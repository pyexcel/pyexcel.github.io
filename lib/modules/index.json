{
  "modules": {
    "BaseHTTPServer": {
      "file": "BaseHTTPServer.py", 
      "imports": [
        "sys", 
        "time", 
        "mimetools", 
        "socket", 
        "SocketServer", 
        "warnings"
      ]
    }, 
    "Bastion": {
      "file": "Bastion.py", 
      "imports": [
        "rexec", 
        "types", 
        "warnings"
      ]
    }, 
    "CGIHTTPServer": {
      "file": "CGIHTTPServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "base64", 
        "binascii", 
        "subprocess", 
        "sys", 
        "copy", 
        "os", 
        "SimpleHTTPServer", 
        "urllib", 
        "pwd", 
        "select"
      ]
    }, 
    "ConfigParser": {
      "file": "ConfigParser.py", 
      "imports": [
        "collections", 
        "re", 
        "UserDict"
      ]
    }, 
    "Cookie": {
      "file": "Cookie.py", 
      "imports": [
        "Cookie", 
        "time.gmtime", 
        "time.time", 
        "doctest", 
        "pickle", 
        "re", 
        "string", 
        "warnings", 
        "cPickle"
      ]
    }, 
    "DocXMLRPCServer": {
      "file": "DocXMLRPCServer.py", 
      "imports": [
        "sys", 
        "inspect", 
        "pydoc", 
        "re", 
        "SimpleXMLRPCServer"
      ]
    }, 
    "HTMLParser": {
      "file": "HTMLParser.py", 
      "imports": [
        "htmlentitydefs", 
        "markupbase", 
        "re"
      ]
    }, 
    "MimeWriter": {
      "file": "MimeWriter.py", 
      "imports": [
        "mimetools", 
        "test.test_MimeWriter", 
        "warnings"
      ]
    }, 
    "Queue": {
      "file": "Queue.py", 
      "imports": [
        "collections", 
        "dummy_threading", 
        "heapq", 
        "time.time"
      ]
    }, 
    "SimpleHTTPServer": {
      "file": "SimpleHTTPServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "cStringIO.StringIO", 
        "cgi", 
        "mimetypes", 
        "os", 
        "posixpath", 
        "shutil", 
        "sys", 
        "StringIO", 
        "urllib"
      ]
    }, 
    "SimpleXMLRPCServer": {
      "file": "SimpleXMLRPCServer.py", 
      "imports": [
        "BaseHTTPServer", 
        "fcntl", 
        "os", 
        "pydoc", 
        "re", 
        "sys", 
        "SocketServer", 
        "traceback", 
        "xmlrpclib"
      ]
    }, 
    "SocketServer": {
      "file": "SocketServer.py", 
      "imports": [
        "cStringIO.StringIO", 
        "dummy_threading", 
        "errno", 
        "os", 
        "socket", 
        "sys", 
        "threading", 
        "StringIO", 
        "traceback", 
        "select"
      ]
    }, 
    "StringIO": {
      "file": "StringIO.py", 
      "imports": [
        "errno.EINVAL", 
        "sys"
      ]
    }, 
    "UserDict": {
      "file": "UserDict.py", 
      "imports": [
        "_abcoll", 
        "copy"
      ]
    }, 
    "UserList": {
      "file": "UserList.py", 
      "imports": [
        "collections"
      ]
    }, 
    "UserString": {
      "file": "UserString.py", 
      "imports": [
        "collections", 
        "os", 
        "sys", 
        "test.test_support", 
        "warnings"
      ]
    }, 
    "_LWPCookieJar": {
      "file": "_LWPCookieJar.py", 
      "imports": [
        "time", 
        "cookielib", 
        "re"
      ]
    }, 
    "_MozillaCookieJar": {
      "file": "_MozillaCookieJar.py", 
      "imports": [
        "time", 
        "cookielib", 
        "re"
      ]
    }, 
    "__future__": {
      "file": "__future__.py", 
      "imports": []
    }, 
    "__init__": {
      "file": "__init__.py", 
      "imports": []
    }, 
    "__phello__.foo": {
      "file": "__phello__.foo.py", 
      "imports": []
    }, 
    "_abcoll": {
      "file": "_abcoll.py", 
      "imports": [
        "sys", 
        "abc"
      ]
    }, 
    "_audioop_build": {
      "file": "_audioop_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_codecs_cn": {
      "file": "_codecs_cn.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_hk": {
      "file": "_codecs_hk.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_iso2022": {
      "file": "_codecs_iso2022.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_jp": {
      "file": "_codecs_jp.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_kr": {
      "file": "_codecs_kr.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_codecs_tw": {
      "file": "_codecs_tw.py", 
      "imports": [
        "_multibytecodec.__getcodec"
      ]
    }, 
    "_collections": {
      "file": "_collections.py", 
      "imports": [
        "threading._get_ident"
      ]
    }, 
    "_csv": {
      "file": "_csv.py", 
      "imports": []
    }, 
    "_ctypes_test": {
      "file": "_ctypes_test.py", 
      "imports": [
        "_ctypes", 
        "cpyext", 
        "imp", 
        "os", 
        "_pypy_testcapi"
      ]
    }, 
    "_curses": {
      "file": "_curses.py", 
      "imports": [
        "_curses_cffi.ffi", 
        "_curses_cffi.lib", 
        "functools", 
        "sys"
      ]
    }, 
    "_curses_build": {
      "file": "_curses_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_curses_panel": {
      "file": "_curses_panel.py", 
      "imports": [
        "_curses"
      ]
    }, 
    "_elementtree": {
      "file": "_elementtree.py", 
      "imports": [
        "xml.etree.ElementTree"
      ]
    }, 
    "_functools": {
      "file": "_functools.py", 
      "imports": []
    }, 
    "_gdbm_build": {
      "file": "_gdbm_build.py", 
      "imports": [
        "cffi", 
        "os", 
        "sys"
      ]
    }, 
    "_marshal": {
      "file": "_marshal.py", 
      "imports": [
        "__pypy__.builtinify", 
        "sys.intern", 
        "types"
      ]
    }, 
    "_md5": {
      "file": "_md5.py", 
      "imports": [
        "copy", 
        "struct"
      ]
    }, 
    "_multiprocessing": {
      "file": "_multiprocessing.py", 
      "imports": [
        "os", 
        "pickle", 
        "select"
      ]
    }, 
    "_osx_support": {
      "file": "_osx_support.py", 
      "imports": [
        "sys", 
        "contextlib", 
        "distutils.log", 
        "os", 
        "re", 
        "tempfile"
      ]
    }, 
    "_pwdgrp_build": {
      "file": "_pwdgrp_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_pyio": {
      "file": "_pyio.py", 
      "imports": [
        "__future__", 
        "_io.FileIO", 
        "array", 
        "errno", 
        "errno.EINTR", 
        "thread.allocate_lock", 
        "abc", 
        "codecs", 
        "dummy_thread", 
        "io", 
        "locale", 
        "os", 
        "warnings"
      ]
    }, 
    "_pypy_interact": {
      "file": "_pypy_interact.py", 
      "imports": [
        "__main__", 
        "code", 
        "os", 
        "sys", 
        "_pypy_irc_topic", 
        "pyrepl.simple_interact"
      ]
    }, 
    "_pypy_irc_topic": {
      "file": "_pypy_irc_topic.py", 
      "imports": [
        "string", 
        "time"
      ]
    }, 
    "_pypy_testcapi": {
      "file": "_pypy_testcapi.py", 
      "imports": [
        "binascii", 
        "distutils.ccompiler", 
        "imp", 
        "os", 
        "sys", 
        "tempfile"
      ]
    }, 
    "_pypy_wait": {
      "file": "_pypy_wait.py", 
      "imports": [
        "ctypes.CDLL", 
        "ctypes.POINTER", 
        "ctypes.byref", 
        "ctypes.c_int", 
        "ctypes.util.find_library", 
        "resource"
      ]
    }, 
    "_scproxy": {
      "file": "_scproxy.py", 
      "imports": [
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_int32", 
        "ctypes.c_int64", 
        "ctypes.c_void_p", 
        "ctypes.cdll", 
        "ctypes.create_string_buffer", 
        "ctypes.pointer", 
        "ctypes.util.find_library", 
        "sys"
      ]
    }, 
    "_sha": {
      "file": "_sha.py", 
      "imports": [
        "copy", 
        "struct"
      ]
    }, 
    "_sha256": {
      "file": "_sha256.py", 
      "imports": [
        "struct"
      ]
    }, 
    "_sha512": {
      "file": "_sha512.py", 
      "imports": [
        "_sha512", 
        "struct"
      ]
    }, 
    "_socket": {
      "file": "_socket.py", 
      "imports": [
        "StringIO", 
        "__future__", 
        "js", 
        "os", 
        "time"
      ]
    }, 
    "_sqlite3": {
      "file": "_sqlite3.py", 
      "imports": [
        "__pypy__.newlist_hint", 
        "_sqlite3_cffi.ffi", 
        "_sqlite3_cffi.lib", 
        "collections", 
        "functools", 
        "sqlite3.dump._iterdump", 
        "string", 
        "sys", 
        "threading._get_ident", 
        "weakref", 
        "datetime"
      ]
    }, 
    "_sqlite3_build": {
      "file": "_sqlite3_build.py", 
      "imports": [
        "cffi.FFI", 
        "os", 
        "sys"
      ]
    }, 
    "_strptime": {
      "file": "_strptime.py", 
      "imports": [
        "thread.allocate_lock", 
        "time", 
        "calendar", 
        "dummy_thread", 
        "locale", 
        "re", 
        "datetime"
      ]
    }, 
    "_structseq": {
      "file": "_structseq.py", 
      "imports": []
    }, 
    "_syslog_build": {
      "file": "_syslog_build.py", 
      "imports": [
        "cffi.FFI"
      ]
    }, 
    "_testcapi": {
      "file": "_testcapi.py", 
      "imports": [
        "_pypy_testcapi", 
        "cpyext", 
        "imp", 
        "os"
      ]
    }, 
    "_threading_local": {
      "file": "_threading_local.py", 
      "imports": [
        "threading"
      ]
    }, 
    "_weakrefset": {
      "file": "_weakrefset.py", 
      "imports": [
        "_weakref.ref"
      ]
    }, 
    "abc": {
      "file": "abc.py", 
      "imports": [
        "_weakrefset", 
        "types"
      ]
    }, 
    "aifc": {
      "file": "aifc.py", 
      "imports": [
        "__builtin__", 
        "audioop", 
        "cl", 
        "math", 
        "sys", 
        "chunk", 
        "struct"
      ]
    }, 
    "antigravity": {
      "file": "antigravity.py", 
      "imports": [
        "webbrowser"
      ]
    }, 
    "anydbm": {
      "file": "anydbm.py", 
      "imports": [
        "whichdb"
      ]
    }, 
    "argparse": {
      "file": "argparse.py", 
      "imports": [
        "sys", 
        "collections", 
        "copy", 
        "gettext", 
        "os", 
        "re", 
        "textwrap", 
        "warnings"
      ]
    }, 
    "ast": {
      "file": "ast.py", 
      "imports": [
        "_ast.*", 
        "_ast.__version__", 
        "collections", 
        "inspect"
      ]
    }, 
    "asynchat": {
      "file": "asynchat.py", 
      "imports": [
        "errno", 
        "sys.py3kwarning", 
        "asyncore", 
        "collections", 
        "socket", 
        "warnings"
      ]
    }, 
    "asyncore": {
      "file": "asyncore.py", 
      "imports": [
        "errno.EAGAIN", 
        "errno.EALREADY", 
        "errno.EBADF", 
        "errno.ECONNABORTED", 
        "errno.ECONNRESET", 
        "errno.EINPROGRESS", 
        "errno.EINTR", 
        "errno.EINVAL", 
        "errno.EISCONN", 
        "errno.ENOTCONN", 
        "errno.EPIPE", 
        "errno.ESHUTDOWN", 
        "errno.EWOULDBLOCK", 
        "errno.errorcode", 
        "fcntl", 
        "sys", 
        "time", 
        "os", 
        "socket", 
        "warnings", 
        "select"
      ]
    }, 
    "atexit": {
      "file": "atexit.py", 
      "imports": [
        "sys", 
        "traceback"
      ]
    }, 
    "athenaHandler": {
      "file": "athenaHandler.py", 
      "imports": [
        "js", 
        "multiprocessing", 
        "nevow.athena", 
        "sys"
      ]
    }, 
    "autostart": {
      "file": "autostart.py", 
      "imports": [
        "webPageLogic"
      ]
    }, 
    "base64": {
      "file": "base64.py", 
      "imports": [
        "binascii", 
        "sys", 
        "getopt", 
        "re", 
        "struct"
      ]
    }, 
    "bdb": {
      "file": "bdb.py", 
      "imports": [
        "__main__", 
        "sys", 
        "fnmatch", 
        "linecache", 
        "os", 
        "repr", 
        "types"
      ]
    }, 
    "binhex": {
      "file": "binhex.py", 
      "imports": [
        "Carbon.File.FInfo", 
        "Carbon.File.FSSpec", 
        "MacOS.openrf", 
        "binascii", 
        "sys", 
        "os", 
        "struct"
      ]
    }, 
    "bisect": {
      "file": "bisect.py", 
      "imports": [
        "_bisect.*"
      ]
    }, 
    "cPickle": {
      "file": "cPickle.py", 
      "imports": [
        "__pypy__.builtinify", 
        "copy_reg", 
        "marshal", 
        "pickle", 
        "struct", 
        "sys", 
        "types"
      ]
    }, 
    "cProfile": {
      "file": "cProfile.py", 
      "imports": [
        "__main__", 
        "_lsprof", 
        "marshal", 
        "sys", 
        "optparse", 
        "os", 
        "pstats", 
        "types"
      ]
    }, 
    "cStringIO": {
      "file": "cStringIO.py", 
      "imports": [
        "StringIO"
      ]
    }, 
    "calendar": {
      "file": "calendar.py", 
      "imports": [
        "sys", 
        "locale", 
        "optparse", 
        "datetime"
      ]
    }, 
    "cgi": {
      "file": "cgi.py", 
      "imports": [
        "cStringIO.StringIO", 
        "operator.attrgetter", 
        "sys", 
        "mimetools", 
        "os", 
        "re", 
        "rfc822", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "urlparse", 
        "UserDict", 
        "warnings"
      ]
    }, 
    "cgitb": {
      "file": "cgitb.py", 
      "imports": [
        "sys", 
        "time", 
        "inspect", 
        "keyword", 
        "linecache", 
        "os", 
        "pydoc", 
        "tempfile", 
        "tokenize", 
        "traceback", 
        "types"
      ]
    }, 
    "chunk": {
      "file": "chunk.py", 
      "imports": [
        "struct"
      ]
    }, 
    "cmd": {
      "file": "cmd.py", 
      "imports": [
        "readline", 
        "sys", 
        "string"
      ]
    }, 
    "code": {
      "file": "code.py", 
      "imports": [
        "readline", 
        "sys", 
        "codeop", 
        "traceback"
      ]
    }, 
    "codecs": {
      "file": "codecs.py", 
      "imports": [
        "__builtin__", 
        "_codecs.*", 
        "sys", 
        "encodings"
      ]
    }, 
    "codeop": {
      "file": "codeop.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "collections": {
      "file": "collections.py", 
      "imports": [
        "__pypy__.newdict", 
        "__pypy__.reversed_dict", 
        "_abcoll", 
        "_collections.defaultdict", 
        "_collections.deque", 
        "itertools.chain", 
        "itertools.imap", 
        "itertools.repeat", 
        "itertools.starmap", 
        "operator.eq", 
        "operator.itemgetter", 
        "sys", 
        "thread.get_ident", 
        "doctest", 
        "dummy_thread", 
        "heapq", 
        "keyword", 
        "cPickle"
      ]
    }, 
    "colorsys": {
      "file": "colorsys.py", 
      "imports": []
    }, 
    "commands": {
      "file": "commands.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "compileall": {
      "file": "compileall.py", 
      "imports": [
        "imp", 
        "sys", 
        "getopt", 
        "os", 
        "py_compile", 
        "re", 
        "struct"
      ]
    }, 
    "compiler": {
      "dir": "compiler"
    }, 
    "compiler.__init__": {
      "file": "compiler/__init__.py", 
      "imports": [
        "compiler.pycodegen", 
        "compiler.transformer", 
        "compiler.visitor", 
        "warnings"
      ]
    }, 
    "compiler.ast": {
      "file": "compiler/ast.py", 
      "imports": [
        "compiler.consts"
      ]
    }, 
    "compiler.consts": {
      "file": "compiler/consts.py", 
      "imports": []
    }, 
    "compiler.future": {
      "file": "compiler/future.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "sys"
      ]
    }, 
    "compiler.misc": {
      "file": "compiler/misc.py", 
      "imports": []
    }, 
    "compiler.pyassem": {
      "file": "compiler/pyassem.py", 
      "imports": [
        "compiler.consts", 
        "compiler.misc", 
        "sys", 
        "dis", 
        "types"
      ]
    }, 
    "compiler.pycodegen": {
      "file": "compiler/pycodegen.py", 
      "imports": [
        "cStringIO.StringIO", 
        "compiler", 
        "compiler.ast", 
        "compiler.consts", 
        "compiler.future", 
        "compiler.misc", 
        "compiler.pyassem", 
        "compiler.symbols", 
        "compiler.syntax", 
        "imp", 
        "marshal", 
        "sys", 
        "os", 
        "pprint", 
        "struct"
      ]
    }, 
    "compiler.symbols": {
      "file": "compiler/symbols.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "compiler.consts", 
        "compiler.misc", 
        "symtable", 
        "sys", 
        "types"
      ]
    }, 
    "compiler.syntax": {
      "file": "compiler/syntax.py", 
      "imports": [
        "compiler", 
        "compiler.ast"
      ]
    }, 
    "compiler.transformer": {
      "file": "compiler/transformer.py", 
      "imports": [
        "compiler.ast", 
        "compiler.consts", 
        "parser", 
        "symbol", 
        "token"
      ]
    }, 
    "compiler.visitor": {
      "file": "compiler/visitor.py", 
      "imports": [
        "compiler.ast"
      ]
    }, 
    "contextlib": {
      "file": "contextlib.py", 
      "imports": [
        "sys", 
        "functools", 
        "warnings"
      ]
    }, 
    "cookielib": {
      "file": "cookielib.py", 
      "imports": [
        "_LWPCookieJar", 
        "_MozillaCookieJar", 
        "calendar", 
        "threading", 
        "time", 
        "copy", 
        "dummy_threading", 
        "httplib", 
        "logging", 
        "re", 
        "StringIO", 
        "traceback", 
        "urllib", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "copy": {
      "file": "copy.py", 
      "imports": [
        "org.python.core.PyStringMap", 
        "sys", 
        "copy_reg", 
        "repr", 
        "types", 
        "weakref"
      ]
    }, 
    "copy_reg": {
      "file": "copy_reg.py", 
      "imports": [
        "types"
      ]
    }, 
    "csv": {
      "file": "csv.py", 
      "imports": [
        "_csv.Dialect", 
        "_csv.Error", 
        "_csv.QUOTE_ALL", 
        "_csv.QUOTE_MINIMAL", 
        "_csv.QUOTE_NONE", 
        "_csv.QUOTE_NONNUMERIC", 
        "_csv.__doc__", 
        "_csv.__version__", 
        "_csv.field_size_limit", 
        "_csv.get_dialect", 
        "_csv.list_dialects", 
        "_csv.reader", 
        "_csv.register_dialect", 
        "_csv.unregister_dialect", 
        "_csv.writer", 
        "cStringIO.StringIO", 
        "functools", 
        "re", 
        "StringIO"
      ]
    }, 
    "ctypes_config_cache": {
      "dir": "ctypes_config_cache"
    }, 
    "ctypes_config_cache.__init__": {
      "file": "ctypes_config_cache/__init__.py", 
      "imports": []
    }, 
    "ctypes_config_cache._locale_32_": {
      "file": "ctypes_config_cache/_locale_32_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._locale_cache": {
      "file": "ctypes_config_cache/_locale_cache.py", 
      "imports": [
        "sys"
      ]
    }, 
    "ctypes_config_cache._resource_32_": {
      "file": "ctypes_config_cache/_resource_32_.py", 
      "imports": [
        "ctypes"
      ]
    }, 
    "ctypes_config_cache._resource_cache": {
      "file": "ctypes_config_cache/_resource_cache.py", 
      "imports": [
        "sys"
      ]
    }, 
    "ctypes_config_cache.dumpcache": {
      "file": "ctypes_config_cache/dumpcache.py", 
      "imports": [
        "ctypes_configure.dumpcache", 
        "os", 
        "sys"
      ]
    }, 
    "ctypes_config_cache.locale.ctc": {
      "file": "ctypes_config_cache/locale.ctc.py", 
      "imports": [
        "ctypes_config_cache.dumpcache", 
        "ctypes_configure.configure.ConstantInteger", 
        "ctypes_configure.configure.DefinedConstantInteger", 
        "ctypes_configure.configure.ExternalCompilationInfo", 
        "ctypes_configure.configure.SimpleType", 
        "ctypes_configure.configure.check_eci", 
        "ctypes_configure.configure.configure"
      ]
    }, 
    "ctypes_config_cache.rebuild": {
      "file": "ctypes_config_cache/rebuild.py", 
      "imports": [
        "os", 
        "py", 
        "rpython.tool.ansi_print.ansi_log", 
        "sys"
      ]
    }, 
    "ctypes_config_cache.resource.ctc": {
      "file": "ctypes_config_cache/resource.ctc.py", 
      "imports": [
        "ctypes.sizeof", 
        "ctypes_config_cache.dumpcache", 
        "ctypes_configure.configure.ConstantInteger", 
        "ctypes_configure.configure.DefinedConstantInteger", 
        "ctypes_configure.configure.ExternalCompilationInfo", 
        "ctypes_configure.configure.SimpleType", 
        "ctypes_configure.configure.configure"
      ]
    }, 
    "datetime": {
      "file": "datetime.py", 
      "imports": [
        "__future__", 
        "_strptime", 
        "math", 
        "struct", 
        "time"
      ]
    }, 
    "dbhash": {
      "file": "dbhash.py", 
      "imports": [
        "bsddb", 
        "sys", 
        "warnings"
      ]
    }, 
    "dbm": {
      "file": "dbm.py", 
      "imports": [
        "ctypes.CDLL", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_char", 
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_void_p", 
        "ctypes.util", 
        "os", 
        "sys"
      ]
    }, 
    "decimal": {
      "file": "decimal.py", 
      "imports": [
        "collections", 
        "itertools.chain", 
        "itertools.repeat", 
        "math", 
        "sys", 
        "threading", 
        "doctest", 
        "locale", 
        "numbers", 
        "re"
      ]
    }, 
    "difflib": {
      "file": "difflib.py", 
      "imports": [
        "collections", 
        "difflib", 
        "doctest", 
        "functools", 
        "heapq", 
        "re"
      ]
    }, 
    "dircache": {
      "file": "dircache.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "dis": {
      "file": "dis.py", 
      "imports": [
        "sys", 
        "opcode", 
        "types"
      ]
    }, 
    "distutils": {
      "dir": "distutils"
    }, 
    "distutils.__init__": {
      "file": "distutils/__init__.py", 
      "imports": []
    }, 
    "distutils.archive_util": {
      "file": "distutils/archive_util.py", 
      "imports": [
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "sys", 
        "os", 
        "tarfile", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.bcppcompiler": {
      "file": "distutils/bcppcompiler.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "os"
      ]
    }, 
    "distutils.ccompiler": {
      "file": "distutils/ccompiler.py", 
      "imports": [
        "distutils.debug", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "os", 
        "re", 
        "tempfile"
      ]
    }, 
    "distutils.cmd": {
      "file": "distutils/cmd.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.debug", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.dist", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.util", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command": {
      "dir": "distutils/command"
    }, 
    "distutils.command.__init__": {
      "file": "distutils/command/__init__.py", 
      "imports": []
    }, 
    "distutils.command.bdist": {
      "file": "distutils/command/bdist.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.bdist_dumb": {
      "file": "distutils/command/bdist_dumb.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.bdist_msi": {
      "file": "distutils/command/bdist_msi.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "distutils.version", 
        "msilib", 
        "msilib.Dialog", 
        "msilib.Directory", 
        "msilib.Feature", 
        "msilib.add_data", 
        "msilib.schema", 
        "msilib.sequence", 
        "msilib.text", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.bdist_rpm": {
      "file": "distutils/command/bdist_rpm.py", 
      "imports": [
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "string"
      ]
    }, 
    "distutils.command.bdist_wininst": {
      "file": "distutils/command/bdist_wininst.py", 
      "imports": [
        "distutils", 
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.msvccompiler", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "time", 
        "os", 
        "string", 
        "struct", 
        "tempfile"
      ]
    }, 
    "distutils.command.build": {
      "file": "distutils/command/build.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.build_clib": {
      "file": "distutils/command/build_clib.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "os"
      ]
    }, 
    "distutils.command.build_ext": {
      "file": "distutils/command/build_ext.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.log", 
        "distutils.msvccompiler", 
        "distutils.sysconfig", 
        "distutils.util", 
        "imp", 
        "sys", 
        "os", 
        "re", 
        "site", 
        "string", 
        "types"
      ]
    }, 
    "distutils.command.build_py": {
      "file": "distutils/command/build_py.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "sys", 
        "glob", 
        "os"
      ]
    }, 
    "distutils.command.build_scripts": {
      "file": "distutils/command/build_scripts.py", 
      "imports": [
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.log", 
        "distutils.util", 
        "os", 
        "re", 
        "stat"
      ]
    }, 
    "distutils.command.check": {
      "file": "distutils/command/check.py", 
      "imports": [
        "distutils.core", 
        "distutils.dist", 
        "distutils.errors", 
        "docutils.frontend", 
        "docutils.nodes", 
        "docutils.parsers.rst.Parser", 
        "docutils.utils.Reporter", 
        "StringIO"
      ]
    }, 
    "distutils.command.clean": {
      "file": "distutils/command/clean.py", 
      "imports": [
        "distutils.core", 
        "distutils.dir_util", 
        "distutils.log", 
        "os"
      ]
    }, 
    "distutils.command.config": {
      "file": "distutils/command/config.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command.install": {
      "file": "distutils/command/install.py", 
      "imports": [
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.util", 
        "sys", 
        "os", 
        "pprint", 
        "site", 
        "string", 
        "types"
      ]
    }, 
    "distutils.command.install_data": {
      "file": "distutils/command/install_data.py", 
      "imports": [
        "distutils.core", 
        "distutils.util", 
        "os"
      ]
    }, 
    "distutils.command.install_egg_info": {
      "file": "distutils/command/install_egg_info.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.dir_util", 
        "distutils.log", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.command.install_headers": {
      "file": "distutils/command/install_headers.py", 
      "imports": [
        "distutils.core"
      ]
    }, 
    "distutils.command.install_lib": {
      "file": "distutils/command/install_lib.py", 
      "imports": [
        "distutils.core", 
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.command.install_scripts": {
      "file": "distutils/command/install_scripts.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.command.register": {
      "file": "distutils/command/register.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "getpass", 
        "urllib2", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "distutils.command.sdist": {
      "file": "distutils/command/sdist.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.core", 
        "distutils.dep_util", 
        "distutils.dir_util", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.file_util", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.text_file", 
        "distutils.util", 
        "sys", 
        "glob", 
        "os", 
        "string", 
        "warnings"
      ]
    }, 
    "distutils.command.upload": {
      "file": "distutils/command/upload.py", 
      "imports": [
        "base64", 
        "cStringIO", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "hashlib", 
        "os", 
        "platform", 
        "socket", 
        "urllib2", 
        "urlparse"
      ]
    }, 
    "distutils.config": {
      "file": "distutils/config.py", 
      "imports": [
        "ConfigParser", 
        "distutils.cmd", 
        "os"
      ]
    }, 
    "distutils.core": {
      "file": "distutils/core.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.config", 
        "distutils.debug", 
        "distutils.dist", 
        "distutils.errors", 
        "distutils.extension", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.cygwinccompiler": {
      "file": "distutils/cygwinccompiler.py", 
      "imports": [
        "copy", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "distutils.version", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.debug": {
      "file": "distutils/debug.py", 
      "imports": [
        "os"
      ]
    }, 
    "distutils.dep_util": {
      "file": "distutils/dep_util.py", 
      "imports": [
        "distutils.errors", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.dir_util": {
      "file": "distutils/dir_util.py", 
      "imports": [
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "errno", 
        "os"
      ]
    }, 
    "distutils.dist": {
      "file": "distutils/dist.py", 
      "imports": [
        "ConfigParser", 
        "distutils.cmd", 
        "distutils.command", 
        "distutils.core", 
        "distutils.debug", 
        "distutils.errors", 
        "distutils.fancy_getopt", 
        "distutils.log", 
        "distutils.util", 
        "distutils.versionpredicate", 
        "sys", 
        "email", 
        "os", 
        "pprint", 
        "re", 
        "warnings"
      ]
    }, 
    "distutils.emxccompiler": {
      "file": "distutils/emxccompiler.py", 
      "imports": [
        "copy", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "distutils.version", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.errors": {
      "file": "distutils/errors.py", 
      "imports": []
    }, 
    "distutils.extension": {
      "file": "distutils/extension.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.text_file", 
        "distutils.util", 
        "sys", 
        "os", 
        "string", 
        "types", 
        "warnings"
      ]
    }, 
    "distutils.fancy_getopt": {
      "file": "distutils/fancy_getopt.py", 
      "imports": [
        "distutils.errors", 
        "sys", 
        "getopt", 
        "re", 
        "string"
      ]
    }, 
    "distutils.file_util": {
      "file": "distutils/file_util.py", 
      "imports": [
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "errno", 
        "os", 
        "stat"
      ]
    }, 
    "distutils.filelist": {
      "file": "distutils/filelist.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "fnmatch", 
        "os", 
        "re", 
        "stat"
      ]
    }, 
    "distutils.log": {
      "file": "distutils/log.py", 
      "imports": [
        "sys"
      ]
    }, 
    "distutils.msvc9compiler": {
      "file": "distutils/msvc9compiler.py", 
      "imports": [
        "_winreg", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.util", 
        "subprocess", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "distutils.msvccompiler": {
      "file": "distutils/msvccompiler.py", 
      "imports": [
        "_winreg", 
        "distutils.ccompiler", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.msvc9compiler", 
        "sys", 
        "win32api", 
        "win32con", 
        "os", 
        "string"
      ]
    }, 
    "distutils.spawn": {
      "file": "distutils/spawn.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "errno", 
        "subprocess", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.sysconfig": {
      "file": "distutils/sysconfig.py", 
      "imports": [
        "distutils.sysconfig_cpython", 
        "distutils.sysconfig_pypy", 
        "sys"
      ]
    }, 
    "distutils.sysconfig_cpython": {
      "file": "distutils/sysconfig_cpython.py", 
      "imports": [
        "_osx_support", 
        "_sysconfigdata.build_time_vars", 
        "distutils.errors", 
        "distutils.text_file", 
        "sys", 
        "os", 
        "re", 
        "string"
      ]
    }, 
    "distutils.sysconfig_pypy": {
      "file": "distutils/sysconfig_pypy.py", 
      "imports": [
        "distutils.errors", 
        "distutils.sysconfig_cpython", 
        "sys", 
        "os", 
        "shlex"
      ]
    }, 
    "distutils.tests": {
      "dir": "distutils/tests"
    }, 
    "distutils.tests.__init__": {
      "file": "distutils/tests/__init__.py", 
      "imports": [
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.setuptools_build_ext": {
      "file": "distutils/tests/setuptools_build_ext.py", 
      "imports": [
        "Pyrex.Distutils.build_ext.build_ext", 
        "distutils.ccompiler", 
        "distutils.command.build_ext", 
        "distutils.errors", 
        "distutils.file_util", 
        "distutils.log", 
        "distutils.sysconfig", 
        "distutils.tests.setuptools_extension", 
        "distutils.util", 
        "dl.RTLD_NOW", 
        "sys", 
        "os"
      ]
    }, 
    "distutils.tests.setuptools_extension": {
      "file": "distutils/tests/setuptools_extension.py", 
      "imports": [
        "Pyrex.Distutils.build_ext.build_ext", 
        "distutils.core", 
        "distutils.extension", 
        "sys"
      ]
    }, 
    "distutils.tests.support": {
      "file": "distutils/tests/support.py", 
      "imports": [
        "copy", 
        "distutils.core", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "unittest", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_archive_util": {
      "file": "distutils/tests/test_archive_util.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "sys", 
        "zlib", 
        "os", 
        "tarfile", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.tests.test_bdist": {
      "file": "distutils/tests/test_bdist.py", 
      "imports": [
        "distutils.command.bdist", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_dumb": {
      "file": "distutils/tests/test_bdist_dumb.py", 
      "imports": [
        "distutils.command.bdist_dumb", 
        "distutils.core", 
        "distutils.tests.support", 
        "sys", 
        "zlib", 
        "os", 
        "test.test_support", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "distutils.tests.test_bdist_msi": {
      "file": "distutils/tests/test_bdist_msi.py", 
      "imports": [
        "distutils.command.bdist_msi", 
        "distutils.tests.support", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_rpm": {
      "file": "distutils/tests/test_bdist_rpm.py", 
      "imports": [
        "distutils.command.bdist_rpm", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_bdist_wininst": {
      "file": "distutils/tests/test_bdist_wininst.py", 
      "imports": [
        "distutils.command.bdist_wininst", 
        "distutils.tests.support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build": {
      "file": "distutils/tests/test_build.py", 
      "imports": [
        "distutils.command.build", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_clib": {
      "file": "distutils/tests/test_build_clib.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.command.build_clib", 
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_ext": {
      "file": "distutils/tests/test_build_ext.py", 
      "imports": [
        "distutils.command.build_ext", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.sysconfig", 
        "distutils.tests.setuptools_build_ext", 
        "distutils.tests.setuptools_extension", 
        "distutils.tests.support", 
        "sys", 
        "xx", 
        "os", 
        "site", 
        "StringIO", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_py": {
      "file": "distutils/tests/test_build_py.py", 
      "imports": [
        "distutils.command.build_py", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_build_scripts": {
      "file": "distutils/tests/test_build_scripts.py", 
      "imports": [
        "distutils.command.build_scripts", 
        "distutils.core", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_ccompiler": {
      "file": "distutils/tests/test_ccompiler.py", 
      "imports": [
        "distutils.ccompiler", 
        "distutils.debug", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_check": {
      "file": "distutils/tests/test_check.py", 
      "imports": [
        "distutils.command.check", 
        "distutils.errors", 
        "distutils.tests.support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_clean": {
      "file": "distutils/tests/test_clean.py", 
      "imports": [
        "distutils.command.clean", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_cmd": {
      "file": "distutils/tests/test_cmd.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.debug", 
        "distutils.dist", 
        "distutils.errors", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_config": {
      "file": "distutils/tests/test_config.py", 
      "imports": [
        "distutils.core", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_config_cmd": {
      "file": "distutils/tests/test_config_cmd.py", 
      "imports": [
        "distutils.command.config", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_core": {
      "file": "distutils/tests/test_core.py", 
      "imports": [
        "distutils.core", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dep_util": {
      "file": "distutils/tests/test_dep_util.py", 
      "imports": [
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.tests.support", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dir_util": {
      "file": "distutils/tests/test_dir_util.py", 
      "imports": [
        "distutils.dir_util", 
        "distutils.log", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "shutil", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_dist": {
      "file": "distutils/tests/test_dist.py", 
      "imports": [
        "distutils.cmd", 
        "distutils.dist", 
        "distutils.tests.support", 
        "distutils.tests.test_dist", 
        "sys", 
        "os", 
        "StringIO", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_file_util": {
      "file": "distutils/tests/test_file_util.py", 
      "imports": [
        "distutils.file_util", 
        "distutils.log", 
        "distutils.tests.support", 
        "os", 
        "shutil", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_filelist": {
      "file": "distutils/tests/test_filelist.py", 
      "imports": [
        "distutils.debug", 
        "distutils.errors", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.tests.support", 
        "os", 
        "re", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install": {
      "file": "distutils/tests/test_install.py", 
      "imports": [
        "distutils.command.build_ext", 
        "distutils.command.install", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "site", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_data": {
      "file": "distutils/tests/test_install_data.py", 
      "imports": [
        "distutils.command.install_data", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_headers": {
      "file": "distutils/tests/test_install_headers.py", 
      "imports": [
        "distutils.command.install_headers", 
        "distutils.tests.support", 
        "sys", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_lib": {
      "file": "distutils/tests/test_install_lib.py", 
      "imports": [
        "distutils.command.install_lib", 
        "distutils.errors", 
        "distutils.extension", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_install_scripts": {
      "file": "distutils/tests/test_install_scripts.py", 
      "imports": [
        "distutils.command.install_scripts", 
        "distutils.core", 
        "distutils.tests.support", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_msvc9compiler": {
      "file": "distutils/tests/test_msvc9compiler.py", 
      "imports": [
        "_winreg", 
        "distutils.errors", 
        "distutils.msvc9compiler", 
        "distutils.msvccompiler", 
        "distutils.tests.support", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_register": {
      "file": "distutils/tests/test_register.py", 
      "imports": [
        "distutils.command.register", 
        "distutils.errors", 
        "distutils.tests.test_config", 
        "docutils", 
        "getpass", 
        "os", 
        "test.test_support", 
        "unittest", 
        "urllib2", 
        "warnings"
      ]
    }, 
    "distutils.tests.test_sdist": {
      "file": "distutils/tests/test_sdist.py", 
      "imports": [
        "distutils.archive_util", 
        "distutils.command.sdist", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.filelist", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.tests.test_config", 
        "zlib", 
        "os", 
        "tarfile", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "distutils.tests.test_spawn": {
      "file": "distutils/tests/test_spawn.py", 
      "imports": [
        "distutils.errors", 
        "distutils.spawn", 
        "distutils.tests.support", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_sysconfig": {
      "file": "distutils/tests/test_sysconfig.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.tests.support", 
        "subprocess", 
        "sys", 
        "os", 
        "shutil", 
        "test.test_support", 
        "test", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_text_file": {
      "file": "distutils/tests/test_text_file.py", 
      "imports": [
        "distutils.tests.support", 
        "distutils.text_file", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_unixccompiler": {
      "file": "distutils/tests/test_unixccompiler.py", 
      "imports": [
        "distutils.sysconfig", 
        "distutils.unixccompiler", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_upload": {
      "file": "distutils/tests/test_upload.py", 
      "imports": [
        "distutils.command.upload", 
        "distutils.core", 
        "distutils.errors", 
        "distutils.tests.test_config", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_util": {
      "file": "distutils/tests/test_util.py", 
      "imports": [
        "distutils.errors", 
        "distutils.util", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_version": {
      "file": "distutils/tests/test_version.py", 
      "imports": [
        "distutils.version", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "distutils.tests.test_versionpredicate": {
      "file": "distutils/tests/test_versionpredicate.py", 
      "imports": [
        "distutils.versionpredicate", 
        "doctest", 
        "test.test_support"
      ]
    }, 
    "distutils.text_file": {
      "file": "distutils/text_file.py", 
      "imports": [
        "sys"
      ]
    }, 
    "distutils.unixccompiler": {
      "file": "distutils/unixccompiler.py", 
      "imports": [
        "_osx_support", 
        "distutils.ccompiler", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "platform", 
        "re", 
        "types"
      ]
    }, 
    "distutils.util": {
      "file": "distutils/util.py", 
      "imports": [
        "_osx_support", 
        "distutils.dep_util", 
        "distutils.errors", 
        "distutils.log", 
        "distutils.spawn", 
        "distutils.sysconfig", 
        "sys", 
        "os", 
        "py_compile", 
        "re", 
        "string", 
        "tempfile", 
        "pwd"
      ]
    }, 
    "distutils.version": {
      "file": "distutils/version.py", 
      "imports": [
        "re", 
        "string", 
        "types"
      ]
    }, 
    "distutils.versionpredicate": {
      "file": "distutils/versionpredicate.py", 
      "imports": [
        "distutils.version", 
        "operator", 
        "re"
      ]
    }, 
    "doctest": {
      "file": "doctest.py", 
      "imports": [
        "__future__", 
        "collections", 
        "difflib", 
        "sys", 
        "inspect", 
        "linecache", 
        "os", 
        "pdb", 
        "re", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "types", 
        "unittest", 
        "warnings"
      ]
    }, 
    "dumbdbm": {
      "file": "dumbdbm.py", 
      "imports": [
        "__builtin__", 
        "os", 
        "UserDict"
      ]
    }, 
    "dummy_thread": {
      "file": "dummy_thread.py", 
      "imports": [
        "traceback"
      ]
    }, 
    "dummy_threading": {
      "file": "dummy_threading.py", 
      "imports": [
        "_threading_local", 
        "collections", 
        "dummy_thread", 
        "itertools.count", 
        "js", 
        "sys", 
        "thread", 
        "time.sleep", 
        "time.time", 
        "traceback", 
        "warnings"
      ]
    }, 
    "email": {
      "dir": "email"
    }, 
    "email.__init__": {
      "file": "email/__init__.py", 
      "imports": [
        "email.mime", 
        "email.parser", 
        "sys"
      ]
    }, 
    "email._parseaddr": {
      "file": "email/_parseaddr.py", 
      "imports": [
        "calendar", 
        "time"
      ]
    }, 
    "email.base64mime": {
      "file": "email/base64mime.py", 
      "imports": [
        "binascii.a2b_base64", 
        "binascii.b2a_base64", 
        "email.utils"
      ]
    }, 
    "email.charset": {
      "file": "email/charset.py", 
      "imports": [
        "codecs", 
        "email.base64mime", 
        "email.encoders", 
        "email.errors", 
        "email.quoprimime"
      ]
    }, 
    "email.encoders": {
      "file": "email/encoders.py", 
      "imports": [
        "base64", 
        "quopri"
      ]
    }, 
    "email.errors": {
      "file": "email/errors.py", 
      "imports": []
    }, 
    "email.feedparser": {
      "file": "email/feedparser.py", 
      "imports": [
        "email.errors", 
        "email.message", 
        "re"
      ]
    }, 
    "email.generator": {
      "file": "email/generator.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.header", 
        "sys", 
        "time", 
        "random", 
        "re", 
        "warnings"
      ]
    }, 
    "email.header": {
      "file": "email/header.py", 
      "imports": [
        "binascii", 
        "email.base64mime", 
        "email.charset", 
        "email.errors", 
        "email.quoprimime", 
        "re"
      ]
    }, 
    "email.iterators": {
      "file": "email/iterators.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys"
      ]
    }, 
    "email.message": {
      "file": "email/message.py", 
      "imports": [
        "binascii", 
        "cStringIO.StringIO", 
        "email.charset", 
        "email.errors", 
        "email.generator", 
        "email.iterators", 
        "email.utils", 
        "re", 
        "uu", 
        "warnings"
      ]
    }, 
    "email.mime": {
      "dir": "email/mime"
    }, 
    "email.mime.__init__": {
      "file": "email/mime/__init__.py", 
      "imports": []
    }, 
    "email.mime.application": {
      "file": "email/mime/application.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.mime.audio": {
      "file": "email/mime/audio.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.encoders", 
        "email.mime.nonmultipart", 
        "sndhdr"
      ]
    }, 
    "email.mime.base": {
      "file": "email/mime/base.py", 
      "imports": [
        "email.message"
      ]
    }, 
    "email.mime.image": {
      "file": "email/mime/image.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart", 
        "imghdr"
      ]
    }, 
    "email.mime.message": {
      "file": "email/mime/message.py", 
      "imports": [
        "email.message", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.mime.multipart": {
      "file": "email/mime/multipart.py", 
      "imports": [
        "email.mime.base"
      ]
    }, 
    "email.mime.nonmultipart": {
      "file": "email/mime/nonmultipart.py", 
      "imports": [
        "email.errors", 
        "email.mime.base"
      ]
    }, 
    "email.mime.text": {
      "file": "email/mime/text.py", 
      "imports": [
        "email.encoders", 
        "email.mime.nonmultipart"
      ]
    }, 
    "email.parser": {
      "file": "email/parser.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email.feedparser", 
        "email.message", 
        "warnings"
      ]
    }, 
    "email.quoprimime": {
      "file": "email/quoprimime.py", 
      "imports": [
        "email.utils", 
        "re", 
        "string"
      ]
    }, 
    "email.test": {
      "dir": "email/test"
    }, 
    "email.test.__init__": {
      "file": "email/test/__init__.py", 
      "imports": []
    }, 
    "email.test.test_email": {
      "file": "email/test/test_email.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "difflib", 
        "email", 
        "email.feedparser", 
        "email.test", 
        "sys", 
        "time", 
        "os", 
        "random", 
        "re", 
        "test.test_support", 
        "textwrap", 
        "unittest", 
        "warnings"
      ]
    }, 
    "email.test.test_email_codecs": {
      "file": "email/test/test_email_codecs.py", 
      "imports": [
        "email.charset", 
        "email.header", 
        "email.message", 
        "email.test.test_email", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "email.test.test_email_codecs_renamed": {
      "file": "email/test/test_email_codecs_renamed.py", 
      "imports": [
        "email.charset", 
        "email.header", 
        "email.message", 
        "email.test.test_email", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "email.test.test_email_renamed": {
      "file": "email/test/test_email_renamed.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "difflib", 
        "email", 
        "email.base64mime", 
        "email.charset", 
        "email.encoders", 
        "email.errors", 
        "email.generator", 
        "email.header", 
        "email.iterators", 
        "email.message", 
        "email.mime.application", 
        "email.mime.audio", 
        "email.mime.base", 
        "email.mime.image", 
        "email.mime.message", 
        "email.mime.multipart", 
        "email.mime.text", 
        "email.parser", 
        "email.quoprimime", 
        "email.test", 
        "email.utils", 
        "sys", 
        "time", 
        "os", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "email.test.test_email_torture": {
      "file": "email/test/test_email_torture.py", 
      "imports": [
        "cStringIO.StringIO", 
        "email", 
        "email.iterators", 
        "email.test.test_email", 
        "sys", 
        "os", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "email.utils": {
      "file": "email/utils.py", 
      "imports": [
        "base64", 
        "email._parseaddr", 
        "email.encoders", 
        "time", 
        "os", 
        "quopri", 
        "random", 
        "re", 
        "socket", 
        "urllib", 
        "warnings"
      ]
    }, 
    "encodings": {
      "dir": "encodings"
    }, 
    "encodings.__init__": {
      "file": "encodings/__init__.py", 
      "imports": [
        "__builtin__", 
        "codecs", 
        "encodings.aliases"
      ]
    }, 
    "encodings.aliases": {
      "file": "encodings/aliases.py", 
      "imports": []
    }, 
    "encodings.ascii": {
      "file": "encodings/ascii.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.base64_codec": {
      "file": "encodings/base64_codec.py", 
      "imports": [
        "base64", 
        "codecs"
      ]
    }, 
    "encodings.big5": {
      "file": "encodings/big5.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_tw"
      ]
    }, 
    "encodings.big5hkscs": {
      "file": "encodings/big5hkscs.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_hk"
      ]
    }, 
    "encodings.bz2_codec": {
      "file": "encodings/bz2_codec.py", 
      "imports": [
        "bz2", 
        "codecs"
      ]
    }, 
    "encodings.charmap": {
      "file": "encodings/charmap.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp037": {
      "file": "encodings/cp037.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1006": {
      "file": "encodings/cp1006.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1026": {
      "file": "encodings/cp1026.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1140": {
      "file": "encodings/cp1140.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1250": {
      "file": "encodings/cp1250.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1251": {
      "file": "encodings/cp1251.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1252": {
      "file": "encodings/cp1252.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1253": {
      "file": "encodings/cp1253.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1254": {
      "file": "encodings/cp1254.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1255": {
      "file": "encodings/cp1255.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1256": {
      "file": "encodings/cp1256.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1257": {
      "file": "encodings/cp1257.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp1258": {
      "file": "encodings/cp1258.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp424": {
      "file": "encodings/cp424.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp437": {
      "file": "encodings/cp437.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp500": {
      "file": "encodings/cp500.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp720": {
      "file": "encodings/cp720.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp737": {
      "file": "encodings/cp737.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp775": {
      "file": "encodings/cp775.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp850": {
      "file": "encodings/cp850.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp852": {
      "file": "encodings/cp852.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp855": {
      "file": "encodings/cp855.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp856": {
      "file": "encodings/cp856.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp857": {
      "file": "encodings/cp857.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp858": {
      "file": "encodings/cp858.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp860": {
      "file": "encodings/cp860.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp861": {
      "file": "encodings/cp861.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp862": {
      "file": "encodings/cp862.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp863": {
      "file": "encodings/cp863.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp864": {
      "file": "encodings/cp864.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp865": {
      "file": "encodings/cp865.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp866": {
      "file": "encodings/cp866.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp869": {
      "file": "encodings/cp869.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp874": {
      "file": "encodings/cp874.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp875": {
      "file": "encodings/cp875.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.cp932": {
      "file": "encodings/cp932.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.cp949": {
      "file": "encodings/cp949.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.cp950": {
      "file": "encodings/cp950.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_tw"
      ]
    }, 
    "encodings.euc_jis_2004": {
      "file": "encodings/euc_jis_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_jisx0213": {
      "file": "encodings/euc_jisx0213.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_jp": {
      "file": "encodings/euc_jp.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.euc_kr": {
      "file": "encodings/euc_kr.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.gb18030": {
      "file": "encodings/gb18030.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.gb2312": {
      "file": "encodings/gb2312.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.gbk": {
      "file": "encodings/gbk.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.hex_codec": {
      "file": "encodings/hex_codec.py", 
      "imports": [
        "binascii", 
        "codecs"
      ]
    }, 
    "encodings.hp_roman8": {
      "file": "encodings/hp_roman8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.hz": {
      "file": "encodings/hz.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_cn"
      ]
    }, 
    "encodings.idna": {
      "file": "encodings/idna.py", 
      "imports": [
        "codecs", 
        "unicodedata.ucd_3_2_0", 
        "re", 
        "stringprep"
      ]
    }, 
    "encodings.iso2022_jp": {
      "file": "encodings/iso2022_jp.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_1": {
      "file": "encodings/iso2022_jp_1.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_2": {
      "file": "encodings/iso2022_jp_2.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_2004": {
      "file": "encodings/iso2022_jp_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_3": {
      "file": "encodings/iso2022_jp_3.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_jp_ext": {
      "file": "encodings/iso2022_jp_ext.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso2022_kr": {
      "file": "encodings/iso2022_kr.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_iso2022"
      ]
    }, 
    "encodings.iso8859_1": {
      "file": "encodings/iso8859_1.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_10": {
      "file": "encodings/iso8859_10.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_11": {
      "file": "encodings/iso8859_11.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_13": {
      "file": "encodings/iso8859_13.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_14": {
      "file": "encodings/iso8859_14.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_15": {
      "file": "encodings/iso8859_15.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_16": {
      "file": "encodings/iso8859_16.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_2": {
      "file": "encodings/iso8859_2.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_3": {
      "file": "encodings/iso8859_3.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_4": {
      "file": "encodings/iso8859_4.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_5": {
      "file": "encodings/iso8859_5.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_6": {
      "file": "encodings/iso8859_6.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_7": {
      "file": "encodings/iso8859_7.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_8": {
      "file": "encodings/iso8859_8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.iso8859_9": {
      "file": "encodings/iso8859_9.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.johab": {
      "file": "encodings/johab.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_kr"
      ]
    }, 
    "encodings.koi8_r": {
      "file": "encodings/koi8_r.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.koi8_u": {
      "file": "encodings/koi8_u.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.latin_1": {
      "file": "encodings/latin_1.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_arabic": {
      "file": "encodings/mac_arabic.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_centeuro": {
      "file": "encodings/mac_centeuro.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_croatian": {
      "file": "encodings/mac_croatian.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_cyrillic": {
      "file": "encodings/mac_cyrillic.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_farsi": {
      "file": "encodings/mac_farsi.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_greek": {
      "file": "encodings/mac_greek.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_iceland": {
      "file": "encodings/mac_iceland.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_latin2": {
      "file": "encodings/mac_latin2.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_roman": {
      "file": "encodings/mac_roman.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_romanian": {
      "file": "encodings/mac_romanian.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mac_turkish": {
      "file": "encodings/mac_turkish.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.mbcs": {
      "file": "encodings/mbcs.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.palmos": {
      "file": "encodings/palmos.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.ptcp154": {
      "file": "encodings/ptcp154.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.punycode": {
      "file": "encodings/punycode.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.quopri_codec": {
      "file": "encodings/quopri_codec.py", 
      "imports": [
        "cStringIO.StringIO", 
        "codecs", 
        "quopri", 
        "StringIO"
      ]
    }, 
    "encodings.raw_unicode_escape": {
      "file": "encodings/raw_unicode_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.rot_13": {
      "file": "encodings/rot_13.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.shift_jis": {
      "file": "encodings/shift_jis.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.shift_jis_2004": {
      "file": "encodings/shift_jis_2004.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.shift_jisx0213": {
      "file": "encodings/shift_jisx0213.py", 
      "imports": [
        "_multibytecodec", 
        "codecs", 
        "_codecs_jp"
      ]
    }, 
    "encodings.string_escape": {
      "file": "encodings/string_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.tis_620": {
      "file": "encodings/tis_620.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.undefined": {
      "file": "encodings/undefined.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.unicode_escape": {
      "file": "encodings/unicode_escape.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.unicode_internal": {
      "file": "encodings/unicode_internal.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_16": {
      "file": "encodings/utf_16.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.utf_16_be": {
      "file": "encodings/utf_16_be.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_16_le": {
      "file": "encodings/utf_16_le.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_32": {
      "file": "encodings/utf_32.py", 
      "imports": [
        "codecs", 
        "sys"
      ]
    }, 
    "encodings.utf_32_be": {
      "file": "encodings/utf_32_be.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_32_le": {
      "file": "encodings/utf_32_le.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_7": {
      "file": "encodings/utf_7.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_8": {
      "file": "encodings/utf_8.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.utf_8_sig": {
      "file": "encodings/utf_8_sig.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "encodings.uu_codec": {
      "file": "encodings/uu_codec.py", 
      "imports": [
        "binascii", 
        "binascii.a2b_uu", 
        "binascii.b2a_uu", 
        "cStringIO.StringIO", 
        "codecs"
      ]
    }, 
    "encodings.zlib_codec": {
      "file": "encodings/zlib_codec.py", 
      "imports": [
        "codecs", 
        "zlib"
      ]
    }, 
    "ensurepip": {
      "dir": "ensurepip"
    }, 
    "ensurepip.__init__": {
      "file": "ensurepip/__init__.py", 
      "imports": [
        "__future__", 
        "argparse", 
        "pip", 
        "ssl", 
        "sys", 
        "os", 
        "pkgutil", 
        "shutil", 
        "tempfile"
      ]
    }, 
    "ensurepip.__main__": {
      "file": "ensurepip/__main__.py", 
      "imports": [
        "ensurepip"
      ]
    }, 
    "ensurepip._uninstall": {
      "file": "ensurepip/_uninstall.py", 
      "imports": [
        "argparse", 
        "ensurepip"
      ]
    }, 
    "filecmp": {
      "file": "filecmp.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.ifilterfalse", 
        "itertools.imap", 
        "itertools.izip", 
        "sys", 
        "getopt", 
        "os", 
        "stat"
      ]
    }, 
    "fileinput": {
      "file": "fileinput.py", 
      "imports": [
        "bz2", 
        "sys", 
        "getopt", 
        "gzip", 
        "io", 
        "os"
      ]
    }, 
    "fnmatch": {
      "file": "fnmatch.py", 
      "imports": [
        "os", 
        "posixpath", 
        "re"
      ]
    }, 
    "formatter": {
      "file": "formatter.py", 
      "imports": [
        "sys"
      ]
    }, 
    "formless": {
      "dir": "formless"
    }, 
    "formless.__init__": {
      "file": "formless/__init__.py", 
      "imports": [
        "formless.annotate", 
        "formless.processors", 
        "nevow"
      ]
    }, 
    "formless.annotate": {
      "file": "formless/annotate.py", 
      "imports": [
        "formless.iformless", 
        "inspect", 
        "os", 
        "sys", 
        "warnings", 
        "zope.interface", 
        "zope.interface.interface", 
        "nevow.util"
      ]
    }, 
    "formless.configurable": {
      "file": "formless/configurable.py", 
      "imports": [
        "formless.annotate", 
        "formless.iformless", 
        "zope.interface", 
        "nevow.tags", 
        "nevow.context", 
        "nevow.inevow"
      ]
    }, 
    "formless.formutils": {
      "file": "formless/formutils.py", 
      "imports": [
        "__future__", 
        "formless.iformless", 
        "twisted.python.components", 
        "zope.interface", 
        "nevow.tags", 
        "nevow.inevow"
      ]
    }, 
    "formless.iformless": {
      "file": "formless/iformless.py", 
      "imports": [
        "formless.annotate", 
        "formless.configurable", 
        "zope.interface", 
        "zope.interface.interface"
      ]
    }, 
    "formless.processors": {
      "file": "formless/processors.py", 
      "imports": [
        "formless", 
        "formless.formutils", 
        "formless.iformless", 
        "twisted.python.components", 
        "warnings", 
        "zope.interface", 
        "nevow.testutil", 
        "nevow.util", 
        "nevow.tags", 
        "nevow.context", 
        "nevow.inevow"
      ]
    }, 
    "formless.test": {
      "dir": "formless/test"
    }, 
    "formless.test.__init__": {
      "file": "formless/test/__init__.py", 
      "imports": []
    }, 
    "formless.test.test_formless": {
      "file": "formless/test/test_formless.py", 
      "imports": [
        "formless", 
        "os", 
        "nevow.testutil"
      ]
    }, 
    "formless.test.test_freeform": {
      "file": "formless/test/test_freeform.py", 
      "imports": [
        "formless", 
        "formless.configurable", 
        "formless.iformless", 
        "formless.webform", 
        "twisted.python.components", 
        "zope.interface", 
        "nevow.util", 
        "nevow.tags", 
        "nevow.context", 
        "nevow.inevow", 
        "nevow.rend", 
        "nevow.test.test_flatstan"
      ]
    }, 
    "formless.webform": {
      "file": "formless/webform.py", 
      "imports": [
        "__future__", 
        "formless.configurable", 
        "formless.formutils", 
        "formless.iformless", 
        "twisted.python.components", 
        "warnings", 
        "zope.interface", 
        "nevow.util", 
        "nevow.tags", 
        "nevow.context", 
        "nevow.stan", 
        "nevow.inevow", 
        "nevow.static"
      ]
    }, 
    "fpformat": {
      "file": "fpformat.py", 
      "imports": [
        "re", 
        "warnings"
      ]
    }, 
    "fractions": {
      "file": "fractions.py", 
      "imports": [
        "__future__", 
        "decimal", 
        "math", 
        "operator", 
        "numbers", 
        "re"
      ]
    }, 
    "ftplib": {
      "file": "ftplib.py", 
      "imports": [
        "SOCKS", 
        "ssl", 
        "sys", 
        "os", 
        "re", 
        "socket"
      ]
    }, 
    "functools": {
      "file": "functools.py", 
      "imports": [
        "_functools"
      ]
    }, 
    "future_builtins": {
      "file": "future_builtins.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.imap", 
        "itertools.izip"
      ]
    }, 
    "gdbm": {
      "file": "gdbm.py", 
      "imports": [
        "_gdbm_cffi.ffi", 
        "_gdbm_cffi.lib", 
        "os", 
        "thread"
      ]
    }, 
    "genericpath": {
      "file": "genericpath.py", 
      "imports": [
        "os", 
        "stat"
      ]
    }, 
    "getopt": {
      "file": "getopt.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "getpass": {
      "file": "getpass.py", 
      "imports": [
        "EasyDialogs.AskPassword", 
        "msvcrt", 
        "sys", 
        "termios", 
        "os", 
        "warnings", 
        "pwd"
      ]
    }, 
    "gettext": {
      "file": "gettext.py", 
      "imports": [
        "__builtin__", 
        "cStringIO.StringIO", 
        "copy", 
        "errno.ENOENT", 
        "sys", 
        "token", 
        "locale", 
        "os", 
        "re", 
        "StringIO", 
        "struct", 
        "tokenize"
      ]
    }, 
    "glob": {
      "file": "glob.py", 
      "imports": [
        "fnmatch", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "greenlet": {
      "file": "greenlet.py", 
      "imports": [
        "_continuation", 
        "sys", 
        "threading.local"
      ]
    }, 
    "grp": {
      "file": "grp.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_pwdgrp_cffi.ffi", 
        "_pwdgrp_cffi.lib", 
        "_structseq", 
        "os"
      ]
    }, 
    "gzip": {
      "file": "gzip.py", 
      "imports": [
        "__builtin__", 
        "errno", 
        "sys", 
        "time", 
        "zlib", 
        "io", 
        "os", 
        "struct", 
        "warnings"
      ]
    }, 
    "hashlib": {
      "file": "hashlib.py", 
      "imports": [
        "_hashlib", 
        "_hashlib.pbkdf2_hmac", 
        "_md5", 
        "_sha", 
        "binascii", 
        "logging", 
        "struct", 
        "_sha256", 
        "_sha512"
      ]
    }, 
    "heapq": {
      "file": "heapq.py", 
      "imports": [
        "_heapq.*", 
        "doctest", 
        "itertools.chain", 
        "itertools.count", 
        "itertools.imap", 
        "itertools.islice", 
        "itertools.izip", 
        "itertools.tee", 
        "operator.itemgetter"
      ]
    }, 
    "helper": {
      "file": "helper.py", 
      "imports": [
        "__builtin__", 
        "pydoc"
      ]
    }, 
    "hmac": {
      "file": "hmac.py", 
      "imports": [
        "hashlib", 
        "operator._compare_digest", 
        "warnings"
      ]
    }, 
    "htmlentitydefs": {
      "file": "htmlentitydefs.py", 
      "imports": []
    }, 
    "htmllib": {
      "file": "htmllib.py", 
      "imports": [
        "formatter", 
        "htmlentitydefs", 
        "sys", 
        "sgmllib", 
        "warnings"
      ]
    }, 
    "httplib": {
      "file": "httplib.py", 
      "imports": [
        "array.array", 
        "cStringIO.StringIO", 
        "ssl", 
        "sys.py3kwarning", 
        "mimetools", 
        "os", 
        "socket", 
        "StringIO", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "identity_dict": {
      "file": "identity_dict.py", 
      "imports": [
        "UserDict", 
        "__pypy__.identity_dict"
      ]
    }, 
    "ihooks": {
      "file": "ihooks.py", 
      "imports": [
        "__builtin__", 
        "imp", 
        "imp.C_BUILTIN", 
        "imp.C_EXTENSION", 
        "imp.PKG_DIRECTORY", 
        "imp.PY_COMPILED", 
        "imp.PY_FROZEN", 
        "imp.PY_SOURCE", 
        "marshal", 
        "sys", 
        "os", 
        "warnings"
      ]
    }, 
    "imaplib": {
      "file": "imaplib.py", 
      "imports": [
        "binascii", 
        "errno", 
        "getopt", 
        "getpass", 
        "hmac", 
        "ssl", 
        "subprocess", 
        "sys", 
        "time", 
        "random", 
        "re", 
        "socket"
      ]
    }, 
    "imghdr": {
      "file": "imghdr.py", 
      "imports": [
        "glob", 
        "sys", 
        "os"
      ]
    }, 
    "importlib": {
      "dir": "importlib"
    }, 
    "importlib.__init__": {
      "file": "importlib/__init__.py", 
      "imports": [
        "sys"
      ]
    }, 
    "imputil": {
      "file": "imputil.py", 
      "imports": [
        "__builtin__", 
        "dos.stat", 
        "imp", 
        "marshal", 
        "nt.stat", 
        "os2.stat", 
        "posix.stat", 
        "sys", 
        "struct", 
        "warnings"
      ]
    }, 
    "inspect": {
      "file": "inspect.py", 
      "imports": [
        "collections", 
        "dis", 
        "imp", 
        "operator.attrgetter", 
        "sys", 
        "linecache", 
        "os", 
        "re", 
        "string", 
        "tokenize", 
        "types"
      ]
    }, 
    "io": {
      "file": "io.py", 
      "imports": [
        "_io", 
        "_io.BlockingIOError", 
        "_io.BufferedRWPair", 
        "_io.BufferedRandom", 
        "_io.BufferedReader", 
        "_io.BufferedWriter", 
        "_io.BytesIO", 
        "_io.DEFAULT_BUFFER_SIZE", 
        "_io.FileIO", 
        "_io.IncrementalNewlineDecoder", 
        "_io.StringIO", 
        "_io.TextIOWrapper", 
        "_io.UnsupportedOperation", 
        "_io.open", 
        "abc"
      ]
    }, 
    "json": {
      "dir": "json"
    }, 
    "json.__init__": {
      "file": "json/__init__.py", 
      "imports": [
        "_pypyjson", 
        "json.decoder", 
        "json.encoder"
      ]
    }, 
    "json.decoder": {
      "file": "json/decoder.py", 
      "imports": [
        "_json.scanstring", 
        "json.scanner", 
        "sys", 
        "re", 
        "struct"
      ]
    }, 
    "json.encoder": {
      "file": "json/encoder.py", 
      "imports": [
        "__pypy__.builders.StringBuilder", 
        "__pypy__.builders.UnicodeBuilder", 
        "_pypyjson.raw_encode_basestring_ascii", 
        "re"
      ]
    }, 
    "json.scanner": {
      "file": "json/scanner.py", 
      "imports": [
        "_json.make_scanner", 
        "re"
      ]
    }, 
    "json.tests": {
      "dir": "json/tests"
    }, 
    "json.tests.__init__": {
      "file": "json/tests/__init__.py", 
      "imports": [
        "doctest", 
        "json", 
        "sys", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "json.tests.test_check_circular": {
      "file": "json/tests/test_check_circular.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_decode": {
      "file": "json/tests/test_decode.py", 
      "imports": [
        "collections", 
        "decimal", 
        "json.tests", 
        "StringIO"
      ]
    }, 
    "json.tests.test_default": {
      "file": "json/tests/test_default.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_dump": {
      "file": "json/tests/test_dump.py", 
      "imports": [
        "cStringIO.StringIO", 
        "json.tests"
      ]
    }, 
    "json.tests.test_encode_basestring_ascii": {
      "file": "json/tests/test_encode_basestring_ascii.py", 
      "imports": [
        "collections", 
        "json.tests"
      ]
    }, 
    "json.tests.test_fail": {
      "file": "json/tests/test_fail.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_float": {
      "file": "json/tests/test_float.py", 
      "imports": [
        "json.tests", 
        "math"
      ]
    }, 
    "json.tests.test_indent": {
      "file": "json/tests/test_indent.py", 
      "imports": [
        "json.tests", 
        "StringIO", 
        "textwrap"
      ]
    }, 
    "json.tests.test_pass1": {
      "file": "json/tests/test_pass1.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_pass2": {
      "file": "json/tests/test_pass2.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_pass3": {
      "file": "json/tests/test_pass3.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_recursion": {
      "file": "json/tests/test_recursion.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_scanstring": {
      "file": "json/tests/test_scanstring.py", 
      "imports": [
        "json.tests", 
        "sys"
      ]
    }, 
    "json.tests.test_separators": {
      "file": "json/tests/test_separators.py", 
      "imports": [
        "json.tests", 
        "textwrap"
      ]
    }, 
    "json.tests.test_speedups": {
      "file": "json/tests/test_speedups.py", 
      "imports": [
        "json.tests"
      ]
    }, 
    "json.tests.test_tool": {
      "file": "json/tests/test_tool.py", 
      "imports": [
        "subprocess", 
        "sys", 
        "os", 
        "test.test_support", 
        "test.script_helper", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "json.tests.test_unicode": {
      "file": "json/tests/test_unicode.py", 
      "imports": [
        "collections", 
        "json.tests"
      ]
    }, 
    "json.tool": {
      "file": "json/tool.py", 
      "imports": [
        "json", 
        "sys"
      ]
    }, 
    "keyword": {
      "file": "keyword.py", 
      "imports": [
        "sys", 
        "re"
      ]
    }, 
    "lib2to3": {
      "dir": "lib2to3"
    }, 
    "lib2to3.__init__": {
      "file": "lib2to3/__init__.py", 
      "imports": []
    }, 
    "lib2to3.__main__": {
      "file": "lib2to3/__main__.py", 
      "imports": [
        "lib2to3.main", 
        "sys"
      ]
    }, 
    "lib2to3.btm_matcher": {
      "file": "lib2to3/btm_matcher.py", 
      "imports": [
        "collections", 
        "itertools", 
        "lib2to3.btm_utils", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "logging"
      ]
    }, 
    "lib2to3.btm_utils": {
      "file": "lib2to3/btm_utils.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixer_base": {
      "file": "lib2to3/fixer_base.py", 
      "imports": [
        "itertools", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pygram", 
        "logging"
      ]
    }, 
    "lib2to3.fixer_util": {
      "file": "lib2to3/fixer_util.py", 
      "imports": [
        "itertools.islice", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes": {
      "dir": "lib2to3/fixes"
    }, 
    "lib2to3.fixes.__init__": {
      "file": "lib2to3/fixes/__init__.py", 
      "imports": []
    }, 
    "lib2to3.fixes.fix_apply": {
      "file": "lib2to3/fixes/fix_apply.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_asserts": {
      "file": "lib2to3/fixes/fix_asserts.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_basestring": {
      "file": "lib2to3/fixes/fix_basestring.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_buffer": {
      "file": "lib2to3/fixes/fix_buffer.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_callable": {
      "file": "lib2to3/fixes/fix_callable.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_dict": {
      "file": "lib2to3/fixes/fix_dict.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_except": {
      "file": "lib2to3/fixes/fix_except.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_exec": {
      "file": "lib2to3/fixes/fix_exec.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_execfile": {
      "file": "lib2to3/fixes/fix_execfile.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_exitfunc": {
      "file": "lib2to3/fixes/fix_exitfunc.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_filter": {
      "file": "lib2to3/fixes/fix_filter.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_funcattrs": {
      "file": "lib2to3/fixes/fix_funcattrs.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_future": {
      "file": "lib2to3/fixes/fix_future.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_getcwdu": {
      "file": "lib2to3/fixes/fix_getcwdu.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_has_key": {
      "file": "lib2to3/fixes/fix_has_key.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_idioms": {
      "file": "lib2to3/fixes/fix_idioms.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_import": {
      "file": "lib2to3/fixes/fix_import.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "os"
      ]
    }, 
    "lib2to3.fixes.fix_imports": {
      "file": "lib2to3/fixes/fix_imports.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_imports2": {
      "file": "lib2to3/fixes/fix_imports2.py", 
      "imports": [
        "lib2to3.fixes.fix_imports"
      ]
    }, 
    "lib2to3.fixes.fix_input": {
      "file": "lib2to3/fixes/fix_input.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp"
      ]
    }, 
    "lib2to3.fixes.fix_intern": {
      "file": "lib2to3/fixes/fix_intern.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_isinstance": {
      "file": "lib2to3/fixes/fix_isinstance.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_itertools": {
      "file": "lib2to3/fixes/fix_itertools.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_itertools_imports": {
      "file": "lib2to3/fixes/fix_itertools_imports.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_long": {
      "file": "lib2to3/fixes/fix_long.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_map": {
      "file": "lib2to3/fixes/fix_map.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_metaclass": {
      "file": "lib2to3/fixes/fix_metaclass.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_methodattrs": {
      "file": "lib2to3/fixes/fix_methodattrs.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_ne": {
      "file": "lib2to3/fixes/fix_ne.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_next": {
      "file": "lib2to3/fixes/fix_next.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram"
      ]
    }, 
    "lib2to3.fixes.fix_nonzero": {
      "file": "lib2to3/fixes/fix_nonzero.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_numliterals": {
      "file": "lib2to3/fixes/fix_numliterals.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_operator": {
      "file": "lib2to3/fixes/fix_operator.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_paren": {
      "file": "lib2to3/fixes/fix_paren.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_print": {
      "file": "lib2to3/fixes/fix_print.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_raise": {
      "file": "lib2to3/fixes/fix_raise.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_raw_input": {
      "file": "lib2to3/fixes/fix_raw_input.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_reduce": {
      "file": "lib2to3/fixes/fix_reduce.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_renames": {
      "file": "lib2to3/fixes/fix_renames.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_repr": {
      "file": "lib2to3/fixes/fix_repr.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_set_literal": {
      "file": "lib2to3/fixes/fix_set_literal.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_standarderror": {
      "file": "lib2to3/fixes/fix_standarderror.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_sys_exc": {
      "file": "lib2to3/fixes/fix_sys_exc.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_throw": {
      "file": "lib2to3/fixes/fix_throw.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_tuple_params": {
      "file": "lib2to3/fixes/fix_tuple_params.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_types": {
      "file": "lib2to3/fixes/fix_types.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_unicode": {
      "file": "lib2to3/fixes/fix_unicode.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.fixes.fix_urllib": {
      "file": "lib2to3/fixes/fix_urllib.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.fixes.fix_imports"
      ]
    }, 
    "lib2to3.fixes.fix_ws_comma": {
      "file": "lib2to3/fixes/fix_ws_comma.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree"
      ]
    }, 
    "lib2to3.fixes.fix_xrange": {
      "file": "lib2to3/fixes/fix_xrange.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp"
      ]
    }, 
    "lib2to3.fixes.fix_xreadlines": {
      "file": "lib2to3/fixes/fix_xreadlines.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.fixes.fix_zip": {
      "file": "lib2to3/fixes/fix_zip.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util"
      ]
    }, 
    "lib2to3.main": {
      "file": "lib2to3/main.py", 
      "imports": [
        "__future__", 
        "difflib", 
        "lib2to3.refactor", 
        "sys", 
        "logging", 
        "optparse", 
        "os", 
        "shutil"
      ]
    }, 
    "lib2to3.patcomp": {
      "file": "lib2to3/patcomp.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.literals", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.pgen2": {
      "dir": "lib2to3/pgen2"
    }, 
    "lib2to3.pgen2.__init__": {
      "file": "lib2to3/pgen2/__init__.py", 
      "imports": []
    }, 
    "lib2to3.pgen2.conv": {
      "file": "lib2to3/pgen2/conv.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "re"
      ]
    }, 
    "lib2to3.pgen2.driver": {
      "file": "lib2to3/pgen2/driver.py", 
      "imports": [
        "codecs", 
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.pgen", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "sys", 
        "logging", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.pgen2.grammar": {
      "file": "lib2to3/pgen2/grammar.py", 
      "imports": [
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "pickle", 
        "pprint"
      ]
    }, 
    "lib2to3.pgen2.literals": {
      "file": "lib2to3/pgen2/literals.py", 
      "imports": [
        "re"
      ]
    }, 
    "lib2to3.pgen2.parse": {
      "file": "lib2to3/pgen2/parse.py", 
      "imports": [
        "lib2to3.pgen2.token"
      ]
    }, 
    "lib2to3.pgen2.pgen": {
      "file": "lib2to3/pgen2/pgen.py", 
      "imports": [
        "lib2to3.pgen2.grammar", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize"
      ]
    }, 
    "lib2to3.pgen2.token": {
      "file": "lib2to3/pgen2/token.py", 
      "imports": []
    }, 
    "lib2to3.pgen2.tokenize": {
      "file": "lib2to3/pgen2/tokenize.py", 
      "imports": [
        "codecs", 
        "lib2to3.pgen2.token", 
        "sys", 
        "re", 
        "string"
      ]
    }, 
    "lib2to3.pygram": {
      "file": "lib2to3/pygram.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree", 
        "os"
      ]
    }, 
    "lib2to3.pytree": {
      "file": "lib2to3/pytree.py", 
      "imports": [
        "lib2to3.pygram", 
        "sys", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "lib2to3.refactor": {
      "file": "lib2to3/refactor.py", 
      "imports": [
        "__future__", 
        "codecs", 
        "collections", 
        "itertools.chain", 
        "lib2to3.btm_matcher", 
        "lib2to3.btm_utils", 
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.driver", 
        "lib2to3.pgen2.token", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "multiprocessing", 
        "operator", 
        "sys", 
        "logging", 
        "os", 
        "StringIO"
      ]
    }, 
    "lib2to3.tests": {
      "dir": "lib2to3/tests"
    }, 
    "lib2to3.tests.__init__": {
      "file": "lib2to3/tests/__init__.py", 
      "imports": [
        "lib2to3.tests.support", 
        "os", 
        "types", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.pytree_idempotency": {
      "file": "lib2to3/tests/pytree_idempotency.py", 
      "imports": [
        "lib2to3.pgen2", 
        "lib2to3.pgen2.driver", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "sys", 
        "logging", 
        "os"
      ]
    }, 
    "lib2to3.tests.support": {
      "file": "lib2to3/tests/support.py", 
      "imports": [
        "lib2to3.pgen2.driver", 
        "lib2to3.pytree", 
        "lib2to3.refactor", 
        "sys", 
        "os", 
        "re", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_all_fixers": {
      "file": "lib2to3/tests/test_all_fixers.py", 
      "imports": [
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_fixers": {
      "file": "lib2to3/tests/test_fixers.py", 
      "imports": [
        "itertools.chain", 
        "lib2to3.fixer_util", 
        "lib2to3.fixes.fix_import", 
        "lib2to3.fixes.fix_imports", 
        "lib2to3.fixes.fix_imports2", 
        "lib2to3.fixes.fix_urllib", 
        "lib2to3.pygram", 
        "lib2to3.pytree", 
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "operator.itemgetter", 
        "os", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_main": {
      "file": "lib2to3/tests/test_main.py", 
      "imports": [
        "codecs", 
        "lib2to3.main", 
        "sys", 
        "logging", 
        "os", 
        "re", 
        "shutil", 
        "StringIO", 
        "tempfile", 
        "unittest"
      ]
    }, 
    "lib2to3.tests.test_parser": {
      "file": "lib2to3/tests/test_parser.py", 
      "imports": [
        "__future__", 
        "lib2to3.pgen2.parse", 
        "lib2to3.pgen2.tokenize", 
        "lib2to3.pygram", 
        "lib2to3.tests.support", 
        "sys", 
        "os"
      ]
    }, 
    "lib2to3.tests.test_pytree": {
      "file": "lib2to3/tests/test_pytree.py", 
      "imports": [
        "__future__", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "sys", 
        "warnings"
      ]
    }, 
    "lib2to3.tests.test_refactor": {
      "file": "lib2to3/tests/test_refactor.py", 
      "imports": [
        "__future__", 
        "codecs", 
        "lib2to3.fixer_base", 
        "lib2to3.pgen2.token", 
        "lib2to3.pygram", 
        "lib2to3.refactor", 
        "lib2to3.tests.support", 
        "myfixes.fix_explicit.FixExplicit", 
        "myfixes.fix_first.FixFirst", 
        "myfixes.fix_last.FixLast", 
        "myfixes.fix_parrot.FixParrot", 
        "myfixes.fix_preorder.FixPreorder", 
        "operator", 
        "sys", 
        "os", 
        "shutil", 
        "StringIO", 
        "tempfile", 
        "unittest", 
        "warnings"
      ]
    }, 
    "lib2to3.tests.test_util": {
      "file": "lib2to3/tests/test_util.py", 
      "imports": [
        "lib2to3.fixer_util", 
        "lib2to3.pgen2.token", 
        "lib2to3.pytree", 
        "lib2to3.tests.support", 
        "os"
      ]
    }, 
    "linecache": {
      "file": "linecache.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "locale": {
      "file": "locale.py", 
      "imports": [
        "_locale", 
        "_locale.*", 
        "encodings", 
        "encodings.aliases", 
        "functools", 
        "operator", 
        "sys", 
        "os", 
        "re"
      ]
    }, 
    "logging": {
      "dir": "logging"
    }, 
    "logging.__init__": {
      "file": "logging/__init__.py", 
      "imports": [
        "atexit", 
        "cStringIO", 
        "codecs", 
        "collections", 
        "sys", 
        "thread", 
        "threading", 
        "time", 
        "os", 
        "traceback", 
        "warnings", 
        "weakref"
      ]
    }, 
    "logging.config": {
      "file": "logging/config.py", 
      "imports": [
        "ConfigParser", 
        "cStringIO", 
        "errno", 
        "io", 
        "json", 
        "logging", 
        "logging.handlers", 
        "sys", 
        "thread", 
        "threading", 
        "os", 
        "re", 
        "socket", 
        "SocketServer", 
        "struct", 
        "tempfile", 
        "traceback", 
        "types", 
        "select"
      ]
    }, 
    "logging.handlers": {
      "file": "logging/handlers.py", 
      "imports": [
        "codecs", 
        "email.utils", 
        "errno", 
        "httplib", 
        "logging", 
        "time", 
        "win32evtlog", 
        "win32evtlogutil", 
        "os", 
        "re", 
        "smtplib", 
        "socket", 
        "stat", 
        "struct", 
        "urllib", 
        "cPickle"
      ]
    }, 
    "macurl2path": {
      "file": "macurl2path.py", 
      "imports": [
        "os", 
        "urllib"
      ]
    }, 
    "mailbox": {
      "file": "mailbox.py", 
      "imports": [
        "calendar", 
        "copy", 
        "email", 
        "email.generator", 
        "email.message", 
        "errno", 
        "fcntl", 
        "sys", 
        "time", 
        "os", 
        "re", 
        "rfc822", 
        "socket", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "mailcap": {
      "file": "mailcap.py", 
      "imports": [
        "sys", 
        "os"
      ]
    }, 
    "markupbase": {
      "file": "markupbase.py", 
      "imports": [
        "re"
      ]
    }, 
    "marshal": {
      "file": "marshal.py", 
      "imports": [
        "_marshal"
      ]
    }, 
    "md5": {
      "file": "md5.py", 
      "imports": [
        "hashlib", 
        "warnings"
      ]
    }, 
    "mhlib": {
      "file": "mhlib.py", 
      "imports": [
        "bisect", 
        "cStringIO.StringIO", 
        "sys", 
        "mimetools", 
        "multifile", 
        "os", 
        "re", 
        "shutil", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "mimetools": {
      "file": "mimetools.py", 
      "imports": [
        "base64", 
        "dummy_thread", 
        "sys", 
        "thread", 
        "time", 
        "os", 
        "quopri", 
        "rfc822", 
        "socket", 
        "tempfile", 
        "uu", 
        "warnings"
      ]
    }, 
    "mimetypes": {
      "file": "mimetypes.py", 
      "imports": [
        "_winreg", 
        "getopt", 
        "sys", 
        "os", 
        "posixpath", 
        "urllib"
      ]
    }, 
    "mimify": {
      "file": "mimify.py", 
      "imports": [
        "base64", 
        "getopt", 
        "sys", 
        "os", 
        "re", 
        "warnings"
      ]
    }, 
    "modulefinder": {
      "file": "modulefinder.py", 
      "imports": [
        "__future__", 
        "dis", 
        "getopt", 
        "imp", 
        "marshal", 
        "sys", 
        "os", 
        "struct", 
        "types"
      ]
    }, 
    "mttest": {
      "file": "mttest.py", 
      "imports": [
        "js", 
        "multiprocessing", 
        "os"
      ]
    }, 
    "multifile": {
      "file": "multifile.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "multiprocessing": {
      "dir": "multiprocessing"
    }, 
    "multiprocessing.__init__": {
      "file": "multiprocessing/__init__.py", 
      "imports": [
        "multiprocessing.connection", 
        "multiprocessing.forking", 
        "multiprocessing.managers", 
        "multiprocessing.pool", 
        "multiprocessing.process", 
        "multiprocessing.queues", 
        "multiprocessing.reduction", 
        "multiprocessing.sharedctypes", 
        "multiprocessing.synchronize", 
        "multiprocessing.util", 
        "os", 
        "sys"
      ]
    }, 
    "multiprocessing.connection": {
      "file": "multiprocessing/connection.py", 
      "imports": [
        "errno", 
        "hmac", 
        "itertools", 
        "multiprocessing", 
        "multiprocessing.forking", 
        "multiprocessing.util", 
        "os", 
        "socket", 
        "sys", 
        "tempfile", 
        "time", 
        "xmlrpclib", 
        "_multiprocessing"
      ]
    }, 
    "multiprocessing.dummy": {
      "dir": "multiprocessing/dummy"
    }, 
    "multiprocessing.dummy.__init__": {
      "file": "multiprocessing/dummy/__init__.py", 
      "imports": [
        "Queue", 
        "array", 
        "itertools", 
        "multiprocessing", 
        "multiprocessing.dummy.connection", 
        "multiprocessing.pool", 
        "sys", 
        "threading", 
        "threading.BoundedSemaphore", 
        "threading.Event", 
        "threading.Lock", 
        "threading.RLock", 
        "threading.Semaphore", 
        "weakref"
      ]
    }, 
    "multiprocessing.dummy.connection": {
      "file": "multiprocessing/dummy/connection.py", 
      "imports": [
        "Queue"
      ]
    }, 
    "multiprocessing.forking": {
      "file": "multiprocessing/forking.py", 
      "imports": [
        "errno", 
        "functools", 
        "imp", 
        "js", 
        "multiprocessing.process", 
        "multiprocessing.util", 
        "os", 
        "pickle", 
        "random", 
        "sys", 
        "time", 
        "signal"
      ]
    }, 
    "multiprocessing.heap": {
      "file": "multiprocessing/heap.py", 
      "imports": [
        "bisect", 
        "itertools", 
        "mmap", 
        "multiprocessing.forking", 
        "multiprocessing.util", 
        "os", 
        "sys", 
        "tempfile", 
        "threading", 
        "_multiprocessing"
      ]
    }, 
    "multiprocessing.managers": {
      "file": "multiprocessing/managers.py", 
      "imports": [
        "Queue", 
        "array", 
        "cPickle", 
        "multiprocessing", 
        "multiprocessing.connection", 
        "multiprocessing.forking", 
        "multiprocessing.process", 
        "multiprocessing.util", 
        "os", 
        "pickle", 
        "sys", 
        "threading", 
        "traceback", 
        "weakref"
      ]
    }, 
    "multiprocessing.pool": {
      "file": "multiprocessing/pool.py", 
      "imports": [
        "Queue", 
        "collections", 
        "itertools", 
        "multiprocessing", 
        "multiprocessing.dummy", 
        "multiprocessing.queues", 
        "multiprocessing.util", 
        "threading", 
        "time"
      ]
    }, 
    "multiprocessing.process": {
      "file": "multiprocessing/process.py", 
      "imports": [
        "itertools", 
        "multiprocessing.forking", 
        "multiprocessing.util", 
        "os", 
        "sys", 
        "traceback", 
        "signal"
      ]
    }, 
    "multiprocessing.queues": {
      "file": "multiprocessing/queues.py", 
      "imports": [
        "Queue", 
        "atexit", 
        "collections", 
        "multiprocessing", 
        "multiprocessing.forking", 
        "multiprocessing.synchronize", 
        "multiprocessing.util", 
        "os", 
        "sys", 
        "threading", 
        "time", 
        "traceback", 
        "weakref"
      ]
    }, 
    "multiprocessing.reduction": {
      "file": "multiprocessing/reduction.py", 
      "imports": [
        "_subprocess", 
        "multiprocessing", 
        "multiprocessing.connection", 
        "multiprocessing.forking", 
        "multiprocessing.util", 
        "os", 
        "socket", 
        "sys", 
        "threading", 
        "traceback", 
        "_multiprocessing"
      ]
    }, 
    "multiprocessing.sharedctypes": {
      "file": "multiprocessing/sharedctypes.py", 
      "imports": [
        "ctypes", 
        "multiprocessing", 
        "multiprocessing.forking", 
        "multiprocessing.heap", 
        "sys", 
        "weakref"
      ]
    }, 
    "multiprocessing.synchronize": {
      "file": "multiprocessing/synchronize.py", 
      "imports": [
        "js", 
        "multiprocessing.forking", 
        "multiprocessing.process", 
        "multiprocessing.util", 
        "os", 
        "sys", 
        "threading", 
        "time.sleep", 
        "time.time"
      ]
    }, 
    "multiprocessing.util": {
      "file": "multiprocessing/util.py", 
      "imports": [
        "atexit", 
        "itertools", 
        "logging", 
        "multiprocessing.process", 
        "os", 
        "shutil", 
        "subprocess", 
        "tempfile", 
        "threading", 
        "traceback", 
        "weakref"
      ]
    }, 
    "mutex": {
      "file": "mutex.py", 
      "imports": [
        "collections", 
        "warnings"
      ]
    }, 
    "my_test_runner": {
      "file": "my_test_runner.py", 
      "imports": [
        "doctest"
      ]
    }, 
    "netrc": {
      "file": "netrc.py", 
      "imports": [
        "os", 
        "shlex", 
        "stat", 
        "pwd"
      ]
    }, 
    "nevow": {
      "dir": "nevow"
    }, 
    "nevow.__init__": {
      "file": "nevow/__init__.py", 
      "imports": [
        "nevow._version", 
        "nevow.flat", 
        "nevow.flat.flatmdom", 
        "nevow.i18n", 
        "nevow.query", 
        "nevow.util", 
        "sys", 
        "twisted.python.components", 
        "twisted.python.versions"
      ]
    }, 
    "nevow._flat": {
      "file": "nevow/_flat.py", 
      "imports": [
        "nevow.context", 
        "nevow.flat", 
        "nevow.flat.flatstan", 
        "nevow.flat.ten", 
        "nevow.inevow", 
        "nevow.stan", 
        "nevow.tags", 
        "nevow.url", 
        "sys.exc_info", 
        "traceback", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "types"
      ]
    }, 
    "nevow._version": {
      "file": "nevow/_version.py", 
      "imports": []
    }, 
    "nevow._widget_plugin": {
      "file": "nevow/_widget_plugin.py", 
      "imports": [
        "nevow.appserver", 
        "nevow.athena", 
        "nevow.loaders", 
        "nevow.tags", 
        "twisted.application.internet", 
        "twisted.python.reflect", 
        "twisted.python.usage"
      ]
    }, 
    "nevow.accessors": {
      "file": "nevow/accessors.py", 
      "imports": [
        "nevow.inevow", 
        "nevow.util", 
        "twisted.python.components", 
        "zope.interface"
      ]
    }, 
    "nevow.appserver": {
      "file": "nevow/appserver.py", 
      "imports": [
        "cgi", 
        "nevow.context", 
        "nevow.failure", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.rend", 
        "nevow.stan", 
        "nevow.url", 
        "twisted.internet.defer", 
        "twisted.protocols", 
        "twisted.python.components", 
        "twisted.python.log", 
        "twisted.web.http", 
        "twisted.web.server", 
        "urllib", 
        "zope.interface"
      ]
    }, 
    "nevow.athena": {
      "file": "nevow/athena.py", 
      "imports": [
        "itertools", 
        "nevow._flat", 
        "nevow.flat", 
        "nevow.guard", 
        "nevow.inevow", 
        "nevow.json", 
        "nevow.loaders", 
        "nevow.page", 
        "nevow.plugins", 
        "nevow.rend", 
        "nevow.stan", 
        "nevow.static", 
        "nevow.tags", 
        "nevow.testutil", 
        "nevow.url", 
        "nevow.useragent", 
        "nevow.util", 
        "os", 
        "re", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.reactor", 
        "twisted.plugin", 
        "twisted.python.context", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.util", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "nevow.blocks": {
      "file": "nevow/blocks.py", 
      "imports": [
        "nevow.inevow", 
        "nevow.static", 
        "nevow.tags", 
        "zope.interface"
      ]
    }, 
    "nevow.canvas": {
      "file": "nevow/canvas.py", 
      "imports": [
        "itertools.count", 
        "nevow.appserver", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.stan", 
        "nevow.static", 
        "nevow.tags", 
        "nevow.url", 
        "nevow.util", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "nevow.compression": {
      "file": "nevow/compression.py", 
      "imports": [
        "gzip", 
        "nevow.appserver", 
        "nevow.inevow", 
        "nevow.rend", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "zope.interface"
      ]
    }, 
    "nevow.compy": {
      "file": "nevow/compy.py", 
      "imports": [
        "nevow.flat", 
        "nevow.util", 
        "twisted.python.components", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "nevow.context": {
      "file": "nevow/context.py", 
      "imports": [
        "__future__", 
        "nevow.inevow", 
        "nevow.stan", 
        "nevow.util", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "nevow.dirlist": {
      "file": "nevow/dirlist.py", 
      "imports": [
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.static", 
        "nevow.tags", 
        "os", 
        "stat", 
        "urllib"
      ]
    }, 
    "nevow.entities": {
      "file": "nevow/entities.py", 
      "imports": [
        "nevow.stan", 
        "types"
      ]
    }, 
    "nevow.errors": {
      "file": "nevow/errors.py", 
      "imports": []
    }, 
    "nevow.events": {
      "file": "nevow/events.py", 
      "imports": []
    }, 
    "nevow.failure": {
      "file": "nevow/failure.py", 
      "imports": [
        "linecache", 
        "nevow.tags", 
        "re", 
        "twisted.python.failure", 
        "types"
      ]
    }, 
    "nevow.flat": {
      "dir": "nevow/flat"
    }, 
    "nevow.flat.__init__": {
      "file": "nevow/flat/__init__.py", 
      "imports": [
        "nevow.flat.ten", 
        "nevow.flat.twist", 
        "twisted"
      ]
    }, 
    "nevow.flat.flatmdom": {
      "file": "nevow/flat/flatmdom.py", 
      "imports": [
        "__future__", 
        "nevow.flat", 
        "nevow.stan", 
        "nevow.util"
      ]
    }, 
    "nevow.flat.flatsax": {
      "file": "nevow/flat/flatsax.py", 
      "imports": [
        "cStringIO", 
        "nevow", 
        "nevow.stan", 
        "sys", 
        "xml", 
        "xml.sax", 
        "xml.sax.handler"
      ]
    }, 
    "nevow.flat.flatstan": {
      "file": "nevow/flat/flatstan.py", 
      "imports": [
        "__future__", 
        "nevow.accessors", 
        "nevow.context", 
        "nevow.failure", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.livepage", 
        "nevow.stan", 
        "nevow.tags", 
        "nevow.util", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "urllib", 
        "warnings"
      ]
    }, 
    "nevow.flat.ten": {
      "file": "nevow/flat/ten.py", 
      "imports": [
        "__future__", 
        "nevow.context", 
        "nevow.inevow", 
        "nevow.tags", 
        "nevow.testutil", 
        "nevow.util", 
        "twisted.python.components", 
        "types", 
        "warnings", 
        "zope.interface.declarations", 
        "zope.interface.interface"
      ]
    }, 
    "nevow.flat.twist": {
      "file": "nevow/flat/twist.py", 
      "imports": [
        "nevow.flat", 
        "twisted.internet.defer"
      ]
    }, 
    "nevow.guard": {
      "file": "nevow/guard.py", 
      "imports": [
        "StringIO", 
        "hashlib", 
        "md5", 
        "nevow.inevow", 
        "nevow.stan", 
        "nevow.url", 
        "random", 
        "time", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.protocols", 
        "twisted.python.components", 
        "twisted.python.log", 
        "twisted.web.http", 
        "zope.interface"
      ]
    }, 
    "nevow.i18n": {
      "file": "nevow/i18n.py", 
      "imports": [
        "gettext", 
        "nevow.inevow", 
        "zope.interface"
      ]
    }, 
    "nevow.inevow": {
      "file": "nevow/inevow.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "nevow.itaglibrary": {
      "file": "nevow/itaglibrary.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "nevow.json": {
      "file": "nevow/json.py", 
      "imports": [
        "nevow._flat", 
        "nevow.athena", 
        "nevow.inevow", 
        "nevow.page", 
        "nevow.rend", 
        "nevow.tags", 
        "re", 
        "types"
      ]
    }, 
    "nevow.jsutil": {
      "file": "nevow/jsutil.py", 
      "imports": [
        "nevow", 
        "nevow.athena", 
        "twisted.python.procutils", 
        "twisted.python.util"
      ]
    }, 
    "nevow.livepage": {
      "file": "nevow/livepage.py", 
      "imports": [
        "itertools", 
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.rend", 
        "nevow.stan", 
        "nevow.static", 
        "nevow.tags", 
        "nevow.testutil", 
        "nevow.url", 
        "nevow.util", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.task", 
        "twisted.python.log", 
        "types", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "nevow.livetest": {
      "file": "nevow/livetest.py", 
      "imports": [
        "nevow.entities", 
        "nevow.livepage", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.static", 
        "nevow.tags", 
        "nevow.util", 
        "twisted.internet.defer", 
        "twisted.internet.reactor"
      ]
    }, 
    "nevow.livetrial": {
      "dir": "nevow/livetrial"
    }, 
    "nevow.livetrial.__init__": {
      "file": "nevow/livetrial/__init__.py", 
      "imports": []
    }, 
    "nevow.livetrial.runner": {
      "file": "nevow/livetrial/runner.py", 
      "imports": [
        "nevow.athena", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.static", 
        "nevow.tags", 
        "nevow.url", 
        "nevow.util", 
        "twisted.python.filepath"
      ]
    }, 
    "nevow.livetrial.testcase": {
      "file": "nevow/livetrial/testcase.py", 
      "imports": [
        "inspect", 
        "nevow.athena", 
        "nevow.loaders", 
        "nevow.page", 
        "nevow.tags", 
        "twisted.trial.runner", 
        "twisted.trial.unittest", 
        "types", 
        "warnings"
      ]
    }, 
    "nevow.loaders": {
      "file": "nevow/loaders.py", 
      "imports": [
        "nevow.flat", 
        "nevow.flat.flatsax", 
        "nevow.inevow", 
        "nevow.util", 
        "os", 
        "twisted.python.reflect", 
        "twisted.web.microdom", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "nevow.page": {
      "file": "nevow/page.py", 
      "imports": [
        "nevow._flat", 
        "nevow.errors", 
        "nevow.flat.ten", 
        "nevow.inevow", 
        "nevow.rend", 
        "nevow.util", 
        "zope.interface"
      ]
    }, 
    "nevow.plugins": {
      "dir": "nevow/plugins"
    }, 
    "nevow.plugins.__init__": {
      "file": "nevow/plugins/__init__.py", 
      "imports": [
        "os", 
        "sys"
      ]
    }, 
    "nevow.plugins.nevow_package": {
      "file": "nevow/plugins/nevow_package.py", 
      "imports": [
        "nevow", 
        "nevow.athena", 
        "twisted.python.util"
      ]
    }, 
    "nevow.query": {
      "file": "nevow/query.py", 
      "imports": [
        "nevow.inevow", 
        "nevow.stan", 
        "twisted.python.components", 
        "zope.interface"
      ]
    }, 
    "nevow.rend": {
      "file": "nevow/rend.py", 
      "imports": [
        "cStringIO", 
        "formless", 
        "formless.annotate", 
        "formless.configurable", 
        "formless.iformless", 
        "formless.webform", 
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.static", 
        "nevow.tags", 
        "nevow.url", 
        "nevow.util", 
        "random", 
        "time.time", 
        "twisted.python.components", 
        "twisted.python.reflect", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "nevow.scripts": {
      "dir": "nevow/scripts"
    }, 
    "nevow.scripts.__init__": {
      "file": "nevow/scripts/__init__.py", 
      "imports": []
    }, 
    "nevow.scripts.nit": {
      "file": "nevow/scripts/nit.py", 
      "imports": [
        "nevow.appserver", 
        "nevow.livetrial.runner", 
        "nevow.livetrial.testcase", 
        "sys", 
        "twisted.internet.reactor", 
        "twisted.python.log", 
        "twisted.python.usage"
      ]
    }, 
    "nevow.scripts.xmlgettext": {
      "file": "nevow/scripts/xmlgettext.py", 
      "imports": [
        "cStringIO", 
        "nevow", 
        "twisted.application.app", 
        "twisted.python.usage", 
        "xml.dom.pulldom"
      ]
    }, 
    "nevow.stan": {
      "file": "nevow/stan.py", 
      "imports": [
        "__future__", 
        "nevow.inevow", 
        "zope.interface"
      ]
    }, 
    "nevow.static": {
      "file": "nevow/static.py", 
      "imports": [
        "cStringIO", 
        "errno", 
        "mimetypes", 
        "nevow.appserver", 
        "nevow.dirlist", 
        "nevow.inevow", 
        "nevow.rend", 
        "os", 
        "string", 
        "time", 
        "traceback", 
        "twisted.internet.abstract", 
        "twisted.protocols", 
        "twisted.python.components", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.python.threadable", 
        "twisted.python.util", 
        "twisted.spread.pb", 
        "twisted.web.error", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.util", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "nevow.taglibrary": {
      "dir": "nevow/taglibrary"
    }, 
    "nevow.taglibrary.__init__": {
      "file": "nevow/taglibrary/__init__.py", 
      "imports": []
    }, 
    "nevow.taglibrary.cal": {
      "file": "nevow/taglibrary/cal.py", 
      "imports": [
        "calendar", 
        "datetime", 
        "nevow.itaglibrary", 
        "nevow.rend", 
        "nevow.static", 
        "nevow.tags", 
        "nevow.url"
      ]
    }, 
    "nevow.taglibrary.livetags": {
      "file": "nevow/taglibrary/livetags.py", 
      "imports": [
        "nevow.livepage", 
        "nevow.static", 
        "nevow.tags"
      ]
    }, 
    "nevow.taglibrary.progressbar": {
      "file": "nevow/taglibrary/progressbar.py", 
      "imports": [
        "nevow.static", 
        "nevow.taglibrary.livetags", 
        "nevow.tags", 
        "nevow.util"
      ]
    }, 
    "nevow.taglibrary.tabbedPane": {
      "file": "nevow/taglibrary/tabbedPane.py", 
      "imports": [
        "nevow.athena", 
        "nevow.inevow", 
        "nevow.livepage", 
        "nevow.loaders", 
        "nevow.static", 
        "nevow.tags", 
        "nevow.util"
      ]
    }, 
    "nevow.tags": {
      "file": "nevow/tags.py", 
      "imports": [
        "nevow.stan"
      ]
    }, 
    "nevow.test": {
      "dir": "nevow/test"
    }, 
    "nevow.test.__init__": {
      "file": "nevow/test/__init__.py", 
      "imports": []
    }, 
    "nevow.test.acceptance": {
      "dir": "nevow/test/acceptance"
    }, 
    "nevow.test.acceptance.__init__": {
      "file": "nevow/test/acceptance/__init__.py", 
      "imports": []
    }, 
    "nevow.test.acceptance.reconnect": {
      "file": "nevow/test/acceptance/reconnect.py", 
      "imports": [
        "itertools", 
        "nevow._widget_plugin", 
        "nevow.athena", 
        "nevow.loaders", 
        "nevow.tags"
      ]
    }, 
    "nevow.test.acceptance.tabbedpane": {
      "file": "nevow/test/acceptance/tabbedpane.py", 
      "imports": [
        "nevow.athena", 
        "nevow.loaders", 
        "nevow.taglibrary.tabbedPane", 
        "nevow.tags"
      ]
    }, 
    "nevow.test.livetest_athena": {
      "file": "nevow/test/livetest_athena.py", 
      "imports": [
        "nevow.athena", 
        "nevow.inevow", 
        "nevow.livetrial.testcase", 
        "nevow.loaders", 
        "nevow.page", 
        "nevow.tags", 
        "nevow.test.test_json", 
        "nevow.testutil", 
        "twisted.internet.defer", 
        "zope.interface"
      ]
    }, 
    "nevow.test.livetest_runtime": {
      "file": "nevow/test/livetest_runtime.py", 
      "imports": [
        "nevow.livetrial.testcase", 
        "nevow.loaders", 
        "nevow.tags"
      ]
    }, 
    "nevow.test.test_accessors": {
      "file": "nevow/test/test_accessors.py", 
      "imports": [
        "nevow.accessors", 
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.rend", 
        "nevow.stan", 
        "nevow.tags", 
        "nevow.testutil"
      ]
    }, 
    "nevow.test.test_appserver": {
      "file": "nevow/test/test_appserver.py", 
      "imports": [
        "cStringIO", 
        "nevow.appserver", 
        "nevow.context", 
        "nevow.inevow", 
        "nevow.rend", 
        "nevow.testutil", 
        "nevow.util", 
        "twisted.internet.address", 
        "twisted.internet.protocol", 
        "twisted.trial.unittest", 
        "twisted.web.resource", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_athena": {
      "file": "nevow/test/test_athena.py", 
      "imports": [
        "itertools.izip", 
        "nevow._widget_plugin", 
        "nevow.appserver", 
        "nevow.athena", 
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.json", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.tags", 
        "nevow.testutil", 
        "nevow.url", 
        "os", 
        "sets", 
        "twisted.application.internet", 
        "twisted.application.service", 
        "twisted.internet.defer", 
        "twisted.plugin", 
        "twisted.plugins.nevow_widget", 
        "twisted.python.reflect", 
        "twisted.python.usage", 
        "twisted.python.util", 
        "twisted.trial.unittest", 
        "xml.dom.minidom"
      ]
    }, 
    "nevow.test.test_compression": {
      "file": "nevow/test/test_compression.py", 
      "imports": [
        "StringIO", 
        "gzip", 
        "nevow.appserver", 
        "nevow.compression", 
        "nevow.context", 
        "nevow.inevow", 
        "nevow.rend", 
        "nevow.testutil", 
        "twisted.internet.defer", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_consolejstest": {
      "file": "nevow/test/test_consolejstest.py", 
      "imports": [
        "nevow.athena", 
        "nevow.jsutil", 
        "nevow.testutil", 
        "textwrap", 
        "twisted.internet.utils", 
        "twisted.trial.unittest"
      ]
    }, 
    "nevow.test.test_context": {
      "file": "nevow/test/test_context.py", 
      "imports": [
        "itertools", 
        "nevow.context", 
        "nevow.inevow", 
        "nevow.tags", 
        "nevow.testutil", 
        "time", 
        "twisted.python.components", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_disktemplate": {
      "file": "nevow/test/test_disktemplate.py", 
      "imports": [
        "nevow.context", 
        "nevow.flat", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.tags", 
        "nevow.testutil", 
        "nevow.util", 
        "twisted.internet.defer"
      ]
    }, 
    "nevow.test.test_element": {
      "file": "nevow/test/test_element.py", 
      "imports": [
        "nevow.context", 
        "nevow.errors", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.page", 
        "nevow.rend", 
        "nevow.tags", 
        "nevow.testutil", 
        "twisted.internet.defer", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "nevow.test.test_errorhandler": {
      "file": "nevow/test/test_errorhandler.py", 
      "imports": [
        "nevow.appserver", 
        "nevow.context", 
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.tags", 
        "nevow.testutil", 
        "twisted.internet.defer", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_flatsax": {
      "file": "nevow/test/test_flatsax.py", 
      "imports": [
        "nevow.flat", 
        "nevow.flat.flatsax", 
        "nevow.stan", 
        "nevow.testutil"
      ]
    }, 
    "nevow.test.test_flatstan": {
      "file": "nevow/test/test_flatstan.py", 
      "imports": [
        "nevow.context", 
        "nevow.entities", 
        "nevow.flat", 
        "nevow.flat.twist", 
        "nevow.inevow", 
        "nevow.rend", 
        "nevow.stan", 
        "nevow.tags", 
        "nevow.testutil", 
        "twisted.internet.defer", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_flatten": {
      "file": "nevow/test/test_flatten.py", 
      "imports": [
        "nevow.flat.ten", 
        "nevow.tags", 
        "nevow.testutil", 
        "unicodedata"
      ]
    }, 
    "nevow.test.test_guard": {
      "file": "nevow/test/test_guard.py", 
      "imports": [
        "cStringIO", 
        "gc", 
        "nevow.appserver", 
        "nevow.context", 
        "nevow.guard", 
        "nevow.inevow", 
        "nevow.rend", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.portal", 
        "twisted.internet.address", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_howtolistings": {
      "file": "nevow/test/test_howtolistings.py", 
      "imports": [
        "chatthing.chatterbox.ChatRoom", 
        "chatthing.chatterbox.ChatterElement", 
        "echothing.echobox.EchoElement", 
        "nevow._widget_plugin", 
        "nevow.athena", 
        "nevow.plugins", 
        "nevow.testutil", 
        "sys", 
        "twisted.python.filepath", 
        "twisted.trial.unittest"
      ]
    }, 
    "nevow.test.test_i18n": {
      "file": "nevow/test/test_i18n.py", 
      "imports": [
        "cStringIO", 
        "nevow.context", 
        "nevow.flat", 
        "nevow.i18n", 
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.tags", 
        "nevow.testutil", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_javascript": {
      "file": "nevow/test/test_javascript.py", 
      "imports": [
        "nevow.testutil"
      ]
    }, 
    "nevow.test.test_json": {
      "file": "nevow/test/test_json.py", 
      "imports": [
        "nevow.athena", 
        "nevow.inevow", 
        "nevow.json", 
        "nevow.loaders", 
        "nevow.page", 
        "nevow.rend", 
        "nevow.tags", 
        "nevow.testutil", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_later": {
      "file": "nevow/test/test_later.py", 
      "imports": [
        "nevow.context", 
        "nevow.flat.twist", 
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.tags", 
        "nevow.testutil", 
        "nevow.util", 
        "twisted.internet.defer"
      ]
    }, 
    "nevow.test.test_livepage": {
      "file": "nevow/test/test_livepage.py", 
      "imports": [
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.livepage", 
        "nevow.loaders", 
        "nevow.tags", 
        "nevow.util", 
        "twisted.trial.unittest"
      ]
    }, 
    "nevow.test.test_loaders": {
      "file": "nevow/test/test_loaders.py", 
      "imports": [
        "nevow.context", 
        "nevow.flat", 
        "nevow.flat.flatstan", 
        "nevow.loaders", 
        "nevow.tags", 
        "os", 
        "twisted.trial.unittest", 
        "twisted.trial.util"
      ]
    }, 
    "nevow.test.test_newflat": {
      "file": "nevow/test/test_newflat.py", 
      "imports": [
        "nevow._flat", 
        "nevow.context", 
        "nevow.entities", 
        "nevow.flat", 
        "nevow.flat.ten", 
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.tags", 
        "nevow.testutil", 
        "nevow.url", 
        "sys", 
        "traceback", 
        "twisted.internet.defer", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_nit": {
      "file": "nevow/test/test_nit.py", 
      "imports": [
        "nevow.appserver", 
        "nevow.livetrial.runner", 
        "nevow.livetrial.testcase", 
        "nevow.scripts.nit", 
        "nevow.testutil", 
        "sys", 
        "twisted.python.failure", 
        "twisted.trial.unittest"
      ]
    }, 
    "nevow.test.test_package": {
      "file": "nevow/test/test_package.py", 
      "imports": [
        "nevow", 
        "twisted.python.versions", 
        "twisted.trial.unittest"
      ]
    }, 
    "nevow.test.test_passobj": {
      "file": "nevow/test/test_passobj.py", 
      "imports": [
        "formless", 
        "pdb", 
        "sys", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_query": {
      "file": "nevow/test/test_query.py", 
      "imports": [
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.stan", 
        "nevow.tags", 
        "nevow.testutil"
      ]
    }, 
    "nevow.test.test_rend": {
      "file": "nevow/test/test_rend.py", 
      "imports": [
        "formless", 
        "formless.annotate", 
        "formless.iformless", 
        "formless.webform", 
        "nevow.appserver", 
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.stan", 
        "nevow.tags", 
        "nevow.testutil", 
        "nevow.url", 
        "nevow.util", 
        "twisted.internet.defer", 
        "twisted.python.reflect", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "twisted.web.twcgi", 
        "zope.interface"
      ]
    }, 
    "nevow.test.test_stan": {
      "file": "nevow/test/test_stan.py", 
      "imports": [
        "nevow.stan", 
        "nevow.testutil"
      ]
    }, 
    "nevow.test.test_static": {
      "file": "nevow/test/test_static.py", 
      "imports": [
        "nevow.context", 
        "nevow.static", 
        "nevow.testutil", 
        "nevow.util", 
        "os", 
        "twisted.trial.unittest"
      ]
    }, 
    "nevow.test.test_tabbedpane": {
      "file": "nevow/test/test_tabbedpane.py", 
      "imports": [
        "nevow.taglibrary.tabbedPane", 
        "twisted.trial.unittest"
      ]
    }, 
    "nevow.test.test_tags": {
      "file": "nevow/test/test_tags.py", 
      "imports": [
        "nevow.context", 
        "nevow.flat", 
        "nevow.tags", 
        "nevow.testutil"
      ]
    }, 
    "nevow.test.test_testutil": {
      "file": "nevow/test/test_testutil.py", 
      "imports": [
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.testutil", 
        "nevow.url", 
        "sys", 
        "twisted.python.filepath", 
        "twisted.trial.unittest", 
        "twisted.web.http", 
        "unittest"
      ]
    }, 
    "nevow.test.test_url": {
      "file": "nevow/test/test_url.py", 
      "imports": [
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.tags", 
        "nevow.testutil", 
        "nevow.url", 
        "nevow.util", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "nevow.test.test_useragent": {
      "file": "nevow/test/test_useragent.py", 
      "imports": [
        "nevow.useragent", 
        "twisted.trial.unittest"
      ]
    }, 
    "nevow.test.test_utils": {
      "file": "nevow/test/test_utils.py", 
      "imports": [
        "itertools.count", 
        "nevow.util", 
        "os", 
        "sets", 
        "twisted.trial.unittest"
      ]
    }, 
    "nevow.testutil": {
      "file": "nevow/testutil.py", 
      "imports": [
        "formless.iformless", 
        "nevow.appserver", 
        "nevow.athena", 
        "nevow.context", 
        "nevow.inevow", 
        "nevow.jsutil", 
        "nevow.loaders", 
        "nevow.tags", 
        "nevow.url", 
        "os", 
        "subunit", 
        "sys", 
        "tempfile", 
        "twisted.internet.defer", 
        "twisted.protocols.basic", 
        "twisted.python.components", 
        "twisted.python.log", 
        "twisted.trial.unittest", 
        "twisted.web.http", 
        "zope.interface", 
        "signal"
      ]
    }, 
    "nevow.url": {
      "file": "nevow/url.py", 
      "imports": [
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.stan", 
        "twisted.web.util", 
        "urllib", 
        "urlparse", 
        "warnings", 
        "weakref", 
        "zope.interface"
      ]
    }, 
    "nevow.useragent": {
      "file": "nevow/useragent.py", 
      "imports": []
    }, 
    "nevow.util": {
      "file": "nevow/util.py", 
      "imports": [
        "inspect", 
        "nevow.inevow", 
        "os", 
        "pkg_resources.resource_filename", 
        "twisted.internet.defer", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.util"
      ]
    }, 
    "nevow.vhost": {
      "file": "nevow/vhost.py", 
      "imports": [
        "nevow.inevow", 
        "nevow.loaders", 
        "nevow.rend", 
        "nevow.stan", 
        "nevow.tags", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "nevow.wsgi": {
      "file": "nevow/wsgi.py", 
      "imports": [
        "cgi", 
        "math", 
        "nevow", 
        "nevow.context", 
        "nevow.flat", 
        "nevow.inevow", 
        "nevow.rend", 
        "nevow.url", 
        "nevow.util", 
        "socket", 
        "sys", 
        "time", 
        "twisted.web.http", 
        "types", 
        "urllib", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "nevow.zomnesrv": {
      "file": "nevow/zomnesrv.py", 
      "imports": [
        "nevow.wsgi", 
        "time", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.protocols.basic", 
        "twisted.python.log", 
        "warnings"
      ]
    }, 
    "new": {
      "file": "new.py", 
      "imports": [
        "types", 
        "warnings"
      ]
    }, 
    "nntplib": {
      "file": "nntplib.py", 
      "imports": [
        "netrc", 
        "os", 
        "re", 
        "socket"
      ]
    }, 
    "nturl2path": {
      "file": "nturl2path.py", 
      "imports": [
        "string", 
        "urllib"
      ]
    }, 
    "numbers": {
      "file": "numbers.py", 
      "imports": [
        "__future__", 
        "abc"
      ]
    }, 
    "odf": {
      "dir": "odf"
    }, 
    "odf.__init__": {
      "file": "odf/__init__.py", 
      "imports": []
    }, 
    "odf.anim": {
      "file": "odf/anim.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.attrconverters": {
      "file": "odf/attrconverters.py", 
      "imports": [
        "odf.namespaces", 
        "re", 
        "types"
      ]
    }, 
    "odf.chart": {
      "file": "odf/chart.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.config": {
      "file": "odf/config.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.dc": {
      "file": "odf/dc.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.dr3d": {
      "file": "odf/dr3d.py", 
      "imports": [
        "odf.draw", 
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.draw": {
      "file": "odf/draw.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.easyliststyle": {
      "file": "odf/easyliststyle.py", 
      "imports": [
        "odf.style", 
        "odf.text", 
        "re"
      ]
    }, 
    "odf.element": {
      "file": "odf/element.py", 
      "imports": [
        "odf.attrconverters", 
        "odf.grammar", 
        "odf.namespaces", 
        "xml.dom", 
        "xml.dom.minicompat"
      ]
    }, 
    "odf.elementtypes": {
      "file": "odf/elementtypes.py", 
      "imports": [
        "odf.namespaces"
      ]
    }, 
    "odf.form": {
      "file": "odf/form.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.grammar": {
      "file": "odf/grammar.py", 
      "imports": [
        "odf.namespaces"
      ]
    }, 
    "odf.load": {
      "file": "odf/load.py", 
      "imports": [
        "cStringIO", 
        "odf.element", 
        "odf.namespaces", 
        "xml.sax", 
        "xml.sax.handler", 
        "xml.sax.saxutils", 
        "xml.sax.xmlreader"
      ]
    }, 
    "odf.manifest": {
      "file": "odf/manifest.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.math": {
      "file": "odf/math.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.meta": {
      "file": "odf/meta.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.namespaces": {
      "file": "odf/namespaces.py", 
      "imports": []
    }, 
    "odf.number": {
      "file": "odf/number.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces", 
        "odf.style"
      ]
    }, 
    "odf.odf2moinmoin": {
      "file": "odf/odf2moinmoin.py", 
      "imports": [
        "odf.elementtypes", 
        "odf.namespaces", 
        "sys", 
        "xml.dom.minidom", 
        "zipfile"
      ]
    }, 
    "odf.odf2xhtml": {
      "file": "odf/odf2xhtml.py", 
      "imports": [
        "odf.namespaces", 
        "odf.opendocument", 
        "xml.dom", 
        "xml.sax.handler", 
        "xml.sax.saxutils"
      ]
    }, 
    "odf.odfmanifest": {
      "file": "odf/odfmanifest.py", 
      "imports": [
        "cStringIO", 
        "sys", 
        "xml.sax", 
        "xml.sax.handler", 
        "xml.sax.saxutils", 
        "xml.sax.xmlreader", 
        "zipfile"
      ]
    }, 
    "odf.office": {
      "file": "odf/office.py", 
      "imports": [
        "odf.draw", 
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.opendocument": {
      "file": "odf/opendocument.py", 
      "imports": [
        "cStringIO", 
        "copy", 
        "mimetypes", 
        "odf.attrconverters", 
        "odf.element", 
        "odf.load", 
        "odf.manifest", 
        "odf.meta", 
        "odf.namespaces", 
        "odf.odfmanifest", 
        "odf.office", 
        "odf.thumbnail", 
        "sys", 
        "time", 
        "xml.sax", 
        "xml.sax.handler", 
        "xml.sax.xmlreader", 
        "zipfile"
      ]
    }, 
    "odf.presentation": {
      "file": "odf/presentation.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.script": {
      "file": "odf/script.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.style": {
      "file": "odf/style.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.svg": {
      "file": "odf/svg.py", 
      "imports": [
        "odf.draw", 
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.table": {
      "file": "odf/table.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "odf.teletype": {
      "file": "odf/teletype.py", 
      "imports": [
        "odf.element", 
        "odf.opendocument", 
        "odf.text"
      ]
    }, 
    "odf.text": {
      "file": "odf/text.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces", 
        "odf.style"
      ]
    }, 
    "odf.thumbnail": {
      "file": "odf/thumbnail.py", 
      "imports": [
        "base64"
      ]
    }, 
    "odf.userfield": {
      "file": "odf/userfield.py", 
      "imports": [
        "odf.namespaces", 
        "odf.opendocument", 
        "odf.text", 
        "sys", 
        "zipfile"
      ]
    }, 
    "odf.xforms": {
      "file": "odf/xforms.py", 
      "imports": [
        "odf.element", 
        "odf.namespaces"
      ]
    }, 
    "opcode": {
      "file": "opcode.py", 
      "imports": []
    }, 
    "optparse": {
      "file": "optparse.py", 
      "imports": [
        "__builtin__", 
        "gettext", 
        "sys", 
        "os", 
        "textwrap", 
        "types"
      ]
    }, 
    "os": {
      "file": "os.py", 
      "imports": [
        "UserDict", 
        "_emx_link.link", 
        "ce", 
        "ce.*", 
        "ce._exit", 
        "copy_reg", 
        "errno", 
        "js", 
        "nt", 
        "nt.*", 
        "nt._exit", 
        "ntpath", 
        "os", 
        "os2", 
        "os2.*", 
        "os2._exit", 
        "os2emxpath", 
        "posix", 
        "posix.*", 
        "posix._exit", 
        "posixpath", 
        "riscos", 
        "riscos.*", 
        "riscos._exit", 
        "riscosenviron._Environ", 
        "riscospath", 
        "subprocess", 
        "sys", 
        "warnings"
      ]
    }, 
    "pdb": {
      "file": "pdb.py", 
      "imports": [
        "__main__", 
        "bdb", 
        "cmd", 
        "linecache", 
        "os", 
        "pdb", 
        "readline", 
        "sys", 
        "pprint", 
        "re", 
        "repr", 
        "shlex", 
        "traceback"
      ]
    }, 
    "pickle": {
      "file": "pickle.py", 
      "imports": [
        "StringIO", 
        "binascii", 
        "cStringIO", 
        "copy_reg", 
        "doctest", 
        "js", 
        "marshal", 
        "org.python.core.PyStringMap", 
        "re", 
        "struct", 
        "sys", 
        "types"
      ]
    }, 
    "pickletools": {
      "file": "pickletools.py", 
      "imports": [
        "cStringIO", 
        "doctest", 
        "pickle", 
        "re", 
        "struct"
      ]
    }, 
    "pipes": {
      "file": "pipes.py", 
      "imports": [
        "os", 
        "re", 
        "string", 
        "tempfile"
      ]
    }, 
    "pkgutil": {
      "file": "pkgutil.py", 
      "imports": [
        "imp", 
        "inspect", 
        "marshal", 
        "os", 
        "sys", 
        "types", 
        "zipimport"
      ]
    }, 
    "platform": {
      "file": "platform.py", 
      "imports": [
        "MacOS", 
        "_winreg", 
        "gestalt", 
        "gestalt.gestalt", 
        "java.lang", 
        "java.lang.System", 
        "os", 
        "subprocess", 
        "sys", 
        "vms_lib", 
        "win32api", 
        "win32api.GetVersionEx", 
        "win32api.RegCloseKey", 
        "win32api.RegOpenKeyEx", 
        "win32api.RegQueryValueEx", 
        "win32con.HKEY_LOCAL_MACHINE", 
        "win32con.VER_NT_WORKSTATION", 
        "win32con.VER_PLATFORM_WIN32_NT", 
        "win32con.VER_PLATFORM_WIN32_WINDOWS", 
        "win32pipe", 
        "plistlib", 
        "re", 
        "socket", 
        "string", 
        "struct", 
        "tempfile"
      ]
    }, 
    "plistlib": {
      "file": "plistlib.py", 
      "imports": [
        "Carbon.File.FSGetResourceForkName", 
        "Carbon.File.FSRef", 
        "Carbon.Files.fsRdPerm", 
        "Carbon.Files.fsRdWrPerm", 
        "Carbon.Res", 
        "binascii", 
        "cStringIO.StringIO", 
        "re", 
        "warnings", 
        "xml.parsers.expat", 
        "datetime"
      ]
    }, 
    "popen2": {
      "file": "popen2.py", 
      "imports": [
        "os", 
        "sys", 
        "warnings"
      ]
    }, 
    "poplib": {
      "file": "poplib.py", 
      "imports": [
        "hashlib", 
        "ssl", 
        "sys", 
        "re", 
        "socket"
      ]
    }, 
    "posixfile": {
      "file": "posixfile.py", 
      "imports": [
        "__builtin__", 
        "fcntl", 
        "os", 
        "posix", 
        "sys", 
        "struct", 
        "types", 
        "warnings"
      ]
    }, 
    "posixpath": {
      "file": "posixpath.py", 
      "imports": [
        "genericpath", 
        "os", 
        "sys", 
        "re", 
        "stat", 
        "warnings", 
        "pwd"
      ]
    }, 
    "pprint": {
      "file": "pprint.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys", 
        "time", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "profile": {
      "file": "profile.py", 
      "imports": [
        "__main__", 
        "marshal", 
        "optparse", 
        "os", 
        "sys", 
        "time", 
        "pstats", 
        "resource"
      ]
    }, 
    "pstats": {
      "file": "pstats.py", 
      "imports": [
        "cmd", 
        "functools", 
        "marshal", 
        "os", 
        "readline", 
        "sys", 
        "time", 
        "re"
      ]
    }, 
    "pty": {
      "file": "pty.py", 
      "imports": [
        "fcntl.I_PUSH", 
        "fcntl.ioctl", 
        "os", 
        "sgi", 
        "tty", 
        "select"
      ]
    }, 
    "pwd": {
      "file": "pwd.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_structseq", 
        "os"
      ]
    }, 
    "py_compile": {
      "file": "py_compile.py", 
      "imports": [
        "__builtin__", 
        "imp", 
        "marshal", 
        "os", 
        "sys", 
        "traceback"
      ]
    }, 
    "pyclbr": {
      "file": "pyclbr.py", 
      "imports": [
        "imp", 
        "operator.itemgetter", 
        "os", 
        "sys", 
        "token.DEDENT", 
        "token.NAME", 
        "token.OP", 
        "tokenize"
      ]
    }, 
    "pydoc": {
      "file": "pydoc.py", 
      "imports": [
        "BaseHTTPServer", 
        "Tkinter", 
        "__builtin__", 
        "collections", 
        "formatter", 
        "getopt", 
        "imp", 
        "inspect", 
        "locale", 
        "mimetools", 
        "nturl2path", 
        "os", 
        "pkgutil", 
        "sys", 
        "threading", 
        "pydoc_data.topics", 
        "re", 
        "repr", 
        "string", 
        "StringIO", 
        "tempfile", 
        "traceback", 
        "tty", 
        "types", 
        "warnings", 
        "webbrowser", 
        "select"
      ]
    }, 
    "pydoc_data": {
      "dir": "pydoc_data"
    }, 
    "pydoc_data.__init__": {
      "file": "pydoc_data/__init__.py", 
      "imports": []
    }, 
    "pydoc_data.topics": {
      "file": "pydoc_data/topics.py", 
      "imports": []
    }, 
    "pyexcel": {
      "dir": "pyexcel"
    }, 
    "pyexcel.__init__": {
      "file": "pyexcel/__init__.py", 
      "imports": [
        "pyexcel.book", 
        "pyexcel.cookbook", 
        "pyexcel.deprecated", 
        "pyexcel.filters", 
        "pyexcel.formatters", 
        "pyexcel.sheets", 
        "pyexcel.sources", 
        "pyexcel.utils", 
        "pyexcel.writers", 
        "pyexcel_io"
      ]
    }, 
    "pyexcel._compact": {
      "file": "pyexcel/_compact.py", 
      "imports": [
        "StringIO", 
        "collections", 
        "io", 
        "ordereddict.OrderedDict", 
        "sys", 
        "urllib", 
        "urllib2"
      ]
    }, 
    "pyexcel.book": {
      "file": "pyexcel/book.py", 
      "imports": [
        "pyexcel._compact", 
        "pyexcel.iterators", 
        "pyexcel.presentation", 
        "pyexcel.sheets", 
        "pyexcel.sources", 
        "pyexcel.utils"
      ]
    }, 
    "pyexcel.constants": {
      "file": "pyexcel/constants.py", 
      "imports": []
    }, 
    "pyexcel.cookbook": {
      "file": "pyexcel/cookbook.py", 
      "imports": [
        "os", 
        "pyexcel._compact", 
        "pyexcel.book", 
        "pyexcel.constants", 
        "pyexcel.sources", 
        "pyexcel.utils", 
        "pyexcel.writers"
      ]
    }, 
    "pyexcel.deprecated": {
      "file": "pyexcel/deprecated.py", 
      "imports": [
        "functools", 
        "pyexcel.sources"
      ]
    }, 
    "pyexcel.ext": {
      "dir": "pyexcel/ext"
    }, 
    "pyexcel.ext.__init__": {
      "file": "pyexcel/ext/__init__.py", 
      "imports": [
        "pyexcel.exthook"
      ]
    }, 
    "pyexcel.exthook": {
      "file": "pyexcel/exthook.py", 
      "imports": [
        "os", 
        "pyexcel._compact", 
        "sys"
      ]
    }, 
    "pyexcel.filters": {
      "file": "pyexcel/filters.py", 
      "imports": [
        "pyexcel._compact"
      ]
    }, 
    "pyexcel.formatters": {
      "file": "pyexcel/formatters.py", 
      "imports": [
        "datetime", 
        "pyexcel._compact", 
        "pyexcel.constants", 
        "types"
      ]
    }, 
    "pyexcel.iterators": {
      "file": "pyexcel/iterators.py", 
      "imports": []
    }, 
    "pyexcel.presentation": {
      "file": "pyexcel/presentation.py", 
      "imports": []
    }, 
    "pyexcel.sheets": {
      "dir": "pyexcel/sheets"
    }, 
    "pyexcel.sheets.__init__": {
      "file": "pyexcel/sheets/__init__.py", 
      "imports": [
        "pyexcel.sheets.filterablesheet", 
        "pyexcel.sheets.formattablesheet", 
        "pyexcel.sheets.matrix", 
        "pyexcel.sheets.nominablesheet", 
        "pyexcel.sheets.sheet"
      ]
    }, 
    "pyexcel.sheets.filterablesheet": {
      "file": "pyexcel/sheets/filterablesheet.py", 
      "imports": [
        "copy", 
        "pyexcel.filters", 
        "pyexcel.sheets.formattablesheet", 
        "pyexcel.sheets.matrix"
      ]
    }, 
    "pyexcel.sheets.formattablesheet": {
      "file": "pyexcel/sheets/formattablesheet.py", 
      "imports": [
        "pyexcel.book", 
        "pyexcel.constants", 
        "pyexcel.formatters", 
        "pyexcel.sheets.matrix", 
        "pyexcel.utils"
      ]
    }, 
    "pyexcel.sheets.matrix": {
      "file": "pyexcel/sheets/matrix.py", 
      "imports": [
        "copy", 
        "pyexcel._compact", 
        "pyexcel.constants", 
        "pyexcel.filters", 
        "pyexcel.formatters", 
        "pyexcel.iterators", 
        "pyexcel.presentation", 
        "re", 
        "texttable"
      ]
    }, 
    "pyexcel.sheets.nominablesheet": {
      "file": "pyexcel/sheets/nominablesheet.py", 
      "imports": [
        "pyexcel._compact", 
        "pyexcel.constants", 
        "pyexcel.filters", 
        "pyexcel.formatters", 
        "pyexcel.iterators", 
        "pyexcel.presentation", 
        "pyexcel.sheets.filterablesheet", 
        "pyexcel.sheets.formattablesheet", 
        "pyexcel.sheets.matrix", 
        "pyexcel.utils", 
        "texttable"
      ]
    }, 
    "pyexcel.sheets.sheet": {
      "file": "pyexcel/sheets/sheet.py", 
      "imports": [
        "pyexcel.sheets.nominablesheet", 
        "pyexcel.sources"
      ]
    }, 
    "pyexcel.sources": {
      "dir": "pyexcel/sources"
    }, 
    "pyexcel.sources.__init__": {
      "file": "pyexcel/sources/__init__.py", 
      "imports": [
        "pyexcel.book", 
        "pyexcel.constants", 
        "pyexcel.sheets", 
        "pyexcel.sources.base", 
        "pyexcel.sources.database", 
        "pyexcel.sources.file", 
        "pyexcel.sources.http", 
        "pyexcel.sources.memory", 
        "re"
      ]
    }, 
    "pyexcel.sources.base": {
      "file": "pyexcel/sources/base.py", 
      "imports": [
        "pyexcel._compact", 
        "pyexcel.constants", 
        "pyexcel_io"
      ]
    }, 
    "pyexcel.sources.database": {
      "file": "pyexcel/sources/database.py", 
      "imports": [
        "pyexcel.constants", 
        "pyexcel.sources.base", 
        "pyexcel.utils", 
        "pyexcel.writers", 
        "pyexcel_io"
      ]
    }, 
    "pyexcel.sources.file": {
      "file": "pyexcel/sources/file.py", 
      "imports": [
        "os", 
        "pyexcel.constants", 
        "pyexcel.sources.base", 
        "pyexcel.writers", 
        "pyexcel_io"
      ]
    }, 
    "pyexcel.sources.http": {
      "file": "pyexcel/sources/http.py", 
      "imports": [
        "js", 
        "pyexcel._compact", 
        "pyexcel.constants", 
        "pyexcel.sources.base", 
        "pyexcel_io"
      ]
    }, 
    "pyexcel.sources.memory": {
      "file": "pyexcel/sources/memory.py", 
      "imports": [
        "pyexcel.constants", 
        "pyexcel.sources.base", 
        "pyexcel.sources.file", 
        "pyexcel.utils", 
        "pyexcel_io"
      ]
    }, 
    "pyexcel.utils": {
      "file": "pyexcel/utils.py", 
      "imports": [
        "datetime", 
        "pyexcel._compact", 
        "pyexcel.constants", 
        "pyexcel.sheets"
      ]
    }, 
    "pyexcel.writers": {
      "file": "pyexcel/writers.py", 
      "imports": [
        "pyexcel._compact", 
        "pyexcel.sheets", 
        "pyexcel.utils", 
        "pyexcel_io"
      ]
    }, 
    "pyexcel_io": {
      "dir": "pyexcel_io"
    }, 
    "pyexcel_io.__init__": {
      "file": "pyexcel_io/__init__.py", 
      "imports": [
        "functools", 
        "pyexcel_io._compact", 
        "pyexcel_io.base", 
        "pyexcel_io.constants", 
        "pyexcel_io.csvbook", 
        "pyexcel_io.csvzipbook", 
        "pyexcel_io.djangobook", 
        "pyexcel_io.sqlbook"
      ]
    }, 
    "pyexcel_io._compact": {
      "file": "pyexcel_io/_compact.py", 
      "imports": [
        "StringIO", 
        "collections", 
        "io", 
        "ordereddict.OrderedDict", 
        "sys"
      ]
    }, 
    "pyexcel_io.base": {
      "file": "pyexcel_io/base.py", 
      "imports": [
        "abc", 
        "collections", 
        "datetime", 
        "ordereddict.OrderedDict", 
        "pyexcel_io._compact", 
        "pyexcel_io.constants", 
        "sys"
      ]
    }, 
    "pyexcel_io.constants": {
      "file": "pyexcel_io/constants.py", 
      "imports": []
    }, 
    "pyexcel_io.csvbook": {
      "file": "pyexcel_io/csvbook.py", 
      "imports": [
        "abc", 
        "codecs", 
        "csv", 
        "glob", 
        "os", 
        "pyexcel_io._compact", 
        "pyexcel_io.base", 
        "pyexcel_io.constants", 
        "re"
      ]
    }, 
    "pyexcel_io.csvzipbook": {
      "file": "pyexcel_io/csvzipbook.py", 
      "imports": [
        "csv", 
        "os", 
        "pyexcel_io._compact", 
        "pyexcel_io.base", 
        "pyexcel_io.constants", 
        "pyexcel_io.csvbook", 
        "zipfile"
      ]
    }, 
    "pyexcel_io.djangobook": {
      "file": "pyexcel_io/djangobook.py", 
      "imports": [
        "pyexcel_io._compact", 
        "pyexcel_io.base", 
        "pyexcel_io.constants"
      ]
    }, 
    "pyexcel_io.sqlbook": {
      "file": "pyexcel_io/sqlbook.py", 
      "imports": [
        "pyexcel_io._compact", 
        "pyexcel_io.base", 
        "pyexcel_io.constants"
      ]
    }, 
    "pyexcel_ods": {
      "dir": "pyexcel_ods"
    }, 
    "pyexcel_ods.__init__": {
      "file": "pyexcel_ods/__init__.py", 
      "imports": [
        "datetime", 
        "pyexcel_io", 
        "odf.text", 
        "odf.opendocument", 
        "odf.table", 
        "odf.namespaces"
      ]
    }, 
    "pyexcel_xls": {
      "dir": "pyexcel_xls"
    }, 
    "pyexcel_xls.__init__": {
      "file": "pyexcel_xls/__init__.py", 
      "imports": [
        "datetime", 
        "pyexcel_io", 
        "sys", 
        "xlrd", 
        "xlwt.Workbook", 
        "xlwt"
      ]
    }, 
    "pyrepl": {
      "dir": "pyrepl"
    }, 
    "pyrepl.__init__": {
      "file": "pyrepl/__init__.py", 
      "imports": []
    }, 
    "pyrepl.cmdrepl": {
      "file": "pyrepl/cmdrepl.py", 
      "imports": [
        "__future__", 
        "cmd", 
        "pyrepl.completer", 
        "pyrepl.completing_reader", 
        "pyrepl.reader"
      ]
    }, 
    "pyrepl.commands": {
      "file": "pyrepl/commands.py", 
      "imports": [
        "os", 
        "pyrepl.input", 
        "sys", 
        "signal"
      ]
    }, 
    "pyrepl.completer": {
      "file": "pyrepl/completer.py", 
      "imports": [
        "__builtin__", 
        "keyword", 
        "re"
      ]
    }, 
    "pyrepl.completing_reader": {
      "file": "pyrepl/completing_reader.py", 
      "imports": [
        "pyrepl.commands", 
        "pyrepl.reader", 
        "re"
      ]
    }, 
    "pyrepl.console": {
      "file": "pyrepl/console.py", 
      "imports": []
    }, 
    "pyrepl.copy_code": {
      "file": "pyrepl/copy_code.py", 
      "imports": [
        "new"
      ]
    }, 
    "pyrepl.curses": {
      "file": "pyrepl/curses.py", 
      "imports": [
        "_curses", 
        "_minimal_curses", 
        "pyrepl.curses", 
        "sys"
      ]
    }, 
    "pyrepl.fancy_termios": {
      "file": "pyrepl/fancy_termios.py", 
      "imports": [
        "termios"
      ]
    }, 
    "pyrepl.historical_reader": {
      "file": "pyrepl/historical_reader.py", 
      "imports": [
        "pyrepl.commands", 
        "pyrepl.input", 
        "pyrepl.reader", 
        "pyrepl.unix_console"
      ]
    }, 
    "pyrepl.input": {
      "file": "pyrepl/input.py", 
      "imports": [
        "pyrepl.keymap", 
        "pyrepl.unicodedata_"
      ]
    }, 
    "pyrepl.keymap": {
      "file": "pyrepl/keymap.py", 
      "imports": []
    }, 
    "pyrepl.keymaps": {
      "file": "pyrepl/keymaps.py", 
      "imports": []
    }, 
    "pyrepl.module_lister": {
      "file": "pyrepl/module_lister.py", 
      "imports": [
        "imp", 
        "os", 
        "sys"
      ]
    }, 
    "pyrepl.pygame_console": {
      "file": "pyrepl/pygame_console.py", 
      "imports": [
        "pygame", 
        "pygame.locals.*", 
        "pyrepl.console", 
        "pyrepl.pygame_keymap", 
        "pyrepl.reader", 
        "types"
      ]
    }, 
    "pyrepl.pygame_keymap": {
      "file": "pyrepl/pygame_keymap.py", 
      "imports": [
        "pygame.locals.*"
      ]
    }, 
    "pyrepl.python_reader": {
      "file": "pyrepl/python_reader.py", 
      "imports": [
        "_tkinter", 
        "atexit", 
        "cPickle", 
        "cocoasupport.CocoaInteracter", 
        "code", 
        "imp", 
        "locale", 
        "new", 
        "os", 
        "pickle", 
        "pyrepl.commands", 
        "pyrepl.completer", 
        "pyrepl.completing_reader", 
        "pyrepl.copy_code", 
        "pyrepl.historical_reader", 
        "pyrepl.module_lister", 
        "pyrepl.pygame_console", 
        "pyrepl.reader", 
        "pyrepl.unix_console", 
        "re", 
        "sys", 
        "traceback", 
        "warnings", 
        "twisted.internet.abstract", 
        "twisted.internet.reactor", 
        "signal"
      ]
    }, 
    "pyrepl.reader": {
      "file": "pyrepl/reader.py", 
      "imports": [
        "_pyrepl_utils.disp_str", 
        "_pyrepl_utils.init_unctrl_map", 
        "pyrepl.commands", 
        "pyrepl.input", 
        "pyrepl.unicodedata_", 
        "pyrepl.unix_console", 
        "re", 
        "types"
      ]
    }, 
    "pyrepl.readline": {
      "file": "pyrepl/readline.py", 
      "imports": [
        "__builtin__", 
        "os", 
        "pyrepl.commands", 
        "pyrepl.completing_reader", 
        "pyrepl.historical_reader", 
        "pyrepl.unix_console", 
        "sys", 
        "warnings"
      ]
    }, 
    "pyrepl.simple_interact": {
      "file": "pyrepl/simple_interact.py", 
      "imports": [
        "__main__", 
        "code", 
        "pyrepl.readline", 
        "sys"
      ]
    }, 
    "pyrepl.unicodedata_": {
      "file": "pyrepl/unicodedata_.py", 
      "imports": [
        "unicodedata.*"
      ]
    }, 
    "pyrepl.unix_console": {
      "file": "pyrepl/unix_console.py", 
      "imports": [
        "errno", 
        "fcntl.ioctl", 
        "os", 
        "pyrepl.console", 
        "pyrepl.curses", 
        "pyrepl.fancy_termios", 
        "pyrepl.unix_eventqueue", 
        "re", 
        "struct", 
        "sys", 
        "termios", 
        "time", 
        "signal", 
        "select"
      ]
    }, 
    "pyrepl.unix_eventqueue": {
      "file": "pyrepl/unix_eventqueue.py", 
      "imports": [
        "os", 
        "pyrepl.console", 
        "pyrepl.curses", 
        "pyrepl.keymap", 
        "termios.VERASE", 
        "termios.tcgetattr"
      ]
    }, 
    "quopri": {
      "file": "quopri.py", 
      "imports": [
        "binascii.a2b_qp", 
        "binascii.b2a_qp", 
        "cStringIO.StringIO", 
        "getopt", 
        "sys"
      ]
    }, 
    "random": {
      "file": "random.py", 
      "imports": [
        "__future__", 
        "_random", 
        "binascii.hexlify", 
        "hashlib", 
        "math.acos", 
        "math.ceil", 
        "math.cos", 
        "math.e", 
        "math.exp", 
        "math.log", 
        "math.pi", 
        "math.sin", 
        "math.sqrt", 
        "os", 
        "time", 
        "warnings"
      ]
    }, 
    "re": {
      "file": "re.py", 
      "imports": [
        "_locale", 
        "copy_reg", 
        "sys", 
        "sre_compile", 
        "sre_constants", 
        "sre_parse"
      ]
    }, 
    "repr": {
      "file": "repr.py", 
      "imports": [
        "__builtin__", 
        "itertools.islice"
      ]
    }, 
    "resource": {
      "file": "resource.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_structseq", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.byref", 
        "ctypes.c_int", 
        "ctypes.c_long", 
        "ctypes_config_cache._resource_cache", 
        "ctypes_support.get_errno", 
        "ctypes_support.standard_c_lib", 
        "errno.EINVAL", 
        "errno.EPERM", 
        "os", 
        "sys"
      ]
    }, 
    "rexec": {
      "file": "rexec.py", 
      "imports": [
        "__builtin__", 
        "code", 
        "getopt", 
        "ihooks", 
        "imp", 
        "os", 
        "readline", 
        "sys", 
        "traceback", 
        "warnings"
      ]
    }, 
    "rfc822": {
      "file": "rfc822.py", 
      "imports": [
        "os", 
        "sys", 
        "time", 
        "warnings"
      ]
    }, 
    "rlcompleter": {
      "file": "rlcompleter.py", 
      "imports": [
        "__builtin__", 
        "__main__", 
        "keyword", 
        "re", 
        "readline"
      ]
    }, 
    "robotparser": {
      "file": "robotparser.py", 
      "imports": [
        "time", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "runpy": {
      "file": "runpy.py", 
      "imports": [
        "imp", 
        "imp.get_loader", 
        "pkgutil", 
        "sys"
      ]
    }, 
    "sched": {
      "file": "sched.py", 
      "imports": [
        "collections", 
        "heapq"
      ]
    }, 
    "scheme": {
      "dir": "scheme"
    }, 
    "scheme.Globals": {
      "file": "scheme/Globals.py", 
      "imports": [
        "scheme.environment"
      ]
    }, 
    "scheme.IF": {
      "file": "scheme/IF.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.macro", 
        "scheme.symbol", 
        "zope.interface"
      ]
    }, 
    "scheme.Lambda": {
      "file": "scheme/Lambda.py", 
      "imports": [
        "scheme", 
        "scheme.macro", 
        "scheme.procedure", 
        "scheme.processer", 
        "time", 
        "zope.interface"
      ]
    }, 
    "scheme.PatternMatcher": {
      "file": "scheme/PatternMatcher.py", 
      "imports": [
        "__future__", 
        "scheme.debug", 
        "scheme.environment", 
        "traceback"
      ]
    }, 
    "scheme.__init__": {
      "file": "scheme/__init__.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.IF", 
        "scheme.Lambda", 
        "scheme.begin", 
        "scheme.builtins", 
        "scheme.callcc", 
        "scheme.case", 
        "scheme.cond", 
        "scheme.define", 
        "scheme.define_syntax", 
        "scheme.environment", 
        "scheme.eval", 
        "scheme.macro", 
        "scheme.parser", 
        "scheme.procedure", 
        "scheme.processer", 
        "scheme.quasiquote", 
        "scheme.quasisyntax", 
        "scheme.quote", 
        "scheme.symbol", 
        "scheme.syntax_case", 
        "scheme.syntax_rules", 
        "scheme.token", 
        "scheme.unquote", 
        "scheme.unquote_splicing", 
        "scheme.utils"
      ]
    }, 
    "scheme.begin": {
      "file": "scheme/begin.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.macro", 
        "zope.interface"
      ]
    }, 
    "scheme.builtins": {
      "file": "scheme/builtins.py", 
      "imports": [
        "__future__", 
        "cStringIO", 
        "cmath", 
        "math", 
        "operator", 
        "os", 
        "scheme", 
        "scheme.Globals", 
        "scheme.debug", 
        "scheme.eval", 
        "scheme.macro", 
        "scheme.procedure", 
        "scheme.repl", 
        "site", 
        "sys", 
        "zope.interface"
      ]
    }, 
    "scheme.callcc": {
      "file": "scheme/callcc.py", 
      "imports": [
        "scheme.macro", 
        "scheme.procedure", 
        "scheme.processer", 
        "scheme.symbol", 
        "scheme.utils", 
        "zope.interface"
      ]
    }, 
    "scheme.case": {
      "file": "scheme/case.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.macro", 
        "scheme.symbol", 
        "zope.interface"
      ]
    }, 
    "scheme.cond": {
      "file": "scheme/cond.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.macro", 
        "zope.interface"
      ]
    }, 
    "scheme.debug": {
      "file": "scheme/debug.py", 
      "imports": []
    }, 
    "scheme.define": {
      "file": "scheme/define.py", 
      "imports": [
        "scheme.macro", 
        "scheme.procedure", 
        "scheme.processer", 
        "scheme.symbol", 
        "zope.interface"
      ]
    }, 
    "scheme.define_syntax": {
      "file": "scheme/define_syntax.py", 
      "imports": [
        "__future__", 
        "scheme.Globals", 
        "scheme.macro", 
        "scheme.symbol", 
        "scheme.syntax", 
        "zope.interface"
      ]
    }, 
    "scheme.environment": {
      "file": "scheme/environment.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.PatternMatcher"
      ]
    }, 
    "scheme.eval": {
      "file": "scheme/eval.py", 
      "imports": [
        "Queue", 
        "cStringIO", 
        "scheme.parser", 
        "scheme.processer", 
        "scheme.utils"
      ]
    }, 
    "scheme.interfaces": {
      "file": "scheme/interfaces.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "scheme.let": {
      "file": "scheme/let.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.debug", 
        "scheme.environment", 
        "scheme.macro", 
        "scheme.procedure", 
        "zope.interface"
      ]
    }, 
    "scheme.let_aster": {
      "file": "scheme/let_aster.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.macro", 
        "zope.interface"
      ]
    }, 
    "scheme.macro": {
      "file": "scheme/macro.py", 
      "imports": [
        "scheme", 
        "scheme.debug", 
        "scheme.environment", 
        "scheme.procedure", 
        "scheme.symbol", 
        "scheme.utils", 
        "zope.interface"
      ]
    }, 
    "scheme.parser": {
      "file": "scheme/parser.py", 
      "imports": [
        "__future__", 
        "cStringIO", 
        "re", 
        "scheme.symbol", 
        "scheme.token"
      ]
    }, 
    "scheme.procedure": {
      "file": "scheme/procedure.py", 
      "imports": [
        "scheme.environment", 
        "scheme.symbol", 
        "scheme.utils", 
        "zope.interface"
      ]
    }, 
    "scheme.processer": {
      "file": "scheme/processer.py", 
      "imports": [
        "Queue", 
        "scheme.Globals", 
        "scheme.debug", 
        "scheme.environment", 
        "scheme.macro", 
        "scheme.procedure", 
        "scheme.symbol", 
        "scheme.syntax_rules", 
        "scheme.utils", 
        "traceback", 
        "zope.interface"
      ]
    }, 
    "scheme.quasiquote": {
      "file": "scheme/quasiquote.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.macro", 
        "scheme.utils", 
        "zope.interface"
      ]
    }, 
    "scheme.quasisyntax": {
      "file": "scheme/quasisyntax.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.macro", 
        "scheme.syntax", 
        "scheme.utils", 
        "zope.interface"
      ]
    }, 
    "scheme.quote": {
      "file": "scheme/quote.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.macro", 
        "scheme.utils", 
        "zope.interface"
      ]
    }, 
    "scheme.repl": {
      "file": "scheme/repl.py", 
      "imports": [
        "Queue", 
        "scheme.debug", 
        "scheme.parser", 
        "scheme.processer", 
        "sys", 
        "time", 
        "traceback"
      ]
    }, 
    "scheme.symbol": {
      "file": "scheme/symbol.py", 
      "imports": [
        "re", 
        "scheme.Globals", 
        "scheme.debug"
      ]
    }, 
    "scheme.syntax": {
      "file": "scheme/syntax.py", 
      "imports": [
        "Queue", 
        "__future__", 
        "scheme.Globals", 
        "scheme.macro", 
        "scheme.processer", 
        "scheme.symbol", 
        "zope.interface"
      ]
    }, 
    "scheme.syntax_case": {
      "file": "scheme/syntax_case.py", 
      "imports": [
        "Queue", 
        "__future__", 
        "scheme.Globals", 
        "scheme.PatternMatcher", 
        "scheme.environment", 
        "scheme.macro", 
        "scheme.procedure", 
        "scheme.symbol", 
        "scheme.syntax", 
        "scheme.utils", 
        "zope.interface"
      ]
    }, 
    "scheme.syntax_rules": {
      "file": "scheme/syntax_rules.py", 
      "imports": [
        "__future__", 
        "scheme.Globals", 
        "scheme.PatternMatcher", 
        "scheme.environment", 
        "scheme.macro", 
        "scheme.symbol", 
        "scheme.syntax", 
        "scheme.utils", 
        "zope.interface"
      ]
    }, 
    "scheme.token": {
      "file": "scheme/token.py", 
      "imports": [
        "scheme.symbol"
      ]
    }, 
    "scheme.unquote": {
      "file": "scheme/unquote.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.begin"
      ]
    }, 
    "scheme.unquote_splicing": {
      "file": "scheme/unquote_splicing.py", 
      "imports": [
        "scheme.Globals", 
        "scheme.begin"
      ]
    }, 
    "scheme.utils": {
      "file": "scheme/utils.py", 
      "imports": [
        "hashlib", 
        "scheme.Globals", 
        "scheme.environment", 
        "scheme.macro", 
        "scheme.symbol", 
        "scheme.syntax", 
        "scheme.unquote", 
        "scheme.unquote_splicing", 
        "time"
      ]
    }, 
    "select": {
      "file": "select.py", 
      "imports": [
        "_socket", 
        "os", 
        "time"
      ]
    }, 
    "sets": {
      "file": "sets.py", 
      "imports": [
        "copy", 
        "itertools.ifilter", 
        "itertools.ifilterfalse", 
        "warnings"
      ]
    }, 
    "setup": {
      "file": "setup.py", 
      "imports": [
        "ez_setup.use_setuptools", 
        "setuptools.find_packages", 
        "setuptools.setup", 
        "sys"
      ]
    }, 
    "sgmllib": {
      "file": "sgmllib.py", 
      "imports": [
        "markupbase", 
        "re", 
        "sys", 
        "warnings"
      ]
    }, 
    "sha": {
      "file": "sha.py", 
      "imports": [
        "hashlib", 
        "warnings"
      ]
    }, 
    "shelve": {
      "file": "shelve.py", 
      "imports": [
        "anydbm", 
        "cStringIO.StringIO", 
        "pickle", 
        "StringIO", 
        "UserDict", 
        "cPickle"
      ]
    }, 
    "shlex": {
      "file": "shlex.py", 
      "imports": [
        "cStringIO.StringIO", 
        "collections", 
        "os", 
        "sys", 
        "StringIO"
      ]
    }, 
    "shutil": {
      "file": "shutil.py", 
      "imports": [
        "collections", 
        "distutils.errors", 
        "distutils.spawn", 
        "errno", 
        "fnmatch", 
        "os", 
        "sys", 
        "stat", 
        "tarfile", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "signal": {
      "file": "signal.py", 
      "imports": []
    }, 
    "site": {
      "file": "site.py", 
      "imports": [
        "__builtin__", 
        "codecs", 
        "distutils.sysconfig", 
        "encodings", 
        "exceptions", 
        "locale", 
        "os", 
        "pydoc", 
        "sitecustomize", 
        "sys", 
        "usercustomize", 
        "sysconfig", 
        "textwrap", 
        "traceback", 
        "zipimport"
      ]
    }, 
    "smtpd": {
      "file": "smtpd.py", 
      "imports": [
        "Mailman.MailList", 
        "Mailman.Message", 
        "Mailman.Utils", 
        "__main__", 
        "asynchat", 
        "asyncore", 
        "cStringIO.StringIO", 
        "errno", 
        "getopt", 
        "os", 
        "sys", 
        "time", 
        "smtplib", 
        "socket", 
        "pwd"
      ]
    }, 
    "smtplib": {
      "file": "smtplib.py", 
      "imports": [
        "base64", 
        "email.base64mime", 
        "email.utils", 
        "hmac", 
        "re", 
        "ssl", 
        "sys", 
        "sys.stderr", 
        "socket"
      ]
    }, 
    "sndhdr": {
      "file": "sndhdr.py", 
      "imports": [
        "aifc", 
        "glob", 
        "os", 
        "sys"
      ]
    }, 
    "socket": {
      "file": "socket.py", 
      "imports": [
        "_socket", 
        "_socket.*", 
        "_ssl", 
        "_ssl.RAND_add", 
        "_ssl.RAND_egd", 
        "_ssl.RAND_status", 
        "_ssl.SSLError", 
        "_ssl.SSL_ERROR_EOF", 
        "_ssl.SSL_ERROR_INVALID_ERROR_CODE", 
        "_ssl.SSL_ERROR_SSL", 
        "_ssl.SSL_ERROR_SYSCALL", 
        "_ssl.SSL_ERROR_WANT_CONNECT", 
        "_ssl.SSL_ERROR_WANT_READ", 
        "_ssl.SSL_ERROR_WANT_WRITE", 
        "_ssl.SSL_ERROR_WANT_X509_LOOKUP", 
        "_ssl.SSL_ERROR_ZERO_RETURN", 
        "cStringIO.StringIO", 
        "errno", 
        "os", 
        "ssl", 
        "sys", 
        "StringIO", 
        "warnings"
      ]
    }, 
    "sre": {
      "file": "sre.py", 
      "imports": [
        "re", 
        "warnings"
      ]
    }, 
    "sre_compile": {
      "file": "sre_compile.py", 
      "imports": [
        "_sre", 
        "array", 
        "sys", 
        "sre_constants", 
        "sre_parse"
      ]
    }, 
    "sre_constants": {
      "file": "sre_constants.py", 
      "imports": [
        "_sre", 
        "_sre.MAXREPEAT"
      ]
    }, 
    "sre_parse": {
      "file": "sre_parse.py", 
      "imports": [
        "__pypy__.newdict", 
        "sre_constants", 
        "sys"
      ]
    }, 
    "stackless": {
      "file": "stackless.py", 
      "imports": [
        "_continuation", 
        "collections", 
        "operator", 
        "threading.local"
      ]
    }, 
    "stat": {
      "file": "stat.py", 
      "imports": []
    }, 
    "statvfs": {
      "file": "statvfs.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "string": {
      "file": "string.py", 
      "imports": [
        "re", 
        "strop.lowercase", 
        "strop.maketrans", 
        "strop.uppercase", 
        "strop.whitespace"
      ]
    }, 
    "stringold": {
      "file": "stringold.py", 
      "imports": [
        "stringold", 
        "strop.lowercase", 
        "strop.maketrans", 
        "strop.uppercase", 
        "strop.whitespace", 
        "warnings"
      ]
    }, 
    "stringprep": {
      "file": "stringprep.py", 
      "imports": [
        "unicodedata.ucd_3_2_0"
      ]
    }, 
    "struct": {
      "file": "struct.py", 
      "imports": [
        "_struct.*", 
        "_struct.__doc__", 
        "_struct._clearcache"
      ]
    }, 
    "subprocess": {
      "file": "subprocess.py", 
      "imports": []
    }, 
    "symbol": {
      "file": "symbol.py", 
      "imports": [
        "sys", 
        "token"
      ]
    }, 
    "sysconfig": {
      "file": "sysconfig.py", 
      "imports": [
        "_osx_support", 
        "imp", 
        "os", 
        "pprint", 
        "re", 
        "sys"
      ]
    }, 
    "syslog": {
      "file": "syslog.py", 
      "imports": [
        "__pypy__.builtinify", 
        "_syslog_cffi.ffi", 
        "_syslog_cffi.lib", 
        "sys"
      ]
    }, 
    "tabnanny": {
      "file": "tabnanny.py", 
      "imports": [
        "getopt", 
        "os", 
        "sys", 
        "tokenize"
      ]
    }, 
    "tarfile": {
      "file": "tarfile.py", 
      "imports": [
        "StringIO", 
        "bz2", 
        "cStringIO.StringIO", 
        "calendar", 
        "copy", 
        "errno", 
        "gzip", 
        "operator", 
        "os", 
        "re", 
        "shutil", 
        "stat", 
        "struct", 
        "sys", 
        "time", 
        "zlib", 
        "warnings", 
        "grp", 
        "pwd"
      ]
    }, 
    "telnetlib": {
      "file": "telnetlib.py", 
      "imports": [
        "errno", 
        "re", 
        "socket", 
        "sys", 
        "thread", 
        "time.time", 
        "select"
      ]
    }, 
    "tempfile": {
      "file": "tempfile.py", 
      "imports": [
        "StringIO", 
        "cStringIO", 
        "dummy_thread", 
        "errno", 
        "fcntl", 
        "io", 
        "os", 
        "random", 
        "thread"
      ]
    }, 
    "test": {
      "dir": "test"
    }, 
    "test.__init__": {
      "file": "test/__init__.py", 
      "imports": []
    }, 
    "test._mock_backport": {
      "file": "test/_mock_backport.py", 
      "imports": [
        "__builtin__", 
        "_io", 
        "functools", 
        "inspect", 
        "java", 
        "pprint", 
        "sys", 
        "types"
      ]
    }, 
    "test.audiotests": {
      "file": "test/audiotests.py", 
      "imports": [
        "array", 
        "base64", 
        "io", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.autotest": {
      "file": "test/autotest.py", 
      "imports": [
        "test.regrtest"
      ]
    }, 
    "test.bad_coding": {
      "file": "test/bad_coding.py", 
      "imports": []
    }, 
    "test.bad_coding2": {
      "file": "test/bad_coding2.py", 
      "imports": []
    }, 
    "test.bad_coding3": {
      "file": "test/bad_coding3.py", 
      "imports": []
    }, 
    "test.badsyntax_future3": {
      "file": "test/badsyntax_future3.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future4": {
      "file": "test/badsyntax_future4.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future5": {
      "file": "test/badsyntax_future5.py", 
      "imports": [
        "__future__", 
        "foo"
      ]
    }, 
    "test.badsyntax_future6": {
      "file": "test/badsyntax_future6.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future7": {
      "file": "test/badsyntax_future7.py", 
      "imports": [
        "__future__", 
        "string"
      ]
    }, 
    "test.badsyntax_future8": {
      "file": "test/badsyntax_future8.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_future9": {
      "file": "test/badsyntax_future9.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.badsyntax_nocaret": {
      "file": "test/badsyntax_nocaret.py", 
      "imports": []
    }, 
    "test.buffer_tests": {
      "file": "test/buffer_tests.py", 
      "imports": [
        "struct", 
        "sys"
      ]
    }, 
    "test.curses_tests": {
      "file": "test/curses_tests.py", 
      "imports": [
        "curses", 
        "curses.textpad"
      ]
    }, 
    "test.doctest_aliases": {
      "file": "test/doctest_aliases.py", 
      "imports": []
    }, 
    "test.double_const": {
      "file": "test/double_const.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.fork_wait": {
      "file": "test/fork_wait.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.gdb_sample": {
      "file": "test/gdb_sample.py", 
      "imports": []
    }, 
    "test.infinite_reload": {
      "file": "test/infinite_reload.py", 
      "imports": [
        "imp", 
        "test.infinite_reload"
      ]
    }, 
    "test.inspect_fodder": {
      "file": "test/inspect_fodder.py", 
      "imports": [
        "inspect", 
        "sys"
      ]
    }, 
    "test.inspect_fodder2": {
      "file": "test/inspect_fodder2.py", 
      "imports": []
    }, 
    "test.leakers": {
      "dir": "test/leakers"
    }, 
    "test.leakers.__init__": {
      "file": "test/leakers/__init__.py", 
      "imports": []
    }, 
    "test.leakers.test_ctypes": {
      "file": "test/leakers/test_ctypes.py", 
      "imports": [
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_int", 
        "gc"
      ]
    }, 
    "test.leakers.test_dictself": {
      "file": "test/leakers/test_dictself.py", 
      "imports": [
        "gc"
      ]
    }, 
    "test.leakers.test_gestalt": {
      "file": "test/leakers/test_gestalt.py", 
      "imports": [
        "MacOS", 
        "gestalt.gestalt", 
        "sys"
      ]
    }, 
    "test.leakers.test_selftype": {
      "file": "test/leakers/test_selftype.py", 
      "imports": [
        "gc"
      ]
    }, 
    "test.list_tests": {
      "file": "test/list_tests.py", 
      "imports": [
        "os", 
        "sys", 
        "test.seq_tests", 
        "test.test_support"
      ]
    }, 
    "test.lock_tests": {
      "file": "test/lock_tests.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "thread.get_ident", 
        "thread.start_new_thread", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.make_ssl_certs": {
      "file": "test/make_ssl_certs.py", 
      "imports": [
        "os", 
        "shutil", 
        "subprocess.*", 
        "sys", 
        "tempfile"
      ]
    }, 
    "test.mapping_tests": {
      "file": "test/mapping_tests.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.mp_fork_bomb": {
      "file": "test/mp_fork_bomb.py", 
      "imports": [
        "multiprocessing"
      ]
    }, 
    "test.outstanding_bugs": {
      "file": "test/outstanding_bugs.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.pickletester": {
      "file": "test/pickletester.py", 
      "imports": [
        "StringIO", 
        "__main__", 
        "cStringIO", 
        "copy_reg", 
        "locale", 
        "os", 
        "pickle", 
        "pickletools", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.profilee": {
      "file": "test/profilee.py", 
      "imports": [
        "sys"
      ]
    }, 
    "test.pyclbr_input": {
      "file": "test/pyclbr_input.py", 
      "imports": []
    }, 
    "test.pydoc_mod": {
      "file": "test/pydoc_mod.py", 
      "imports": []
    }, 
    "test.pydocfodder": {
      "file": "test/pydocfodder.py", 
      "imports": [
        "types"
      ]
    }, 
    "test.pystone": {
      "file": "test/pystone.py", 
      "imports": [
        "sys", 
        "time.time"
      ]
    }, 
    "test.re_tests": {
      "file": "test/re_tests.py", 
      "imports": []
    }, 
    "test.regrtest": {
      "file": "test/regrtest.py", 
      "imports": [
        "Queue", 
        "StringIO", 
        "_abcoll", 
        "_pyio", 
        "_strptime", 
        "copy_reg", 
        "ctypes", 
        "distutils.dir_util", 
        "doctest", 
        "filecmp", 
        "gc", 
        "getopt", 
        "imp", 
        "json", 
        "linecache", 
        "mimetypes", 
        "os", 
        "platform", 
        "random", 
        "re", 
        "shutil", 
        "stat", 
        "struct", 
        "subprocess.PIPE", 
        "subprocess.Popen", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "test.test_timeout", 
        "threading.Thread", 
        "time", 
        "textwrap", 
        "trace", 
        "traceback", 
        "unittest", 
        "urllib", 
        "urllib2", 
        "urlparse", 
        "warnings", 
        "resource", 
        "zipimport"
      ]
    }, 
    "test.relimport": {
      "file": "test/relimport.py", 
      "imports": [
        "test.test_import"
      ]
    }, 
    "test.reperf": {
      "file": "test/reperf.py", 
      "imports": [
        "re", 
        "time"
      ]
    }, 
    "test.sample_doctest": {
      "file": "test/sample_doctest.py", 
      "imports": [
        "doctest"
      ]
    }, 
    "test.sample_doctest_no_docstrings": {
      "file": "test/sample_doctest_no_docstrings.py", 
      "imports": []
    }, 
    "test.sample_doctest_no_doctests": {
      "file": "test/sample_doctest_no_doctests.py", 
      "imports": []
    }, 
    "test.script_helper": {
      "file": "test/script_helper.py", 
      "imports": [
        "contextlib", 
        "os", 
        "py_compile", 
        "re", 
        "shutil", 
        "subprocess", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "zipfile"
      ]
    }, 
    "test.seq_tests": {
      "file": "test/seq_tests.py", 
      "imports": [
        "itertools.chain", 
        "itertools.imap", 
        "sys", 
        "unittest"
      ]
    }, 
    "test.sortperf": {
      "file": "test/sortperf.py", 
      "imports": [
        "marshal", 
        "os", 
        "random", 
        "sys", 
        "tempfile", 
        "time"
      ]
    }, 
    "test.ssl_servers": {
      "file": "test/ssl_servers.py", 
      "imports": [
        "BaseHTTPServer", 
        "SimpleHTTPServer", 
        "argparse", 
        "os", 
        "pprint", 
        "ssl", 
        "sys", 
        "test.test_support", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "test.string_tests": {
      "file": "test/string_tests.py", 
      "imports": [
        "string", 
        "struct", 
        "sys", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "UserList", 
        "_testcapi"
      ]
    }, 
    "test.symlink_support": {
      "file": "test/symlink_support.py", 
      "imports": [
        "ctypes.wintypes", 
        "os", 
        "platform", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_MimeWriter": {
      "file": "test/test_MimeWriter.py", 
      "imports": [
        "MimeWriter", 
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_SimpleHTTPServer": {
      "file": "test/test_SimpleHTTPServer.py", 
      "imports": [
        "SimpleHTTPServer", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_StringIO": {
      "file": "test/test_StringIO.py", 
      "imports": [
        "StringIO", 
        "array", 
        "cStringIO", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test___all__": {
      "file": "test/test___all__.py", 
      "imports": [
        "__future__", 
        "_socket", 
        "locale", 
        "os", 
        "rlcompleter", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test___future__": {
      "file": "test/test___future__.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test__locale": {
      "file": "test/test__locale.py", 
      "imports": [
        "_locale.Error", 
        "_locale.LC_NUMERIC", 
        "_locale.RADIXCHAR", 
        "_locale.THOUSEP", 
        "_locale.localeconv", 
        "_locale.nl_langinfo", 
        "_locale.setlocale", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test__osx_support": {
      "file": "test/test__osx_support.py", 
      "imports": [
        "_osx_support", 
        "os", 
        "platform", 
        "shutil", 
        "stat", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_abc": {
      "file": "test/test_abc.py", 
      "imports": [
        "abc", 
        "inspect", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_abstract_numbers": {
      "file": "test/test_abstract_numbers.py", 
      "imports": [
        "math", 
        "numbers", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_aepack": {
      "file": "test/test_aepack.py", 
      "imports": [
        "Carbon.File", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_aifc": {
      "file": "test/test_aifc.py", 
      "imports": [
        "aifc", 
        "io", 
        "os", 
        "struct", 
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_al": {
      "file": "test/test_al.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_anydbm": {
      "file": "test/test_anydbm.py", 
      "imports": [
        "glob", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_applesingle": {
      "file": "test/test_applesingle.py", 
      "imports": [
        "applesingle", 
        "os", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_argparse": {
      "file": "test/test_argparse.py", 
      "imports": [
        "StringIO", 
        "argparse", 
        "codecs", 
        "gc", 
        "inspect", 
        "os", 
        "shutil", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_array": {
      "file": "test/test_array.py", 
      "imports": [
        "array", 
        "cStringIO", 
        "copy", 
        "gc", 
        "sys", 
        "sys.maxsize", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_ascii_formatd": {
      "file": "test/test_ascii_formatd.py", 
      "imports": [
        "ctypes.byref", 
        "ctypes.c_double", 
        "ctypes.create_string_buffer", 
        "ctypes.pythonapi", 
        "ctypes.sizeof", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ast": {
      "file": "test/test_ast.py", 
      "imports": [
        "ast", 
        "itertools", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_asynchat": {
      "file": "test/test_asynchat.py", 
      "imports": [
        "asynchat", 
        "asyncore", 
        "errno", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_asyncore": {
      "file": "test/test_asyncore.py", 
      "imports": [
        "StringIO", 
        "asyncore", 
        "errno", 
        "os", 
        "socket", 
        "struct", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "warnings", 
        "select"
      ]
    }, 
    "test.test_atexit": {
      "file": "test/test_atexit.py", 
      "imports": [
        "StringIO", 
        "atexit", 
        "imp.reload", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_audioop": {
      "file": "test/test_audioop.py", 
      "imports": [
        "audioop", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_augassign": {
      "file": "test/test_augassign.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_base64": {
      "file": "test/test_base64.py", 
      "imports": [
        "base64", 
        "cStringIO.StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bastion": {
      "file": "test/test_bastion.py", 
      "imports": []
    }, 
    "test.test_bigaddrspace": {
      "file": "test/test_bigaddrspace.py", 
      "imports": [
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bigmem": {
      "file": "test/test_bigmem.py", 
      "imports": [
        "operator", 
        "string", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binascii": {
      "file": "test/test_binascii.py", 
      "imports": [
        "array", 
        "binascii", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binhex": {
      "file": "test/test_binhex.py", 
      "imports": [
        "binhex", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_binop": {
      "file": "test/test_binop.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bisect": {
      "file": "test/test_bisect.py", 
      "imports": [
        "bisect", 
        "gc", 
        "random", 
        "sys", 
        "test.test_bisect", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_bool": {
      "file": "test/test_bool.py", 
      "imports": [
        "marshal", 
        "operator", 
        "os", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_bsddb": {
      "file": "test/test_bsddb.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bsddb185": {
      "file": "test/test_bsddb185.py", 
      "imports": [
        "anydbm", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "whichdb"
      ]
    }, 
    "test.test_bsddb3": {
      "file": "test/test_bsddb3.py", 
      "imports": [
        "bsddb.db", 
        "bsddb.test.test_all", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_buffer": {
      "file": "test/test_buffer.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bufio": {
      "file": "test/test_bufio.py", 
      "imports": [
        "_pyio", 
        "io", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_builtin": {
      "file": "test/test_builtin.py", 
      "imports": [
        "cStringIO", 
        "gc", 
        "marshal", 
        "math.sqrt", 
        "operator.neg", 
        "os", 
        "platform", 
        "random", 
        "string", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest", 
        "UserDict", 
        "UserList", 
        "warnings"
      ]
    }, 
    "test.test_bytes": {
      "file": "test/test_bytes.py", 
      "imports": [
        "copy", 
        "functools", 
        "os", 
        "pickle", 
        "re", 
        "sys", 
        "tempfile", 
        "test.buffer_tests", 
        "test.string_tests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_bz2": {
      "file": "test/test_bz2.py", 
      "imports": [
        "bz2.BZ2Compressor", 
        "bz2.BZ2Decompressor", 
        "bz2.BZ2File", 
        "cStringIO.StringIO", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_calendar": {
      "file": "test/test_calendar.py", 
      "imports": [
        "calendar", 
        "locale", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_call": {
      "file": "test/test_call.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_capi": {
      "file": "test/test_capi.py", 
      "imports": [
        "__future__", 
        "random", 
        "sys", 
        "test.test_support", 
        "thread", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_cd": {
      "file": "test/test_cd.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_cfgparser": {
      "file": "test/test_cfgparser.py", 
      "imports": [
        "ConfigParser", 
        "StringIO", 
        "os", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.test_cgi": {
      "file": "test/test_cgi.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "cgi", 
        "collections", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_charmapcodec": {
      "file": "test/test_charmapcodec.py", 
      "imports": [
        "codecs", 
        "test.test_support", 
        "test.testcodec", 
        "unittest"
      ]
    }, 
    "test.test_cl": {
      "file": "test/test_cl.py", 
      "imports": [
        "test.test_support"
      ]
    }, 
    "test.test_class": {
      "file": "test/test_class.py", 
      "imports": [
        "gc", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_cmath": {
      "file": "test/test_cmath.py", 
      "imports": [
        "cmath", 
        "cmath.phase", 
        "cmath.pi", 
        "cmath.polar", 
        "cmath.rect", 
        "math", 
        "test.test_math", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd": {
      "file": "test/test_cmd.py", 
      "imports": [
        "StringIO", 
        "cmd", 
        "re", 
        "sys", 
        "test.test_cmd", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd_line": {
      "file": "test/test_cmd_line.py", 
      "imports": [
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cmd_line_script": {
      "file": "test/test_cmd_line_script.py", 
      "imports": [
        "os", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_code": {
      "file": "test/test_code.py", 
      "imports": [
        "test.test_code", 
        "test.test_support", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_codeccallbacks": {
      "file": "test/test_codeccallbacks.py", 
      "imports": [
        "codecs", 
        "htmlentitydefs", 
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_cn": {
      "file": "test/test_codecencodings_cn.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_hk": {
      "file": "test/test_codecencodings_hk.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_iso2022": {
      "file": "test/test_codecencodings_iso2022.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_jp": {
      "file": "test/test_codecencodings_jp.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_kr": {
      "file": "test/test_codecencodings_kr.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecencodings_tw": {
      "file": "test/test_codecencodings_tw.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_cn": {
      "file": "test/test_codecmaps_cn.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_hk": {
      "file": "test/test_codecmaps_hk.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_jp": {
      "file": "test/test_codecmaps_jp.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_kr": {
      "file": "test/test_codecmaps_kr.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecmaps_tw": {
      "file": "test/test_codecmaps_tw.py", 
      "imports": [
        "test.test_multibytecodec_support", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_codecs": {
      "file": "test/test_codecs.py", 
      "imports": [
        "StringIO", 
        "array", 
        "bz2", 
        "codecs", 
        "encodings.cp1140", 
        "encodings.idna", 
        "locale", 
        "sys", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_codeop": {
      "file": "test/test_codeop.py", 
      "imports": [
        "cStringIO", 
        "codeop", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_coding": {
      "file": "test/test_coding.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_coercion": {
      "file": "test/test_coercion.py", 
      "imports": [
        "copy", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_collections": {
      "file": "test/test_collections.py", 
      "imports": [
        "collections", 
        "copy", 
        "doctest", 
        "inspect", 
        "keyword", 
        "operator", 
        "pickle", 
        "random", 
        "re", 
        "string", 
        "sys", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_colorsys": {
      "file": "test/test_colorsys.py", 
      "imports": [
        "colorsys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_commands": {
      "file": "test/test_commands.py", 
      "imports": [
        "os", 
        "re", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compare": {
      "file": "test/test_compare.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compile": {
      "file": "test/test_compile.py", 
      "imports": [
        "__builtin__", 
        "__mangled_mod", 
        "__package__.module", 
        "_ast", 
        "math", 
        "sys", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_compileall": {
      "file": "test/test_compileall.py", 
      "imports": [
        "compileall", 
        "imp", 
        "os", 
        "py_compile", 
        "shutil", 
        "struct", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_compiler": {
      "file": "test/test_compiler.py", 
      "imports": [
        "StringIO", 
        "compiler.ast", 
        "math.*", 
        "os", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_complex": {
      "file": "test/test_complex.py", 
      "imports": [
        "math.atan2", 
        "math.copysign", 
        "math.isnan", 
        "random", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_complex_args": {
      "file": "test/test_complex_args.py", 
      "imports": [
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_contains": {
      "file": "test/test_contains.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_contextlib": {
      "file": "test/test_contextlib.py", 
      "imports": [
        "contextlib", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_cookie": {
      "file": "test/test_cookie.py", 
      "imports": [
        "Cookie", 
        "pickle", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cookielib": {
      "file": "test/test_cookielib.py", 
      "imports": [
        "StringIO", 
        "cookielib", 
        "mimetools", 
        "os", 
        "re", 
        "test.test_support", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_copy": {
      "file": "test/test_copy.py", 
      "imports": [
        "copy", 
        "copy_reg", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_copy_reg": {
      "file": "test/test_copy_reg.py", 
      "imports": [
        "copy", 
        "copy_reg", 
        "test.pickletester", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_cpickle": {
      "file": "test/test_cpickle.py", 
      "imports": [
        "cStringIO", 
        "io", 
        "test.pickletester", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_cprofile": {
      "file": "test/test_cprofile.py", 
      "imports": [
        "_lsprof", 
        "cProfile", 
        "sys", 
        "test.test_profile", 
        "test.test_support"
      ]
    }, 
    "test.test_crypt": {
      "file": "test/test_crypt.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_csv": {
      "file": "test/test_csv.py", 
      "imports": [
        "StringIO", 
        "array", 
        "csv", 
        "gc", 
        "io", 
        "itertools", 
        "os", 
        "string", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ctypes": {
      "file": "test/test_ctypes.py", 
      "imports": [
        "ctypes.test", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_curses": {
      "file": "test/test_curses.py", 
      "imports": [
        "curses.ascii", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_datetime": {
      "file": "test/test_datetime.py", 
      "imports": [
        "__future__", 
        "_strptime", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "cPickle", 
        "datetime"
      ]
    }, 
    "test.test_dbm": {
      "file": "test/test_dbm.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_decimal": {
      "file": "test/test_decimal.py", 
      "imports": [
        "copy", 
        "decimal", 
        "locale", 
        "math", 
        "numbers", 
        "operator", 
        "optparse", 
        "os", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest"
      ]
    }, 
    "test.test_decorators": {
      "file": "test/test_decorators.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_defaultdict": {
      "file": "test/test_defaultdict.py", 
      "imports": [
        "collections", 
        "copy", 
        "os", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_deque": {
      "file": "test/test_deque.py", 
      "imports": [
        "collections", 
        "copy", 
        "gc", 
        "random", 
        "struct", 
        "sys", 
        "test.seq_tests", 
        "test.test_deque", 
        "test.test_support", 
        "unittest", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_descr": {
      "file": "test/test_descr.py", 
      "imports": [
        "__builtin__", 
        "abc", 
        "binascii", 
        "cStringIO", 
        "copy", 
        "gc", 
        "operator", 
        "pickle", 
        "popen2", 
        "sys", 
        "test.test_support", 
        "xxsubtype", 
        "types", 
        "unittest", 
        "weakref", 
        "_testcapi", 
        "cPickle"
      ]
    }, 
    "test.test_descrtut": {
      "file": "test/test_descrtut.py", 
      "imports": [
        "pprint", 
        "test.test_descrtut", 
        "test.test_support"
      ]
    }, 
    "test.test_dict": {
      "file": "test/test_dict.py", 
      "imports": [
        "gc", 
        "random", 
        "string", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "UserDict", 
        "weakref"
      ]
    }, 
    "test.test_dictcomps": {
      "file": "test/test_dictcomps.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dictviews": {
      "file": "test/test_dictviews.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_difflib": {
      "file": "test/test_difflib.py", 
      "imports": [
        "difflib", 
        "doctest", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dircache": {
      "file": "test/test_dircache.py", 
      "imports": [
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dis": {
      "file": "test/test_dis.py", 
      "imports": [
        "StringIO", 
        "difflib", 
        "dis", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_distutils": {
      "file": "test/test_distutils.py", 
      "imports": [
        "distutils.tests", 
        "test.test_support"
      ]
    }, 
    "test.test_dl": {
      "file": "test/test_dl.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_doctest": {
      "file": "test/test_doctest.py", 
      "imports": [
        "doctest", 
        "sys", 
        "test.test_doctest", 
        "test.test_support"
      ]
    }, 
    "test.test_doctest2": {
      "file": "test/test_doctest2.py", 
      "imports": [
        "doctest", 
        "sys", 
        "test.test_doctest2", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_docxmlrpc": {
      "file": "test/test_docxmlrpc.py", 
      "imports": [
        "DocXMLRPCServer", 
        "httplib", 
        "socket", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dumbdbm": {
      "file": "test/test_dumbdbm.py", 
      "imports": [
        "dumbdbm", 
        "os", 
        "random", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_dummy_thread": {
      "file": "test/test_dummy_thread.py", 
      "imports": [
        "Queue", 
        "dummy_thread", 
        "random", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_dummy_threading": {
      "file": "test/test_dummy_threading.py", 
      "imports": [
        "dummy_threading", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_email": {
      "file": "test/test_email.py", 
      "imports": [
        "email.test.test_email", 
        "email.test.test_email_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_email_codecs": {
      "file": "test/test_email_codecs.py", 
      "imports": [
        "email.test.test_email_codecs", 
        "email.test.test_email_codecs_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_email_renamed": {
      "file": "test/test_email_renamed.py", 
      "imports": [
        "email.test.test_email_renamed", 
        "test.test_support"
      ]
    }, 
    "test.test_ensurepip": {
      "file": "test/test_ensurepip.py", 
      "imports": [
        "contextlib", 
        "ensurepip", 
        "ensurepip._uninstall", 
        "os", 
        "ssl", 
        "sys", 
        "test._mock_backport", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_enumerate": {
      "file": "test/test_enumerate.py", 
      "imports": [
        "sys", 
        "test.test_iterlen", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_eof": {
      "file": "test/test_eof.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_epoll": {
      "file": "test/test_epoll.py", 
      "imports": [
        "errno", 
        "socket", 
        "test.test_support", 
        "time", 
        "unittest", 
        "select"
      ]
    }, 
    "test.test_errno": {
      "file": "test/test_errno.py", 
      "imports": [
        "errno", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_exception_variations": {
      "file": "test/test_exception_variations.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_exceptions": {
      "file": "test/test_exceptions.py", 
      "imports": [
        "exceptions", 
        "imp.reload", 
        "os", 
        "pickle", 
        "sys", 
        "test.test_pep352", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "cPickle"
      ]
    }, 
    "test.test_extcall": {
      "file": "test/test_extcall.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fcntl": {
      "file": "test/test_fcntl.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_file": {
      "file": "test/test_file.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "array.array", 
        "io", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_file2k": {
      "file": "test/test_file2k.py", 
      "imports": [
        "array.array", 
        "itertools", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "UserList", 
        "weakref", 
        "signal", 
        "select"
      ]
    }, 
    "test.test_file_eintr": {
      "file": "test/test_file_eintr.py", 
      "imports": [
        "_io.FileIO", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "signal", 
        "select"
      ]
    }, 
    "test.test_filecmp": {
      "file": "test/test_filecmp.py", 
      "imports": [
        "filecmp", 
        "os", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fileinput": {
      "file": "test/test_fileinput.py", 
      "imports": [
        "StringIO", 
        "fileinput", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fileio": {
      "file": "test/test_fileio.py", 
      "imports": [
        "__future__", 
        "_io.FileIO", 
        "array.array", 
        "errno", 
        "functools", 
        "msvcrt", 
        "os", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_float": {
      "file": "test/test_float.py", 
      "imports": [
        "fractions", 
        "locale", 
        "math", 
        "math.copysign", 
        "math.isinf", 
        "math.isnan", 
        "math.ldexp", 
        "operator", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fnmatch": {
      "file": "test/test_fnmatch.py", 
      "imports": [
        "fnmatch", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fork1": {
      "file": "test/test_fork1.py", 
      "imports": [
        "imp", 
        "os", 
        "sys", 
        "test.fork_wait", 
        "test.test_support", 
        "time", 
        "signal"
      ]
    }, 
    "test.test_format": {
      "file": "test/test_format.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_fpformat": {
      "file": "test/test_fpformat.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_fractions": {
      "file": "test/test_fractions.py", 
      "imports": [
        "copy", 
        "decimal", 
        "fractions", 
        "math", 
        "numbers", 
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_frozen": {
      "file": "test/test_frozen.py", 
      "imports": [
        "__hello__", 
        "__phello__", 
        "__phello__.foo", 
        "__phello__.spam", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ftplib": {
      "file": "test/test_ftplib.py", 
      "imports": [
        "StringIO", 
        "asynchat", 
        "asyncore", 
        "errno", 
        "ftplib", 
        "os", 
        "socket", 
        "ssl", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_funcattrs": {
      "file": "test/test_funcattrs.py", 
      "imports": [
        "test.test_support", 
        "types", 
        "unittest", 
        "UserDict"
      ]
    }, 
    "test.test_functools": {
      "file": "test/test_functools.py", 
      "imports": [
        "functools", 
        "gc", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_future": {
      "file": "test/test_future.py", 
      "imports": [
        "re", 
        "test.badsyntax_future3", 
        "test.badsyntax_future4", 
        "test.badsyntax_future5", 
        "test.badsyntax_future6", 
        "test.badsyntax_future7", 
        "test.badsyntax_future8", 
        "test.badsyntax_future9", 
        "test.test_future1", 
        "test.test_future2", 
        "test.test_future3", 
        "test.test_future5", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future1": {
      "file": "test/test_future1.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "test.test_future2": {
      "file": "test/test_future2.py", 
      "imports": [
        "__future__", 
        "string"
      ]
    }, 
    "test.test_future3": {
      "file": "test/test_future3.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future4": {
      "file": "test/test_future4.py", 
      "imports": [
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future5": {
      "file": "test/test_future5.py", 
      "imports": [
        "__future__", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_future_builtins": {
      "file": "test/test_future_builtins.py", 
      "imports": [
        "itertools.ifilter", 
        "itertools.imap", 
        "itertools.izip", 
        "test.test_support", 
        "unittest", 
        "future_builtins"
      ]
    }, 
    "test.test_gc": {
      "file": "test/test_gc.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_gdb": {
      "file": "test/test_gdb.py", 
      "imports": [
        "os", 
        "re", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gdbm": {
      "file": "test/test_gdbm.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_generators": {
      "file": "test/test_generators.py", 
      "imports": [
        "test.test_generators", 
        "test.test_support"
      ]
    }, 
    "test.test_genericpath": {
      "file": "test/test_genericpath.py", 
      "imports": [
        "genericpath", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_genexps": {
      "file": "test/test_genexps.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_genexps", 
        "test.test_support"
      ]
    }, 
    "test.test_getargs": {
      "file": "test/test_getargs.py", 
      "imports": [
        "marshal", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_getargs2": {
      "file": "test/test_getargs2.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "warnings", 
        "_testcapi"
      ]
    }, 
    "test.test_getopt": {
      "file": "test/test_getopt.py", 
      "imports": [
        "getopt", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_gettext": {
      "file": "test/test_gettext.py", 
      "imports": [
        "__builtin__", 
        "base64", 
        "gettext", 
        "os", 
        "shutil", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gl": {
      "file": "test/test_gl.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_glob": {
      "file": "test/test_glob.py", 
      "imports": [
        "glob", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_global": {
      "file": "test/test_global.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_grammar": {
      "file": "test/test_grammar.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "sys.*", 
        "sys.argv", 
        "sys.maxint", 
        "sys.path", 
        "test.test_support", 
        "time", 
        "time.time", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_grp": {
      "file": "test/test_grp.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_gzip": {
      "file": "test/test_gzip.py", 
      "imports": [
        "io", 
        "os", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_hash": {
      "file": "test/test_hash.py", 
      "imports": [
        "collections", 
        "os", 
        "struct", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_hashlib": {
      "file": "test/test_hashlib.py", 
      "imports": [
        "_md5", 
        "array", 
        "binascii.unhexlify", 
        "hashlib", 
        "itertools", 
        "string", 
        "sys", 
        "test.test_support", 
        "threading", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_heapq": {
      "file": "test/test_heapq.py", 
      "imports": [
        "gc", 
        "itertools.chain", 
        "itertools.imap", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_hmac": {
      "file": "test/test_hmac.py", 
      "imports": [
        "hashlib", 
        "hmac", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_hotshot": {
      "file": "test/test_hotshot.py", 
      "imports": [
        "_hotshot", 
        "gc", 
        "hotshot.log.ENTER", 
        "hotshot.log.EXIT", 
        "hotshot.log.LINE", 
        "hotshot.stats", 
        "os", 
        "pprint", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_htmllib": {
      "file": "test/test_htmllib.py", 
      "imports": [
        "formatter", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_htmlparser": {
      "file": "test/test_htmlparser.py", 
      "imports": [
        "HTMLParser", 
        "pprint", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_httplib": {
      "file": "test/test_httplib.py", 
      "imports": [
        "StringIO", 
        "array", 
        "errno", 
        "httplib", 
        "os", 
        "socket", 
        "ssl", 
        "test.ssl_servers", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_httpservers": {
      "file": "test/test_httpservers.py", 
      "imports": [
        "BaseHTTPServer", 
        "CGIHTTPServer", 
        "SimpleHTTPServer", 
        "StringIO", 
        "base64", 
        "httplib", 
        "os", 
        "re", 
        "shutil", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "urllib"
      ]
    }, 
    "test.test_idle": {
      "file": "test/test_idle.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_imageop": {
      "file": "test/test_imageop.py", 
      "imports": [
        "imgfile", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "uu"
      ]
    }, 
    "test.test_imaplib": {
      "file": "test/test_imaplib.py", 
      "imports": [
        "SocketServer", 
        "contextlib", 
        "imaplib", 
        "os", 
        "ssl", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_imgfile": {
      "file": "test/test_imgfile.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "uu"
      ]
    }, 
    "test.test_imghdr": {
      "file": "test/test_imghdr.py", 
      "imports": [
        "imghdr", 
        "io", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_imp": {
      "file": "test/test_imp.py", 
      "imports": [
        "imp", 
        "marshal", 
        "os", 
        "test.test_support", 
        "thread", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_import": {
      "file": "test/test_import.py", 
      "imports": [
        "RAnDoM", 
        "errno", 
        "imp", 
        "marshal", 
        "os", 
        "py_compile", 
        "random", 
        "shutil", 
        "socket", 
        "stat", 
        "struct", 
        "sys", 
        "test", 
        "test.double_const", 
        "test.infinite_reload", 
        "test.relimport", 
        "test.script_helper", 
        "test.symlink_support", 
        "test.test_import", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_importhooks": {
      "file": "test/test_importhooks.py", 
      "imports": [
        "hooktestmodule", 
        "hooktestpackage", 
        "hooktestpackage.futrel", 
        "hooktestpackage.newabs", 
        "hooktestpackage.newrel", 
        "hooktestpackage.oldabs", 
        "hooktestpackage.sub", 
        "hooktestpackage.sub.subber", 
        "hooktestpackage.sub.subber.subest", 
        "imp", 
        "os", 
        "reloadmodule", 
        "sub", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_importlib": {
      "file": "test/test_importlib.py", 
      "imports": [
        "contextlib", 
        "imp", 
        "importlib", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_index": {
      "file": "test/test_index.py", 
      "imports": [
        "operator", 
        "sys.maxint", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_inspect": {
      "file": "test/test_inspect.py", 
      "imports": [
        "__builtin__", 
        "abc", 
        "inspect", 
        "linecache", 
        "re", 
        "sys", 
        "test.inspect_fodder", 
        "test.inspect_fodder2", 
        "test.test_support", 
        "unicodedata", 
        "types", 
        "unittest", 
        "UserDict", 
        "UserList"
      ]
    }, 
    "test.test_int": {
      "file": "test/test_int.py", 
      "imports": [
        "math", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_int_literal": {
      "file": "test/test_int_literal.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_io": {
      "file": "test/test_io.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "abc", 
        "array", 
        "codecs", 
        "collections", 
        "contextlib", 
        "errno", 
        "fcntl", 
        "io", 
        "itertools.count", 
        "itertools.cycle", 
        "os", 
        "random", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "UserList", 
        "warnings", 
        "weakref", 
        "signal"
      ]
    }, 
    "test.test_ioctl": {
      "file": "test/test_ioctl.py", 
      "imports": [
        "array", 
        "os", 
        "pty", 
        "struct", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_isinstance": {
      "file": "test/test_isinstance.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_iter": {
      "file": "test/test_iter.py", 
      "imports": [
        "operator.add", 
        "operator.countOf", 
        "operator.indexOf", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_iterlen": {
      "file": "test/test_iterlen.py", 
      "imports": [
        "__builtin__.len", 
        "collections", 
        "itertools.repeat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_itertools": {
      "file": "test/test_itertools.py", 
      "imports": [
        "copy", 
        "decimal", 
        "fractions", 
        "functools", 
        "gc", 
        "itertools.*", 
        "operator", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_iterlen", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_json": {
      "file": "test/test_json.py", 
      "imports": [
        "json.tests", 
        "test.test_support"
      ]
    }, 
    "test.test_kqueue": {
      "file": "test/test_kqueue.py", 
      "imports": [
        "errno", 
        "socket", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "select"
      ]
    }, 
    "test.test_largefile": {
      "file": "test/test_largefile.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "io", 
        "os", 
        "stat", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "signal"
      ]
    }, 
    "test.test_lib2to3": {
      "file": "test/test_lib2to3.py", 
      "imports": [
        "lib2to3.tests.test_fixers", 
        "lib2to3.tests.test_main", 
        "lib2to3.tests.test_parser", 
        "lib2to3.tests.test_pytree", 
        "lib2to3.tests.test_refactor", 
        "lib2to3.tests.test_util", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_linecache": {
      "file": "test/test_linecache.py", 
      "imports": [
        "linecache", 
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_linuxaudiodev": {
      "file": "test/test_linuxaudiodev.py", 
      "imports": [
        "audioop", 
        "errno", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_list": {
      "file": "test/test_list.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.list_tests", 
        "test.test_support"
      ]
    }, 
    "test.test_locale": {
      "file": "test/test_locale.py", 
      "imports": [
        "codecs", 
        "locale", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_logging": {
      "file": "test/test_logging.py", 
      "imports": [
        "SocketServer", 
        "cStringIO", 
        "codecs", 
        "gc", 
        "json", 
        "logging", 
        "logging.config", 
        "logging.handlers", 
        "os", 
        "random", 
        "re", 
        "socket", 
        "struct", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "time", 
        "textwrap", 
        "unittest", 
        "warnings", 
        "weakref", 
        "cPickle", 
        "select"
      ]
    }, 
    "test.test_long": {
      "file": "test/test_long.py", 
      "imports": [
        "math", 
        "random", 
        "sys", 
        "test.test_int", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_long_future": {
      "file": "test/test_long_future.py", 
      "imports": [
        "__future__", 
        "math", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_longexp": {
      "file": "test/test_longexp.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macos": {
      "file": "test/test_macos.py", 
      "imports": [
        "os", 
        "subprocess", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macostools": {
      "file": "test/test_macostools.py", 
      "imports": [
        "Carbon.File", 
        "macostools", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macpath": {
      "file": "test/test_macpath.py", 
      "imports": [
        "macpath", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_macurl2path": {
      "file": "test/test_macurl2path.py", 
      "imports": [
        "macurl2path", 
        "unittest"
      ]
    }, 
    "test.test_mailbox": {
      "file": "test/test_mailbox.py", 
      "imports": [
        "StringIO", 
        "email", 
        "email.message", 
        "email.parser", 
        "fcntl", 
        "glob", 
        "mailbox", 
        "os", 
        "re", 
        "shutil", 
        "socket", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_marshal": {
      "file": "test/test_marshal.py", 
      "imports": [
        "marshal", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_math": {
      "file": "test/test_math.py", 
      "imports": [
        "doctest", 
        "math", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "sys.float_info", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_md5": {
      "file": "test/test_md5.py", 
      "imports": [
        "md5", 
        "string", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_memoryio": {
      "file": "test/test_memoryio.py", 
      "imports": [
        "__future__", 
        "__main__", 
        "_pyio", 
        "array", 
        "io", 
        "pickle", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_memoryview": {
      "file": "test/test_memoryview.py", 
      "imports": [
        "array", 
        "gc", 
        "io", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_mhlib": {
      "file": "test/test_mhlib.py", 
      "imports": [
        "StringIO", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_mimetools": {
      "file": "test/test_mimetools.py", 
      "imports": [
        "StringIO", 
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_mimetypes": {
      "file": "test/test_mimetypes.py", 
      "imports": [
        "StringIO", 
        "_winreg", 
        "mimetypes", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_minidom": {
      "file": "test/test_minidom.py", 
      "imports": [
        "StringIO", 
        "pickle", 
        "test.test_support", 
        "unittest", 
        "xml.parsers.expat", 
        "xml.dom.pulldom", 
        "xml.dom.minidom", 
        "xml.dom"
      ]
    }, 
    "test.test_mmap": {
      "file": "test/test_mmap.py", 
      "imports": [
        "itertools", 
        "os", 
        "re", 
        "socket", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_module": {
      "file": "test/test_module.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_modulefinder": {
      "file": "test/test_modulefinder.py", 
      "imports": [
        "__future__", 
        "distutils.dir_util", 
        "modulefinder", 
        "os", 
        "sets", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_msilib": {
      "file": "test/test_msilib.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multibytecodec": {
      "file": "test/test_multibytecodec.py", 
      "imports": [
        "StringIO", 
        "_multibytecodec", 
        "codecs", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multibytecodec_support": {
      "file": "test/test_multibytecodec_support.py", 
      "imports": [
        "StringIO", 
        "codecs", 
        "htmlentitydefs", 
        "httplib", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_multifile": {
      "file": "test/test_multifile.py", 
      "imports": [
        "cStringIO", 
        "test.test_support"
      ]
    }, 
    "test.test_multiprocessing": {
      "file": "test/test_multiprocessing.py", 
      "imports": [
        "Queue", 
        "StringIO", 
        "array", 
        "ctypes.Structure", 
        "ctypes.c_double", 
        "ctypes.c_int", 
        "errno", 
        "gc", 
        "json", 
        "logging", 
        "msvcrt", 
        "multiprocessing.connection", 
        "multiprocessing.dummy", 
        "multiprocessing.forking", 
        "multiprocessing.heap", 
        "multiprocessing.managers", 
        "multiprocessing.managers.BaseManager", 
        "multiprocessing.managers.BaseProxy", 
        "multiprocessing.managers.RemoteError", 
        "multiprocessing.pool", 
        "multiprocessing.pool.MaybeEncodingError", 
        "multiprocessing.reduction", 
        "multiprocessing.sharedctypes.Value", 
        "multiprocessing.sharedctypes.copy", 
        "multiprocessing.util", 
        "os", 
        "random", 
        "socket", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "signal"
      ]
    }, 
    "test.test_mutants": {
      "file": "test/test_mutants.py", 
      "imports": [
        "os", 
        "random", 
        "test.test_support"
      ]
    }, 
    "test.test_mutex": {
      "file": "test/test_mutex.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_netrc": {
      "file": "test/test_netrc.py", 
      "imports": [
        "netrc", 
        "os", 
        "sys", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_new": {
      "file": "test/test_new.py", 
      "imports": [
        "Spam", 
        "__builtin__", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_nis": {
      "file": "test/test_nis.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_nntplib": {
      "file": "test/test_nntplib.py", 
      "imports": [
        "nntplib", 
        "socket", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_normalization": {
      "file": "test/test_normalization.py", 
      "imports": [
        "httplib", 
        "os", 
        "sys", 
        "test.test_support", 
        "unicodedata.normalize", 
        "unicodedata.unidata_version", 
        "unittest"
      ]
    }, 
    "test.test_ntpath": {
      "file": "test/test_ntpath.py", 
      "imports": [
        "nt", 
        "ntpath", 
        "os", 
        "sys", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_old_mailbox": {
      "file": "test/test_old_mailbox.py", 
      "imports": [
        "email.parser", 
        "mailbox", 
        "os", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_opcodes": {
      "file": "test/test_opcodes.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_openpty": {
      "file": "test/test_openpty.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_operator": {
      "file": "test/test_operator.py", 
      "imports": [
        "gc", 
        "operator", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_optparse": {
      "file": "test/test_optparse.py", 
      "imports": [
        "StringIO", 
        "copy", 
        "optparse", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_os": {
      "file": "test/test_os.py", 
      "imports": [
        "ctypes", 
        "ctypes.wintypes", 
        "errno", 
        "mmap", 
        "msvcrt", 
        "os", 
        "stat", 
        "subprocess", 
        "sys", 
        "test.mapping_tests", 
        "test.script_helper", 
        "test.test_support", 
        "time", 
        "unittest", 
        "uuid", 
        "warnings", 
        "resource", 
        "signal"
      ]
    }, 
    "test.test_ossaudiodev": {
      "file": "test/test_ossaudiodev.py", 
      "imports": [
        "audioop", 
        "errno", 
        "ossaudiodev.AFMT_S16_NE", 
        "sunau", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_parser": {
      "file": "test/test_parser.py", 
      "imports": [
        "parser", 
        "struct", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pdb": {
      "file": "test/test_pdb.py", 
      "imports": [
        "imp", 
        "os", 
        "subprocess", 
        "sys", 
        "test.test_doctest", 
        "test.test_pdb", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_peepholer": {
      "file": "test/test_peepholer.py", 
      "imports": [
        "cStringIO.StringIO", 
        "dis", 
        "gc", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep247": {
      "file": "test/test_pep247.py", 
      "imports": [
        "hmac", 
        "md5", 
        "sha", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_pep263": {
      "file": "test/test_pep263.py", 
      "imports": [
        "test.bad_coding3", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep277": {
      "file": "test/test_pep277.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unicodedata.normalize", 
        "unittest"
      ]
    }, 
    "test.test_pep292": {
      "file": "test/test_pep292.py", 
      "imports": [
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pep352": {
      "file": "test/test_pep352.py", 
      "imports": [
        "__builtin__", 
        "exceptions", 
        "os", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_pickle": {
      "file": "test/test_pickle.py", 
      "imports": [
        "cStringIO.StringIO", 
        "pickle", 
        "test.pickletester", 
        "test.test_support"
      ]
    }, 
    "test.test_pickletools": {
      "file": "test/test_pickletools.py", 
      "imports": [
        "pickle", 
        "pickletools", 
        "test.pickletester", 
        "test.test_support"
      ]
    }, 
    "test.test_pipes": {
      "file": "test/test_pipes.py", 
      "imports": [
        "os", 
        "pipes", 
        "string", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pkg": {
      "file": "test/test_pkg.py", 
      "imports": [
        "os", 
        "sys", 
        "t1", 
        "t2.sub", 
        "t2.sub.subsub", 
        "t2.sub.subsub.spam", 
        "t3.sub.subsub", 
        "t5", 
        "t6", 
        "t7", 
        "t7.sub", 
        "t7.sub.subsub", 
        "t7.sub.subsub.spam", 
        "t8", 
        "tempfile", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_pkgimport": {
      "file": "test/test_pkgimport.py", 
      "imports": [
        "os", 
        "random", 
        "string", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pkgutil": {
      "file": "test/test_pkgutil.py", 
      "imports": [
        "foo", 
        "imp", 
        "os", 
        "pkgutil", 
        "shutil", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "zipfile", 
        "zipimport"
      ]
    }, 
    "test.test_platform": {
      "file": "test/test_platform.py", 
      "imports": [
        "gestalt", 
        "os", 
        "platform", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_plistlib": {
      "file": "test/test_plistlib.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "os", 
        "plistlib", 
        "test.test_support", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_poll": {
      "file": "test/test_poll.py", 
      "imports": [
        "os", 
        "random", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "_testcapi", 
        "select"
      ]
    }, 
    "test.test_popen": {
      "file": "test/test_popen.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_popen2": {
      "file": "test/test_popen2.py", 
      "imports": [
        "os", 
        "popen2", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_poplib": {
      "file": "test/test_poplib.py", 
      "imports": [
        "asynchat", 
        "asyncore", 
        "errno", 
        "os", 
        "poplib", 
        "socket", 
        "ssl", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_posix": {
      "file": "test/test_posix.py", 
      "imports": [
        "errno", 
        "os", 
        "platform", 
        "shutil", 
        "stat", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "unittest", 
        "warnings", 
        "pwd"
      ]
    }, 
    "test.test_posixpath": {
      "file": "test/test_posixpath.py", 
      "imports": [
        "os", 
        "posixpath", 
        "test.test_genericpath", 
        "test.test_support", 
        "unittest", 
        "pwd"
      ]
    }, 
    "test.test_pow": {
      "file": "test/test_pow.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pprint": {
      "file": "test/test_pprint.py", 
      "imports": [
        "pprint", 
        "test.test_set", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_print": {
      "file": "test/test_print.py", 
      "imports": [
        "StringIO", 
        "__future__", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_profile": {
      "file": "test/test_profile.py", 
      "imports": [
        "StringIO", 
        "profile", 
        "pstats", 
        "sys", 
        "test.profilee", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_property": {
      "file": "test/test_property.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pstats": {
      "file": "test/test_pstats.py", 
      "imports": [
        "pstats", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pty": {
      "file": "test/test_pty.py", 
      "imports": [
        "errno", 
        "os", 
        "pty", 
        "socket", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "signal", 
        "select"
      ]
    }, 
    "test.test_pwd": {
      "file": "test/test_pwd.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_py3kwarn": {
      "file": "test/test_py3kwarn.py", 
      "imports": [
        "operator.add", 
        "operator.isCallable", 
        "operator.sequenceIncludes", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "UserString", 
        "warnings"
      ]
    }, 
    "test.test_py_compile": {
      "file": "test/test_py_compile.py", 
      "imports": [
        "imp", 
        "os", 
        "py_compile", 
        "shutil", 
        "tempfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_pyclbr": {
      "file": "test/test_pyclbr.py", 
      "imports": [
        "commands", 
        "pyclbr", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest"
      ]
    }, 
    "test.test_pydoc": {
      "file": "test/test_pydoc.py", 
      "imports": [
        "__builtin__", 
        "collections", 
        "contextlib", 
        "difflib", 
        "inspect", 
        "keyword", 
        "nturl2path", 
        "os", 
        "pkgutil", 
        "pydoc", 
        "re", 
        "sys", 
        "test.pydoc_mod", 
        "test.pydocfodder", 
        "test.script_helper", 
        "test.test_support", 
        "types", 
        "unittest", 
        "xml.etree"
      ]
    }, 
    "test.test_pyexpat": {
      "file": "test/test_pyexpat.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "xml.parsers.expat"
      ]
    }, 
    "test.test_queue": {
      "file": "test/test_queue.py", 
      "imports": [
        "Queue", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_quopri": {
      "file": "test/test_quopri.py", 
      "imports": [
        "cStringIO", 
        "quopri", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_random": {
      "file": "test/test_random.py", 
      "imports": [
        "functools", 
        "math.exp", 
        "math.fsum", 
        "math.ldexp", 
        "math.log", 
        "math.pi", 
        "math.sin", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_re": {
      "file": "test/test_re.py", 
      "imports": [
        "_sre", 
        "_sre.MAXREPEAT", 
        "array", 
        "locale", 
        "pickle", 
        "re", 
        "sre", 
        "sre_constants", 
        "string", 
        "sys", 
        "test.re_tests", 
        "test.test_support", 
        "traceback", 
        "unittest", 
        "weakref", 
        "cPickle"
      ]
    }, 
    "test.test_readline": {
      "file": "test/test_readline.py", 
      "imports": [
        "os", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_repr": {
      "file": "test/test_repr.py", 
      "imports": [
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.bar", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.baz", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.foo", 
        "areallylongpackageandmodulenametotestreprtruncation.areallylongpackageandmodulenametotestreprtruncation.qux", 
        "array.array", 
        "collections", 
        "os", 
        "repr", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_resource": {
      "file": "test/test_resource.py", 
      "imports": [
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_rfc822": {
      "file": "test/test_rfc822.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_richcmp": {
      "file": "test/test_richcmp.py", 
      "imports": [
        "operator", 
        "random", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_rlcompleter": {
      "file": "test/test_rlcompleter.py", 
      "imports": [
        "__builtin__", 
        "rlcompleter", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_robotparser": {
      "file": "test/test_robotparser.py", 
      "imports": [
        "StringIO", 
        "robotparser", 
        "test.test_support", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_runpy": {
      "file": "test/test_runpy.py", 
      "imports": [
        "os", 
        "re", 
        "runpy", 
        "sys", 
        "tempfile", 
        "test.script_helper", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sax": {
      "file": "test/test_sax.py", 
      "imports": [
        "cStringIO.StringIO", 
        "io", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "xml.sax.xmlreader", 
        "xml.sax.saxutils", 
        "xml.sax.handler", 
        "xml.sax.expatreader", 
        "xml.sax"
      ]
    }, 
    "test.test_scope": {
      "file": "test/test_scope.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_scriptpackages": {
      "file": "test/test_scriptpackages.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_select": {
      "file": "test/test_select.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "select"
      ]
    }, 
    "test.test_set": {
      "file": "test/test_set.py", 
      "imports": [
        "collections", 
        "copy", 
        "gc", 
        "itertools.chain", 
        "itertools.imap", 
        "operator", 
        "pickle", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_setcomps": {
      "file": "test/test_setcomps.py", 
      "imports": [
        "gc", 
        "sys", 
        "test.test_setcomps", 
        "test.test_support"
      ]
    }, 
    "test.test_sets": {
      "file": "test/test_sets.py", 
      "imports": [
        "copy", 
        "doctest", 
        "operator", 
        "pickle", 
        "random", 
        "sets", 
        "test.test_sets", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sgmllib": {
      "file": "test/test_sgmllib.py", 
      "imports": [
        "pprint", 
        "re", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sha": {
      "file": "test/test_sha.py", 
      "imports": [
        "sha", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_shelve": {
      "file": "test/test_shelve.py", 
      "imports": [
        "glob", 
        "os", 
        "shelve", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_shlex": {
      "file": "test/test_shlex.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "shlex", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_shutil": {
      "file": "test/test_shutil.py", 
      "imports": [
        "distutils.spawn", 
        "errno", 
        "os", 
        "shutil", 
        "stat", 
        "sys", 
        "tarfile", 
        "tempfile", 
        "test.test_support", 
        "zlib", 
        "unittest", 
        "warnings", 
        "zipfile", 
        "grp", 
        "pwd"
      ]
    }, 
    "test.test_signal": {
      "file": "test/test_signal.py", 
      "imports": [
        "contextlib", 
        "errno", 
        "fcntl", 
        "gc", 
        "os", 
        "pickle", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "time", 
        "traceback", 
        "unittest", 
        "signal", 
        "select"
      ]
    }, 
    "test.test_site": {
      "file": "test/test_site.py", 
      "imports": [
        "__builtin__", 
        "copy", 
        "encodings", 
        "locale", 
        "os", 
        "re", 
        "site", 
        "sitecustomize", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_slice": {
      "file": "test/test_slice.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_smtplib": {
      "file": "test/test_smtplib.py", 
      "imports": [
        "StringIO", 
        "asyncore", 
        "email.utils", 
        "smtpd", 
        "smtplib", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "select"
      ]
    }, 
    "test.test_smtpnet": {
      "file": "test/test_smtpnet.py", 
      "imports": [
        "smtplib", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_socket": {
      "file": "test/test_socket.py", 
      "imports": [
        "Queue", 
        "_socket", 
        "array", 
        "contextlib", 
        "errno", 
        "itertools", 
        "math", 
        "os", 
        "socket", 
        "sys", 
        "test.test_support", 
        "thread", 
        "threading", 
        "time", 
        "traceback", 
        "unittest", 
        "weakref", 
        "_testcapi", 
        "signal", 
        "select"
      ]
    }, 
    "test.test_socketserver": {
      "file": "test/test_socketserver.py", 
      "imports": [
        "SocketServer", 
        "contextlib", 
        "errno", 
        "imp", 
        "os", 
        "socket", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "unittest", 
        "signal", 
        "select"
      ]
    }, 
    "test.test_softspace": {
      "file": "test/test_softspace.py", 
      "imports": [
        "StringIO", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sort": {
      "file": "test/test_sort.py", 
      "imports": [
        "gc", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_spwd": {
      "file": "test/test_spwd.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sqlite": {
      "file": "test/test_sqlite.py", 
      "imports": [
        "sqlite3.test.dbapi", 
        "sqlite3.test.dump", 
        "sqlite3.test.factory", 
        "sqlite3.test.hooks", 
        "sqlite3.test.py25tests", 
        "sqlite3.test.regression", 
        "sqlite3.test.transactions", 
        "sqlite3.test.types", 
        "sqlite3.test.userfunctions", 
        "test.test_support"
      ]
    }, 
    "test.test_ssl": {
      "file": "test/test_ssl.py", 
      "imports": [
        "asyncore", 
        "contextlib", 
        "errno", 
        "functools", 
        "gc", 
        "os", 
        "platform", 
        "pprint", 
        "socket", 
        "sys", 
        "tempfile", 
        "test.ssl_servers", 
        "test.test_support", 
        "threading", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2", 
        "weakref", 
        "datetime", 
        "select"
      ]
    }, 
    "test.test_startfile": {
      "file": "test/test_startfile.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "time.sleep", 
        "unittest"
      ]
    }, 
    "test.test_stat": {
      "file": "test/test_stat.py", 
      "imports": [
        "os", 
        "stat", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_str": {
      "file": "test/test_str.py", 
      "imports": [
        "struct", 
        "sys", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_strftime": {
      "file": "test/test_strftime.py", 
      "imports": [
        "calendar", 
        "java", 
        "locale", 
        "re", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_string": {
      "file": "test/test_string.py", 
      "imports": [
        "string", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "UserList"
      ]
    }, 
    "test.test_stringprep": {
      "file": "test/test_stringprep.py", 
      "imports": [
        "stringprep", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_strop": {
      "file": "test/test_strop.py", 
      "imports": [
        "strop", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_strptime": {
      "file": "test/test_strptime.py", 
      "imports": [
        "_strptime", 
        "locale", 
        "re", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "datetime"
      ]
    }, 
    "test.test_strtod": {
      "file": "test/test_strtod.py", 
      "imports": [
        "random", 
        "re", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_struct": {
      "file": "test/test_struct.py", 
      "imports": [
        "array", 
        "binascii", 
        "inspect", 
        "math", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_structmembers": {
      "file": "test/test_structmembers.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_structseq": {
      "file": "test/test_structseq.py", 
      "imports": [
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_subprocess": {
      "file": "test/test_subprocess.py", 
      "imports": [
        "errno", 
        "os", 
        "re", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "resource", 
        "signal"
      ]
    }, 
    "test.test_sunau": {
      "file": "test/test_sunau.py", 
      "imports": [
        "sunau", 
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sunaudiodev": {
      "file": "test/test_sunaudiodev.py", 
      "imports": [
        "os", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sundry": {
      "file": "test/test_sundry.py", 
      "imports": [
        "CGIHTTPServer", 
        "audiodev", 
        "bdb", 
        "cgitb", 
        "code", 
        "compileall", 
        "distutils.bcppcompiler", 
        "distutils.ccompiler", 
        "distutils.command.bdist", 
        "distutils.command.bdist_dumb", 
        "distutils.command.bdist_msi", 
        "distutils.command.bdist_rpm", 
        "distutils.command.bdist_wininst", 
        "distutils.command.build", 
        "distutils.command.build_clib", 
        "distutils.command.build_ext", 
        "distutils.command.clean", 
        "distutils.command.config", 
        "distutils.command.install_data", 
        "distutils.command.install_egg_info", 
        "distutils.command.install_headers", 
        "distutils.command.install_lib", 
        "distutils.command.register", 
        "distutils.command.sdist", 
        "distutils.command.upload", 
        "distutils.cygwinccompiler", 
        "distutils.emxccompiler", 
        "distutils.filelist", 
        "distutils.msvccompiler", 
        "distutils.text_file", 
        "distutils.unixccompiler", 
        "encodings", 
        "formatter", 
        "getpass", 
        "htmlentitydefs", 
        "ihooks", 
        "imputil", 
        "keyword", 
        "linecache", 
        "mailcap", 
        "mimify", 
        "nntplib", 
        "nturl2path", 
        "opcode", 
        "os2emxpath", 
        "pdb", 
        "posixfile", 
        "pstats", 
        "py_compile", 
        "rexec", 
        "sched", 
        "sndhdr", 
        "statvfs", 
        "stringold", 
        "sunau", 
        "sunaudio", 
        "symbol", 
        "sys", 
        "tabnanny", 
        "test.test_support", 
        "token", 
        "timeit", 
        "toaiff", 
        "tty", 
        "unittest", 
        "webbrowser", 
        "xml"
      ]
    }, 
    "test.test_support": {
      "file": "test/test_support.py", 
      "imports": [
        "StringIO", 
        "Tkinter.Tk", 
        "contextlib", 
        "ctypes", 
        "ctypes.Structure", 
        "ctypes.c_int", 
        "ctypes.cdll", 
        "ctypes.pointer", 
        "ctypes.util.find_library", 
        "ctypes.wintypes", 
        "doctest", 
        "errno", 
        "functools", 
        "gc", 
        "importlib", 
        "locale", 
        "os", 
        "pdb", 
        "platform", 
        "re", 
        "shutil", 
        "socket", 
        "struct", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test", 
        "thread", 
        "time", 
        "traceback", 
        "unittest", 
        "urllib2", 
        "urlparse", 
        "UserDict", 
        "warnings", 
        "_testcapi"
      ]
    }, 
    "test.test_symtable": {
      "file": "test/test_symtable.py", 
      "imports": [
        "symtable", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_syntax": {
      "file": "test/test_syntax.py", 
      "imports": [
        "re", 
        "test.test_support", 
        "test.test_syntax", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_sys": {
      "file": "test/test_sys.py", 
      "imports": [
        "__builtin__", 
        "_ast", 
        "cStringIO", 
        "codecs", 
        "encodings.iso8859_3", 
        "imp", 
        "inspect", 
        "operator", 
        "os", 
        "re", 
        "struct", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "thread", 
        "threading", 
        "traceback", 
        "types", 
        "unittest", 
        "weakref", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_sys_setprofile": {
      "file": "test/test_sys_setprofile.py", 
      "imports": [
        "gc", 
        "pprint", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sys_settrace": {
      "file": "test/test_sys_settrace.py", 
      "imports": [
        "difflib", 
        "gc", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_sysconfig": {
      "file": "test/test_sysconfig.py", 
      "imports": [
        "_osx_support", 
        "copy", 
        "os", 
        "shutil", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_tarfile": {
      "file": "test/test_tarfile.py", 
      "imports": [
        "StringIO", 
        "bz2", 
        "errno", 
        "gzip", 
        "hashlib", 
        "os", 
        "shutil", 
        "sys", 
        "tarfile", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_tcl": {
      "file": "test/test_tcl.py", 
      "imports": [
        "Tkinter.Tcl", 
        "_tkinter.TclError", 
        "os", 
        "subprocess.PIPE", 
        "subprocess.Popen", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_telnetlib": {
      "file": "test/test_telnetlib.py", 
      "imports": [
        "Queue", 
        "socket", 
        "telnetlib", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_tempfile": {
      "file": "test/test_tempfile.py", 
      "imports": [
        "contextlib", 
        "errno", 
        "io", 
        "os", 
        "re", 
        "shutil", 
        "stat", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "signal"
      ]
    }, 
    "test.test_textwrap": {
      "file": "test/test_textwrap.py", 
      "imports": [
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_thread": {
      "file": "test/test_thread.py", 
      "imports": [
        "os", 
        "random", 
        "sys", 
        "test.lock_tests", 
        "test.test_support", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_threaded_import": {
      "file": "test/test_threaded_import.py", 
      "imports": [
        "imp", 
        "random", 
        "sys", 
        "test.test_support", 
        "test.threaded_import_hangers", 
        "unittest"
      ]
    }, 
    "test.test_threadedtempfile": {
      "file": "test/test_threadedtempfile.py", 
      "imports": [
        "StringIO", 
        "tempfile", 
        "test.test_support", 
        "traceback", 
        "unittest"
      ]
    }, 
    "test.test_threading": {
      "file": "test/test_threading.py", 
      "imports": [
        "ctypes", 
        "os", 
        "random", 
        "re", 
        "subprocess", 
        "sys", 
        "test.lock_tests", 
        "test.script_helper", 
        "test.test_support", 
        "time", 
        "unittest", 
        "weakref", 
        "_testcapi"
      ]
    }, 
    "test.test_threading_local": {
      "file": "test/test_threading_local.py", 
      "imports": [
        "_threading_local", 
        "doctest", 
        "gc", 
        "test.test_support", 
        "thread._local", 
        "time", 
        "unittest", 
        "weakref"
      ]
    }, 
    "test.test_threadsignals": {
      "file": "test/test_threadsignals.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "signal"
      ]
    }, 
    "test.test_time": {
      "file": "test/test_time.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_timeout": {
      "file": "test/test_timeout.py", 
      "imports": [
        "socket", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_tk": {
      "file": "test/test_tk.py", 
      "imports": [
        "os", 
        "runtktests", 
        "test.test_support"
      ]
    }, 
    "test.test_tokenize": {
      "file": "test/test_tokenize.py", 
      "imports": [
        "StringIO", 
        "os", 
        "test.test_support", 
        "test.test_tokenize", 
        "tokenize", 
        "unittest"
      ]
    }, 
    "test.test_tools": {
      "file": "test/test_tools.py", 
      "imports": [
        "os", 
        "shutil", 
        "subprocess", 
        "sys", 
        "sysconfig", 
        "tempfile", 
        "test.script_helper", 
        "test.test_support", 
        "textwrap", 
        "unittest"
      ]
    }, 
    "test.test_trace": {
      "file": "test/test_trace.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "test.tracedmodules.testmod", 
        "trace", 
        "unittest"
      ]
    }, 
    "test.test_traceback": {
      "file": "test/test_traceback.py", 
      "imports": [
        "StringIO", 
        "imp.reload", 
        "os", 
        "sys", 
        "tempfile", 
        "test.badsyntax_nocaret", 
        "test.test_support", 
        "test_bug737473", 
        "time", 
        "traceback", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_transformer": {
      "file": "test/test_transformer.py", 
      "imports": [
        "compiler", 
        "compiler.ast", 
        "compiler.transformer", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ttk_guionly": {
      "file": "test/test_ttk_guionly.py", 
      "imports": [
        "Tkinter", 
        "_tkinter.TclError", 
        "os", 
        "runtktests", 
        "test.test_support", 
        "ttk", 
        "unittest"
      ]
    }, 
    "test.test_ttk_textonly": {
      "file": "test/test_ttk_textonly.py", 
      "imports": [
        "os", 
        "runtktests", 
        "test.test_support"
      ]
    }, 
    "test.test_tuple": {
      "file": "test/test_tuple.py", 
      "imports": [
        "gc", 
        "test.seq_tests", 
        "test.test_support"
      ]
    }, 
    "test.test_typechecks": {
      "file": "test/test_typechecks.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_types": {
      "file": "test/test_types.py", 
      "imports": [
        "array", 
        "locale", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_ucn": {
      "file": "test/test_ucn.py", 
      "imports": [
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest", 
        "_testcapi"
      ]
    }, 
    "test.test_unary": {
      "file": "test/test_unary.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_undocumented_details": {
      "file": "test/test_undocumented_details.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_unicode": {
      "file": "test/test_unicode.py", 
      "imports": [
        "codecs", 
        "ctypes.c_int", 
        "ctypes.c_long", 
        "ctypes.c_longlong", 
        "ctypes.c_size_t", 
        "ctypes.c_ssize_t", 
        "ctypes.c_uint", 
        "ctypes.c_ulong", 
        "ctypes.c_ulonglong", 
        "ctypes.c_void_p", 
        "ctypes.py_object", 
        "ctypes.pythonapi", 
        "ctypes.sizeof", 
        "imp", 
        "struct", 
        "sys", 
        "test.string_tests", 
        "test.test_support", 
        "unittest", 
        "_testcapi", 
        "datetime"
      ]
    }, 
    "test.test_unicode_file": {
      "file": "test/test_unicode_file.py", 
      "imports": [
        "glob", 
        "os", 
        "shutil", 
        "sys", 
        "test.test_support", 
        "time", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_unicodedata": {
      "file": "test/test_unicodedata.py", 
      "imports": [
        "hashlib", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "unicodedata", 
        "unittest"
      ]
    }, 
    "test.test_unittest": {
      "file": "test/test_unittest.py", 
      "imports": [
        "test.test_support", 
        "unittest.test"
      ]
    }, 
    "test.test_univnewlines": {
      "file": "test/test_univnewlines.py", 
      "imports": [
        "__future__", 
        "_pyio", 
        "io", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_univnewlines2k": {
      "file": "test/test_univnewlines2k.py", 
      "imports": [
        "os", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_unpack": {
      "file": "test/test_unpack.py", 
      "imports": [
        "test.test_support", 
        "test.test_unpack"
      ]
    }, 
    "test.test_urllib": {
      "file": "test/test_urllib.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "httplib", 
        "mimetools", 
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "unittest", 
        "urllib", 
        "warnings"
      ]
    }, 
    "test.test_urllib2": {
      "file": "test/test_urllib2.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "cookielib", 
        "copy", 
        "ftplib", 
        "httplib", 
        "mimetools", 
        "os", 
        "rfc822", 
        "socket", 
        "ssl", 
        "string", 
        "test.test_cookielib", 
        "test.test_support", 
        "test.test_urllib2", 
        "unittest", 
        "urllib", 
        "urllib2"
      ]
    }, 
    "test.test_urllib2_localnet": {
      "file": "test/test_urllib2_localnet.py", 
      "imports": [
        "BaseHTTPServer", 
        "base64", 
        "hashlib", 
        "os", 
        "ssl", 
        "test.ssl_servers", 
        "test.test_support", 
        "unittest", 
        "urllib2", 
        "urlparse"
      ]
    }, 
    "test.test_urllib2net": {
      "file": "test/test_urllib2net.py", 
      "imports": [
        "httplib", 
        "logging", 
        "os", 
        "socket", 
        "sys", 
        "test.test_support", 
        "test.test_urllib2", 
        "time", 
        "unittest", 
        "urllib2"
      ]
    }, 
    "test.test_urllibnet": {
      "file": "test/test_urllibnet.py", 
      "imports": [
        "os", 
        "socket", 
        "ssl", 
        "sys", 
        "test.test_support", 
        "time", 
        "unittest", 
        "urllib"
      ]
    }, 
    "test.test_urlparse": {
      "file": "test/test_urlparse.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "urlparse"
      ]
    }, 
    "test.test_userdict": {
      "file": "test/test_userdict.py", 
      "imports": [
        "test.mapping_tests", 
        "test.test_support", 
        "UserDict"
      ]
    }, 
    "test.test_userlist": {
      "file": "test/test_userlist.py", 
      "imports": [
        "test.list_tests", 
        "test.test_support", 
        "UserList"
      ]
    }, 
    "test.test_userstring": {
      "file": "test/test_userstring.py", 
      "imports": [
        "string", 
        "test.string_tests", 
        "test.test_support", 
        "UserString", 
        "warnings"
      ]
    }, 
    "test.test_uu": {
      "file": "test/test_uu.py", 
      "imports": [
        "cStringIO", 
        "codecs", 
        "os", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "uu"
      ]
    }, 
    "test.test_uuid": {
      "file": "test/test_uuid.py", 
      "imports": [
        "io", 
        "os", 
        "test.test_support", 
        "unittest", 
        "uuid"
      ]
    }, 
    "test.test_wait3": {
      "file": "test/test_wait3.py", 
      "imports": [
        "os", 
        "test.fork_wait", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_wait4": {
      "file": "test/test_wait4.py", 
      "imports": [
        "os", 
        "sys", 
        "test.fork_wait", 
        "test.test_support", 
        "time"
      ]
    }, 
    "test.test_warnings": {
      "file": "test/test_warnings.py", 
      "imports": [
        "StringIO", 
        "contextlib", 
        "linecache", 
        "os", 
        "subprocess", 
        "sys", 
        "test.script_helper", 
        "test.test_support", 
        "test.warning_tests", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_wave": {
      "file": "test/test_wave.py", 
      "imports": [
        "sys", 
        "test.audiotests", 
        "test.test_support", 
        "wave", 
        "unittest"
      ]
    }, 
    "test.test_weakref": {
      "file": "test/test_weakref.py", 
      "imports": [
        "contextlib", 
        "copy", 
        "gc", 
        "operator", 
        "sys", 
        "test.mapping_tests", 
        "test.test_support", 
        "unittest", 
        "UserList", 
        "weakref"
      ]
    }, 
    "test.test_weakset": {
      "file": "test/test_weakset.py", 
      "imports": [
        "collections", 
        "contextlib", 
        "copy", 
        "gc", 
        "operator", 
        "os", 
        "random", 
        "string", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings", 
        "weakref"
      ]
    }, 
    "test.test_whichdb": {
      "file": "test/test_whichdb.py", 
      "imports": [
        "glob", 
        "os", 
        "test.test_support", 
        "unittest", 
        "whichdb"
      ]
    }, 
    "test.test_winreg": {
      "file": "test/test_winreg.py", 
      "imports": [
        "_winreg.*", 
        "errno", 
        "os", 
        "platform", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_winsound": {
      "file": "test/test_winsound.py", 
      "imports": [
        "_winreg", 
        "os", 
        "subprocess", 
        "test.test_support", 
        "time", 
        "unittest"
      ]
    }, 
    "test.test_with": {
      "file": "test/test_with.py", 
      "imports": [
        "collections", 
        "contextlib", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_wsgiref": {
      "file": "test/test_wsgiref.py", 
      "imports": [
        "SocketServer", 
        "StringIO", 
        "__future__", 
        "os", 
        "re", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "wsgiref.validate", 
        "wsgiref.util", 
        "wsgiref.simple_server", 
        "wsgiref.headers", 
        "wsgiref.handlers"
      ]
    }, 
    "test.test_xdrlib": {
      "file": "test/test_xdrlib.py", 
      "imports": [
        "test.test_support", 
        "unittest", 
        "xdrlib"
      ]
    }, 
    "test.test_xml_etree": {
      "file": "test/test_xml_etree.py", 
      "imports": [
        "StringIO", 
        "cgi", 
        "sys", 
        "test.test_support", 
        "test.test_xml_etree", 
        "xml.etree.ElementTree", 
        "xml.etree.ElementPath"
      ]
    }, 
    "test.test_xml_etree_c": {
      "file": "test/test_xml_etree_c.py", 
      "imports": [
        "test.test_support", 
        "test.test_xml_etree", 
        "test.test_xml_etree_c", 
        "unittest"
      ]
    }, 
    "test.test_xmllib": {
      "file": "test/test_xmllib.py", 
      "imports": [
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.test_xmlrpc": {
      "file": "test/test_xmlrpc.py", 
      "imports": [
        "SimpleXMLRPCServer", 
        "StringIO", 
        "base64", 
        "gzip", 
        "httplib", 
        "mimetools", 
        "os", 
        "re", 
        "socket", 
        "sys", 
        "test.test_support", 
        "threading", 
        "time", 
        "unittest", 
        "xmlrpclib", 
        "datetime"
      ]
    }, 
    "test.test_xpickle": {
      "file": "test/test_xpickle.py", 
      "imports": [
        "os", 
        "pickle", 
        "subprocess", 
        "sys", 
        "test.test_support", 
        "types", 
        "unittest", 
        "cPickle"
      ]
    }, 
    "test.test_xrange": {
      "file": "test/test_xrange.py", 
      "imports": [
        "itertools", 
        "pickle", 
        "sys", 
        "test.test_support", 
        "unittest", 
        "warnings"
      ]
    }, 
    "test.test_zipfile": {
      "file": "test/test_zipfile.py", 
      "imports": [
        "StringIO", 
        "email", 
        "io", 
        "os", 
        "random", 
        "struct", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "zlib", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipfile64": {
      "file": "test/test_zipfile64.py", 
      "imports": [
        "os", 
        "sys", 
        "tempfile", 
        "test.test_support", 
        "time", 
        "zlib", 
        "unittest", 
        "zipfile"
      ]
    }, 
    "test.test_zipimport": {
      "file": "test/test_zipimport.py", 
      "imports": [
        "StringIO", 
        "doctest", 
        "imp", 
        "inspect", 
        "linecache", 
        "marshal", 
        "os", 
        "struct", 
        "sys", 
        "test.test_importhooks", 
        "test.test_support", 
        "time", 
        "zlib", 
        "traceback", 
        "unittest", 
        "zipfile", 
        "zipimport"
      ]
    }, 
    "test.test_zipimport_support": {
      "file": "test/test_zipimport_support.py", 
      "imports": [
        "doctest", 
        "inspect", 
        "linecache", 
        "os", 
        "pdb", 
        "sys", 
        "test.sample_doctest", 
        "test.sample_doctest_no_docstrings", 
        "test.sample_doctest_no_doctests", 
        "test.script_helper", 
        "test.test_doctest", 
        "test.test_importhooks", 
        "test.test_support", 
        "test_zipped_doctest", 
        "zip_pkg", 
        "textwrap", 
        "warnings", 
        "zipfile", 
        "zipimport"
      ]
    }, 
    "test.test_zlib": {
      "file": "test/test_zlib.py", 
      "imports": [
        "binascii", 
        "mmap", 
        "os", 
        "random", 
        "sys", 
        "test.test_support", 
        "unittest"
      ]
    }, 
    "test.testall": {
      "file": "test/testall.py", 
      "imports": [
        "sys", 
        "test.regrtest", 
        "warnings"
      ]
    }, 
    "test.testcodec": {
      "file": "test/testcodec.py", 
      "imports": [
        "codecs"
      ]
    }, 
    "test.tf_inherit_check": {
      "file": "test/tf_inherit_check.py", 
      "imports": [
        "os", 
        "sys"
      ]
    }, 
    "test.threaded_import_hangers": {
      "file": "test/threaded_import_hangers.py", 
      "imports": [
        "os", 
        "tempfile", 
        "threading"
      ]
    }, 
    "test.time_hashlib": {
      "file": "test/time_hashlib.py", 
      "imports": [
        "_hashlib", 
        "hashlib", 
        "sys", 
        "time"
      ]
    }, 
    "test.tracedmodules": {
      "dir": "test/tracedmodules"
    }, 
    "test.tracedmodules.__init__": {
      "file": "test/tracedmodules/__init__.py", 
      "imports": []
    }, 
    "test.tracedmodules.testmod": {
      "file": "test/tracedmodules/testmod.py", 
      "imports": []
    }, 
    "test.warning_tests": {
      "file": "test/warning_tests.py", 
      "imports": [
        "warnings"
      ]
    }, 
    "test.win_console_handler": {
      "file": "test/win_console_handler.py", 
      "imports": [
        "ctypes", 
        "ctypes.WINFUNCTYPE", 
        "ctypes.wintypes", 
        "mmap", 
        "sys", 
        "signal"
      ]
    }, 
    "test.xmltests": {
      "file": "test/xmltests.py", 
      "imports": [
        "sys", 
        "test.test_support"
      ]
    }, 
    "tests": {
      "dir": "tests"
    }, 
    "tests.__init__": {
      "file": "tests/__init__.py", 
      "imports": []
    }, 
    "tests.base": {
      "file": "tests/base.py", 
      "imports": [
        "os"
      ]
    }, 
    "tests.test_biffh": {
      "file": "tests/test_biffh.py", 
      "imports": [
        "StringIO", 
        "io", 
        "sys", 
        "unittest", 
        "xlrd.biffh"
      ]
    }, 
    "tests.test_cell": {
      "file": "tests/test_cell.py", 
      "imports": [
        "os", 
        "sys", 
        "tests.base", 
        "unittest", 
        "xlrd"
      ]
    }, 
    "tests.test_formats": {
      "file": "tests/test_formats.py", 
      "imports": [
        "os", 
        "sys", 
        "tests.base", 
        "unittest", 
        "xlrd"
      ]
    }, 
    "tests.test_formulas": {
      "file": "tests/test_formulas.py", 
      "imports": [
        "os", 
        "sys", 
        "tests.base", 
        "unittest", 
        "xlrd"
      ]
    }, 
    "tests.test_open_workbook": {
      "file": "tests/test_open_workbook.py", 
      "imports": [
        "os", 
        "tests.base", 
        "unittest", 
        "xlrd"
      ]
    }, 
    "tests.test_sheet": {
      "file": "tests/test_sheet.py", 
      "imports": [
        "os", 
        "sys", 
        "tests.base", 
        "types", 
        "unittest", 
        "xlrd"
      ]
    }, 
    "tests.test_workbook": {
      "file": "tests/test_workbook.py", 
      "imports": [
        "os", 
        "sys", 
        "tests.base", 
        "unittest", 
        "xlrd", 
        "xlrd.book", 
        "xlrd.sheet"
      ]
    }, 
    "tests.test_xldate": {
      "file": "tests/test_xldate.py", 
      "imports": [
        "sys", 
        "unittest", 
        "xlrd.xldate"
      ]
    }, 
    "tests.test_xldate_to_datetime": {
      "file": "tests/test_xldate_to_datetime.py", 
      "imports": [
        "datetime", 
        "unittest", 
        "xlrd.xldate"
      ]
    }, 
    "tests.test_xlsx_comments": {
      "file": "tests/test_xlsx_comments.py", 
      "imports": [
        "os", 
        "tests.base", 
        "unittest", 
        "xlrd"
      ]
    }, 
    "tests.test_xlsx_parse": {
      "file": "tests/test_xlsx_parse.py", 
      "imports": [
        "tests.base", 
        "unittest", 
        "xlrd"
      ]
    }, 
    "texttable": {
      "file": "texttable.py", 
      "imports": [
        "functools", 
        "optik.textwrap", 
        "optparse", 
        "string", 
        "sys", 
        "textwrap"
      ]
    }, 
    "textwrap": {
      "file": "textwrap.py", 
      "imports": [
        "re", 
        "string"
      ]
    }, 
    "this": {
      "file": "this.py", 
      "imports": []
    }, 
    "threading": {
      "file": "threading.py", 
      "imports": [
        "_threading_local", 
        "collections", 
        "dummy_thread", 
        "itertools.count", 
        "js", 
        "sys", 
        "thread", 
        "time.sleep", 
        "time.time", 
        "traceback", 
        "warnings"
      ]
    }, 
    "timeit": {
      "file": "timeit.py", 
      "imports": [
        "gc", 
        "getopt", 
        "linecache", 
        "os", 
        "sys", 
        "time", 
        "traceback"
      ]
    }, 
    "toaiff": {
      "file": "toaiff.py", 
      "imports": [
        "os", 
        "pipes", 
        "sndhdr", 
        "tempfile", 
        "warnings"
      ]
    }, 
    "token": {
      "file": "token.py", 
      "imports": [
        "re", 
        "sys"
      ]
    }, 
    "tokenize": {
      "file": "tokenize.py", 
      "imports": [
        "itertools.chain", 
        "re", 
        "string", 
        "sys", 
        "token"
      ]
    }, 
    "tputil": {
      "file": "tputil.py", 
      "imports": [
        "__pypy__.tproxy", 
        "types"
      ]
    }, 
    "trace": {
      "file": "trace.py", 
      "imports": [
        "__main__", 
        "dis", 
        "gc", 
        "getopt", 
        "inspect", 
        "linecache", 
        "os", 
        "pickle", 
        "re", 
        "sys", 
        "threading", 
        "time", 
        "token", 
        "tokenize", 
        "cPickle"
      ]
    }, 
    "traceback": {
      "file": "traceback.py", 
      "imports": [
        "linecache", 
        "sys", 
        "types"
      ]
    }, 
    "tty": {
      "file": "tty.py", 
      "imports": [
        "termios.*"
      ]
    }, 
    "twisted": {
      "dir": "twisted"
    }, 
    "twisted.__init__": {
      "file": "twisted/__init__.py", 
      "imports": [
        "sys", 
        "twisted._version", 
        "twisted.python.compat", 
        "twisted.python.deprecate", 
        "twisted.python.versions", 
        "zope.interface"
      ]
    }, 
    "twisted._version": {
      "file": "twisted/_version.py", 
      "imports": [
        "twisted.python.versions"
      ]
    }, 
    "twisted.application": {
      "dir": "twisted/application"
    }, 
    "twisted.application.__init__": {
      "file": "twisted/application/__init__.py", 
      "imports": []
    }, 
    "twisted.application.app": {
      "file": "twisted/application/app.py", 
      "imports": [
        "cProfile", 
        "getpass", 
        "hotshot.stats", 
        "operator.attrgetter", 
        "os", 
        "pdb", 
        "profile", 
        "pstats", 
        "sys", 
        "threading", 
        "traceback", 
        "twisted.application.reactors", 
        "twisted.application.service", 
        "twisted.copyright", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.persisted.sob", 
        "twisted.plugin", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.logfile", 
        "twisted.python.reflect", 
        "twisted.python.runtime", 
        "twisted.python.usage", 
        "twisted.python.util", 
        "twisted.python.versions", 
        "signal"
      ]
    }, 
    "twisted.application.internet": {
      "file": "twisted/application/internet.py", 
      "imports": [
        "twisted.application.service", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.python.deprecate", 
        "twisted.python.log", 
        "twisted.python.versions", 
        "types"
      ]
    }, 
    "twisted.application.reactors": {
      "file": "twisted/application/reactors.py", 
      "imports": [
        "twisted.internet.reactor", 
        "twisted.plugin", 
        "twisted.python.reflect", 
        "zope.interface"
      ]
    }, 
    "twisted.application.service": {
      "file": "twisted/application/service.py", 
      "imports": [
        "twisted.internet.defer", 
        "twisted.persisted.sob", 
        "twisted.plugin", 
        "twisted.python.components", 
        "twisted.python.reflect", 
        "zope.interface"
      ]
    }, 
    "twisted.application.strports": {
      "file": "twisted/application/strports.py", 
      "imports": [
        "twisted.application.internet", 
        "twisted.internet.endpoints", 
        "twisted.internet.reactor", 
        "twisted.python.deprecate", 
        "twisted.python.versions", 
        "warnings"
      ]
    }, 
    "twisted.application.test": {
      "dir": "twisted/application/test"
    }, 
    "twisted.application.test.__init__": {
      "file": "twisted/application/test/__init__.py", 
      "imports": []
    }, 
    "twisted.application.test.test_internet": {
      "file": "twisted/application/test/test_internet.py", 
      "imports": [
        "pickle", 
        "twisted.application.internet", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.task", 
        "twisted.python.failure", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.conch": {
      "dir": "twisted/conch"
    }, 
    "twisted.conch.__init__": {
      "file": "twisted/conch/__init__.py", 
      "imports": [
        "twisted.conch._version"
      ]
    }, 
    "twisted.conch._version": {
      "file": "twisted/conch/_version.py", 
      "imports": [
        "twisted.python.versions"
      ]
    }, 
    "twisted.conch.avatar": {
      "file": "twisted/conch/avatar.py", 
      "imports": [
        "twisted.conch.error", 
        "twisted.conch.interfaces", 
        "twisted.conch.ssh.connection", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.checkers": {
      "file": "twisted/conch/checkers.py", 
      "imports": [
        "base64", 
        "binascii", 
        "crypt", 
        "errno", 
        "pwd", 
        "shadow", 
        "spwd", 
        "twisted.conch.error", 
        "twisted.conch.ssh.keys", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.pamauth", 
        "twisted.internet.defer", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.util", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.client": {
      "dir": "twisted/conch/client"
    }, 
    "twisted.conch.client.__init__": {
      "file": "twisted/conch/client/__init__.py", 
      "imports": []
    }, 
    "twisted.conch.client.agent": {
      "file": "twisted/conch/client/agent.py", 
      "imports": [
        "os", 
        "twisted.conch.ssh.agent", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.keys", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.log"
      ]
    }, 
    "twisted.conch.client.connect": {
      "file": "twisted/conch/client/connect.py", 
      "imports": [
        "twisted.conch.client.direct"
      ]
    }, 
    "twisted.conch.client.default": {
      "file": "twisted/conch/client/default.py", 
      "imports": [
        "base64", 
        "getpass", 
        "os", 
        "sys", 
        "twisted.conch.client.agent", 
        "twisted.conch.client.knownhosts", 
        "twisted.conch.error", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.userauth", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.filepath", 
        "twisted.python.log"
      ]
    }, 
    "twisted.conch.client.direct": {
      "file": "twisted/conch/client/direct.py", 
      "imports": [
        "twisted.conch.error", 
        "twisted.conch.ssh.transport", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.log"
      ]
    }, 
    "twisted.conch.client.knownhosts": {
      "file": "twisted/conch/client/knownhosts.py", 
      "imports": [
        "binascii.Error", 
        "binascii.b2a_base64", 
        "hashlib", 
        "hmac", 
        "twisted.conch.error", 
        "twisted.conch.interfaces", 
        "twisted.conch.ssh.keys", 
        "twisted.internet.defer", 
        "twisted.python.log", 
        "twisted.python.randbytes", 
        "twisted.python.util", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.client.options": {
      "file": "twisted/conch/client/options.py", 
      "imports": [
        "sys", 
        "twisted.conch.ssh.transport", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.conch.endpoints": {
      "file": "twisted/conch/endpoints.py", 
      "imports": [
        "os", 
        "struct", 
        "twisted.conch.client.agent", 
        "twisted.conch.client.default", 
        "twisted.conch.client.knownhosts", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.transport", 
        "twisted.conch.ssh.userauth", 
        "twisted.internet.defer", 
        "twisted.internet.endpoints", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.error": {
      "file": "twisted/conch/error.py", 
      "imports": [
        "twisted.cred.error"
      ]
    }, 
    "twisted.conch.insults": {
      "dir": "twisted/conch/insults"
    }, 
    "twisted.conch.insults.__init__": {
      "file": "twisted/conch/insults/__init__.py", 
      "imports": [
        "twisted.python.deprecate", 
        "twisted.python.versions"
      ]
    }, 
    "twisted.conch.insults.client": {
      "file": "twisted/conch/insults/client.py", 
      "imports": [
        "twisted.internet.protocol", 
        "twisted.internet.reactor"
      ]
    }, 
    "twisted.conch.insults.colors": {
      "file": "twisted/conch/insults/colors.py", 
      "imports": []
    }, 
    "twisted.conch.insults.helper": {
      "file": "twisted/conch/insults/helper.py", 
      "imports": [
        "re", 
        "string", 
        "twisted.conch.insults.insults", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python._textattributes", 
        "twisted.python.deprecate", 
        "twisted.python.log", 
        "twisted.python.versions", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.insults.insults": {
      "file": "twisted/conch/insults/insults.py", 
      "imports": [
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.insults.text": {
      "file": "twisted/conch/insults/text.py", 
      "imports": [
        "twisted.conch.insults.helper", 
        "twisted.conch.insults.insults", 
        "twisted.python._textattributes", 
        "twisted.python.deprecate", 
        "twisted.python.versions"
      ]
    }, 
    "twisted.conch.insults.window": {
      "file": "twisted/conch/insults/window.py", 
      "imports": [
        "array", 
        "twisted.conch.insults.helper", 
        "twisted.conch.insults.insults", 
        "twisted.python.text"
      ]
    }, 
    "twisted.conch.interfaces": {
      "file": "twisted/conch/interfaces.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "twisted.conch.ls": {
      "file": "twisted/conch/ls.py", 
      "imports": [
        "array", 
        "stat", 
        "time.localtime", 
        "time.strftime", 
        "time.time"
      ]
    }, 
    "twisted.conch.manhole": {
      "file": "twisted/conch/manhole.py", 
      "imports": [
        "StringIO", 
        "code", 
        "sys", 
        "tokenize", 
        "twisted.conch.recvline", 
        "twisted.internet.defer", 
        "twisted.python.htmlizer"
      ]
    }, 
    "twisted.conch.manhole_ssh": {
      "file": "twisted/conch/manhole_ssh.py", 
      "imports": [
        "twisted.conch.avatar", 
        "twisted.conch.error", 
        "twisted.conch.insults.insults", 
        "twisted.conch.interfaces", 
        "twisted.conch.ssh.factory", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.session", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.portal", 
        "twisted.python.components", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.manhole_tap": {
      "file": "twisted/conch/manhole_tap.py", 
      "imports": [
        "twisted.application.service", 
        "twisted.application.strports", 
        "twisted.conch.insults.insults", 
        "twisted.conch.interfaces", 
        "twisted.conch.manhole", 
        "twisted.conch.manhole_ssh", 
        "twisted.conch.ssh.session", 
        "twisted.conch.telnet", 
        "twisted.cred.checkers", 
        "twisted.cred.portal", 
        "twisted.internet.protocol", 
        "twisted.python.usage", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.mixin": {
      "file": "twisted/conch/mixin.py", 
      "imports": [
        "twisted.internet.reactor"
      ]
    }, 
    "twisted.conch.openssh_compat": {
      "dir": "twisted/conch/openssh_compat"
    }, 
    "twisted.conch.openssh_compat.__init__": {
      "file": "twisted/conch/openssh_compat/__init__.py", 
      "imports": []
    }, 
    "twisted.conch.openssh_compat.factory": {
      "file": "twisted/conch/openssh_compat/factory.py", 
      "imports": [
        "errno", 
        "os", 
        "twisted.conch.openssh_compat.primes", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.factory", 
        "twisted.conch.ssh.keys", 
        "twisted.python.log", 
        "twisted.python.util"
      ]
    }, 
    "twisted.conch.openssh_compat.primes": {
      "file": "twisted/conch/openssh_compat/primes.py", 
      "imports": []
    }, 
    "twisted.conch.recvline": {
      "file": "twisted/conch/recvline.py", 
      "imports": [
        "string", 
        "twisted.conch.insults.helper", 
        "twisted.conch.insults.insults", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.scripts": {
      "dir": "twisted/conch/scripts"
    }, 
    "twisted.conch.scripts.__init__": {
      "file": "twisted/conch/scripts/__init__.py", 
      "imports": []
    }, 
    "twisted.conch.scripts.cftp": {
      "file": "twisted/conch/scripts/cftp.py", 
      "imports": [
        "fcntl", 
        "fnmatch", 
        "getpass", 
        "glob", 
        "os", 
        "pwd", 
        "stat", 
        "struct", 
        "sys", 
        "tty", 
        "twisted.conch.client.connect", 
        "twisted.conch.client.default", 
        "twisted.conch.client.options", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.filetransfer", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.internet.utils", 
        "twisted.protocols.basic", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.conch.scripts.ckeygen": {
      "file": "twisted/conch/scripts/ckeygen.py", 
      "imports": [
        "Crypto.PublicKey.DSA", 
        "Crypto.PublicKey.RSA", 
        "getpass", 
        "os", 
        "socket", 
        "sys", 
        "termios", 
        "twisted.conch.ssh.keys", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.randbytes", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.conch.scripts.conch": {
      "file": "twisted/conch/scripts/conch.py", 
      "imports": [
        "errno", 
        "fcntl", 
        "getpass", 
        "os", 
        "struct", 
        "sys", 
        "tty", 
        "twisted.conch.client.connect", 
        "twisted.conch.client.default", 
        "twisted.conch.client.options", 
        "twisted.conch.error", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.forwarding", 
        "twisted.conch.ssh.session", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.internet.task", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.usage", 
        "signal"
      ]
    }, 
    "twisted.conch.scripts.tkconch": {
      "file": "twisted/conch/scripts/tkconch.py", 
      "imports": [
        "Tkinter", 
        "base64", 
        "getpass", 
        "os", 
        "string", 
        "struct", 
        "sys", 
        "tkFileDialog", 
        "tkFont", 
        "tkMessageBox", 
        "twisted.conch.client.default", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.forwarding", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.session", 
        "twisted.conch.ssh.transport", 
        "twisted.conch.ssh.userauth", 
        "twisted.conch.ui.tkvt100", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.tksupport", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.usage", 
        "signal"
      ]
    }, 
    "twisted.conch.ssh": {
      "dir": "twisted/conch/ssh"
    }, 
    "twisted.conch.ssh.__init__": {
      "file": "twisted/conch/ssh/__init__.py", 
      "imports": []
    }, 
    "twisted.conch.ssh.address": {
      "file": "twisted/conch/ssh/address.py", 
      "imports": [
        "twisted.internet.interfaces", 
        "twisted.python.util", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.ssh.agent": {
      "file": "twisted/conch/ssh/agent.py", 
      "imports": [
        "struct", 
        "twisted.conch.error", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.keys", 
        "twisted.internet.defer", 
        "twisted.internet.protocol"
      ]
    }, 
    "twisted.conch.ssh.channel": {
      "file": "twisted/conch/ssh/channel.py", 
      "imports": [
        "twisted.internet.interfaces", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.ssh.common": {
      "file": "twisted/conch/ssh/common.py", 
      "imports": [
        "Crypto.Util", 
        "__builtin__", 
        "gmpy", 
        "struct", 
        "twisted.python.randbytes", 
        "warnings"
      ]
    }, 
    "twisted.conch.ssh.connection": {
      "file": "twisted/conch/ssh/connection.py", 
      "imports": [
        "string", 
        "struct", 
        "twisted.conch.error", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.service", 
        "twisted.internet.defer", 
        "twisted.python.log"
      ]
    }, 
    "twisted.conch.ssh.factory": {
      "file": "twisted/conch/ssh/factory.py", 
      "imports": [
        "random", 
        "twisted.conch.error", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.transport", 
        "twisted.conch.ssh.userauth", 
        "twisted.internet.protocol", 
        "twisted.python.log"
      ]
    }, 
    "twisted.conch.ssh.filetransfer": {
      "file": "twisted/conch/ssh/filetransfer.py", 
      "imports": [
        "errno", 
        "struct", 
        "twisted.conch.interfaces", 
        "twisted.conch.ssh.common", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.ssh.forwarding": {
      "file": "twisted/conch/ssh/forwarding.py", 
      "imports": [
        "struct", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.common", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.log"
      ]
    }, 
    "twisted.conch.ssh.keys": {
      "file": "twisted/conch/ssh/keys.py", 
      "imports": [
        "Crypto.Cipher.AES", 
        "Crypto.Cipher.DES3", 
        "Crypto.PublicKey.DSA", 
        "Crypto.PublicKey.RSA", 
        "Crypto.Util", 
        "base64", 
        "hashlib", 
        "itertools", 
        "pyasn1.codec.ber.decoder", 
        "pyasn1.codec.ber.encoder", 
        "pyasn1.error.PyAsn1Error", 
        "pyasn1.type.univ", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.sexpy", 
        "twisted.python.randbytes"
      ]
    }, 
    "twisted.conch.ssh.service": {
      "file": "twisted/conch/ssh/service.py", 
      "imports": [
        "twisted.python.log"
      ]
    }, 
    "twisted.conch.ssh.session": {
      "file": "twisted/conch/ssh/session.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "twisted.conch.interfaces", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.connection", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.python.log", 
        "zope.interface", 
        "signal"
      ]
    }, 
    "twisted.conch.ssh.sexpy": {
      "file": "twisted/conch/ssh/sexpy.py", 
      "imports": []
    }, 
    "twisted.conch.ssh.transport": {
      "file": "twisted/conch/ssh/transport.py", 
      "imports": [
        "Crypto.Util", 
        "array", 
        "hashlib", 
        "hmac", 
        "string", 
        "struct", 
        "twisted.conch.error", 
        "twisted.conch.ssh.address", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.keys", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.python.log", 
        "twisted.python.randbytes", 
        "zlib"
      ]
    }, 
    "twisted.conch.ssh.userauth": {
      "file": "twisted/conch/ssh/userauth.py", 
      "imports": [
        "struct", 
        "twisted.conch.error", 
        "twisted.conch.interfaces", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.service", 
        "twisted.conch.ssh.transport", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.python.failure", 
        "twisted.python.log"
      ]
    }, 
    "twisted.conch.stdio": {
      "file": "twisted/conch/stdio.py", 
      "imports": [
        "os", 
        "sys", 
        "termios", 
        "tty", 
        "twisted.conch.insults.insults", 
        "twisted.conch.manhole", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect"
      ]
    }, 
    "twisted.conch.tap": {
      "file": "twisted/conch/tap.py", 
      "imports": [
        "twisted.application.strports", 
        "twisted.conch.checkers", 
        "twisted.conch.openssh_compat.factory", 
        "twisted.conch.unix", 
        "twisted.cred.checkers", 
        "twisted.cred.pamauth", 
        "twisted.cred.portal", 
        "twisted.cred.strcred", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.conch.telnet": {
      "file": "twisted/conch/telnet.py", 
      "imports": [
        "struct", 
        "twisted.cred.credentials", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.protocols.basic", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.test": {
      "dir": "twisted/conch/test"
    }, 
    "twisted.conch.test.__init__": {
      "file": "twisted/conch/test/__init__.py", 
      "imports": []
    }, 
    "twisted.conch.test.keydata": {
      "file": "twisted/conch/test/keydata.py", 
      "imports": []
    }, 
    "twisted.conch.test.test_address": {
      "file": "twisted/conch/test/test_address.py", 
      "imports": [
        "twisted.conch.ssh.address", 
        "twisted.internet.address", 
        "twisted.internet.test.test_address", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_agent": {
      "file": "twisted/conch/test/test_agent.py", 
      "imports": [
        "Crypto.Cipher.DES3", 
        "OpenSSL", 
        "pyasn1", 
        "struct", 
        "twisted.conch.error", 
        "twisted.conch.ssh.agent", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.test.keydata", 
        "twisted.test.iosim", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_cftp": {
      "file": "twisted/conch/test/test_cftp.py", 
      "imports": [
        "StringIO", 
        "getpass", 
        "locale", 
        "operator", 
        "os", 
        "struct", 
        "sys", 
        "time", 
        "tty", 
        "twisted.conch.ls", 
        "twisted.conch.scripts.cftp", 
        "twisted.conch.test.test_conch", 
        "twisted.conch.test.test_filetransfer", 
        "twisted.conch.test.test_ssh", 
        "twisted.conch.unix", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.internet.utils", 
        "twisted.python.fakepwd", 
        "twisted.python.log", 
        "twisted.python.procutils", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_channel": {
      "file": "twisted/conch/test/test_channel.py", 
      "imports": [
        "twisted.conch.ssh.channel", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_checkers": {
      "file": "twisted/conch/test/test_checkers.py", 
      "imports": [
        "Crypto.Cipher.DES3", 
        "base64", 
        "crypt", 
        "os", 
        "pyasn1", 
        "twisted.conch.checkers", 
        "twisted.conch.error", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.test.keydata", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.python.failure", 
        "twisted.python.fakepwd", 
        "twisted.python.filepath", 
        "twisted.python.util", 
        "twisted.test.test_process", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_ckeygen": {
      "file": "twisted/conch/test/test_ckeygen.py", 
      "imports": [
        "Crypto", 
        "StringIO", 
        "getpass", 
        "pyasn1", 
        "sys", 
        "twisted.conch.scripts.ckeygen", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.test.keydata", 
        "twisted.python.filepath", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_conch": {
      "file": "twisted/conch/test/test_conch.py", 
      "imports": [
        "itertools.count", 
        "os", 
        "socket", 
        "sys", 
        "twisted.conch.avatar", 
        "twisted.conch.error", 
        "twisted.conch.scripts.conch", 
        "twisted.conch.ssh.session", 
        "twisted.conch.test.keydata", 
        "twisted.conch.test.test_ssh", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.python.log", 
        "twisted.python.procutils", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.test.test_connection": {
      "file": "twisted/conch/test/test_connection.py", 
      "imports": [
        "struct", 
        "twisted.conch.error", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.test.test_userauth", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_default": {
      "file": "twisted/conch/test/test_default.py", 
      "imports": [
        "Crypto.Cipher.DES3", 
        "pyasn1", 
        "twisted.conch.client.agent", 
        "twisted.conch.client.default", 
        "twisted.conch.client.options", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.test.keydata", 
        "twisted.python.filepath", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_endpoints": {
      "file": "twisted/conch/test/test_endpoints.py", 
      "imports": [
        "Crypto.Cipher.AES", 
        "errno.ENOSYS", 
        "os", 
        "pyasn1.type", 
        "struct", 
        "twisted.conch.avatar", 
        "twisted.conch.checkers", 
        "twisted.conch.client.knownhosts", 
        "twisted.conch.endpoints", 
        "twisted.conch.error", 
        "twisted.conch.interfaces", 
        "twisted.conch.ssh.agent", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.factory", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.transport", 
        "twisted.conch.ssh.userauth", 
        "twisted.conch.test.keydata", 
        "twisted.cred.checkers", 
        "twisted.cred.portal", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.python.failure", 
        "twisted.python.fakepwd", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.test.iosim", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.conch.test.test_filetransfer": {
      "file": "twisted/conch/test/test_filetransfer.py", 
      "imports": [
        "os", 
        "re", 
        "struct", 
        "sys", 
        "twisted.conch.avatar", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.filetransfer", 
        "twisted.conch.ssh.session", 
        "twisted.conch.unix", 
        "twisted.internet.defer", 
        "twisted.protocols.loopback", 
        "twisted.python.components", 
        "twisted.trial.unittest", 
        "warnings"
      ]
    }, 
    "twisted.conch.test.test_helper": {
      "file": "twisted/conch/test/test_helper.py", 
      "imports": [
        "twisted.conch.insults.helper", 
        "twisted.conch.insults.insults", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_insults": {
      "file": "twisted/conch/test/test_insults.py", 
      "imports": [
        "twisted.conch.insults.client", 
        "twisted.conch.insults.colors", 
        "twisted.conch.insults.insults", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_keys": {
      "file": "twisted/conch/test/test_keys.py", 
      "imports": [
        "Crypto.Cipher.DES3", 
        "base64", 
        "hashlib", 
        "os", 
        "pyasn1", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.sexpy", 
        "twisted.conch.test.keydata", 
        "twisted.python.randbytes", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_knownhosts": {
      "file": "twisted/conch/test/test_knownhosts.py", 
      "imports": [
        "Crypto", 
        "binascii.Error", 
        "binascii.a2b_base64", 
        "binascii.b2a_base64", 
        "os", 
        "pyasn1", 
        "twisted.conch.client.default", 
        "twisted.conch.client.knownhosts", 
        "twisted.conch.error", 
        "twisted.conch.interfaces", 
        "twisted.conch.ssh.keys", 
        "twisted.internet.defer", 
        "twisted.python.filepath", 
        "twisted.test.testutils", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.conch.test.test_manhole": {
      "file": "twisted/conch/test/test_manhole.py", 
      "imports": [
        "traceback", 
        "twisted.conch.insults.insults", 
        "twisted.conch.manhole", 
        "twisted.conch.test.test_recvline", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_mixin": {
      "file": "twisted/conch/test/test_mixin.py", 
      "imports": [
        "time", 
        "twisted.conch.mixin", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_openssh_compat": {
      "file": "twisted/conch/test/test_openssh_compat.py", 
      "imports": [
        "Crypto.Cipher.DES3", 
        "os", 
        "pyasn1", 
        "twisted.conch.openssh_compat.factory", 
        "twisted.conch.test.keydata", 
        "twisted.python.filepath", 
        "twisted.test.test_process", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_recvline": {
      "file": "twisted/conch/test/test_recvline.py", 
      "imports": [
        "os", 
        "sys", 
        "twisted.conch.insults.helper", 
        "twisted.conch.insults.insults", 
        "twisted.conch.manhole_ssh", 
        "twisted.conch.recvline", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.session", 
        "twisted.conch.ssh.transport", 
        "twisted.conch.ssh.userauth", 
        "twisted.conch.stdio", 
        "twisted.conch.telnet", 
        "twisted.conch.test.test_telnet", 
        "twisted.cred.checkers", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.reactor", 
        "twisted.protocols.loopback", 
        "twisted.python.components", 
        "twisted.python.reflect", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_scripts": {
      "file": "twisted/conch/test/test_scripts.py", 
      "imports": [
        "Crypto", 
        "Tkinter", 
        "pyasn1", 
        "tty", 
        "twisted.python.test.test_shellcomp", 
        "twisted.scripts.test.test_scripts", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_session": {
      "file": "twisted/conch/test/test_session.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.session", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.protocol", 
        "twisted.python.components", 
        "twisted.python.failure", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "signal"
      ]
    }, 
    "twisted.conch.test.test_ssh": {
      "file": "twisted/conch/test/test_ssh.py", 
      "imports": [
        "Crypto.Cipher.DES3", 
        "gmpy", 
        "pyasn1", 
        "struct", 
        "twisted.conch.avatar", 
        "twisted.conch.checkers", 
        "twisted.conch.error", 
        "twisted.conch.ssh.channel", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.connection", 
        "twisted.conch.ssh.factory", 
        "twisted.conch.ssh.forwarding", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.session", 
        "twisted.conch.ssh.transport", 
        "twisted.conch.ssh.userauth", 
        "twisted.conch.test.keydata", 
        "twisted.conch.test.test_recvline", 
        "twisted.cred.error", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.components", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_tap": {
      "file": "twisted/conch/test/test_tap.py", 
      "imports": [
        "Crypto.Cipher.DES3", 
        "pyasn1", 
        "twisted.application.internet", 
        "twisted.conch.openssh_compat.factory", 
        "twisted.conch.tap", 
        "twisted.conch.unix", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.pamauth", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_telnet": {
      "file": "twisted/conch/test/test_telnet.py", 
      "imports": [
        "twisted.conch.telnet", 
        "twisted.internet.defer", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.conch.test.test_text": {
      "file": "twisted/conch/test/test_text.py", 
      "imports": [
        "twisted.conch.insults.helper", 
        "twisted.conch.insults.text", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_transport": {
      "file": "twisted/conch/test/test_transport.py", 
      "imports": [
        "Crypto.Cipher.DES3", 
        "hashlib", 
        "pyasn1", 
        "struct", 
        "twisted.conch.error", 
        "twisted.conch.ssh.address", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.factory", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.service", 
        "twisted.conch.ssh.transport", 
        "twisted.conch.test.keydata", 
        "twisted.internet.defer", 
        "twisted.protocols.loopback", 
        "twisted.python.randbytes", 
        "twisted.python.reflect", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.test.test_userauth": {
      "file": "twisted/conch/test/test_userauth.py", 
      "imports": [
        "Crypto.Cipher.DES3", 
        "pyasn1", 
        "twisted.conch.checkers", 
        "twisted.conch.error", 
        "twisted.conch.ssh.common", 
        "twisted.conch.ssh.keys", 
        "twisted.conch.ssh.transport", 
        "twisted.conch.ssh.userauth", 
        "twisted.conch.test.keydata", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.task", 
        "twisted.protocols.loopback", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.conch.test.test_window": {
      "file": "twisted/conch/test/test_window.py", 
      "imports": [
        "twisted.conch.insults.window", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.conch.ttymodes": {
      "file": "twisted/conch/ttymodes.py", 
      "imports": [
        "tty"
      ]
    }, 
    "twisted.conch.ui": {
      "dir": "twisted/conch/ui"
    }, 
    "twisted.conch.ui.__init__": {
      "file": "twisted/conch/ui/__init__.py", 
      "imports": []
    }, 
    "twisted.conch.ui.ansi": {
      "file": "twisted/conch/ui/ansi.py", 
      "imports": [
        "string", 
        "twisted.python.log"
      ]
    }, 
    "twisted.conch.ui.tkvt100": {
      "file": "twisted/conch/ui/tkvt100.py", 
      "imports": [
        "Tkinter", 
        "string", 
        "tkFont", 
        "twisted.conch.ui.ansi"
      ]
    }, 
    "twisted.conch.unix": {
      "file": "twisted/conch/unix.py", 
      "imports": [
        "fcntl", 
        "grp", 
        "os", 
        "pty", 
        "pwd", 
        "socket", 
        "struct", 
        "time", 
        "tty", 
        "twisted.conch.avatar", 
        "twisted.conch.error", 
        "twisted.conch.interfaces", 
        "twisted.conch.ls", 
        "twisted.conch.ssh.filetransfer", 
        "twisted.conch.ssh.forwarding", 
        "twisted.conch.ssh.session", 
        "twisted.conch.ttymodes", 
        "twisted.cred.portal", 
        "twisted.internet.error", 
        "twisted.internet.reactor", 
        "twisted.python.components", 
        "twisted.python.log", 
        "utmp", 
        "zope.interface"
      ]
    }, 
    "twisted.copyright": {
      "file": "twisted/copyright.py", 
      "imports": [
        "__future__", 
        "twisted"
      ]
    }, 
    "twisted.cred": {
      "dir": "twisted/cred"
    }, 
    "twisted.cred.__init__": {
      "file": "twisted/cred/__init__.py", 
      "imports": []
    }, 
    "twisted.cred._digest": {
      "file": "twisted/cred/_digest.py", 
      "imports": [
        "hashlib"
      ]
    }, 
    "twisted.cred.checkers": {
      "file": "twisted/cred/checkers.py", 
      "imports": [
        "os", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.pamauth", 
        "twisted.internet.defer", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.cred.credentials": {
      "file": "twisted/cred/credentials.py", 
      "imports": [
        "hashlib", 
        "hmac", 
        "random", 
        "re", 
        "time", 
        "twisted.cred._digest", 
        "twisted.cred.error", 
        "twisted.python.randbytes", 
        "zope.interface"
      ]
    }, 
    "twisted.cred.error": {
      "file": "twisted/cred/error.py", 
      "imports": []
    }, 
    "twisted.cred.pamauth": {
      "file": "twisted/cred/pamauth.py", 
      "imports": [
        "PAM", 
        "getpass", 
        "os", 
        "threading", 
        "traceback", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.threads"
      ]
    }, 
    "twisted.cred.portal": {
      "file": "twisted/cred/portal.py", 
      "imports": [
        "twisted.cred.error", 
        "twisted.internet.defer", 
        "twisted.python.failure", 
        "twisted.python.reflect", 
        "zope.interface"
      ]
    }, 
    "twisted.cred.strcred": {
      "file": "twisted/cred/strcred.py", 
      "imports": [
        "sys", 
        "twisted.plugin", 
        "twisted.python.usage", 
        "zope.interface"
      ]
    }, 
    "twisted.enterprise": {
      "dir": "twisted/enterprise"
    }, 
    "twisted.enterprise.__init__": {
      "file": "twisted/enterprise/__init__.py", 
      "imports": []
    }, 
    "twisted.enterprise.adbapi": {
      "file": "twisted/enterprise/adbapi.py", 
      "imports": [
        "sys", 
        "thread", 
        "twisted.internet.reactor", 
        "twisted.internet.threads", 
        "twisted.python.deprecate", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.threadpool", 
        "twisted.python.versions"
      ]
    }, 
    "twisted.internet": {
      "dir": "twisted/internet"
    }, 
    "twisted.internet.__init__": {
      "file": "twisted/internet/__init__.py", 
      "imports": []
    }, 
    "twisted.internet._baseprocess": {
      "file": "twisted/internet/_baseprocess.py", 
      "imports": [
        "twisted.persisted.styles", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect"
      ]
    }, 
    "twisted.internet._dumbwin32proc": {
      "file": "twisted/internet/_dumbwin32proc.py", 
      "imports": [
        "os", 
        "pywintypes", 
        "sys", 
        "twisted.internet._baseprocess", 
        "twisted.internet._pollingfile", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.python.win32", 
        "win32api", 
        "win32con", 
        "win32event", 
        "win32file", 
        "win32pipe", 
        "win32process", 
        "win32security", 
        "zope.interface"
      ]
    }, 
    "twisted.internet._glibbase": {
      "file": "twisted/internet/_glibbase.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.internet.base", 
        "twisted.internet.interfaces", 
        "twisted.internet.posixbase", 
        "twisted.internet.selectreactor", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.internet._newtls": {
      "file": "twisted/internet/_newtls.py", 
      "imports": [
        "__future__", 
        "twisted.internet.abstract", 
        "twisted.internet.interfaces", 
        "twisted.protocols.tls", 
        "zope.interface"
      ]
    }, 
    "twisted.internet._pollingfile": {
      "file": "twisted/internet/_pollingfile.py", 
      "imports": [
        "pywintypes", 
        "twisted.internet.interfaces", 
        "win32api", 
        "win32file", 
        "win32pipe", 
        "zope.interface"
      ]
    }, 
    "twisted.internet._posixserialport": {
      "file": "twisted/internet/_posixserialport.py", 
      "imports": [
        "errno", 
        "os", 
        "serial", 
        "serial.EIGHTBITS", 
        "serial.FIVEBITS", 
        "serial.PARITY_EVEN", 
        "serial.PARITY_NONE", 
        "serial.PARITY_ODD", 
        "serial.SEVENBITS", 
        "serial.SIXBITS", 
        "serial.STOPBITS_ONE", 
        "serial.STOPBITS_TWO", 
        "twisted.internet.abstract", 
        "twisted.internet.fdesc", 
        "twisted.internet.main", 
        "twisted.internet.serialport"
      ]
    }, 
    "twisted.internet._posixstdio": {
      "file": "twisted/internet/_posixstdio.py", 
      "imports": [
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.process", 
        "twisted.internet.reactor", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.internet._signals": {
      "file": "twisted/internet/_signals.py", 
      "imports": [
        "__future__", 
        "signal"
      ]
    }, 
    "twisted.internet._ssl": {
      "file": "twisted/internet/_ssl.py", 
      "imports": []
    }, 
    "twisted.internet._sslverify": {
      "file": "twisted/internet/_sslverify.py", 
      "imports": [
        "OpenSSL.SSL", 
        "OpenSSL.SSL.SSL_CB_HANDSHAKE_DONE", 
        "OpenSSL.SSL.SSL_CB_HANDSHAKE_START", 
        "OpenSSL._util.binding", 
        "OpenSSL.crypto", 
        "OpenSSL.version", 
        "__future__", 
        "hashlib", 
        "idna", 
        "itertools", 
        "service_identity.VerificationError", 
        "service_identity.pyopenssl.verify_hostname", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.python.compat", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.util", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.internet._threadedselect": {
      "file": "twisted/internet/_threadedselect.py", 
      "imports": [
        "Queue", 
        "errno.EBADF", 
        "errno.EINTR", 
        "sys", 
        "threading.Thread", 
        "time.sleep", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.posixbase", 
        "twisted.internet.selectreactor", 
        "twisted.persisted.styles", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.python.threadable", 
        "zope.interface", 
        "select"
      ]
    }, 
    "twisted.internet._win32serialport": {
      "file": "twisted/internet/_win32serialport.py", 
      "imports": [
        "serial", 
        "serial.EIGHTBITS", 
        "serial.FIVEBITS", 
        "serial.PARITY_EVEN", 
        "serial.PARITY_NONE", 
        "serial.PARITY_ODD", 
        "serial.SEVENBITS", 
        "serial.SIXBITS", 
        "serial.STOPBITS_ONE", 
        "serial.STOPBITS_TWO", 
        "twisted.internet.abstract", 
        "twisted.internet.serialport", 
        "win32event", 
        "win32file"
      ]
    }, 
    "twisted.internet._win32stdio": {
      "file": "twisted/internet/_win32stdio.py", 
      "imports": [
        "msvcrt", 
        "os", 
        "twisted.internet._pollingfile", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.reactor", 
        "twisted.python.failure", 
        "win32api", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.abstract": {
      "file": "twisted/internet/abstract.py", 
      "imports": [
        "__future__", 
        "socket", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.reactor", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.reflect", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.address": {
      "file": "twisted/internet/address.py", 
      "imports": [
        "__future__", 
        "os", 
        "twisted.internet.interfaces", 
        "twisted.python.util", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.base": {
      "file": "twisted/internet/base.py", 
      "imports": [
        "__future__", 
        "heapq", 
        "socket", 
        "sys", 
        "traceback", 
        "twisted.internet.abstract", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.fdesc", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.threads", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.runtime", 
        "twisted.python.threadable", 
        "twisted.python.threadpool", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.cfreactor": {
      "file": "twisted/internet/cfreactor.py", 
      "imports": [
        "CFNetwork.CFSocketCreateRunLoopSource", 
        "CFNetwork.CFSocketCreateWithNative", 
        "CFNetwork.CFSocketDisableCallBacks", 
        "CFNetwork.CFSocketEnableCallBacks", 
        "CFNetwork.CFSocketInvalidate", 
        "CFNetwork.CFSocketSetSocketFlags", 
        "CFNetwork.kCFSocketAutomaticallyReenableReadCallBack", 
        "CFNetwork.kCFSocketAutomaticallyReenableWriteCallBack", 
        "CFNetwork.kCFSocketConnectCallBack", 
        "CFNetwork.kCFSocketReadCallBack", 
        "CFNetwork.kCFSocketWriteCallBack", 
        "CoreFoundation.CFAbsoluteTimeGetCurrent", 
        "CoreFoundation.CFRunLoopAddSource", 
        "CoreFoundation.CFRunLoopAddTimer", 
        "CoreFoundation.CFRunLoopGetMain", 
        "CoreFoundation.CFRunLoopRemoveSource", 
        "CoreFoundation.CFRunLoopRun", 
        "CoreFoundation.CFRunLoopStop", 
        "CoreFoundation.CFRunLoopTimerCreate", 
        "CoreFoundation.CFRunLoopTimerInvalidate", 
        "CoreFoundation.kCFAllocatorDefault", 
        "CoreFoundation.kCFRunLoopCommonModes", 
        "sys", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.posixbase", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.default": {
      "file": "twisted/internet/default.py", 
      "imports": [
        "__future__", 
        "twisted.internet.epollreactor", 
        "twisted.internet.pollreactor", 
        "twisted.internet.selectreactor", 
        "twisted.python.runtime"
      ]
    }, 
    "twisted.internet.defer": {
      "file": "twisted/internet/defer.py", 
      "imports": [
        "__future__", 
        "functools", 
        "sys.exc_info", 
        "traceback", 
        "twisted.internet.reactor", 
        "twisted.python.compat", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.lockfile", 
        "twisted.python.log", 
        "types", 
        "warnings"
      ]
    }, 
    "twisted.internet.endpoints": {
      "file": "twisted/internet/endpoints.py", 
      "imports": [
        "__future__", 
        "os", 
        "re", 
        "socket", 
        "twisted.internet.abstract", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.fdesc", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.ssl", 
        "twisted.internet.stdio", 
        "twisted.internet.task", 
        "twisted.internet.threads", 
        "twisted.plugin", 
        "twisted.python.compat", 
        "twisted.python.components", 
        "twisted.python.constants", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.systemd", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.epollreactor": {
      "file": "twisted/internet/epollreactor.py", 
      "imports": [
        "__future__", 
        "errno", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.posixbase", 
        "twisted.internet.task", 
        "twisted.python.log", 
        "zope.interface", 
        "select"
      ]
    }, 
    "twisted.internet.error": {
      "file": "twisted/internet/error.py", 
      "imports": [
        "__future__", 
        "errno", 
        "socket", 
        "twisted.python.deprecate", 
        "twisted.python.versions"
      ]
    }, 
    "twisted.internet.fdesc": {
      "file": "twisted/internet/fdesc.py", 
      "imports": [
        "errno", 
        "fcntl", 
        "os", 
        "twisted.internet.main"
      ]
    }, 
    "twisted.internet.gireactor": {
      "file": "twisted/internet/gireactor.py", 
      "imports": [
        "__future__", 
        "gi.pygtkcompat", 
        "gi.repository.GLib", 
        "gi.repository.Gtk", 
        "twisted.internet._glibbase", 
        "twisted.internet.error", 
        "twisted.internet.main", 
        "twisted.python.compat", 
        "twisted.python.modules", 
        "twisted.python.runtime"
      ]
    }, 
    "twisted.internet.glib2reactor": {
      "file": "twisted/internet/glib2reactor.py", 
      "imports": [
        "twisted.internet.gtk2reactor", 
        "twisted.internet.main"
      ]
    }, 
    "twisted.internet.gtk2reactor": {
      "file": "twisted/internet/gtk2reactor.py", 
      "imports": [
        "gobject", 
        "gtk", 
        "pygtk", 
        "sys", 
        "twisted.internet._glibbase", 
        "twisted.internet.main", 
        "twisted.python.runtime"
      ]
    }, 
    "twisted.internet.gtk3reactor": {
      "file": "twisted/internet/gtk3reactor.py", 
      "imports": [
        "__future__", 
        "os", 
        "twisted.internet.gireactor", 
        "twisted.internet.main", 
        "twisted.python.runtime"
      ]
    }, 
    "twisted.internet.gtkreactor": {
      "file": "twisted/internet/gtkreactor.py", 
      "imports": [
        "gtk", 
        "pygtk", 
        "sys", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.posixbase", 
        "twisted.internet.selectreactor", 
        "twisted.python.deprecate", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.python.versions", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.inotify": {
      "file": "twisted/internet/inotify.py", 
      "imports": [
        "os", 
        "struct", 
        "twisted.internet.abstract", 
        "twisted.internet.fdesc", 
        "twisted.python._inotify", 
        "twisted.python.log"
      ]
    }, 
    "twisted.internet.interfaces": {
      "file": "twisted/internet/interfaces.py", 
      "imports": [
        "__future__", 
        "twisted.python.deprecate", 
        "twisted.python.versions", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.iocpreactor": {
      "dir": "twisted/internet/iocpreactor"
    }, 
    "twisted.internet.iocpreactor.__init__": {
      "file": "twisted/internet/iocpreactor/__init__.py", 
      "imports": [
        "twisted.internet.iocpreactor.reactor"
      ]
    }, 
    "twisted.internet.iocpreactor.abstract": {
      "file": "twisted/internet/iocpreactor/abstract.py", 
      "imports": [
        "errno", 
        "twisted.internet.abstract", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.iocpreactor", 
        "twisted.internet.iocpreactor.const", 
        "twisted.internet.main", 
        "twisted.internet.reactor", 
        "twisted.python.failure", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.iocpreactor.const": {
      "file": "twisted/internet/iocpreactor/const.py", 
      "imports": []
    }, 
    "twisted.internet.iocpreactor.interfaces": {
      "file": "twisted/internet/iocpreactor/interfaces.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "twisted.internet.iocpreactor.reactor": {
      "file": "twisted/internet/iocpreactor/reactor.py", 
      "imports": [
        "socket", 
        "sys", 
        "twisted.internet._dumbwin32proc", 
        "twisted.internet.base", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.iocpreactor", 
        "twisted.internet.iocpreactor.const", 
        "twisted.internet.iocpreactor.tcp", 
        "twisted.internet.iocpreactor.udp", 
        "twisted.internet.main", 
        "twisted.internet.win32eventreactor", 
        "twisted.protocols.tls", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.iocpreactor.setup": {
      "file": "twisted/internet/iocpreactor/setup.py", 
      "imports": [
        "Cython.Distutils.build_ext", 
        "distutils.core", 
        "distutils.extension"
      ]
    }, 
    "twisted.internet.iocpreactor.tcp": {
      "file": "twisted/internet/iocpreactor/tcp.py", 
      "imports": [
        "errno", 
        "operator", 
        "socket", 
        "struct", 
        "twisted.internet._newtls", 
        "twisted.internet.abstract", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.iocpreactor", 
        "twisted.internet.iocpreactor.abstract", 
        "twisted.internet.iocpreactor.const", 
        "twisted.internet.iocpreactor.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.tcp", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.util", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.iocpreactor.udp": {
      "file": "twisted/internet/iocpreactor/udp.py", 
      "imports": [
        "errno", 
        "operator", 
        "socket", 
        "struct", 
        "twisted.internet.abstract", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.iocpreactor", 
        "twisted.internet.iocpreactor.abstract", 
        "twisted.internet.iocpreactor.const", 
        "twisted.internet.iocpreactor.interfaces", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.kqreactor": {
      "file": "twisted/internet/kqreactor.py", 
      "imports": [
        "errno", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.posixbase", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "zope.interface", 
        "select"
      ]
    }, 
    "twisted.internet.main": {
      "file": "twisted/internet/main.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.internet", 
        "twisted.internet.error"
      ]
    }, 
    "twisted.internet.pollreactor": {
      "file": "twisted/internet/pollreactor.py", 
      "imports": [
        "__future__", 
        "errno", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.posixbase", 
        "twisted.python.log", 
        "zope.interface", 
        "select"
      ]
    }, 
    "twisted.internet.posixbase": {
      "file": "twisted/internet/posixbase.py", 
      "imports": [
        "__future__", 
        "errno", 
        "os", 
        "socket", 
        "sys", 
        "twisted.internet._dumbwin32proc", 
        "twisted.internet._signals", 
        "twisted.internet.base", 
        "twisted.internet.error", 
        "twisted.internet.fdesc", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.process", 
        "twisted.internet.ssl", 
        "twisted.internet.tcp", 
        "twisted.internet.udp", 
        "twisted.internet.unix", 
        "twisted.protocols.tls", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.python.util", 
        "win32process", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.process": {
      "file": "twisted/internet/process.py", 
      "imports": [
        "errno", 
        "fcntl", 
        "gc", 
        "os", 
        "pty", 
        "resource", 
        "stat", 
        "sys", 
        "termios", 
        "traceback", 
        "twisted.internet._baseprocess", 
        "twisted.internet.abstract", 
        "twisted.internet.error", 
        "twisted.internet.fdesc", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.util", 
        "zope.interface", 
        "signal", 
        "select"
      ]
    }, 
    "twisted.internet.protocol": {
      "file": "twisted/internet/protocol.py", 
      "imports": [
        "__future__", 
        "random", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.python.components", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.pyuisupport": {
      "file": "twisted/internet/pyuisupport.py", 
      "imports": [
        "pyui", 
        "twisted.internet.reactor"
      ]
    }, 
    "twisted.internet.qtreactor": {
      "file": "twisted/internet/qtreactor.py", 
      "imports": [
        "twisted.plugins.twisted_qtstub", 
        "warnings"
      ]
    }, 
    "twisted.internet.reactor": {
      "file": "twisted/internet/reactor.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.internet.default"
      ]
    }, 
    "twisted.internet.selectreactor": {
      "file": "twisted/internet/selectreactor.py", 
      "imports": [
        "__future__", 
        "errno.EBADF", 
        "errno.EINTR", 
        "socket", 
        "sys", 
        "time.sleep", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.posixbase", 
        "twisted.internet.win32eventreactor", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "zope.interface", 
        "select"
      ]
    }, 
    "twisted.internet.serialport": {
      "file": "twisted/internet/serialport.py", 
      "imports": [
        "os", 
        "serial", 
        "serial.EIGHTBITS", 
        "serial.FIVEBITS", 
        "serial.PARITY_EVEN", 
        "serial.PARITY_NONE", 
        "serial.PARITY_ODD", 
        "serial.SEVENBITS", 
        "serial.SIXBITS", 
        "serial.STOPBITS_ONE", 
        "serial.STOPBITS_TWO", 
        "sys", 
        "twisted.internet._posixserialport", 
        "twisted.internet._win32serialport"
      ]
    }, 
    "twisted.internet.ssl": {
      "file": "twisted/internet/ssl.py", 
      "imports": [
        "OpenSSL.SSL", 
        "__future__", 
        "twisted.internet._sslverify", 
        "twisted.internet.interfaces", 
        "twisted.internet.tcp", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.stdio": {
      "file": "twisted/internet/stdio.py", 
      "imports": [
        "twisted.internet._posixstdio", 
        "twisted.internet._win32stdio", 
        "twisted.python.runtime"
      ]
    }, 
    "twisted.internet.task": {
      "file": "twisted/internet/task.py", 
      "imports": [
        "__future__", 
        "sys", 
        "time", 
        "twisted.internet.base", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.tcp": {
      "file": "twisted/internet/tcp.py", 
      "imports": [
        "__future__", 
        "errno.EAGAIN", 
        "errno.EALREADY", 
        "errno.ECONNABORTED", 
        "errno.ECONNRESET", 
        "errno.EINPROGRESS", 
        "errno.EINTR", 
        "errno.EINVAL", 
        "errno.EISCONN", 
        "errno.EMFILE", 
        "errno.ENFILE", 
        "errno.ENOBUFS", 
        "errno.ENOMEM", 
        "errno.ENOTCONN", 
        "errno.EPERM", 
        "errno.EWOULDBLOCK", 
        "errno.WSAEALREADY", 
        "errno.WSAECONNRESET", 
        "errno.WSAEINPROGRESS", 
        "errno.WSAEINTR", 
        "errno.WSAEINVAL", 
        "errno.WSAEISCONN", 
        "errno.WSAEMFILE", 
        "errno.WSAENOBUFS", 
        "errno.WSAENOTCONN", 
        "errno.WSAEWOULDBLOCK", 
        "errno.errorcode", 
        "operator", 
        "os", 
        "socket", 
        "struct", 
        "sys", 
        "twisted.internet._newtls", 
        "twisted.internet.abstract", 
        "twisted.internet.address", 
        "twisted.internet.base", 
        "twisted.internet.error", 
        "twisted.internet.fdesc", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.task", 
        "twisted.python.compat", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.runtime", 
        "twisted.python.util", 
        "twisted.python.versions", 
        "twisted.python.win32", 
        "types", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.test": {
      "dir": "twisted/internet/test"
    }, 
    "twisted.internet.test.__init__": {
      "file": "twisted/internet/test/__init__.py", 
      "imports": []
    }, 
    "twisted.internet.test._posixifaces": {
      "file": "twisted/internet/test/_posixifaces.py", 
      "imports": [
        "__future__", 
        "ctypes.CDLL", 
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_ubyte", 
        "ctypes.c_uint32", 
        "ctypes.c_uint8", 
        "ctypes.c_ushort", 
        "ctypes.c_void_p", 
        "ctypes.cast", 
        "ctypes.pointer", 
        "ctypes.util.find_library", 
        "socket", 
        "sys", 
        "twisted.python.compat"
      ]
    }, 
    "twisted.internet.test._win32ifaces": {
      "file": "twisted/internet/test/_win32ifaces.py", 
      "imports": [
        "ctypes.POINTER", 
        "ctypes.Structure", 
        "ctypes.WinDLL", 
        "ctypes.byref", 
        "ctypes.c_int", 
        "ctypes.c_void_p", 
        "ctypes.cast", 
        "ctypes.create_string_buffer", 
        "ctypes.string_at", 
        "socket"
      ]
    }, 
    "twisted.internet.test.connectionmixins": {
      "file": "twisted/internet/test/connectionmixins.py", 
      "imports": [
        "__future__", 
        "gc.collect", 
        "socket", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.test.reactormixins", 
        "twisted.python.context", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.test.test_tcp", 
        "twisted.trial.unittest", 
        "weakref", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.internet.test.fakeendpoint": {
      "file": "twisted/internet/test/fakeendpoint.py", 
      "imports": [
        "twisted.internet.interfaces", 
        "twisted.plugin", 
        "zope.interface.declarations"
      ]
    }, 
    "twisted.internet.test.modulehelpers": {
      "file": "twisted/internet/test/modulehelpers.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.internet", 
        "twisted.test.test_twisted"
      ]
    }, 
    "twisted.internet.test.process_gireactornocompat": {
      "file": "twisted/internet/test/process_gireactornocompat.py", 
      "imports": [
        "gobject", 
        "sys", 
        "twisted.internet.gireactor", 
        "twisted.python.modules"
      ]
    }, 
    "twisted.internet.test.process_helper": {
      "file": "twisted/internet/test/process_helper.py", 
      "imports": [
        "os", 
        "sys", 
        "win32api", 
        "win32process"
      ]
    }, 
    "twisted.internet.test.reactormixins": {
      "file": "twisted/internet/test/reactormixins.py", 
      "imports": [
        "__future__", 
        "os", 
        "time", 
        "twisted.internet.cfreactor", 
        "twisted.internet.process", 
        "twisted.internet.reactor", 
        "twisted.python.compat", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "signal"
      ]
    }, 
    "twisted.internet.test.test_abstract": {
      "file": "twisted/internet/test/test_abstract.py", 
      "imports": [
        "__future__", 
        "twisted.internet.abstract", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_address": {
      "file": "twisted/internet/test/test_address.py", 
      "imports": [
        "__future__", 
        "os", 
        "re", 
        "twisted.internet.address", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_base": {
      "file": "twisted/internet/test/test_base.py", 
      "imports": [
        "Queue", 
        "queue.Queue", 
        "socket", 
        "twisted.internet.base", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.task", 
        "twisted.python.threadpool", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.test.test_baseprocess": {
      "file": "twisted/internet/test/test_baseprocess.py", 
      "imports": [
        "twisted.internet._baseprocess", 
        "twisted.python.deprecate", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_core": {
      "file": "twisted/internet/test/test_core.py", 
      "imports": [
        "__future__", 
        "inspect", 
        "time", 
        "twisted.internet.abstract", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.test.reactormixins", 
        "signal"
      ]
    }, 
    "twisted.internet.test.test_default": {
      "file": "twisted/internet/test/test_default.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.internet.default", 
        "twisted.internet.epollreactor", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.internet.test.test_main", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "select"
      ]
    }, 
    "twisted.internet.test.test_endpoints": {
      "file": "twisted/internet/test/test_endpoints.py", 
      "imports": [
        "OpenSSL.SSL.ContextType", 
        "OpenSSL.SSL.SSLv23_METHOD", 
        "OpenSSL.SSL.TLSv1_METHOD", 
        "__future__", 
        "errno.EPERM", 
        "socket", 
        "sys", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.endpoints", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.internet.stdio", 
        "twisted.internet.task", 
        "twisted.internet.threads", 
        "twisted.plugin", 
        "twisted.plugins", 
        "twisted.protocols.basic", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.modules", 
        "twisted.python.runtime", 
        "twisted.python.systemd", 
        "twisted.test", 
        "twisted.test.proto_helpers", 
        "twisted.test.test_sslverify", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.internet.test.test_epollreactor": {
      "file": "twisted/internet/test/test_epollreactor.py", 
      "imports": [
        "__future__", 
        "twisted.internet.epollreactor", 
        "twisted.internet.error", 
        "twisted.internet.task", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_fdset": {
      "file": "twisted/internet/test/test_fdset.py", 
      "imports": [
        "os", 
        "socket", 
        "traceback", 
        "twisted.internet.abstract", 
        "twisted.internet.interfaces", 
        "twisted.internet.tcp", 
        "twisted.internet.test.reactormixins", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.test.test_filedescriptor": {
      "file": "twisted/internet/test/test_filedescriptor.py", 
      "imports": [
        "__future__", 
        "twisted.internet.abstract", 
        "twisted.internet.interfaces", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.internet.test.test_gireactor": {
      "file": "twisted/internet/test/test_gireactor.py", 
      "imports": [
        "__future__", 
        "gi.repository.Gio", 
        "gi.repository.Gtk", 
        "gobject", 
        "os", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.gireactor", 
        "twisted.internet.gtk3reactor", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.test.reactormixins", 
        "twisted.python.filepath", 
        "twisted.python.runtime", 
        "twisted.test.test_twisted", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_glibbase": {
      "file": "twisted/internet/test/test_glibbase.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.internet._glibbase", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_gtkreactor": {
      "file": "twisted/internet/test/test_gtkreactor.py", 
      "imports": [
        "sys", 
        "twisted.internet.gtkreactor", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_inlinecb": {
      "file": "twisted/internet/test/test_inlinecb.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_inotify": {
      "file": "twisted/internet/test/test_inotify.py", 
      "imports": [
        "os", 
        "twisted.internet.defer", 
        "twisted.internet.inotify", 
        "twisted.internet.reactor", 
        "twisted.python._inotify", 
        "twisted.python.filepath", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_iocp": {
      "file": "twisted/internet/test/test_iocp.py", 
      "imports": [
        "array.array", 
        "errno", 
        "socket", 
        "struct", 
        "twisted.internet.interfaces", 
        "twisted.internet.iocpreactor", 
        "twisted.internet.iocpreactor.abstract", 
        "twisted.internet.iocpreactor.const", 
        "twisted.internet.iocpreactor.interfaces", 
        "twisted.internet.iocpreactor.reactor", 
        "twisted.internet.iocpreactor.tcp", 
        "twisted.internet.iocpreactor.udp", 
        "twisted.python.log", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.internet.test.test_main": {
      "file": "twisted/internet/test/test_main.py", 
      "imports": [
        "__future__", 
        "twisted.internet.error", 
        "twisted.internet.main", 
        "twisted.internet.reactor", 
        "twisted.internet.test.modulehelpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_newtls": {
      "file": "twisted/internet/test/test_newtls.py", 
      "imports": [
        "__future__", 
        "twisted.internet._newtls", 
        "twisted.internet.test.connectionmixins", 
        "twisted.internet.test.reactormixins", 
        "twisted.internet.test.test_tcp", 
        "twisted.internet.test.test_tls", 
        "twisted.protocols.tls", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_pollingfile": {
      "file": "twisted/internet/test/test_pollingfile.py", 
      "imports": [
        "twisted.internet._pollingfile", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_posixbase": {
      "file": "twisted/internet/test/test_posixbase.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.posixbase", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.tcp", 
        "twisted.internet.unix", 
        "twisted.python.compat", 
        "twisted.test.test_unix", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_posixprocess": {
      "file": "twisted/internet/test/test_posixprocess.py", 
      "imports": [
        "errno", 
        "fcntl", 
        "os", 
        "sys", 
        "twisted.internet.process", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_process": {
      "file": "twisted/internet/test/test_process.py", 
      "imports": [
        "os", 
        "sys", 
        "threading", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.process", 
        "twisted.internet.protocol", 
        "twisted.internet.test.reactormixins", 
        "twisted.internet.utils", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "signal"
      ]
    }, 
    "twisted.internet.test.test_protocol": {
      "file": "twisted/internet/test/test_protocol.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.python.failure", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.internet.test.test_qtreactor": {
      "file": "twisted/internet/test/test_qtreactor.py", 
      "imports": [
        "sys", 
        "twisted.internet.qtreactor", 
        "twisted.internet.utils", 
        "twisted.plugins.twisted_qtstub", 
        "twisted.python.runtime", 
        "twisted.python.util", 
        "twisted.trial.unittest", 
        "win32process"
      ]
    }, 
    "twisted.internet.test.test_serialport": {
      "file": "twisted/internet/test/test_serialport.py", 
      "imports": [
        "twisted.internet.error", 
        "twisted.internet.protocol", 
        "twisted.internet.serialport", 
        "twisted.python.failure", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_sigchld": {
      "file": "twisted/internet/test/test_sigchld.py", 
      "imports": [
        "__future__", 
        "errno", 
        "os", 
        "twisted.internet._signals", 
        "twisted.internet.fdesc", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "signal"
      ]
    }, 
    "twisted.internet.test.test_socket": {
      "file": "twisted/internet/test/test_socket.py", 
      "imports": [
        "errno", 
        "socket", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.test.reactormixins", 
        "twisted.python.log", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.internet.test.test_stdio": {
      "file": "twisted/internet/test/test_stdio.py", 
      "imports": [
        "twisted.internet._posixstdio", 
        "twisted.internet.protocol", 
        "twisted.internet.test.reactormixins", 
        "twisted.python.runtime"
      ]
    }, 
    "twisted.internet.test.test_tcp": {
      "file": "twisted/internet/test/test_tcp.py", 
      "imports": [
        "OpenSSL.SSL", 
        "__future__", 
        "errno", 
        "functools", 
        "socket", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.endpoints", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.ssl", 
        "twisted.internet.tcp", 
        "twisted.internet.test._posixifaces", 
        "twisted.internet.test._win32ifaces", 
        "twisted.internet.test.connectionmixins", 
        "twisted.internet.test.reactormixins", 
        "twisted.internet.test.test_core", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.test.test_tcp", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.internet.test.test_threads": {
      "file": "twisted/internet/test/test_threads.py", 
      "imports": [
        "__future__", 
        "gc", 
        "threading", 
        "twisted.internet.interfaces", 
        "twisted.internet.test.reactormixins", 
        "twisted.python.threadable", 
        "twisted.python.threadpool", 
        "weakref"
      ]
    }, 
    "twisted.internet.test.test_time": {
      "file": "twisted/internet/test/test_time.py", 
      "imports": [
        "gobject", 
        "twisted.internet.interfaces", 
        "twisted.internet.test.reactormixins", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_tls": {
      "file": "twisted/internet/test/test_tls.py", 
      "imports": [
        "OpenSSL.crypto.FILETYPE_PEM", 
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.endpoints", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.ssl", 
        "twisted.internet.task", 
        "twisted.internet.test.connectionmixins", 
        "twisted.internet.test.reactormixins", 
        "twisted.internet.test.test_core", 
        "twisted.internet.test.test_tcp", 
        "twisted.protocols.tls", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.test.test_udp": {
      "file": "twisted/internet/test/test_udp.py", 
      "imports": [
        "__future__", 
        "socket", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.test.connectionmixins", 
        "twisted.internet.test.reactormixins", 
        "twisted.python.context", 
        "twisted.python.log", 
        "twisted.test.test_udp", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.internet.test.test_udp_internals": {
      "file": "twisted/internet/test/test_udp_internals.py", 
      "imports": [
        "__future__", 
        "errno.ECONNREFUSED", 
        "errno.EWOULDBLOCK", 
        "errno.WSAECONNREFUSED", 
        "errno.WSAEWOULDBLOCK", 
        "socket", 
        "twisted.internet.protocol", 
        "twisted.internet.udp", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.internet.test.test_unix": {
      "file": "twisted/internet/test/test_unix.py", 
      "imports": [
        "hashlib", 
        "os", 
        "pprint", 
        "socket", 
        "stat", 
        "tempfile", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.endpoints", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.task", 
        "twisted.internet.test.connectionmixins", 
        "twisted.internet.test.reactormixins", 
        "twisted.internet.test.test_core", 
        "twisted.internet.test.test_tcp", 
        "twisted.python", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.test.test_win32events": {
      "file": "twisted/internet/test/test_win32events.py", 
      "imports": [
        "thread.get_ident", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.test.reactormixins", 
        "twisted.python.failure", 
        "twisted.python.threadable", 
        "win32event", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.internet.threads": {
      "file": "twisted/internet/threads.py", 
      "imports": [
        "Queue", 
        "__future__", 
        "queue", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.python.compat", 
        "twisted.python.failure"
      ]
    }, 
    "twisted.internet.tksupport": {
      "file": "twisted/internet/tksupport.py", 
      "imports": [
        "Tkinter", 
        "tkMessageBox", 
        "tkSimpleDialog", 
        "twisted.internet.task", 
        "twisted.python.log", 
        "twisted.python.util"
      ]
    }, 
    "twisted.internet.udp": {
      "file": "twisted/internet/udp.py", 
      "imports": [
        "__future__", 
        "errno.EAGAIN", 
        "errno.ECONNREFUSED", 
        "errno.EINTR", 
        "errno.EMSGSIZE", 
        "errno.EWOULDBLOCK", 
        "errno.WSAECONNREFUSED", 
        "errno.WSAECONNRESET", 
        "errno.WSAEINPROGRESS", 
        "errno.WSAEINTR", 
        "errno.WSAEMSGSIZE", 
        "errno.WSAENETRESET", 
        "errno.WSAETIMEDOUT", 
        "errno.WSAEWOULDBLOCK", 
        "operator", 
        "socket", 
        "struct", 
        "twisted.internet.abstract", 
        "twisted.internet.address", 
        "twisted.internet.base", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.unix": {
      "file": "twisted/internet/unix.py", 
      "imports": [
        "errno.EAGAIN", 
        "errno.ECONNREFUSED", 
        "errno.EINTR", 
        "errno.EMSGSIZE", 
        "errno.ENOBUFS", 
        "errno.EWOULDBLOCK", 
        "os", 
        "socket", 
        "stat", 
        "struct", 
        "sys", 
        "twisted.internet.address", 
        "twisted.internet.base", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.protocol", 
        "twisted.internet.tcp", 
        "twisted.internet.udp", 
        "twisted.python", 
        "twisted.python.failure", 
        "twisted.python.lockfile", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.util", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.utils": {
      "file": "twisted/internet/utils.py", 
      "imports": [
        "StringIO", 
        "__future__", 
        "cStringIO", 
        "functools", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "warnings"
      ]
    }, 
    "twisted.internet.win32eventreactor": {
      "file": "twisted/internet/win32eventreactor.py", 
      "imports": [
        "sys", 
        "threading.Thread", 
        "time", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.posixbase", 
        "twisted.internet.threads", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.threadable", 
        "warnings", 
        "weakref", 
        "win32event.CreateEvent", 
        "win32event.MsgWaitForMultipleObjects", 
        "win32event.QS_ALLEVENTS", 
        "win32event.QS_ALLINPUT", 
        "win32event.WAIT_OBJECT_0", 
        "win32event.WAIT_TIMEOUT", 
        "win32file.FD_ACCEPT", 
        "win32file.FD_CLOSE", 
        "win32file.FD_CONNECT", 
        "win32file.FD_READ", 
        "win32file.WSAEnumNetworkEvents", 
        "win32file.WSAEventSelect", 
        "win32gui", 
        "zope.interface"
      ]
    }, 
    "twisted.internet.wxreactor": {
      "file": "twisted/internet/wxreactor.py", 
      "imports": [
        "Queue", 
        "twisted.internet._threadedselect", 
        "twisted.internet.main", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "wx.CallAfter", 
        "wx.PySimpleApp", 
        "wx.Timer", 
        "wxPython.wx.wxCallAfter", 
        "wxPython.wx.wxPySimpleApp", 
        "wxPython.wx.wxTimer", 
        "signal"
      ]
    }, 
    "twisted.internet.wxsupport": {
      "file": "twisted/internet/wxsupport.py", 
      "imports": [
        "twisted.internet.reactor", 
        "twisted.python.runtime", 
        "warnings", 
        "wxPython.wx.wxApp"
      ]
    }, 
    "twisted.manhole": {
      "dir": "twisted/manhole"
    }, 
    "twisted.manhole.__init__": {
      "file": "twisted/manhole/__init__.py", 
      "imports": []
    }, 
    "twisted.manhole._inspectro": {
      "file": "twisted/manhole/_inspectro.py", 
      "imports": [
        "gnome", 
        "gobject", 
        "gtk", 
        "gtk.glade", 
        "sys", 
        "time", 
        "twisted.manhole.ui.gtk2manhole", 
        "twisted.protocols.policies", 
        "twisted.python.components", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.util", 
        "types", 
        "zope.interface"
      ]
    }, 
    "twisted.manhole.explorer": {
      "file": "twisted/manhole/explorer.py", 
      "imports": [
        "UserDict", 
        "inspect", 
        "string", 
        "sys", 
        "twisted.python.reflect", 
        "twisted.spread.pb", 
        "types"
      ]
    }, 
    "twisted.manhole.gladereactor": {
      "file": "twisted/manhole/gladereactor.py", 
      "imports": [
        "gobject", 
        "gtk", 
        "gtk.glade", 
        "twisted.internet.gtk2reactor", 
        "twisted.internet.main", 
        "twisted.manhole._inspectro", 
        "twisted.python.reflect", 
        "twisted.python.util"
      ]
    }, 
    "twisted.manhole.service": {
      "file": "twisted/manhole/service.py", 
      "imports": [
        "linecache", 
        "string", 
        "sys", 
        "traceback", 
        "twisted.application.service", 
        "twisted.copyright", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.manhole.explorer", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.spread.pb", 
        "zope.interface"
      ]
    }, 
    "twisted.manhole.telnet": {
      "file": "twisted/manhole/telnet.py", 
      "imports": [
        "copy", 
        "string", 
        "sys", 
        "twisted.internet.protocol", 
        "twisted.protocols.telnet", 
        "twisted.python.failure", 
        "twisted.python.log"
      ]
    }, 
    "twisted.manhole.test": {
      "dir": "twisted/manhole/test"
    }, 
    "twisted.manhole.test.__init__": {
      "file": "twisted/manhole/test/__init__.py", 
      "imports": []
    }, 
    "twisted.manhole.test.test_explorer": {
      "file": "twisted/manhole/test/test_explorer.py", 
      "imports": [
        "twisted.manhole.explorer", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.manhole.ui": {
      "dir": "twisted/manhole/ui"
    }, 
    "twisted.manhole.ui.__init__": {
      "file": "twisted/manhole/ui/__init__.py", 
      "imports": []
    }, 
    "twisted.manhole.ui.gtk2manhole": {
      "file": "twisted/manhole/ui/gtk2manhole.py", 
      "imports": [
        "code", 
        "gtk", 
        "inspect", 
        "os", 
        "pdb", 
        "sys", 
        "twisted.copyright", 
        "twisted.internet.reactor", 
        "twisted.manhole.service", 
        "twisted.python.components", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.rebuild", 
        "twisted.python.reflect", 
        "twisted.python.util", 
        "twisted.spread.pb", 
        "twisted.spread.ui.gtk2util", 
        "types", 
        "zope.interface"
      ]
    }, 
    "twisted.manhole.ui.test": {
      "dir": "twisted/manhole/ui/test"
    }, 
    "twisted.manhole.ui.test.__init__": {
      "file": "twisted/manhole/ui/test/__init__.py", 
      "imports": []
    }, 
    "twisted.manhole.ui.test.test_gtk2manhole": {
      "file": "twisted/manhole/ui/test/test_gtk2manhole.py", 
      "imports": [
        "gtk", 
        "pygtk", 
        "twisted.manhole.ui.gtk2manhole", 
        "twisted.python.reflect", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.persisted": {
      "dir": "twisted/persisted"
    }, 
    "twisted.persisted.__init__": {
      "file": "twisted/persisted/__init__.py", 
      "imports": []
    }, 
    "twisted.persisted.aot": {
      "file": "twisted/persisted/aot.py", 
      "imports": [
        "copy_reg", 
        "re", 
        "tokenize", 
        "twisted.persisted.crefutil", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "types"
      ]
    }, 
    "twisted.persisted.crefutil": {
      "file": "twisted/persisted/crefutil.py", 
      "imports": [
        "traceback", 
        "twisted.internet.defer", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "types"
      ]
    }, 
    "twisted.persisted.dirdbm": {
      "file": "twisted/persisted/dirdbm.py", 
      "imports": [
        "base64", 
        "cPickle", 
        "glob", 
        "os", 
        "pickle", 
        "types"
      ]
    }, 
    "twisted.persisted.sob": {
      "file": "twisted/persisted/sob.py", 
      "imports": [
        "Crypto.Cipher.AES", 
        "StringIO", 
        "cPickle", 
        "cStringIO", 
        "hashlib", 
        "os", 
        "pickle", 
        "sys", 
        "twisted.persisted.aot", 
        "twisted.persisted.styles", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "zope.interface"
      ]
    }, 
    "twisted.persisted.styles": {
      "file": "twisted/persisted/styles.py", 
      "imports": [
        "StringIO", 
        "cStringIO", 
        "copy", 
        "copy_reg", 
        "gc", 
        "inspect", 
        "sys", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "types"
      ]
    }, 
    "twisted.persisted.test": {
      "dir": "twisted/persisted/test"
    }, 
    "twisted.persisted.test.__init__": {
      "file": "twisted/persisted/test/__init__.py", 
      "imports": []
    }, 
    "twisted.persisted.test.test_styles": {
      "file": "twisted/persisted/test/test_styles.py", 
      "imports": [
        "twisted.persisted.styles", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.plugin": {
      "file": "twisted/plugin.py", 
      "imports": [
        "cPickle", 
        "os", 
        "pickle", 
        "sys", 
        "twisted.plugins", 
        "twisted.python.components", 
        "twisted.python.log", 
        "twisted.python.modules", 
        "twisted.python.reflect", 
        "zope.interface"
      ]
    }, 
    "twisted.plugins": {
      "dir": "twisted/plugins"
    }, 
    "twisted.plugins.__init__": {
      "file": "twisted/plugins/__init__.py", 
      "imports": [
        "twisted.plugin"
      ]
    }, 
    "twisted.plugins.axiom_plugins": {
      "file": "twisted/plugins/axiom_plugins.py", 
      "imports": [
        "axiom.listversions.checkSystemVersion", 
        "axiom.store.Store", 
        "twisted.application.service", 
        "twisted.plugin", 
        "twisted.python.usage", 
        "zope.interface"
      ]
    }, 
    "twisted.plugins.cred_anonymous": {
      "file": "twisted/plugins/cred_anonymous.py", 
      "imports": [
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.strcred", 
        "twisted.plugin", 
        "zope.interface"
      ]
    }, 
    "twisted.plugins.cred_file": {
      "file": "twisted/plugins/cred_file.py", 
      "imports": [
        "sys", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.strcred", 
        "twisted.plugin", 
        "twisted.python.filepath", 
        "zope.interface"
      ]
    }, 
    "twisted.plugins.cred_memory": {
      "file": "twisted/plugins/cred_memory.py", 
      "imports": [
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.strcred", 
        "twisted.plugin", 
        "zope.interface"
      ]
    }, 
    "twisted.plugins.cred_sshkeys": {
      "file": "twisted/plugins/cred_sshkeys.py", 
      "imports": [
        "twisted.conch.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.strcred", 
        "twisted.plugin", 
        "zope.interface"
      ]
    }, 
    "twisted.plugins.cred_unix": {
      "file": "twisted/plugins/cred_unix.py", 
      "imports": [
        "crypt", 
        "pwd", 
        "spwd", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.strcred", 
        "twisted.internet.defer", 
        "twisted.plugin", 
        "zope.interface"
      ]
    }, 
    "twisted.plugins.nevow_widget": {
      "file": "twisted/plugins/nevow_widget.py", 
      "imports": [
        "twisted.application.service", 
        "twisted.scripts"
      ]
    }, 
    "twisted.plugins.twisted_conch": {
      "file": "twisted/plugins/twisted_conch.py", 
      "imports": [
        "twisted.application.service"
      ]
    }, 
    "twisted.plugins.twisted_core": {
      "file": "twisted/plugins/twisted_core.py", 
      "imports": [
        "twisted.internet.endpoints"
      ]
    }, 
    "twisted.plugins.twisted_ftp": {
      "file": "twisted/plugins/twisted_ftp.py", 
      "imports": [
        "twisted.application.service"
      ]
    }, 
    "twisted.plugins.twisted_inet": {
      "file": "twisted/plugins/twisted_inet.py", 
      "imports": [
        "twisted.application.service"
      ]
    }, 
    "twisted.plugins.twisted_manhole": {
      "file": "twisted/plugins/twisted_manhole.py", 
      "imports": [
        "twisted.application.service"
      ]
    }, 
    "twisted.plugins.twisted_portforward": {
      "file": "twisted/plugins/twisted_portforward.py", 
      "imports": [
        "twisted.application.service"
      ]
    }, 
    "twisted.plugins.twisted_qtstub": {
      "file": "twisted/plugins/twisted_qtstub.py", 
      "imports": [
        "twisted.application.reactors", 
        "warnings"
      ]
    }, 
    "twisted.plugins.twisted_reactors": {
      "file": "twisted/plugins/twisted_reactors.py", 
      "imports": [
        "twisted.application.reactors"
      ]
    }, 
    "twisted.plugins.twisted_socks": {
      "file": "twisted/plugins/twisted_socks.py", 
      "imports": [
        "twisted.application.service"
      ]
    }, 
    "twisted.plugins.twisted_telnet": {
      "file": "twisted/plugins/twisted_telnet.py", 
      "imports": [
        "twisted.application.service"
      ]
    }, 
    "twisted.plugins.twisted_trial": {
      "file": "twisted/plugins/twisted_trial.py", 
      "imports": [
        "twisted.plugin", 
        "twisted.trial.itrial", 
        "zope.interface"
      ]
    }, 
    "twisted.plugins.twisted_web": {
      "file": "twisted/plugins/twisted_web.py", 
      "imports": [
        "twisted.application.service"
      ]
    }, 
    "twisted.positioning": {
      "dir": "twisted/positioning"
    }, 
    "twisted.positioning.__init__": {
      "file": "twisted/positioning/__init__.py", 
      "imports": []
    }, 
    "twisted.positioning._sentence": {
      "file": "twisted/positioning/_sentence.py", 
      "imports": []
    }, 
    "twisted.positioning.base": {
      "file": "twisted/positioning/base.py", 
      "imports": [
        "functools", 
        "operator.attrgetter", 
        "twisted.positioning.ipositioning", 
        "twisted.python.constants", 
        "twisted.python.util", 
        "zope.interface"
      ]
    }, 
    "twisted.positioning.ipositioning": {
      "file": "twisted/positioning/ipositioning.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "twisted.positioning.nmea": {
      "file": "twisted/positioning/nmea.py", 
      "imports": [
        "datetime", 
        "itertools", 
        "operator", 
        "twisted.positioning._sentence", 
        "twisted.positioning.base", 
        "twisted.positioning.ipositioning", 
        "twisted.protocols.basic", 
        "twisted.python.compat", 
        "twisted.python.constants", 
        "zope.interface"
      ]
    }, 
    "twisted.positioning.test": {
      "dir": "twisted/positioning/test"
    }, 
    "twisted.positioning.test.__init__": {
      "file": "twisted/positioning/test/__init__.py", 
      "imports": []
    }, 
    "twisted.positioning.test.receiver": {
      "file": "twisted/positioning/test/receiver.py", 
      "imports": [
        "twisted.positioning.base", 
        "twisted.positioning.ipositioning"
      ]
    }, 
    "twisted.positioning.test.test_base": {
      "file": "twisted/positioning/test/test_base.py", 
      "imports": [
        "twisted.positioning.base", 
        "twisted.positioning.ipositioning", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.positioning.test.test_nmea": {
      "file": "twisted/positioning/test/test_nmea.py", 
      "imports": [
        "datetime", 
        "operator.attrgetter", 
        "twisted.positioning.base", 
        "twisted.positioning.ipositioning", 
        "twisted.positioning.nmea", 
        "twisted.positioning.test.receiver", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.positioning.test.test_sentence": {
      "file": "twisted/positioning/test/test_sentence.py", 
      "imports": [
        "itertools", 
        "twisted.positioning._sentence", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.protocols": {
      "dir": "twisted/protocols"
    }, 
    "twisted.protocols.__init__": {
      "file": "twisted/protocols/__init__.py", 
      "imports": []
    }, 
    "twisted.protocols.amp": {
      "file": "twisted/protocols/amp.py", 
      "imports": [
        "cStringIO", 
        "datetime", 
        "decimal", 
        "itertools.count", 
        "struct", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.ssl", 
        "twisted.protocols.basic", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "types", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.protocols.basic": {
      "file": "twisted/protocols/basic.py", 
      "imports": [
        "__future__", 
        "io", 
        "math", 
        "re", 
        "struct", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.python.compat", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.protocols.dict": {
      "file": "twisted/protocols/dict.py", 
      "imports": [
        "StringIO", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.protocols.basic", 
        "twisted.python.log"
      ]
    }, 
    "twisted.protocols.finger": {
      "file": "twisted/protocols/finger.py", 
      "imports": [
        "twisted.protocols.basic"
      ]
    }, 
    "twisted.protocols.ftp": {
      "file": "twisted/protocols/ftp.py", 
      "imports": [
        "errno", 
        "fnmatch", 
        "grp", 
        "operator", 
        "os", 
        "pwd", 
        "re", 
        "stat", 
        "time", 
        "twisted.copyright", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.protocols.basic", 
        "twisted.protocols.policies", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.protocols.gps": {
      "dir": "twisted/protocols/gps"
    }, 
    "twisted.protocols.gps.__init__": {
      "file": "twisted/protocols/gps/__init__.py", 
      "imports": []
    }, 
    "twisted.protocols.gps.nmea": {
      "file": "twisted/protocols/gps/nmea.py", 
      "imports": [
        "functools", 
        "operator", 
        "twisted.protocols.basic"
      ]
    }, 
    "twisted.protocols.gps.rockwell": {
      "file": "twisted/protocols/gps/rockwell.py", 
      "imports": [
        "math", 
        "struct", 
        "twisted.internet.protocol", 
        "twisted.python.log"
      ]
    }, 
    "twisted.protocols.htb": {
      "file": "twisted/protocols/htb.py", 
      "imports": [
        "time.time", 
        "twisted.protocols.pcp", 
        "zope.interface"
      ]
    }, 
    "twisted.protocols.ident": {
      "file": "twisted/protocols/ident.py", 
      "imports": [
        "pwd", 
        "struct", 
        "twisted.internet.defer", 
        "twisted.protocols.basic", 
        "twisted.python.failure", 
        "twisted.python.log"
      ]
    }, 
    "twisted.protocols.loopback": {
      "file": "twisted/protocols/loopback.py", 
      "imports": [
        "__future__", 
        "tempfile", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.protocols.policies", 
        "twisted.python.failure", 
        "zope.interface"
      ]
    }, 
    "twisted.protocols.memcache": {
      "file": "twisted/protocols/memcache.py", 
      "imports": [
        "collections", 
        "twisted.internet.defer", 
        "twisted.protocols.basic", 
        "twisted.protocols.policies", 
        "twisted.python.log"
      ]
    }, 
    "twisted.protocols.mice": {
      "dir": "twisted/protocols/mice"
    }, 
    "twisted.protocols.mice.__init__": {
      "file": "twisted/protocols/mice/__init__.py", 
      "imports": []
    }, 
    "twisted.protocols.mice.mouseman": {
      "file": "twisted/protocols/mice/mouseman.py", 
      "imports": [
        "twisted.internet.protocol"
      ]
    }, 
    "twisted.protocols.pcp": {
      "file": "twisted/protocols/pcp.py", 
      "imports": [
        "twisted.internet.interfaces", 
        "zope.interface"
      ]
    }, 
    "twisted.protocols.policies": {
      "file": "twisted/protocols/policies.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.protocols.portforward": {
      "file": "twisted/protocols/portforward.py", 
      "imports": [
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.log"
      ]
    }, 
    "twisted.protocols.postfix": {
      "file": "twisted/protocols/postfix.py", 
      "imports": [
        "UserDict", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.protocols.basic", 
        "twisted.protocols.policies", 
        "twisted.python.log", 
        "urllib"
      ]
    }, 
    "twisted.protocols.shoutcast": {
      "file": "twisted/protocols/shoutcast.py", 
      "imports": [
        "twisted.copyright", 
        "twisted.web.http"
      ]
    }, 
    "twisted.protocols.sip": {
      "file": "twisted/protocols/sip.py", 
      "imports": [
        "hashlib", 
        "random", 
        "socket", 
        "sys", 
        "time", 
        "twisted.cred", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.protocols.basic", 
        "twisted.python.deprecate", 
        "twisted.python.log", 
        "twisted.python.util", 
        "twisted.python.versions", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.protocols.socks": {
      "file": "twisted/protocols/socks.py", 
      "imports": [
        "socket", 
        "string", 
        "struct", 
        "time", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.log"
      ]
    }, 
    "twisted.protocols.stateful": {
      "file": "twisted/protocols/stateful.py", 
      "imports": [
        "StringIO", 
        "cStringIO", 
        "twisted.internet.protocol"
      ]
    }, 
    "twisted.protocols.telnet": {
      "file": "twisted/protocols/telnet.py", 
      "imports": [
        "StringIO", 
        "cStringIO", 
        "twisted.copyright", 
        "twisted.internet.protocol", 
        "warnings"
      ]
    }, 
    "twisted.protocols.test": {
      "dir": "twisted/protocols/test"
    }, 
    "twisted.protocols.test.__init__": {
      "file": "twisted/protocols/test/__init__.py", 
      "imports": []
    }, 
    "twisted.protocols.test.test_basic": {
      "file": "twisted/protocols/test/test_basic.py", 
      "imports": [
        "__future__", 
        "io", 
        "struct", 
        "sys", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.task", 
        "twisted.protocols.basic", 
        "twisted.python.compat", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.protocols.test.test_tls": {
      "file": "twisted/protocols/test/test_tls.py", 
      "imports": [
        "OpenSSL.SSL.ConnectionType", 
        "OpenSSL.SSL.Context", 
        "OpenSSL.SSL.Error", 
        "OpenSSL.SSL.TLSv1_METHOD", 
        "OpenSSL.SSL.WantReadError", 
        "OpenSSL.crypto.X509Type", 
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.ssl", 
        "twisted.internet.task", 
        "twisted.protocols.loopback", 
        "twisted.protocols.tls", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.test.proto_helpers", 
        "twisted.test.ssl_helpers", 
        "twisted.test.test_tcp", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.protocols.tls": {
      "file": "twisted/protocols/tls.py", 
      "imports": [
        "OpenSSL.SSL.Connection", 
        "OpenSSL.SSL.Context", 
        "OpenSSL.SSL.Error", 
        "OpenSSL.SSL.TLSv1_METHOD", 
        "OpenSSL.SSL.WantReadError", 
        "OpenSSL.SSL.ZeroReturnError", 
        "__future__", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.protocol", 
        "twisted.internet.task", 
        "twisted.protocols.policies", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "zope.interface"
      ]
    }, 
    "twisted.protocols.wire": {
      "file": "twisted/protocols/wire.py", 
      "imports": [
        "struct", 
        "time", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "zope.interface"
      ]
    }, 
    "twisted.python": {
      "dir": "twisted/python"
    }, 
    "twisted.python.__init__": {
      "file": "twisted/python/__init__.py", 
      "imports": []
    }, 
    "twisted.python._inotify": {
      "file": "twisted/python/_inotify.py", 
      "imports": [
        "ctypes", 
        "ctypes.util"
      ]
    }, 
    "twisted.python._release": {
      "file": "twisted/python/_release.py", 
      "imports": [
        "datetime", 
        "os", 
        "pydoctor.driver.main", 
        "re", 
        "sys", 
        "tarfile", 
        "tempfile", 
        "textwrap", 
        "twisted.python.compat", 
        "twisted.python.dist", 
        "twisted.python.filepath", 
        "twisted.python.usage", 
        "twisted.python.versions", 
        "subprocess"
      ]
    }, 
    "twisted.python._shellcomp": {
      "file": "twisted/python/_shellcomp.py", 
      "imports": [
        "getopt", 
        "inspect", 
        "itertools", 
        "twisted.python.reflect", 
        "twisted.python.usage", 
        "twisted.python.util"
      ]
    }, 
    "twisted.python._textattributes": {
      "file": "twisted/python/_textattributes.py", 
      "imports": [
        "twisted.python.util"
      ]
    }, 
    "twisted.python.compat": {
      "file": "twisted/python/compat.py", 
      "imports": [
        "__future__", 
        "functools", 
        "io", 
        "socket", 
        "string", 
        "struct", 
        "sys", 
        "types"
      ]
    }, 
    "twisted.python.components": {
      "file": "twisted/python/components.py", 
      "imports": [
        "__future__", 
        "pprint", 
        "twisted.python.compat", 
        "twisted.python.reflect", 
        "zope.interface.adapter", 
        "zope.interface.declarations", 
        "zope.interface.interface"
      ]
    }, 
    "twisted.python.constants": {
      "file": "twisted/python/constants.py", 
      "imports": [
        "__future__", 
        "functools", 
        "itertools.count", 
        "operator.and_", 
        "operator.or_", 
        "operator.xor"
      ]
    }, 
    "twisted.python.context": {
      "file": "twisted/python/context.py", 
      "imports": [
        "__future__", 
        "_threading_local"
      ]
    }, 
    "twisted.python.deprecate": {
      "file": "twisted/python/deprecate.py", 
      "imports": [
        "__future__", 
        "dis", 
        "functools", 
        "inspect", 
        "sys", 
        "twisted.python.versions", 
        "warnings"
      ]
    }, 
    "twisted.python.dist": {
      "file": "twisted/python/dist.py", 
      "imports": [
        "distutils.command.build_ext", 
        "distutils.command.build_scripts", 
        "distutils.command.install_data", 
        "distutils.core", 
        "distutils.errors", 
        "fnmatch", 
        "os", 
        "platform", 
        "sys", 
        "twisted.copyright", 
        "twisted.python.compat"
      ]
    }, 
    "twisted.python.dist3": {
      "file": "twisted/python/dist3.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "twisted.python.failure": {
      "file": "twisted/python/failure.py", 
      "imports": [
        "__future__", 
        "inspect", 
        "linecache", 
        "opcode", 
        "pdb", 
        "sys", 
        "twisted.python.compat", 
        "twisted.python.log", 
        "twisted.python.reflect"
      ]
    }, 
    "twisted.python.fakepwd": {
      "file": "twisted/python/fakepwd.py", 
      "imports": []
    }, 
    "twisted.python.filepath": {
      "file": "twisted/python/filepath.py", 
      "imports": [
        "__future__", 
        "base64", 
        "errno", 
        "glob", 
        "hashlib", 
        "os", 
        "stat", 
        "twisted.python.compat", 
        "twisted.python.runtime", 
        "twisted.python.util", 
        "twisted.python.win32", 
        "zope.interface"
      ]
    }, 
    "twisted.python.finalize": {
      "file": "twisted/python/finalize.py", 
      "imports": [
        "gc", 
        "weakref"
      ]
    }, 
    "twisted.python.formmethod": {
      "file": "twisted/python/formmethod.py", 
      "imports": [
        "calendar"
      ]
    }, 
    "twisted.python.hashlib": {
      "file": "twisted/python/hashlib.py", 
      "imports": [
        "__future__", 
        "hashlib", 
        "warnings"
      ]
    }, 
    "twisted.python.hook": {
      "file": "twisted/python/hook.py", 
      "imports": []
    }, 
    "twisted.python.htmlizer": {
      "file": "twisted/python/htmlizer.py", 
      "imports": [
        "cgi", 
        "keyword", 
        "sys", 
        "tokenize", 
        "twisted.python.reflect"
      ]
    }, 
    "twisted.python.lockfile": {
      "file": "twisted/python/lockfile.py", 
      "imports": [
        "errno", 
        "os", 
        "pywintypes", 
        "time.time", 
        "twisted.python.runtime", 
        "win32api.OpenProcess"
      ]
    }, 
    "twisted.python.log": {
      "file": "twisted/python/log.py", 
      "imports": [
        "__future__", 
        "datetime", 
        "logging", 
        "sys", 
        "time", 
        "twisted.python.compat", 
        "twisted.python.context", 
        "twisted.python.failure", 
        "twisted.python.reflect", 
        "twisted.python.threadable", 
        "twisted.python.util", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.python.logfile": {
      "file": "twisted/python/logfile.py", 
      "imports": [
        "glob", 
        "os", 
        "stat", 
        "time", 
        "twisted.python.threadable"
      ]
    }, 
    "twisted.python.modules": {
      "file": "twisted/python/modules.py", 
      "imports": [
        "inspect", 
        "os", 
        "sys", 
        "twisted.python.components", 
        "twisted.python.filepath", 
        "twisted.python.reflect", 
        "twisted.python.zippath", 
        "warnings", 
        "zipimport", 
        "zope.interface"
      ]
    }, 
    "twisted.python.monkey": {
      "file": "twisted/python/monkey.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "twisted.python.procutils": {
      "file": "twisted/python/procutils.py", 
      "imports": [
        "os"
      ]
    }, 
    "twisted.python.randbytes": {
      "file": "twisted/python/randbytes.py", 
      "imports": [
        "__future__", 
        "os", 
        "random", 
        "string", 
        "twisted.python.compat", 
        "warnings"
      ]
    }, 
    "twisted.python.rebuild": {
      "file": "twisted/python/rebuild.py", 
      "imports": [
        "gc", 
        "linecache", 
        "sys", 
        "time", 
        "twisted.python.components", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "types"
      ]
    }, 
    "twisted.python.reflect": {
      "file": "twisted/python/reflect.py", 
      "imports": [
        "__future__", 
        "collections", 
        "os", 
        "pickle", 
        "re", 
        "sys", 
        "traceback", 
        "twisted.python.compat", 
        "twisted.python.deprecate", 
        "twisted.python.versions", 
        "types", 
        "warnings", 
        "weakref"
      ]
    }, 
    "twisted.python.release": {
      "file": "twisted/python/release.py", 
      "imports": [
        "os"
      ]
    }, 
    "twisted.python.roots": {
      "file": "twisted/python/roots.py", 
      "imports": [
        "twisted.python.reflect", 
        "types"
      ]
    }, 
    "twisted.python.runtime": {
      "file": "twisted/python/runtime.py", 
      "imports": [
        "__future__", 
        "imp", 
        "os", 
        "sys", 
        "time", 
        "twisted.python._inotify", 
        "twisted.python.compat", 
        "warnings"
      ]
    }, 
    "twisted.python.shortcut": {
      "file": "twisted/python/shortcut.py", 
      "imports": [
        "os", 
        "pythoncom", 
        "win32com.shell.shell"
      ]
    }, 
    "twisted.python.syslog": {
      "file": "twisted/python/syslog.py", 
      "imports": [
        "twisted.python.log"
      ]
    }, 
    "twisted.python.systemd": {
      "file": "twisted/python/systemd.py", 
      "imports": [
        "os"
      ]
    }, 
    "twisted.python.test": {
      "dir": "twisted/python/test"
    }, 
    "twisted.python.test.__init__": {
      "file": "twisted/python/test/__init__.py", 
      "imports": []
    }, 
    "twisted.python.test.deprecatedattributes": {
      "file": "twisted/python/test/deprecatedattributes.py", 
      "imports": [
        "__future__", 
        "twisted.python.deprecate", 
        "twisted.python.versions"
      ]
    }, 
    "twisted.python.test.modules_helpers": {
      "file": "twisted/python/test/modules_helpers.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.python.filepath"
      ]
    }, 
    "twisted.python.test.pullpipe": {
      "file": "twisted/python/test/pullpipe.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "twisted.python"
      ]
    }, 
    "twisted.python.test.test_components": {
      "file": "twisted/python/test/test_components.py", 
      "imports": [
        "__future__", 
        "twisted.python.compat", 
        "twisted.python.components", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.adapter"
      ]
    }, 
    "twisted.python.test.test_constants": {
      "file": "twisted/python/test/test_constants.py", 
      "imports": [
        "__future__", 
        "twisted.python.constants", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_deprecate": {
      "file": "twisted/python/test/test_deprecate.py", 
      "imports": [
        "__future__", 
        "importlib", 
        "inspect", 
        "os", 
        "package.module", 
        "sys", 
        "twisted.python.deprecate", 
        "twisted.python.filepath", 
        "twisted.python.test.deprecatedattributes", 
        "twisted.python.test.modules_helpers", 
        "twisted.python.versions", 
        "twisted.trial.unittest", 
        "twisted_private_helper.module", 
        "twisted_renamed_helper.module", 
        "types", 
        "warnings"
      ]
    }, 
    "twisted.python.test.test_dist": {
      "file": "twisted/python/test/test_dist.py", 
      "imports": [
        "distutils.core", 
        "os", 
        "sys", 
        "twisted.python.dist", 
        "twisted.python.filepath", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_dist3": {
      "file": "twisted/python/test/test_dist3.py", 
      "imports": [
        "__future__", 
        "os", 
        "twisted", 
        "twisted.python.dist3", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_fakepwd": {
      "file": "twisted/python/test/test_fakepwd.py", 
      "imports": [
        "operator.getitem", 
        "os", 
        "pwd", 
        "spwd", 
        "twisted.python.fakepwd", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_hashlib": {
      "file": "twisted/python/test/test_hashlib.py", 
      "imports": [
        "twisted.python.hashlib", 
        "twisted.trial.unittest", 
        "twisted.trial.util"
      ]
    }, 
    "twisted.python.test.test_htmlizer": {
      "file": "twisted/python/test/test_htmlizer.py", 
      "imports": [
        "StringIO", 
        "twisted.python.htmlizer", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_inotify": {
      "file": "twisted/python/test/test_inotify.py", 
      "imports": [
        "ctypes.c_char_p", 
        "ctypes.c_int", 
        "ctypes.c_uint32", 
        "twisted.python._inotify", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_release": {
      "file": "twisted/python/test/test_release.py", 
      "imports": [
        "StringIO", 
        "datetime", 
        "glob", 
        "operator", 
        "os", 
        "pydoctor.driver", 
        "sys", 
        "tarfile", 
        "textwrap", 
        "twisted.python._release", 
        "twisted.python.compat", 
        "twisted.python.filepath", 
        "twisted.python.procutils", 
        "twisted.python.release", 
        "twisted.python.versions", 
        "twisted.trial.unittest", 
        "twisted.web.microdom"
      ]
    }, 
    "twisted.python.test.test_runtime": {
      "file": "twisted/python/test/test_runtime.py", 
      "imports": [
        "__future__", 
        "sys", 
        "threading", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "twisted.trial.util"
      ]
    }, 
    "twisted.python.test.test_sendmsg": {
      "file": "twisted/python/test/test_sendmsg.py", 
      "imports": [
        "errno", 
        "os", 
        "socket", 
        "struct", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python", 
        "twisted.python.filepath", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_shellcomp": {
      "file": "twisted/python/test/test_shellcomp.py", 
      "imports": [
        "cStringIO", 
        "sys", 
        "twisted.python._shellcomp", 
        "twisted.python.reflect", 
        "twisted.python.usage", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_syslog": {
      "file": "twisted/python/test/test_syslog.py", 
      "imports": [
        "twisted.python.failure", 
        "twisted.python.syslog", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_systemd": {
      "file": "twisted/python/test/test_systemd.py", 
      "imports": [
        "os", 
        "twisted.python.systemd", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_textattributes": {
      "file": "twisted/python/test/test_textattributes.py", 
      "imports": [
        "twisted.python._textattributes", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_urlpath": {
      "file": "twisted/python/test/test_urlpath.py", 
      "imports": [
        "twisted.python.urlpath", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_util": {
      "file": "twisted/python/test/test_util.py", 
      "imports": [
        "__future__", 
        "errno", 
        "grp", 
        "os", 
        "pwd", 
        "shutil", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.compat", 
        "twisted.python.reflect", 
        "twisted.python.util", 
        "twisted.test.test_process", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "warnings"
      ]
    }, 
    "twisted.python.test.test_versions": {
      "file": "twisted/python/test/test_versions.py", 
      "imports": [
        "__future__", 
        "io", 
        "operator", 
        "sys", 
        "twisted.python.filepath", 
        "twisted.python.versions", 
        "twisted.trial.unittest", 
        "twisted_python_versions_package"
      ]
    }, 
    "twisted.python.test.test_win32": {
      "file": "twisted/python/test/test_win32.py", 
      "imports": [
        "twisted.python.runtime", 
        "twisted.python.win32", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.python.test.test_zippath": {
      "file": "twisted/python/test/test_zippath.py", 
      "imports": [
        "os", 
        "twisted.python.zippath", 
        "twisted.test.test_paths", 
        "zipfile"
      ]
    }, 
    "twisted.python.test.test_zipstream": {
      "file": "twisted/python/test/test_zipstream.py", 
      "imports": [
        "random", 
        "twisted.python.filepath", 
        "twisted.python.hashlib", 
        "twisted.python.zipstream", 
        "twisted.trial.unittest", 
        "zipfile"
      ]
    }, 
    "twisted.python.text": {
      "file": "twisted/python/text.py", 
      "imports": []
    }, 
    "twisted.python.threadable": {
      "file": "twisted/python/threadable.py", 
      "imports": [
        "__future__", 
        "functools", 
        "threading"
      ]
    }, 
    "twisted.python.threadpool": {
      "file": "twisted/python/threadpool.py", 
      "imports": [
        "Queue", 
        "__future__", 
        "contextlib", 
        "copy", 
        "queue.Queue", 
        "threading", 
        "twisted.python.context", 
        "twisted.python.failure", 
        "twisted.python.log"
      ]
    }, 
    "twisted.python.urlpath": {
      "file": "twisted/python/urlpath.py", 
      "imports": [
        "urllib", 
        "urlparse"
      ]
    }, 
    "twisted.python.usage": {
      "file": "twisted/python/usage.py", 
      "imports": [
        "__main__", 
        "getopt", 
        "os", 
        "sys", 
        "twisted.copyright", 
        "twisted.python._shellcomp", 
        "twisted.python.reflect", 
        "twisted.python.text", 
        "twisted.python.util"
      ]
    }, 
    "twisted.python.util": {
      "file": "twisted/python/util.py", 
      "imports": [
        "UserDict", 
        "__future__", 
        "cStringIO", 
        "errno", 
        "getpass", 
        "grp", 
        "os", 
        "pwd", 
        "sys", 
        "twisted", 
        "twisted.python", 
        "twisted.python.compat", 
        "twisted.python.reflect", 
        "warnings"
      ]
    }, 
    "twisted.python.versions": {
      "file": "twisted/python/versions.py", 
      "imports": [
        "__future__", 
        "os", 
        "sys", 
        "twisted.python.compat", 
        "xml.dom.minidom"
      ]
    }, 
    "twisted.python.win32": {
      "file": "twisted/python/win32.py", 
      "imports": [
        "__future__", 
        "ctypes.WinError", 
        "os", 
        "re", 
        "socket", 
        "twisted.python.runtime", 
        "win32api", 
        "win32api.FormatMessage", 
        "win32con"
      ]
    }, 
    "twisted.python.zippath": {
      "file": "twisted/python/zippath.py", 
      "imports": [
        "errno", 
        "os", 
        "sys", 
        "time", 
        "twisted.python.filepath", 
        "twisted.python.zipstream", 
        "zipfile", 
        "zope.interface"
      ]
    }, 
    "twisted.python.zipstream": {
      "file": "twisted/python/zipstream.py", 
      "imports": [
        "os", 
        "struct", 
        "zipfile", 
        "zlib"
      ]
    }, 
    "twisted.scripts": {
      "dir": "twisted/scripts"
    }, 
    "twisted.scripts.__init__": {
      "file": "twisted/scripts/__init__.py", 
      "imports": [
        "twisted.python.deprecate", 
        "twisted.python.versions"
      ]
    }, 
    "twisted.scripts._twistd_unix": {
      "file": "twisted/scripts/_twistd_unix.py", 
      "imports": [
        "errno", 
        "os", 
        "sys", 
        "twisted.application.app", 
        "twisted.application.service", 
        "twisted.copyright", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.python.log", 
        "twisted.python.logfile", 
        "twisted.python.syslog", 
        "twisted.python.usage", 
        "twisted.python.util", 
        "signal"
      ]
    }, 
    "twisted.scripts._twistw": {
      "file": "twisted/scripts/_twistw.py", 
      "imports": [
        "os", 
        "sys", 
        "twisted.application.app", 
        "twisted.application.internet", 
        "twisted.application.service", 
        "twisted.copyright", 
        "twisted.python.log"
      ]
    }, 
    "twisted.scripts.htmlizer": {
      "file": "twisted/scripts/htmlizer.py", 
      "imports": [
        "os", 
        "sys", 
        "twisted.copyright", 
        "twisted.python.htmlizer", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.scripts.manhole": {
      "file": "twisted/scripts/manhole.py", 
      "imports": [
        "sys", 
        "twisted.internet.gtk2reactor", 
        "twisted.internet.reactor", 
        "twisted.manhole.ui.gtk2manhole", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.scripts.tap2deb": {
      "file": "twisted/scripts/tap2deb.py", 
      "imports": [
        "email.utils", 
        "os", 
        "shutil", 
        "sys", 
        "twisted.python.filepath", 
        "twisted.python.usage", 
        "subprocess"
      ]
    }, 
    "twisted.scripts.tap2rpm": {
      "file": "twisted/scripts/tap2rpm.py", 
      "imports": [
        "StringIO", 
        "glob", 
        "os", 
        "shutil", 
        "sys", 
        "tarfile", 
        "tempfile", 
        "time", 
        "twisted.python.deprecate", 
        "twisted.python.log", 
        "twisted.python.usage", 
        "twisted.python.versions", 
        "warnings", 
        "subprocess"
      ]
    }, 
    "twisted.scripts.tapconvert": {
      "file": "twisted/scripts/tapconvert.py", 
      "imports": [
        "getpass", 
        "sys", 
        "twisted.application.app", 
        "twisted.persisted.sob", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.scripts.test": {
      "dir": "twisted/scripts/test"
    }, 
    "twisted.scripts.test.__init__": {
      "file": "twisted/scripts/test/__init__.py", 
      "imports": []
    }, 
    "twisted.scripts.test.test_scripts": {
      "file": "twisted/scripts/test/test_scripts.py", 
      "imports": [
        "os", 
        "sys.executable", 
        "twisted.copyright", 
        "twisted.python.filepath", 
        "twisted.python.modules", 
        "twisted.python.test.test_shellcomp", 
        "twisted.scripts.tapconvert", 
        "twisted.scripts.tkunzip", 
        "twisted.trial.unittest", 
        "subprocess"
      ]
    }, 
    "twisted.scripts.test.test_tap2deb": {
      "file": "twisted/scripts/test/test_tap2deb.py", 
      "imports": [
        "twisted.python.filepath", 
        "twisted.python.procutils", 
        "twisted.python.usage", 
        "twisted.scripts.tap2deb", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.scripts.test.test_tap2rpm": {
      "file": "twisted/scripts/test/test_tap2rpm.py", 
      "imports": [
        "os", 
        "twisted.internet.utils", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.procutils", 
        "twisted.python.versions", 
        "twisted.scripts.tap2rpm", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.scripts.tkunzip": {
      "file": "twisted/scripts/tkunzip.py", 
      "imports": [
        "Tkinter", 
        "Tkinter.*", 
        "compileall", 
        "distutils.sysconfig", 
        "os", 
        "py_compile", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.tksupport", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.procutils", 
        "twisted.python.usage", 
        "twisted.python.util", 
        "twisted.python.zipstream", 
        "twisted.scripts.tkunzip", 
        "zipfile"
      ]
    }, 
    "twisted.scripts.trial": {
      "file": "twisted/scripts/trial.py", 
      "imports": [
        "__future__", 
        "gc", 
        "inspect", 
        "os", 
        "pdb", 
        "random", 
        "readline", 
        "sys", 
        "time", 
        "trace", 
        "twisted.application.app", 
        "twisted.internet.defer", 
        "twisted.plugin", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.reflect", 
        "twisted.python.usage", 
        "twisted.python.util", 
        "twisted.trial._dist.disttrial", 
        "twisted.trial.itrial", 
        "twisted.trial.reporter", 
        "twisted.trial.runner", 
        "warnings"
      ]
    }, 
    "twisted.scripts.twistd": {
      "file": "twisted/scripts/twistd.py", 
      "imports": [
        "twisted.application.app", 
        "twisted.python.runtime", 
        "twisted.scripts._twistd_unix", 
        "twisted.scripts._twistw"
      ]
    }, 
    "twisted.spread": {
      "dir": "twisted/spread"
    }, 
    "twisted.spread.__init__": {
      "file": "twisted/spread/__init__.py", 
      "imports": []
    }, 
    "twisted.spread.banana": {
      "file": "twisted/spread/banana.py", 
      "imports": [
        "cStringIO", 
        "copy", 
        "struct", 
        "twisted.internet.protocol", 
        "twisted.persisted.styles", 
        "twisted.python.log"
      ]
    }, 
    "twisted.spread.flavors": {
      "file": "twisted/spread/flavors.py", 
      "imports": [
        "sys", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.spread.jelly", 
        "twisted.spread.pb", 
        "zope.interface"
      ]
    }, 
    "twisted.spread.interfaces": {
      "file": "twisted/spread/interfaces.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "twisted.spread.jelly": {
      "file": "twisted/spread/jelly.py", 
      "imports": [
        "copy", 
        "datetime", 
        "decimal", 
        "functools", 
        "pickle", 
        "sets", 
        "twisted.persisted.crefutil", 
        "twisted.python.reflect", 
        "twisted.spread.interfaces", 
        "types", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.spread.pb": {
      "file": "twisted/spread/pb.py", 
      "imports": [
        "hashlib", 
        "random", 
        "twisted.cred.credentials", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.persisted.styles", 
        "twisted.python.components", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.spread.banana", 
        "twisted.spread.flavors", 
        "twisted.spread.interfaces", 
        "twisted.spread.jelly", 
        "types", 
        "zope.interface"
      ]
    }, 
    "twisted.spread.publish": {
      "file": "twisted/spread/publish.py", 
      "imports": [
        "time", 
        "twisted.internet.defer", 
        "twisted.spread.banana", 
        "twisted.spread.flavors", 
        "twisted.spread.jelly"
      ]
    }, 
    "twisted.spread.ui": {
      "dir": "twisted/spread/ui"
    }, 
    "twisted.spread.ui.__init__": {
      "file": "twisted/spread/ui/__init__.py", 
      "imports": []
    }, 
    "twisted.spread.ui.gtk2util": {
      "file": "twisted/spread/ui/gtk2util.py", 
      "imports": [
        "gtk", 
        "gtk.glade", 
        "twisted.copyright", 
        "twisted.cred.credentials", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.reactor", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.util", 
        "twisted.spread.pb"
      ]
    }, 
    "twisted.spread.ui.tktree": {
      "file": "twisted/spread/ui/tktree.py", 
      "imports": [
        "Tkinter.*", 
        "os"
      ]
    }, 
    "twisted.spread.ui.tkutil": {
      "file": "twisted/spread/ui/tkutil.py", 
      "imports": [
        "Tkinter.*", 
        "string", 
        "tkFileDialog._Dialog", 
        "tkSimpleDialog._QueryString", 
        "twisted.copyright", 
        "twisted.internet.reactor", 
        "twisted.spread.pb"
      ]
    }, 
    "twisted.spread.util": {
      "file": "twisted/spread/util.py", 
      "imports": [
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.protocols.basic", 
        "twisted.python.failure", 
        "twisted.spread.pb", 
        "zope.interface"
      ]
    }, 
    "twisted.tap": {
      "dir": "twisted/tap"
    }, 
    "twisted.tap.__init__": {
      "file": "twisted/tap/__init__.py", 
      "imports": []
    }, 
    "twisted.tap.ftp": {
      "file": "twisted/tap/ftp.py", 
      "imports": [
        "twisted.application.internet", 
        "twisted.cred.checkers", 
        "twisted.cred.portal", 
        "twisted.cred.strcred", 
        "twisted.protocols.ftp", 
        "twisted.python.deprecate", 
        "twisted.python.usage", 
        "twisted.python.versions", 
        "warnings"
      ]
    }, 
    "twisted.tap.manhole": {
      "file": "twisted/tap/manhole.py", 
      "imports": [
        "twisted.application.strports", 
        "twisted.cred.checkers", 
        "twisted.cred.portal", 
        "twisted.manhole.service", 
        "twisted.python.usage", 
        "twisted.python.util", 
        "twisted.spread.pb"
      ]
    }, 
    "twisted.tap.portforward": {
      "file": "twisted/tap/portforward.py", 
      "imports": [
        "twisted.application.strports", 
        "twisted.protocols.portforward", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.tap.socks": {
      "file": "twisted/tap/socks.py", 
      "imports": [
        "twisted.application.internet", 
        "twisted.protocols.socks", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.tap.telnet": {
      "file": "twisted/tap/telnet.py", 
      "imports": [
        "twisted.application.strports", 
        "twisted.manhole.telnet", 
        "twisted.python.usage"
      ]
    }, 
    "twisted.test": {
      "dir": "twisted/test"
    }, 
    "twisted.test.__init__": {
      "file": "twisted/test/__init__.py", 
      "imports": []
    }, 
    "twisted.test._preamble": {
      "file": "twisted/test/_preamble.py", 
      "imports": [
        "os", 
        "sys"
      ]
    }, 
    "twisted.test.crash_test_dummy": {
      "file": "twisted/test/crash_test_dummy.py", 
      "imports": [
        "twisted.python.components", 
        "zope.interface"
      ]
    }, 
    "twisted.test.iosim": {
      "file": "twisted/test/iosim.py", 
      "imports": [
        "OpenSSL.SSL.Error", 
        "__future__", 
        "itertools", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.python.failure", 
        "zope.interface"
      ]
    }, 
    "twisted.test.mock_win32process": {
      "file": "twisted/test/mock_win32process.py", 
      "imports": [
        "win32process"
      ]
    }, 
    "twisted.test.myrebuilder1": {
      "file": "twisted/test/myrebuilder1.py", 
      "imports": []
    }, 
    "twisted.test.myrebuilder2": {
      "file": "twisted/test/myrebuilder2.py", 
      "imports": []
    }, 
    "twisted.test.plugin_basic": {
      "file": "twisted/test/plugin_basic.py", 
      "imports": [
        "twisted.plugin", 
        "twisted.test.test_plugin", 
        "zope.interface"
      ]
    }, 
    "twisted.test.plugin_extra1": {
      "file": "twisted/test/plugin_extra1.py", 
      "imports": [
        "twisted.plugin", 
        "twisted.test.test_plugin", 
        "zope.interface"
      ]
    }, 
    "twisted.test.plugin_extra2": {
      "file": "twisted/test/plugin_extra2.py", 
      "imports": [
        "twisted.plugin", 
        "twisted.test.test_plugin", 
        "zope.interface"
      ]
    }, 
    "twisted.test.process_cmdline": {
      "file": "twisted/test/process_cmdline.py", 
      "imports": [
        "sys"
      ]
    }, 
    "twisted.test.process_echoer": {
      "file": "twisted/test/process_echoer.py", 
      "imports": [
        "sys"
      ]
    }, 
    "twisted.test.process_fds": {
      "file": "twisted/test/process_fds.py", 
      "imports": [
        "os", 
        "sys"
      ]
    }, 
    "twisted.test.process_linger": {
      "file": "twisted/test/process_linger.py", 
      "imports": [
        "os", 
        "sys", 
        "time"
      ]
    }, 
    "twisted.test.process_reader": {
      "file": "twisted/test/process_reader.py", 
      "imports": [
        "sys"
      ]
    }, 
    "twisted.test.process_signal": {
      "file": "twisted/test/process_signal.py", 
      "imports": [
        "sys", 
        "signal"
      ]
    }, 
    "twisted.test.process_stdinreader": {
      "file": "twisted/test/process_stdinreader.py", 
      "imports": [
        "msvcrt", 
        "os", 
        "sys", 
        "time"
      ]
    }, 
    "twisted.test.process_tester": {
      "file": "twisted/test/process_tester.py", 
      "imports": [
        "os", 
        "sys"
      ]
    }, 
    "twisted.test.process_tty": {
      "file": "twisted/test/process_tty.py", 
      "imports": []
    }, 
    "twisted.test.process_twisted": {
      "file": "twisted/test/process_twisted.py", 
      "imports": [
        "os", 
        "sys", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.log", 
        "zope.interface"
      ]
    }, 
    "twisted.test.proto_helpers": {
      "file": "twisted/test/proto_helpers.py", 
      "imports": [
        "__future__", 
        "io", 
        "socket", 
        "twisted.internet.abstract", 
        "twisted.internet.address", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.task", 
        "twisted.protocols.basic", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.test.reflect_helper_IE": {
      "file": "twisted/test/reflect_helper_IE.py", 
      "imports": [
        "idonotexist"
      ]
    }, 
    "twisted.test.reflect_helper_VE": {
      "file": "twisted/test/reflect_helper_VE.py", 
      "imports": []
    }, 
    "twisted.test.reflect_helper_ZDE": {
      "file": "twisted/test/reflect_helper_ZDE.py", 
      "imports": []
    }, 
    "twisted.test.ssl_helpers": {
      "file": "twisted/test/ssl_helpers.py", 
      "imports": [
        "OpenSSL.SSL", 
        "__future__", 
        "twisted.internet.ssl", 
        "twisted.python.compat", 
        "twisted.python.filepath"
      ]
    }, 
    "twisted.test.stdio_test_consumer": {
      "file": "twisted/test/stdio_test_consumer.py", 
      "imports": [
        "sys", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.protocols.basic", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.test._preamble"
      ]
    }, 
    "twisted.test.stdio_test_halfclose": {
      "file": "twisted/test/stdio_test_halfclose.py", 
      "imports": [
        "sys", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.test._preamble", 
        "zope.interface"
      ]
    }, 
    "twisted.test.stdio_test_hostpeer": {
      "file": "twisted/test/stdio_test_hostpeer.py", 
      "imports": [
        "sys", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.reflect", 
        "twisted.test._preamble"
      ]
    }, 
    "twisted.test.stdio_test_lastwrite": {
      "file": "twisted/test/stdio_test_lastwrite.py", 
      "imports": [
        "sys", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.reflect", 
        "twisted.test._preamble"
      ]
    }, 
    "twisted.test.stdio_test_loseconn": {
      "file": "twisted/test/stdio_test_loseconn.py", 
      "imports": [
        "sys", 
        "twisted.internet.error", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.test._preamble"
      ]
    }, 
    "twisted.test.stdio_test_producer": {
      "file": "twisted/test/stdio_test_producer.py", 
      "imports": [
        "sys", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.test._preamble"
      ]
    }, 
    "twisted.test.stdio_test_write": {
      "file": "twisted/test/stdio_test_write.py", 
      "imports": [
        "sys", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.reflect", 
        "twisted.test._preamble"
      ]
    }, 
    "twisted.test.stdio_test_writeseq": {
      "file": "twisted/test/stdio_test_writeseq.py", 
      "imports": [
        "sys", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.reflect", 
        "twisted.test._preamble"
      ]
    }, 
    "twisted.test.test_abstract": {
      "file": "twisted/test/test_abstract.py", 
      "imports": [
        "__future__", 
        "twisted.internet.abstract", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_adbapi": {
      "file": "twisted/test/test_adbapi.py", 
      "imports": [
        "MySQLdb", 
        "gadfly", 
        "kinterbasdb", 
        "os", 
        "psycopg", 
        "pyPgSQL.PgSQL", 
        "sqlite", 
        "stat", 
        "twisted.enterprise.adbapi", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.python.failure", 
        "twisted.trial.unittest", 
        "types"
      ]
    }, 
    "twisted.test.test_amp": {
      "file": "twisted/test/test_amp.py", 
      "imports": [
        "datetime", 
        "decimal", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.protocols.amp", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.test.iosim", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.test.test_application": {
      "file": "twisted/test/test_application.py", 
      "imports": [
        "StringIO", 
        "copy", 
        "os", 
        "pickle", 
        "twisted.application.app", 
        "twisted.application.internet", 
        "twisted.application.reactors", 
        "twisted.application.service", 
        "twisted.internet", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.persisted.sob", 
        "twisted.protocols.basic", 
        "twisted.protocols.wire", 
        "twisted.python.test.modules_helpers", 
        "twisted.python.usage", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_banana": {
      "file": "twisted/test/test_banana.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "twisted.internet.main", 
        "twisted.internet.protocol", 
        "twisted.python.failure", 
        "twisted.spread.banana", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_compat": {
      "file": "twisted/test/test_compat.py", 
      "imports": [
        "__future__", 
        "socket", 
        "sys", 
        "traceback", 
        "twisted.python.compat", 
        "twisted.python.filepath", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_context": {
      "file": "twisted/test/test_context.py", 
      "imports": [
        "__future__", 
        "twisted.python.context", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_cooperator": {
      "file": "twisted/test/test_cooperator.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_defer": {
      "file": "twisted/test/test_defer.py", 
      "imports": [
        "__future__", 
        "gc", 
        "re", 
        "traceback", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.trial.unittest", 
        "warnings"
      ]
    }, 
    "twisted.test.test_defgen": {
      "file": "twisted/test/test_defgen.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_dict": {
      "file": "twisted/test/test_dict.py", 
      "imports": [
        "twisted.protocols.dict", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_digestauth": {
      "file": "twisted/test/test_digestauth.py", 
      "imports": [
        "hashlib", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.internet.address", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.test.test_dirdbm": {
      "file": "twisted/test/test_dirdbm.py", 
      "imports": [
        "glob", 
        "os", 
        "shutil", 
        "time", 
        "twisted.persisted.dirdbm", 
        "twisted.python.rebuild", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_doc": {
      "file": "twisted/test/test_doc.py", 
      "imports": [
        "glob", 
        "inspect", 
        "os", 
        "twisted.python.modules", 
        "twisted.python.reflect", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_error": {
      "file": "twisted/test/test_error.py", 
      "imports": [
        "__future__", 
        "errno", 
        "socket", 
        "twisted.internet.error", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_explorer": {
      "file": "twisted/test/test_explorer.py", 
      "imports": [
        "twisted.manhole.explorer", 
        "twisted.trial.unittest", 
        "types"
      ]
    }, 
    "twisted.test.test_factories": {
      "file": "twisted/test/test_factories.py", 
      "imports": [
        "__future__", 
        "pickle", 
        "twisted.internet.protocol", 
        "twisted.internet.task", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_failure": {
      "file": "twisted/test/test_failure.py", 
      "imports": [
        "__future__", 
        "linecache", 
        "pdb", 
        "re", 
        "sys", 
        "traceback", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.reflect", 
        "twisted.test", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_fdesc": {
      "file": "twisted/test/test_fdesc.py", 
      "imports": [
        "errno", 
        "fcntl", 
        "os", 
        "sys", 
        "traceback", 
        "twisted.internet.fdesc", 
        "twisted.python.util", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_finger": {
      "file": "twisted/test/test_finger.py", 
      "imports": [
        "twisted.protocols.finger", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_formmethod": {
      "file": "twisted/test/test_formmethod.py", 
      "imports": [
        "twisted.python.formmethod", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_ftp": {
      "file": "twisted/test/test_ftp.py", 
      "imports": [
        "StringIO", 
        "errno", 
        "getpass", 
        "os", 
        "pwd", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.protocols.basic", 
        "twisted.protocols.ftp", 
        "twisted.protocols.loopback", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.randbytes", 
        "twisted.python.runtime", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.test.test_ftp_options": {
      "file": "twisted/test/test_ftp_options.py", 
      "imports": [
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.python.filepath", 
        "twisted.python.versions", 
        "twisted.tap.ftp", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_hook": {
      "file": "twisted/test/test_hook.py", 
      "imports": [
        "twisted.python.hook", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_htb": {
      "file": "twisted/test/test_htb.py", 
      "imports": [
        "twisted.protocols.htb", 
        "twisted.test.test_pcp", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_ident": {
      "file": "twisted/test/test_ident.py", 
      "imports": [
        "struct", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.protocols.ident", 
        "twisted.python.failure", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_internet": {
      "file": "twisted/test/test_internet.py", 
      "imports": [
        "__future__", 
        "os", 
        "sys", 
        "time", 
        "twisted.internet.abstract", 
        "twisted.internet.base", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.python.compat", 
        "twisted.python.runtime", 
        "twisted.python.util", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_iosim": {
      "file": "twisted/test/test_iosim.py", 
      "imports": [
        "twisted.test.iosim", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_iutils": {
      "file": "twisted/test/test_iutils.py", 
      "imports": [
        "__future__", 
        "os", 
        "stat", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.internet.utils", 
        "twisted.python.compat", 
        "twisted.python.runtime", 
        "twisted.python.test.test_util", 
        "twisted.trial.unittest", 
        "warnings", 
        "signal"
      ]
    }, 
    "twisted.test.test_jelly": {
      "file": "twisted/test/test_jelly.py", 
      "imports": [
        "datetime", 
        "decimal", 
        "twisted.spread.jelly", 
        "twisted.spread.pb", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_lockfile": {
      "file": "twisted/test/test_lockfile.py", 
      "imports": [
        "errno", 
        "os", 
        "pywintypes", 
        "twisted.python.lockfile", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "win32api.OpenProcess"
      ]
    }, 
    "twisted.test.test_log": {
      "file": "twisted/test/test_log.py", 
      "imports": [
        "__future__", 
        "calendar", 
        "logging", 
        "os", 
        "sys", 
        "time", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.trial.unittest", 
        "warnings"
      ]
    }, 
    "twisted.test.test_logfile": {
      "file": "twisted/test/test_logfile.py", 
      "imports": [
        "errno", 
        "os", 
        "stat", 
        "time", 
        "twisted.python.logfile", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_loopback": {
      "file": "twisted/test/test_loopback.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.protocols.basic", 
        "twisted.protocols.loopback", 
        "twisted.python.compat", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "zope.interface"
      ]
    }, 
    "twisted.test.test_manhole": {
      "file": "twisted/test/test_manhole.py", 
      "imports": [
        "twisted.manhole.service", 
        "twisted.spread.util", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_memcache": {
      "file": "twisted/test/test_memcache.py", 
      "imports": [
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.task", 
        "twisted.protocols.memcache", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_modules": {
      "file": "twisted/test/test_modules.py", 
      "imports": [
        "compileall", 
        "itertools", 
        "sys", 
        "twisted", 
        "twisted.python.filepath", 
        "twisted.python.modules", 
        "twisted.python.reflect", 
        "twisted.python.test.modules_helpers", 
        "twisted.python.test.test_zippath", 
        "twisted.trial.unittest", 
        "zipfile"
      ]
    }, 
    "twisted.test.test_monkey": {
      "file": "twisted/test/test_monkey.py", 
      "imports": [
        "__future__", 
        "twisted.python.monkey", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_newcred": {
      "file": "twisted/test/test_newcred.py", 
      "imports": [
        "crypt.crypt", 
        "hmac", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.pamauth", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.python.components", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.test.test_nmea": {
      "file": "twisted/test/test_nmea.py", 
      "imports": [
        "StringIO", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.protocols.gps.nmea", 
        "twisted.python.reflect", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_paths": {
      "file": "twisted/test/test_paths.py", 
      "imports": [
        "__future__", 
        "contextlib", 
        "errno", 
        "os", 
        "pickle", 
        "pprint", 
        "stat", 
        "time", 
        "twisted.python.compat", 
        "twisted.python.filepath", 
        "twisted.python.runtime", 
        "twisted.python.win32", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.test.test_pb": {
      "file": "twisted/test/test_pb.py", 
      "imports": [
        "cStringIO", 
        "gc", 
        "os", 
        "sys", 
        "time", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.main", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.protocols.policies", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.spread.jelly", 
        "twisted.spread.pb", 
        "twisted.spread.publish", 
        "twisted.spread.util", 
        "twisted.trial.unittest", 
        "weakref", 
        "zope.interface"
      ]
    }, 
    "twisted.test.test_pbfailure": {
      "file": "twisted/test/test_pbfailure.py", 
      "imports": [
        "StringIO", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.python.log", 
        "twisted.spread.flavors", 
        "twisted.spread.jelly", 
        "twisted.spread.pb", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_pcp": {
      "file": "twisted/test/test_pcp.py", 
      "imports": [
        "StringIO", 
        "twisted.protocols.pcp", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_persisted": {
      "file": "twisted/test/test_persisted.py", 
      "imports": [
        "StringIO", 
        "cPickle", 
        "cStringIO", 
        "pickle", 
        "sets", 
        "sys", 
        "twisted.persisted.aot", 
        "twisted.persisted.crefutil", 
        "twisted.persisted.styles", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_plugin": {
      "file": "twisted/test/test_plugin.py", 
      "imports": [
        "compileall", 
        "dummy.plugins", 
        "errno", 
        "mypackage", 
        "mypackage.testplugin", 
        "os", 
        "plugindummy.plugins", 
        "sys", 
        "time", 
        "twisted.plugin", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.util", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.test.test_policies": {
      "file": "twisted/test/test_policies.py", 
      "imports": [
        "__future__", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.protocols.policies", 
        "twisted.python.compat", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.test.test_postfix": {
      "file": "twisted/test/test_postfix.py", 
      "imports": [
        "twisted.protocols.postfix", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_process": {
      "file": "twisted/test/test_process.py", 
      "imports": [
        "StringIO", 
        "errno", 
        "fcntl", 
        "gc", 
        "glob", 
        "gzip", 
        "operator", 
        "os", 
        "stat", 
        "sys", 
        "twisted.internet._dumbwin32proc", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.process", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.log", 
        "twisted.python.procutils", 
        "twisted.python.runtime", 
        "twisted.python.util", 
        "twisted.test.mock_win32process", 
        "twisted.test.process_tester", 
        "twisted.trial.unittest", 
        "win32api", 
        "zope.interface.verify", 
        "signal"
      ]
    }, 
    "twisted.test.test_protocols": {
      "file": "twisted/test/test_protocols.py", 
      "imports": [
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.protocols.portforward", 
        "twisted.protocols.wire", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_randbytes": {
      "file": "twisted/test/test_randbytes.py", 
      "imports": [
        "__future__", 
        "os", 
        "twisted.python.randbytes", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_rebuild": {
      "file": "twisted/test/test_rebuild.py", 
      "imports": [
        "os", 
        "shutil", 
        "sys", 
        "time", 
        "twisted.python.components", 
        "twisted.python.rebuild", 
        "twisted.python.util", 
        "twisted.spread.banana", 
        "twisted.test.crash_test_dummy", 
        "twisted.test.test_rebuild", 
        "twisted.trial.unittest", 
        "twisted_rebuild_fakelib.myrebuilder", 
        "types"
      ]
    }, 
    "twisted.test.test_reflect": {
      "file": "twisted/test/test_reflect.py", 
      "imports": [
        "__future__", 
        "collections", 
        "os", 
        "twisted", 
        "twisted.python", 
        "twisted.python.compat", 
        "twisted.python.monkey", 
        "twisted.python.reflect", 
        "twisted.python.versions", 
        "twisted.trial.unittest", 
        "weakref"
      ]
    }, 
    "twisted.test.test_roots": {
      "file": "twisted/test/test_roots.py", 
      "imports": [
        "twisted.python.roots", 
        "twisted.trial.unittest", 
        "types"
      ]
    }, 
    "twisted.test.test_setup": {
      "file": "twisted/test/test_setup.py", 
      "imports": [
        "__future__", 
        "os", 
        "sys", 
        "twisted", 
        "twisted.python.dist", 
        "twisted.python.filepath", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_shortcut": {
      "file": "twisted/test/test_shortcut.py", 
      "imports": [
        "os", 
        "sys", 
        "twisted.python.shortcut", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_sip": {
      "file": "twisted/test/test_sip.py", 
      "imports": [
        "twisted.cred", 
        "twisted.cred.checkers", 
        "twisted.cred.portal", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.utils", 
        "twisted.protocols.sip", 
        "twisted.python.versions", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "zope.interface"
      ]
    }, 
    "twisted.test.test_sob": {
      "file": "twisted/test/test_sob.py", 
      "imports": [
        "Crypto.Cipher.AES", 
        "os", 
        "sys", 
        "twisted.persisted.sob", 
        "twisted.python.components", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_socks": {
      "file": "twisted/test/test_socks.py", 
      "imports": [
        "socket", 
        "struct", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.reactor", 
        "twisted.protocols.socks", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_ssl": {
      "file": "twisted/test/test_ssl.py", 
      "imports": [
        "OpenSSL.SSL", 
        "OpenSSL.crypto", 
        "__future__", 
        "errno", 
        "os", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.protocols.basic", 
        "twisted.protocols.tls", 
        "twisted.python.filepath", 
        "twisted.python.runtime", 
        "twisted.test.ssl_helpers", 
        "twisted.test.test_tcp", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_sslverify": {
      "file": "twisted/test/test_sslverify.py", 
      "imports": [
        "OpenSSL.SSL", 
        "OpenSSL.crypto.FILETYPE_PEM", 
        "OpenSSL.crypto.PKey", 
        "OpenSSL.crypto.TYPE_RSA", 
        "OpenSSL.crypto.X509", 
        "__future__", 
        "itertools", 
        "twisted.internet._sslverify", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.protocols.tls", 
        "twisted.python.compat", 
        "twisted.python.constants", 
        "twisted.python.filepath", 
        "twisted.test.iosim", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.test.test_stateful": {
      "file": "twisted/test/test_stateful.py", 
      "imports": [
        "struct", 
        "twisted.protocols.stateful", 
        "twisted.protocols.test.test_basic", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_stdio": {
      "file": "twisted/test/test_stdio.py", 
      "imports": [
        "itertools", 
        "os", 
        "sys", 
        "twisted", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.stdio", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.test.test_tcp", 
        "twisted.trial.unittest", 
        "win32process"
      ]
    }, 
    "twisted.test.test_strcred": {
      "file": "twisted/test/test_strcred.py", 
      "imports": [
        "Crypto", 
        "StringIO", 
        "crypt", 
        "os", 
        "pwd", 
        "pyasn1", 
        "spwd", 
        "twisted.conch", 
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.strcred", 
        "twisted.plugin", 
        "twisted.plugins.cred_anonymous", 
        "twisted.plugins.cred_file", 
        "twisted.python.fakepwd", 
        "twisted.python.filepath", 
        "twisted.python.usage", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_strerror": {
      "file": "twisted/test/test_strerror.py", 
      "imports": [
        "ctypes.WinError", 
        "os", 
        "socket", 
        "twisted.internet.tcp", 
        "twisted.python.runtime", 
        "twisted.python.win32", 
        "twisted.trial.unittest", 
        "win32api.FormatMessage"
      ]
    }, 
    "twisted.test.test_stringtransport": {
      "file": "twisted/test/test_stringtransport.py", 
      "imports": [
        "twisted.internet.address", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.test.test_strports": {
      "file": "twisted/test/test_strports.py", 
      "imports": [
        "twisted.application.internet", 
        "twisted.application.strports", 
        "twisted.internet.endpoints", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.test.test_endpoints", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_task": {
      "file": "twisted/test/test_task.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.internet.test.modulehelpers", 
        "twisted.python.failure", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_tcp": {
      "file": "twisted/test/test_tcp.py", 
      "imports": [
        "__future__", 
        "errno", 
        "functools", 
        "random", 
        "resource", 
        "socket", 
        "sys", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.protocols.policies", 
        "twisted.python.log", 
        "twisted.python.util", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.test.test_tcp_internals": {
      "file": "twisted/test/test_tcp_internals.py", 
      "imports": [
        "__future__", 
        "errno", 
        "os", 
        "resource", 
        "socket", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.process", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.tcp", 
        "twisted.python.compat", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_text": {
      "file": "twisted/test/test_text.py", 
      "imports": [
        "cStringIO", 
        "twisted.python.text", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_threadable": {
      "file": "twisted/test/test_threadable.py", 
      "imports": [
        "__future__", 
        "pickle", 
        "sys", 
        "threading", 
        "twisted.python.compat", 
        "twisted.python.threadable", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_threadpool": {
      "file": "twisted/test/test_threadpool.py", 
      "imports": [
        "__future__", 
        "gc", 
        "pickle", 
        "threading", 
        "time", 
        "twisted.python.compat", 
        "twisted.python.context", 
        "twisted.python.failure", 
        "twisted.python.threadable", 
        "twisted.python.threadpool", 
        "twisted.trial.unittest", 
        "weakref"
      ]
    }, 
    "twisted.test.test_threads": {
      "file": "twisted/test/test_threads.py", 
      "imports": [
        "__future__", 
        "os", 
        "sys", 
        "threading", 
        "time", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.threads", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.threadable", 
        "twisted.python.threadpool", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_tpfile": {
      "file": "twisted/test/test_tpfile.py", 
      "imports": [
        "StringIO", 
        "twisted.internet.abstract", 
        "twisted.internet.protocol", 
        "twisted.protocols.basic", 
        "twisted.protocols.loopback", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_twistd": {
      "file": "twisted/test/test_twistd.py", 
      "imports": [
        "StringIO", 
        "cPickle", 
        "cProfile", 
        "errno", 
        "grp", 
        "hotshot", 
        "hotshot.stats", 
        "inspect", 
        "os", 
        "pickle", 
        "profile", 
        "pstats", 
        "pwd", 
        "sys", 
        "twisted.application.app", 
        "twisted.application.reactors", 
        "twisted.application.service", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.test.modulehelpers", 
        "twisted.plugin", 
        "twisted.python.components", 
        "twisted.python.fakepwd", 
        "twisted.python.log", 
        "twisted.python.syslog", 
        "twisted.python.usage", 
        "twisted.scripts._twistd_unix", 
        "twisted.scripts.twistd", 
        "twisted.test.test_process", 
        "twisted.trial.unittest", 
        "zope.interface", 
        "zope.interface.verify", 
        "signal"
      ]
    }, 
    "twisted.test.test_twisted": {
      "file": "twisted/test/test_twisted.py", 
      "imports": [
        "__future__", 
        "pkg_resources", 
        "sys", 
        "twisted", 
        "twisted.python.compat", 
        "twisted.python.reflect", 
        "twisted.trial.unittest", 
        "types", 
        "unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.test.test_udp": {
      "file": "twisted/test/test_udp.py", 
      "imports": [
        "__future__", 
        "os", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.udp", 
        "twisted.python.compat", 
        "twisted.python.runtime", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.test_unix": {
      "file": "twisted/test/test_unix.py", 
      "imports": [
        "os", 
        "socket", 
        "stat", 
        "sys", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.utils", 
        "twisted.python.lockfile", 
        "twisted.test.test_tcp", 
        "twisted.trial.unittest", 
        "types"
      ]
    }, 
    "twisted.test.test_usage": {
      "file": "twisted/test/test_usage.py", 
      "imports": [
        "twisted.python.usage", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.test.testutils": {
      "file": "twisted/test/testutils.py", 
      "imports": [
        "io", 
        "twisted.internet.protocol", 
        "xml.dom.minidom"
      ]
    }, 
    "twisted.trial": {
      "dir": "twisted/trial"
    }, 
    "twisted.trial.__init__": {
      "file": "twisted/trial/__init__.py", 
      "imports": []
    }, 
    "twisted.trial._asyncrunner": {
      "file": "twisted/trial/_asyncrunner.py", 
      "imports": [
        "doctest", 
        "gc", 
        "twisted.python.components", 
        "twisted.trial._synctest", 
        "twisted.trial.itrial", 
        "twisted.trial.reporter", 
        "zope.interface"
      ]
    }, 
    "twisted.trial._asynctest": {
      "file": "twisted/trial/_asynctest.py", 
      "imports": [
        "__future__", 
        "inspect", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.utils", 
        "twisted.python.failure", 
        "twisted.trial._synctest", 
        "twisted.trial.itrial", 
        "twisted.trial.util", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.trial._dist": {
      "dir": "twisted/trial/_dist"
    }, 
    "twisted.trial._dist.__init__": {
      "file": "twisted/trial/_dist/__init__.py", 
      "imports": []
    }, 
    "twisted.trial._dist.distreporter": {
      "file": "twisted/trial/_dist/distreporter.py", 
      "imports": [
        "twisted.python.components", 
        "twisted.trial.itrial", 
        "zope.interface"
      ]
    }, 
    "twisted.trial._dist.disttrial": {
      "file": "twisted/trial/_dist/disttrial.py", 
      "imports": [
        "os", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.python.filepath", 
        "twisted.python.modules", 
        "twisted.trial._dist", 
        "twisted.trial._dist.distreporter", 
        "twisted.trial._dist.worker", 
        "twisted.trial.reporter", 
        "twisted.trial.unittest", 
        "twisted.trial.util"
      ]
    }, 
    "twisted.trial._dist.managercommands": {
      "file": "twisted/trial/_dist/managercommands.py", 
      "imports": [
        "twisted.protocols.amp"
      ]
    }, 
    "twisted.trial._dist.options": {
      "file": "twisted/trial/_dist/options.py", 
      "imports": [
        "twisted.application.app", 
        "twisted.python.filepath", 
        "twisted.python.usage", 
        "twisted.scripts.trial"
      ]
    }, 
    "twisted.trial._dist.test": {
      "dir": "twisted/trial/_dist/test"
    }, 
    "twisted.trial._dist.test.__init__": {
      "file": "twisted/trial/_dist/test/__init__.py", 
      "imports": []
    }, 
    "twisted.trial._dist.test.test_distreporter": {
      "file": "twisted/trial/_dist/test/test_distreporter.py", 
      "imports": [
        "cStringIO", 
        "twisted.trial._dist.distreporter", 
        "twisted.trial.reporter", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial._dist.test.test_disttrial": {
      "file": "twisted/trial/_dist/test/test_disttrial.py", 
      "imports": [
        "cStringIO", 
        "os", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.main", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.python.failure", 
        "twisted.python.lockfile", 
        "twisted.test.test_cooperator", 
        "twisted.trial._dist.distreporter", 
        "twisted.trial._dist.disttrial", 
        "twisted.trial._dist.worker", 
        "twisted.trial.reporter", 
        "twisted.trial.runner", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial._dist.test.test_options": {
      "file": "twisted/trial/_dist/test/test_options.py", 
      "imports": [
        "gc", 
        "os", 
        "sys", 
        "twisted.trial._dist.options", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial._dist.test.test_worker": {
      "file": "twisted/trial/_dist/test/test_worker.py", 
      "imports": [
        "cStringIO", 
        "os", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.main", 
        "twisted.protocols.amp", 
        "twisted.python.failure", 
        "twisted.scripts.trial", 
        "twisted.test.proto_helpers", 
        "twisted.trial._dist.managercommands", 
        "twisted.trial._dist.worker", 
        "twisted.trial._dist.workercommands", 
        "twisted.trial.reporter", 
        "twisted.trial.unittest", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.trial._dist.test.test_workerreporter": {
      "file": "twisted/trial/_dist/test/test_workerreporter.py", 
      "imports": [
        "twisted.python.failure", 
        "twisted.trial._dist.managercommands", 
        "twisted.trial._dist.workerreporter", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial._dist.test.test_workertrial": {
      "file": "twisted/trial/_dist/test/test_workertrial.py", 
      "imports": [
        "cStringIO", 
        "errno", 
        "os", 
        "sys", 
        "twisted.protocols.amp", 
        "twisted.test.proto_helpers", 
        "twisted.trial._dist", 
        "twisted.trial._dist.managercommands", 
        "twisted.trial._dist.workercommands", 
        "twisted.trial._dist.workertrial", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial._dist.worker": {
      "file": "twisted/trial/_dist/worker.py", 
      "imports": [
        "os", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.protocols.amp", 
        "twisted.python.failure", 
        "twisted.python.reflect", 
        "twisted.trial._dist", 
        "twisted.trial._dist.managercommands", 
        "twisted.trial._dist.workercommands", 
        "twisted.trial._dist.workerreporter", 
        "twisted.trial.runner", 
        "twisted.trial.unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.trial._dist.workercommands": {
      "file": "twisted/trial/_dist/workercommands.py", 
      "imports": [
        "twisted.protocols.amp"
      ]
    }, 
    "twisted.trial._dist.workerreporter": {
      "file": "twisted/trial/_dist/workerreporter.py", 
      "imports": [
        "twisted.python.failure", 
        "twisted.python.reflect", 
        "twisted.trial._dist.managercommands", 
        "twisted.trial.reporter"
      ]
    }, 
    "twisted.trial._dist.workertrial": {
      "file": "twisted/trial/_dist/workertrial.py", 
      "imports": [
        "errno", 
        "os", 
        "sys", 
        "twisted.internet.protocol", 
        "twisted.python.log", 
        "twisted.trial._dist", 
        "twisted.trial._dist.managercommands", 
        "twisted.trial._dist.options", 
        "twisted.trial._dist.worker"
      ]
    }, 
    "twisted.trial._synctest": {
      "file": "twisted/trial/_synctest.py", 
      "imports": [
        "__future__", 
        "dis", 
        "inspect", 
        "os", 
        "pprint", 
        "sys", 
        "tempfile", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.monkey", 
        "twisted.python.util", 
        "twisted.trial.itrial", 
        "twisted.trial.util", 
        "types", 
        "unittest", 
        "warnings"
      ]
    }, 
    "twisted.trial.itrial": {
      "file": "twisted/trial/itrial.py", 
      "imports": [
        "__future__", 
        "zope.interface"
      ]
    }, 
    "twisted.trial.reporter": {
      "file": "twisted/trial/reporter.py", 
      "imports": [
        "__future__", 
        "collections", 
        "curses", 
        "os", 
        "pywintypes", 
        "subunit.TestProtocolClient", 
        "sys", 
        "time", 
        "twisted.python.components", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.python.util", 
        "twisted.trial.itrial", 
        "twisted.trial.util", 
        "unittest", 
        "warnings", 
        "win32console", 
        "win32console.FOREGROUND_BLUE", 
        "win32console.FOREGROUND_GREEN", 
        "win32console.FOREGROUND_INTENSITY", 
        "win32console.FOREGROUND_RED", 
        "win32console.GetStdHandle", 
        "win32console.STD_OUTPUT_HANDLE", 
        "zope.interface"
      ]
    }, 
    "twisted.trial.runner": {
      "file": "twisted/trial/runner.py", 
      "imports": [
        "doctest", 
        "imp", 
        "inspect", 
        "os", 
        "sys", 
        "time", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.modules", 
        "twisted.python.reflect", 
        "twisted.python.versions", 
        "twisted.trial.itrial", 
        "twisted.trial.reporter", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "types", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.trial.test": {
      "dir": "twisted/trial/test"
    }, 
    "twisted.trial.test.__init__": {
      "file": "twisted/trial/test/__init__.py", 
      "imports": []
    }, 
    "twisted.trial.test.detests": {
      "file": "twisted/trial/test/detests.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.internet.threads", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.erroneous": {
      "file": "twisted/trial/test/erroneous.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.trial.unittest", 
        "twisted.trial.util"
      ]
    }, 
    "twisted.trial.test.mockcustomsuite": {
      "file": "twisted/trial/test/mockcustomsuite.py", 
      "imports": [
        "twisted.trial.runner", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.mockcustomsuite2": {
      "file": "twisted/trial/test/mockcustomsuite2.py", 
      "imports": [
        "twisted.trial.runner", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.mockcustomsuite3": {
      "file": "twisted/trial/test/mockcustomsuite3.py", 
      "imports": [
        "twisted.trial.runner", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.mockdoctest": {
      "file": "twisted/trial/test/mockdoctest.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "twisted.trial.test.moduleself": {
      "file": "twisted/trial/test/moduleself.py", 
      "imports": [
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.moduletest": {
      "file": "twisted/trial/test/moduletest.py", 
      "imports": []
    }, 
    "twisted.trial.test.novars": {
      "file": "twisted/trial/test/novars.py", 
      "imports": []
    }, 
    "twisted.trial.test.ordertests": {
      "file": "twisted/trial/test/ordertests.py", 
      "imports": [
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.packages": {
      "file": "twisted/trial/test/packages.py", 
      "imports": [
        "__future__", 
        "os", 
        "sys", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.sample": {
      "file": "twisted/trial/test/sample.py", 
      "imports": [
        "twisted.python.util", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.scripttest": {
      "file": "twisted/trial/test/scripttest.py", 
      "imports": []
    }, 
    "twisted.trial.test.skipping": {
      "file": "twisted/trial/test/skipping.py", 
      "imports": [
        "__future__", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.suppression": {
      "file": "twisted/trial/test/suppression.py", 
      "imports": [
        "__future__", 
        "twisted.python.compat", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "warnings"
      ]
    }, 
    "twisted.trial.test.test_assertions": {
      "file": "twisted/trial/test/test_assertions.py", 
      "imports": [
        "__future__", 
        "pprint", 
        "twisted.internet.defer", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.reflect", 
        "twisted.python.util", 
        "twisted.python.versions", 
        "twisted.trial.unittest", 
        "unittest", 
        "warnings"
      ]
    }, 
    "twisted.trial.test.test_asyncassertions": {
      "file": "twisted/trial/test/test_asyncassertions.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.python.failure", 
        "twisted.trial.unittest", 
        "unittest"
      ]
    }, 
    "twisted.trial.test.test_deferred": {
      "file": "twisted/trial/test/test_deferred.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.trial.reporter", 
        "twisted.trial.test.detests", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "unittest"
      ]
    }, 
    "twisted.trial.test.test_doctest": {
      "file": "twisted/trial/test/test_doctest.py", 
      "imports": [
        "twisted.trial.itrial", 
        "twisted.trial.reporter", 
        "twisted.trial.runner", 
        "twisted.trial.test.mockdoctest", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.test_keyboard": {
      "file": "twisted/trial/test/test_keyboard.py", 
      "imports": [
        "StringIO", 
        "twisted.trial.reporter", 
        "twisted.trial.runner", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.test_loader": {
      "file": "twisted/trial/test/test_loader.py", 
      "imports": [
        "goodpackage", 
        "goodpackage.test_sample", 
        "hashlib", 
        "os", 
        "package", 
        "sys", 
        "twisted", 
        "twisted.python.filepath", 
        "twisted.python.modules", 
        "twisted.python.test.test_zippath", 
        "twisted.python.util", 
        "twisted.trial.itrial", 
        "twisted.trial.reporter", 
        "twisted.trial.runner", 
        "twisted.trial.test.erroneous", 
        "twisted.trial.test.mockcustomsuite", 
        "twisted.trial.test.mockcustomsuite2", 
        "twisted.trial.test.mockcustomsuite3", 
        "twisted.trial.test.packages", 
        "twisted.trial.test.sample", 
        "twisted.trial.unittest", 
        "uberpackage"
      ]
    }, 
    "twisted.trial.test.test_log": {
      "file": "twisted/trial/test/test_log.py", 
      "imports": [
        "__future__", 
        "time", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.trial.reporter", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.test_output": {
      "file": "twisted/trial/test/test_output.py", 
      "imports": [
        "StringIO", 
        "os", 
        "twisted.scripts.trial", 
        "twisted.trial.reporter", 
        "twisted.trial.runner", 
        "twisted.trial.test.packages"
      ]
    }, 
    "twisted.trial.test.test_plugins": {
      "file": "twisted/trial/test/test_plugins.py", 
      "imports": [
        "twisted.plugin", 
        "twisted.trial.itrial", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.test_pyunitcompat": {
      "file": "twisted/trial/test/test_pyunitcompat.py", 
      "imports": [
        "__future__", 
        "sys", 
        "traceback", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.trial.itrial", 
        "twisted.trial.unittest", 
        "unittest", 
        "zope.interface"
      ]
    }, 
    "twisted.trial.test.test_reporter": {
      "file": "twisted/trial/test/test_reporter.py", 
      "imports": [
        "StringIO", 
        "__future__", 
        "errno", 
        "inspect", 
        "os", 
        "re", 
        "sys", 
        "twisted.internet.utils", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.trial.itrial", 
        "twisted.trial.reporter", 
        "twisted.trial.runner", 
        "twisted.trial.test.erroneous", 
        "twisted.trial.test.sample", 
        "twisted.trial.unittest", 
        "twisted.trial.util"
      ]
    }, 
    "twisted.trial.test.test_runner": {
      "file": "twisted/trial/test/test_runner.py", 
      "imports": [
        "StringIO", 
        "os", 
        "pdb", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.plugin", 
        "twisted.plugins.twisted_trial", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.scripts.trial", 
        "twisted.trial.itrial", 
        "twisted.trial.reporter", 
        "twisted.trial.runner", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.trial.test.test_script": {
      "file": "twisted/trial/test/test_script.py", 
      "imports": [
        "StringIO", 
        "gc", 
        "imaplib", 
        "re", 
        "smtplib", 
        "sys", 
        "textwrap", 
        "twisted.python.filepath", 
        "twisted.python.usage", 
        "twisted.python.util", 
        "twisted.scripts.trial", 
        "twisted.trial._dist.disttrial", 
        "twisted.trial.runner", 
        "twisted.trial.test.test_loader", 
        "twisted.trial.unittest", 
        "twisted_toptobottom_temp.test_missing", 
        "types"
      ]
    }, 
    "twisted.trial.test.test_suppression": {
      "file": "twisted/trial/test/test_suppression.py", 
      "imports": [
        "__future__", 
        "twisted.trial.test.suppression", 
        "twisted.trial.unittest", 
        "unittest"
      ]
    }, 
    "twisted.trial.test.test_testcase": {
      "file": "twisted/trial/test/test_testcase.py", 
      "imports": [
        "__future__", 
        "twisted.trial.unittest"
      ]
    }, 
    "twisted.trial.test.test_tests": {
      "file": "twisted/trial/test/test_tests.py", 
      "imports": [
        "__future__", 
        "gc", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.python.compat", 
        "twisted.trial.reporter", 
        "twisted.trial.runner", 
        "twisted.trial.test.erroneous", 
        "twisted.trial.test.skipping", 
        "twisted.trial.test.suppression", 
        "twisted.trial.test.test_reporter", 
        "twisted.trial.test.test_suppression", 
        "twisted.trial.test.weird", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "unittest", 
        "weakref"
      ]
    }, 
    "twisted.trial.test.test_util": {
      "file": "twisted/trial/test/test_util.py", 
      "imports": [
        "__future__", 
        "os", 
        "sys", 
        "twisted.internet.base", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.trial.test.suppression", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "zope.interface"
      ]
    }, 
    "twisted.trial.test.test_warning": {
      "file": "twisted/trial/test/test_warning.py", 
      "imports": [
        "__future__", 
        "importlib", 
        "sys", 
        "twisted.python.compat", 
        "twisted.python.filepath", 
        "twisted.trial.unittest", 
        "twisted_private_helper.missingsourcefile", 
        "twisted_private_helper.module", 
        "twisted_renamed_helper.module", 
        "unittest", 
        "warnings"
      ]
    }, 
    "twisted.trial.test.weird": {
      "file": "twisted/trial/test/weird.py", 
      "imports": [
        "__future__", 
        "twisted.internet.defer", 
        "unittest"
      ]
    }, 
    "twisted.trial.unittest": {
      "file": "twisted/trial/unittest.py", 
      "imports": [
        "__future__", 
        "twisted.python.compat", 
        "twisted.trial._asyncrunner", 
        "twisted.trial._asynctest", 
        "twisted.trial._synctest"
      ]
    }, 
    "twisted.trial.util": {
      "file": "twisted/trial/util.py", 
      "imports": [
        "__future__", 
        "profile", 
        "random", 
        "sys", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.internet.utils", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.filepath", 
        "twisted.python.lockfile", 
        "twisted.python.versions"
      ]
    }, 
    "twisted.web": {
      "dir": "twisted/web"
    }, 
    "twisted.web.__init__": {
      "file": "twisted/web/__init__.py", 
      "imports": [
        "twisted.web._version"
      ]
    }, 
    "twisted.web._auth": {
      "dir": "twisted/web/_auth"
    }, 
    "twisted.web._auth.__init__": {
      "file": "twisted/web/_auth/__init__.py", 
      "imports": []
    }, 
    "twisted.web._auth.basic": {
      "file": "twisted/web/_auth/basic.py", 
      "imports": [
        "binascii", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.web.iweb", 
        "zope.interface"
      ]
    }, 
    "twisted.web._auth.digest": {
      "file": "twisted/web/_auth/digest.py", 
      "imports": [
        "twisted.cred.credentials", 
        "twisted.web.iweb", 
        "zope.interface"
      ]
    }, 
    "twisted.web._auth.wrapper": {
      "file": "twisted/web/_auth/wrapper.py", 
      "imports": [
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.python.components", 
        "twisted.python.log", 
        "twisted.web.resource", 
        "twisted.web.util", 
        "zope.interface"
      ]
    }, 
    "twisted.web._element": {
      "file": "twisted/web/_element.py", 
      "imports": [
        "twisted.web.error", 
        "twisted.web.iweb", 
        "zope.interface"
      ]
    }, 
    "twisted.web._flatten": {
      "file": "twisted/web/_flatten.py", 
      "imports": [
        "cStringIO", 
        "sys.exc_info", 
        "traceback", 
        "twisted.internet.defer", 
        "twisted.web._stan", 
        "twisted.web.error", 
        "twisted.web.iweb", 
        "types"
      ]
    }, 
    "twisted.web._newclient": {
      "file": "twisted/web/_newclient.py", 
      "imports": [
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.protocols.basic", 
        "twisted.python.components", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.web.http", 
        "twisted.web.http_headers", 
        "twisted.web.iweb", 
        "zope.interface"
      ]
    }, 
    "twisted.web._responses": {
      "file": "twisted/web/_responses.py", 
      "imports": [
        "__future__"
      ]
    }, 
    "twisted.web._stan": {
      "file": "twisted/web/_stan.py", 
      "imports": []
    }, 
    "twisted.web._version": {
      "file": "twisted/web/_version.py", 
      "imports": [
        "twisted.python.versions"
      ]
    }, 
    "twisted.web.client": {
      "file": "twisted/web/client.py", 
      "imports": [
        "OpenSSL.SSL", 
        "__future__", 
        "functools", 
        "os", 
        "twisted.internet.defer", 
        "twisted.internet.endpoints", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.internet.task", 
        "twisted.python.compat", 
        "twisted.python.components", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.util", 
        "twisted.python.versions", 
        "twisted.web._newclient", 
        "twisted.web.error", 
        "twisted.web.http", 
        "twisted.web.http_headers", 
        "twisted.web.iweb", 
        "types", 
        "urllib", 
        "urlparse", 
        "warnings", 
        "zlib", 
        "zope.interface"
      ]
    }, 
    "twisted.web.demo": {
      "file": "twisted/web/demo.py", 
      "imports": [
        "twisted.web.static"
      ]
    }, 
    "twisted.web.distrib": {
      "file": "twisted/web/distrib.py", 
      "imports": [
        "cStringIO", 
        "copy", 
        "os", 
        "pwd", 
        "twisted.internet.address", 
        "twisted.internet.reactor", 
        "twisted.persisted.styles", 
        "twisted.python.log", 
        "twisted.spread.banana", 
        "twisted.spread.pb", 
        "twisted.web.html", 
        "twisted.web.http", 
        "twisted.web.http_headers", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.static", 
        "types", 
        "xml.dom.minidom"
      ]
    }, 
    "twisted.web.domhelpers": {
      "file": "twisted/web/domhelpers.py", 
      "imports": [
        "StringIO", 
        "twisted.web.microdom"
      ]
    }, 
    "twisted.web.error": {
      "file": "twisted/web/error.py", 
      "imports": [
        "__future__", 
        "collections", 
        "traceback", 
        "twisted.web._responses", 
        "twisted.web.template"
      ]
    }, 
    "twisted.web.guard": {
      "file": "twisted/web/guard.py", 
      "imports": [
        "twisted.web._auth.basic", 
        "twisted.web._auth.digest", 
        "twisted.web._auth.wrapper"
      ]
    }, 
    "twisted.web.html": {
      "file": "twisted/web/html.py", 
      "imports": [
        "cgi", 
        "twisted.python.compat", 
        "twisted.python.log"
      ]
    }, 
    "twisted.web.http": {
      "file": "twisted/web/http.py", 
      "imports": [
        "__future__", 
        "base64", 
        "binascii", 
        "calendar", 
        "cgi", 
        "io", 
        "math", 
        "os", 
        "socket", 
        "tempfile", 
        "time", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.protocols.basic", 
        "twisted.protocols.policies", 
        "twisted.python.compat", 
        "twisted.python.components", 
        "twisted.python.log", 
        "twisted.web._responses", 
        "twisted.web.http_headers", 
        "twisted.web.iweb", 
        "urllib", 
        "urlparse", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.web.http_headers": {
      "file": "twisted/web/http_headers.py", 
      "imports": [
        "__future__", 
        "collections", 
        "twisted.python.compat"
      ]
    }, 
    "twisted.web.iweb": {
      "file": "twisted/web/iweb.py", 
      "imports": [
        "twisted.cred.credentials", 
        "twisted.internet.interfaces", 
        "twisted.python.compat", 
        "zope.interface"
      ]
    }, 
    "twisted.web.microdom": {
      "file": "twisted/web/microdom.py", 
      "imports": [
        "cStringIO", 
        "re", 
        "twisted.python.util", 
        "twisted.web.sux", 
        "types"
      ]
    }, 
    "twisted.web.proxy": {
      "file": "twisted/web/proxy.py", 
      "imports": [
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "twisted.web.resource": {
      "file": "twisted/web/resource.py", 
      "imports": [
        "__future__", 
        "twisted.python.compat", 
        "twisted.python.components", 
        "twisted.python.reflect", 
        "twisted.web._responses", 
        "twisted.web.error", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.web.rewrite": {
      "file": "twisted/web/rewrite.py", 
      "imports": [
        "twisted.web.resource"
      ]
    }, 
    "twisted.web.script": {
      "file": "twisted/web/script.py", 
      "imports": [
        "StringIO", 
        "cStringIO", 
        "os", 
        "quixote.ptl_compile", 
        "traceback", 
        "twisted.copyright", 
        "twisted.python.compat", 
        "twisted.web.html", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.static"
      ]
    }, 
    "twisted.web.server": {
      "file": "twisted/web/server.py", 
      "imports": [
        "__future__", 
        "cgi", 
        "copy", 
        "hashlib", 
        "html.escape", 
        "os", 
        "random", 
        "twisted.copyright", 
        "twisted.internet.address", 
        "twisted.internet.reactor", 
        "twisted.python.compat", 
        "twisted.python.components", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.logfile", 
        "twisted.python.reflect", 
        "twisted.python.urlpath", 
        "twisted.python.versions", 
        "twisted.spread.pb", 
        "twisted.web.error", 
        "twisted.web.html", 
        "twisted.web.http", 
        "twisted.web.iweb", 
        "twisted.web.resource", 
        "twisted.web.util", 
        "urllib", 
        "zope.interface"
      ]
    }, 
    "twisted.web.soap": {
      "file": "twisted/web/soap.py", 
      "imports": [
        "SOAPpy", 
        "twisted.internet.defer", 
        "twisted.web.client", 
        "twisted.web.resource", 
        "twisted.web.server"
      ]
    }, 
    "twisted.web.static": {
      "file": "twisted/web/static.py", 
      "imports": [
        "__future__", 
        "cgi", 
        "errno", 
        "itertools", 
        "mimetypes", 
        "os", 
        "time", 
        "twisted.internet.abstract", 
        "twisted.internet.interfaces", 
        "twisted.persisted.styles", 
        "twisted.python.components", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.python.util", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.util", 
        "urllib", 
        "warnings", 
        "zope.interface"
      ]
    }, 
    "twisted.web.sux": {
      "file": "twisted/web/sux.py", 
      "imports": [
        "twisted.internet.protocol", 
        "twisted.python.reflect"
      ]
    }, 
    "twisted.web.tap": {
      "file": "twisted/web/tap.py", 
      "imports": [
        "os", 
        "twisted.application.internet", 
        "twisted.application.service", 
        "twisted.application.strports", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.python.reflect", 
        "twisted.python.threadpool", 
        "twisted.python.usage", 
        "twisted.spread.pb", 
        "twisted.web.demo", 
        "twisted.web.distrib", 
        "twisted.web.script", 
        "twisted.web.server", 
        "twisted.web.static", 
        "twisted.web.twcgi", 
        "twisted.web.wsgi"
      ]
    }, 
    "twisted.web.template": {
      "file": "twisted/web/template.py", 
      "imports": [
        "cStringIO", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.web._element", 
        "twisted.web._flatten", 
        "twisted.web._stan", 
        "twisted.web.iweb", 
        "twisted.web.util", 
        "warnings", 
        "xml.sax", 
        "xml.sax.handler", 
        "zope.interface"
      ]
    }, 
    "twisted.web.test": {
      "dir": "twisted/web/test"
    }, 
    "twisted.web.test.__init__": {
      "file": "twisted/web/test/__init__.py", 
      "imports": []
    }, 
    "twisted.web.test._util": {
      "file": "twisted/web/test/_util.py", 
      "imports": [
        "twisted.internet.defer", 
        "twisted.python.failure", 
        "twisted.trial.unittest", 
        "twisted.web._flatten", 
        "twisted.web.error", 
        "twisted.web.server"
      ]
    }, 
    "twisted.web.test.requesthelper": {
      "file": "twisted/web/test/requesthelper.py", 
      "imports": [
        "__future__", 
        "io", 
        "twisted.internet.address", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.web.http_headers", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "zope.interface"
      ]
    }, 
    "twisted.web.test.test_agent": {
      "file": "twisted/web/test/test_agent.py", 
      "imports": [
        "StringIO", 
        "cookielib", 
        "twisted.internet._sslverify", 
        "twisted.internet.defer", 
        "twisted.internet.endpoints", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.internet.ssl", 
        "twisted.internet.task", 
        "twisted.protocols.tls", 
        "twisted.python.components", 
        "twisted.python.deprecate", 
        "twisted.python.failure", 
        "twisted.python.versions", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "twisted.web._newclient", 
        "twisted.web.client", 
        "twisted.web.error", 
        "twisted.web.http_headers", 
        "twisted.web.iweb", 
        "zlib", 
        "zope.interface.declarations", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.web.test.test_cgi": {
      "file": "twisted/web/test/test_cgi.py", 
      "imports": [
        "os", 
        "sys", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.util", 
        "twisted.trial.unittest", 
        "twisted.web.client", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.test._util", 
        "twisted.web.test.test_web", 
        "twisted.web.twcgi"
      ]
    }, 
    "twisted.web.test.test_distrib": {
      "file": "twisted/web/test/test_distrib.py", 
      "imports": [
        "os", 
        "pwd", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.spread.banana", 
        "twisted.spread.pb", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "twisted.web.client", 
        "twisted.web.distrib", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.static", 
        "twisted.web.test._util", 
        "twisted.web.test.test_web", 
        "xml.dom.minidom", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.web.test.test_domhelpers": {
      "file": "twisted/web/test/test_domhelpers.py", 
      "imports": [
        "twisted.trial.unittest", 
        "twisted.web.domhelpers", 
        "twisted.web.microdom", 
        "xml.dom.minidom"
      ]
    }, 
    "twisted.web.test.test_error": {
      "file": "twisted/web/test/test_error.py", 
      "imports": [
        "twisted.trial.unittest", 
        "twisted.web.error"
      ]
    }, 
    "twisted.web.test.test_flatten": {
      "file": "twisted/web/test/test_flatten.py", 
      "imports": [
        "sys", 
        "traceback", 
        "twisted.internet.defer", 
        "twisted.test.testutils", 
        "twisted.trial.unittest", 
        "twisted.web.error", 
        "twisted.web.iweb", 
        "twisted.web.template", 
        "twisted.web.test._util", 
        "xml.etree.cElementTree", 
        "zope.interface"
      ]
    }, 
    "twisted.web.test.test_http": {
      "file": "twisted/web/test/test_http.py", 
      "imports": [
        "base64", 
        "cgi", 
        "random", 
        "twisted.internet.error", 
        "twisted.internet.task", 
        "twisted.protocols.loopback", 
        "twisted.python.compat", 
        "twisted.python.failure", 
        "twisted.test.proto_helpers", 
        "twisted.test.test_internet", 
        "twisted.trial.unittest", 
        "twisted.web.http", 
        "twisted.web.http_headers", 
        "twisted.web.test.requesthelper", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "twisted.web.test.test_http_headers": {
      "file": "twisted/web/test/test_http_headers.py", 
      "imports": [
        "__future__", 
        "sys", 
        "twisted.python.compat", 
        "twisted.trial.unittest", 
        "twisted.web.http_headers"
      ]
    }, 
    "twisted.web.test.test_httpauth": {
      "file": "twisted/web/test/test_httpauth.py", 
      "imports": [
        "twisted.cred.checkers", 
        "twisted.cred.credentials", 
        "twisted.cred.error", 
        "twisted.cred.portal", 
        "twisted.internet.address", 
        "twisted.internet.error", 
        "twisted.python.failure", 
        "twisted.trial.unittest", 
        "twisted.web._auth.basic", 
        "twisted.web._auth.digest", 
        "twisted.web._auth.wrapper", 
        "twisted.web.iweb", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.static", 
        "twisted.web.test.test_web", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.web.test.test_newclient": {
      "file": "twisted/web/test/test_newclient.py", 
      "imports": [
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.interfaces", 
        "twisted.internet.protocol", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "twisted.web._newclient", 
        "twisted.web.http", 
        "twisted.web.http_headers", 
        "twisted.web.iweb", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.web.test.test_proxy": {
      "file": "twisted/web/test/test_proxy.py", 
      "imports": [
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "twisted.web.proxy", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.test.test_web"
      ]
    }, 
    "twisted.web.test.test_resource": {
      "file": "twisted/web/test/test_resource.py", 
      "imports": [
        "twisted.trial.unittest", 
        "twisted.web.error", 
        "twisted.web.resource", 
        "twisted.web.test.requesthelper"
      ]
    }, 
    "twisted.web.test.test_script": {
      "file": "twisted/web/test/test_script.py", 
      "imports": [
        "os", 
        "twisted.trial.unittest", 
        "twisted.web.http", 
        "twisted.web.script", 
        "twisted.web.test._util", 
        "twisted.web.test.test_web"
      ]
    }, 
    "twisted.web.test.test_soap": {
      "file": "twisted/web/test/test_soap.py", 
      "imports": [
        "SOAPpy", 
        "twisted.internet.defer", 
        "twisted.internet.reactor", 
        "twisted.trial.unittest", 
        "twisted.web.error", 
        "twisted.web.server", 
        "twisted.web.soap"
      ]
    }, 
    "twisted.web.test.test_stan": {
      "file": "twisted/web/test/test_stan.py", 
      "imports": [
        "twisted.trial.unittest", 
        "twisted.web.template"
      ]
    }, 
    "twisted.web.test.test_static": {
      "file": "twisted/web/test/test_static.py", 
      "imports": [
        "StringIO", 
        "inspect", 
        "mimetypes", 
        "os", 
        "re", 
        "twisted.internet.abstract", 
        "twisted.internet.interfaces", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.python.runtime", 
        "twisted.trial.unittest", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.script", 
        "twisted.web.server", 
        "twisted.web.static", 
        "twisted.web.test._util", 
        "twisted.web.test.test_web", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.web.test.test_tap": {
      "file": "twisted/web/test/test_tap.py", 
      "imports": [
        "os", 
        "stat", 
        "twisted.application.strports", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.python.filepath", 
        "twisted.python.threadpool", 
        "twisted.python.usage", 
        "twisted.spread.pb", 
        "twisted.trial.unittest", 
        "twisted.web.distrib", 
        "twisted.web.script", 
        "twisted.web.server", 
        "twisted.web.static", 
        "twisted.web.tap", 
        "twisted.web.twcgi", 
        "twisted.web.wsgi"
      ]
    }, 
    "twisted.web.test.test_template": {
      "file": "twisted/web/test/test_template.py", 
      "imports": [
        "cStringIO", 
        "twisted.internet.defer", 
        "twisted.python.filepath", 
        "twisted.trial.unittest", 
        "twisted.trial.util", 
        "twisted.web._element", 
        "twisted.web.error", 
        "twisted.web.iweb", 
        "twisted.web.server", 
        "twisted.web.template", 
        "twisted.web.test._util", 
        "twisted.web.test.test_web", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.web.test.test_util": {
      "file": "twisted/web/test/test_util.py", 
      "imports": [
        "twisted.internet.defer", 
        "twisted.python.failure", 
        "twisted.trial.unittest", 
        "twisted.web.error", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.template", 
        "twisted.web.test.requesthelper", 
        "twisted.web.util"
      ]
    }, 
    "twisted.web.test.test_vhost": {
      "file": "twisted/web/test/test_vhost.py", 
      "imports": [
        "twisted.internet.defer", 
        "twisted.trial.unittest", 
        "twisted.web.http", 
        "twisted.web.static", 
        "twisted.web.test._util", 
        "twisted.web.test.test_web", 
        "twisted.web.vhost"
      ]
    }, 
    "twisted.web.test.test_web": {
      "file": "twisted/web/test/test_web.py", 
      "imports": [
        "os", 
        "twisted.internet.address", 
        "twisted.internet.reactor", 
        "twisted.internet.task", 
        "twisted.python.compat", 
        "twisted.python.filepath", 
        "twisted.trial.unittest", 
        "twisted.web.error", 
        "twisted.web.http", 
        "twisted.web.iweb", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.static", 
        "twisted.web.test.requesthelper", 
        "zlib", 
        "zope.interface", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.web.test.test_webclient": {
      "file": "twisted/web/test/test_webclient.py", 
      "imports": [
        "__future__", 
        "errno.ENOSPC", 
        "os", 
        "twisted.internet.defer", 
        "twisted.internet.interfaces", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.protocols.policies", 
        "twisted.python.compat", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.test", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "twisted.web.client", 
        "twisted.web.error", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.static", 
        "twisted.web.test.test_web", 
        "twisted.web.util", 
        "urllib", 
        "urlparse"
      ]
    }, 
    "twisted.web.test.test_wsgi": {
      "file": "twisted/web/test/test_wsgi.py", 
      "imports": [
        "StringIO", 
        "cStringIO", 
        "sys.exc_info", 
        "tempfile", 
        "thread.get_ident", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.reactor", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.threadpool", 
        "twisted.trial.unittest", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.test.test_web", 
        "twisted.web.wsgi", 
        "urllib", 
        "zope.interface.verify"
      ]
    }, 
    "twisted.web.test.test_xml": {
      "file": "twisted/web/test/test_xml.py", 
      "imports": [
        "struct", 
        "twisted.trial.unittest", 
        "twisted.web.domhelpers", 
        "twisted.web.microdom", 
        "twisted.web.sux"
      ]
    }, 
    "twisted.web.test.test_xmlrpc": {
      "file": "twisted/web/test/test_xmlrpc.py", 
      "imports": [
        "StringIO", 
        "datetime", 
        "twisted.internet.defer", 
        "twisted.internet.error", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.python.failure", 
        "twisted.test.proto_helpers", 
        "twisted.trial.unittest", 
        "twisted.web.client", 
        "twisted.web.error", 
        "twisted.web.http", 
        "twisted.web.server", 
        "twisted.web.static", 
        "twisted.web.test.test_web", 
        "twisted.web.xmlrpc", 
        "xmlrpclib"
      ]
    }, 
    "twisted.web.twcgi": {
      "file": "twisted/web/twcgi.py", 
      "imports": [
        "os", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.python.filepath", 
        "twisted.python.log", 
        "twisted.spread.pb", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.static", 
        "urllib"
      ]
    }, 
    "twisted.web.util": {
      "file": "twisted/web/util.py", 
      "imports": [
        "cStringIO", 
        "linecache", 
        "twisted.python.deprecate", 
        "twisted.python.modules", 
        "twisted.python.reflect", 
        "twisted.python.urlpath", 
        "twisted.python.versions", 
        "twisted.web.html", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "twisted.web.template", 
        "types"
      ]
    }, 
    "twisted.web.vhost": {
      "file": "twisted/web/vhost.py", 
      "imports": [
        "twisted.python.roots", 
        "twisted.web.resource"
      ]
    }, 
    "twisted.web.wsgi": {
      "file": "twisted/web/wsgi.py", 
      "imports": [
        "sys.exc_info", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "zope.interface"
      ]
    }, 
    "twisted.web.xmlrpc": {
      "file": "twisted/web/xmlrpc.py", 
      "imports": [
        "base64", 
        "twisted.internet.defer", 
        "twisted.internet.protocol", 
        "twisted.internet.reactor", 
        "twisted.internet.ssl", 
        "twisted.python.failure", 
        "twisted.python.log", 
        "twisted.python.reflect", 
        "twisted.web.http", 
        "twisted.web.resource", 
        "twisted.web.server", 
        "urlparse", 
        "xmlrpclib"
      ]
    }, 
    "types": {
      "file": "types.py", 
      "imports": [
        "sys"
      ]
    }, 
    "unittest": {
      "dir": "unittest"
    }, 
    "unittest.__init__": {
      "file": "unittest/__init__.py", 
      "imports": [
        "unittest.case", 
        "unittest.loader", 
        "unittest.main", 
        "unittest.result", 
        "unittest.runner", 
        "unittest.signals", 
        "unittest.suite"
      ]
    }, 
    "unittest.__main__": {
      "file": "unittest/__main__.py", 
      "imports": [
        "sys", 
        "unittest.main"
      ]
    }, 
    "unittest.case": {
      "file": "unittest/case.py", 
      "imports": [
        "collections", 
        "difflib", 
        "functools", 
        "pprint", 
        "re", 
        "sys", 
        "types", 
        "unittest.result", 
        "unittest.util", 
        "warnings"
      ]
    }, 
    "unittest.loader": {
      "file": "unittest/loader.py", 
      "imports": [
        "fnmatch", 
        "functools", 
        "os", 
        "re", 
        "sys", 
        "traceback", 
        "types", 
        "unittest.case", 
        "unittest.suite"
      ]
    }, 
    "unittest.main": {
      "file": "unittest/main.py", 
      "imports": [
        "getopt", 
        "optparse", 
        "os", 
        "sys", 
        "types", 
        "unittest.loader", 
        "unittest.runner", 
        "unittest.signals"
      ]
    }, 
    "unittest.result": {
      "file": "unittest/result.py", 
      "imports": [
        "StringIO", 
        "functools", 
        "os", 
        "sys", 
        "traceback", 
        "unittest.util"
      ]
    }, 
    "unittest.runner": {
      "file": "unittest/runner.py", 
      "imports": [
        "sys", 
        "time", 
        "unittest.result", 
        "unittest.signals"
      ]
    }, 
    "unittest.signals": {
      "file": "unittest/signals.py", 
      "imports": [
        "functools", 
        "weakref", 
        "signal"
      ]
    }, 
    "unittest.suite": {
      "file": "unittest/suite.py", 
      "imports": [
        "sys", 
        "unittest.case", 
        "unittest.util"
      ]
    }, 
    "unittest.test": {
      "dir": "unittest/test"
    }, 
    "unittest.test.__init__": {
      "file": "unittest/test/__init__.py", 
      "imports": [
        "os", 
        "sys", 
        "unittest"
      ]
    }, 
    "unittest.test.dummy": {
      "file": "unittest/test/dummy.py", 
      "imports": []
    }, 
    "unittest.test.support": {
      "file": "unittest/test/support.py", 
      "imports": [
        "unittest"
      ]
    }, 
    "unittest.test.test_assertions": {
      "file": "unittest/test/test_assertions.py", 
      "imports": [
        "unittest", 
        "datetime"
      ]
    }, 
    "unittest.test.test_break": {
      "file": "unittest/test/test_break.py", 
      "imports": [
        "cStringIO.StringIO", 
        "gc", 
        "os", 
        "sys", 
        "unittest", 
        "weakref", 
        "signal"
      ]
    }, 
    "unittest.test.test_case": {
      "file": "unittest/test/test_case.py", 
      "imports": [
        "copy", 
        "difflib", 
        "pickle", 
        "pprint", 
        "re", 
        "sys", 
        "unittest", 
        "unittest.test", 
        "unittest.test.support"
      ]
    }, 
    "unittest.test.test_discovery": {
      "file": "unittest/test/test_discovery.py", 
      "imports": [
        "os", 
        "re", 
        "sys", 
        "unittest", 
        "unittest.test"
      ]
    }, 
    "unittest.test.test_functiontestcase": {
      "file": "unittest/test/test_functiontestcase.py", 
      "imports": [
        "unittest", 
        "unittest.test.support"
      ]
    }, 
    "unittest.test.test_loader": {
      "file": "unittest/test/test_loader.py", 
      "imports": [
        "sys", 
        "types", 
        "unittest"
      ]
    }, 
    "unittest.test.test_program": {
      "file": "unittest/test/test_program.py", 
      "imports": [
        "cStringIO.StringIO", 
        "os", 
        "sys", 
        "unittest", 
        "unittest.test"
      ]
    }, 
    "unittest.test.test_result": {
      "file": "unittest/test/test_result.py", 
      "imports": [
        "StringIO", 
        "sys", 
        "textwrap", 
        "traceback", 
        "unittest", 
        "unittest.test"
      ]
    }, 
    "unittest.test.test_runner": {
      "file": "unittest/test/test_runner.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "pickle", 
        "unittest", 
        "unittest.test.support"
      ]
    }, 
    "unittest.test.test_setups": {
      "file": "unittest/test/test_setups.py", 
      "imports": [
        "cStringIO.StringIO", 
        "sys", 
        "unittest"
      ]
    }, 
    "unittest.test.test_skipping": {
      "file": "unittest/test/test_skipping.py", 
      "imports": [
        "unittest", 
        "unittest.test.support"
      ]
    }, 
    "unittest.test.test_suite": {
      "file": "unittest/test/test_suite.py", 
      "imports": [
        "sys", 
        "unittest", 
        "unittest.test.support"
      ]
    }, 
    "unittest.util": {
      "file": "unittest/util.py", 
      "imports": [
        "collections"
      ]
    }, 
    "urllib": {
      "file": "urllib.py", 
      "imports": [
        "StringIO", 
        "_winreg", 
        "base64", 
        "cStringIO.StringIO", 
        "email.utils", 
        "fnmatch", 
        "ftplib", 
        "getpass", 
        "httplib", 
        "mimetools", 
        "mimetypes", 
        "nturl2path", 
        "os", 
        "re", 
        "rourl2path.pathname2url", 
        "rourl2path.url2pathname", 
        "socket", 
        "ssl", 
        "string", 
        "sys", 
        "tempfile", 
        "time", 
        "urlparse", 
        "warnings", 
        "_scproxy"
      ]
    }, 
    "urllib2": {
      "file": "urllib2.py", 
      "imports": [
        "StringIO", 
        "base64", 
        "bisect", 
        "cStringIO.StringIO", 
        "cookielib", 
        "email.utils", 
        "ftplib", 
        "hashlib", 
        "httplib", 
        "mimetools", 
        "mimetypes", 
        "os", 
        "posixpath", 
        "random", 
        "re", 
        "socket", 
        "ssl", 
        "sys", 
        "time", 
        "types", 
        "urllib", 
        "urlparse", 
        "warnings"
      ]
    }, 
    "urlparse": {
      "file": "urlparse.py", 
      "imports": [
        "collections", 
        "re"
      ]
    }, 
    "user": {
      "file": "user.py", 
      "imports": [
        "os", 
        "warnings"
      ]
    }, 
    "uu": {
      "file": "uu.py", 
      "imports": [
        "binascii", 
        "optparse", 
        "os", 
        "sys"
      ]
    }, 
    "uuid": {
      "file": "uuid.py", 
      "imports": [
        "ctypes", 
        "ctypes.util", 
        "hashlib", 
        "netbios", 
        "os", 
        "random", 
        "re", 
        "socket", 
        "struct", 
        "sys", 
        "time", 
        "win32wnet"
      ]
    }, 
    "warnings": {
      "file": "warnings.py", 
      "imports": [
        "_warnings.default_action", 
        "_warnings.filters", 
        "_warnings.once_registry", 
        "_warnings.warn", 
        "_warnings.warn_explicit", 
        "linecache", 
        "re", 
        "sys", 
        "types"
      ]
    }, 
    "weakref": {
      "file": "weakref.py", 
      "imports": [
        "UserDict", 
        "_weakref.CallableProxyType", 
        "_weakref.ProxyType", 
        "_weakref.ReferenceType", 
        "_weakref.getweakrefcount", 
        "_weakref.getweakrefs", 
        "_weakref.proxy", 
        "_weakref.ref", 
        "_weakrefset", 
        "copy", 
        "exceptions.ReferenceError"
      ]
    }, 
    "webPageLogic": {
      "file": "webPageLogic.py", 
      "imports": [
        "nevow.loaders", 
        "nevow.static", 
        "os", 
        "time", 
        "twisted.python.filepath", 
        "athenaHandler"
      ]
    }, 
    "webbrowser": {
      "file": "webbrowser.py", 
      "imports": [
        "copy", 
        "getopt", 
        "glob", 
        "os", 
        "shlex", 
        "socket", 
        "stat", 
        "subprocess", 
        "sys", 
        "tempfile", 
        "time", 
        "pwd"
      ]
    }, 
    "whichdb": {
      "file": "whichdb.py", 
      "imports": [
        "os", 
        "struct", 
        "sys", 
        "dbm"
      ]
    }, 
    "wsgiref": {
      "dir": "wsgiref"
    }, 
    "wsgiref.__init__": {
      "file": "wsgiref/__init__.py", 
      "imports": []
    }, 
    "wsgiref.handlers": {
      "file": "wsgiref/handlers.py", 
      "imports": [
        "os", 
        "sys", 
        "time", 
        "traceback", 
        "types", 
        "wsgiref.headers", 
        "wsgiref.util"
      ]
    }, 
    "wsgiref.headers": {
      "file": "wsgiref/headers.py", 
      "imports": [
        "re", 
        "types"
      ]
    }, 
    "wsgiref.simple_server": {
      "file": "wsgiref/simple_server.py", 
      "imports": [
        "BaseHTTPServer", 
        "StringIO", 
        "sys", 
        "urllib", 
        "webbrowser", 
        "wsgiref.handlers"
      ]
    }, 
    "wsgiref.util": {
      "file": "wsgiref/util.py", 
      "imports": [
        "StringIO", 
        "posixpath", 
        "urllib"
      ]
    }, 
    "wsgiref.validate": {
      "file": "wsgiref/validate.py", 
      "imports": [
        "re", 
        "sys", 
        "types", 
        "warnings"
      ]
    }, 
    "xdrlib": {
      "file": "xdrlib.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "functools", 
        "struct"
      ]
    }, 
    "xlrd": {
      "dir": "xlrd"
    }, 
    "xlrd.__init__": {
      "file": "xlrd/__init__.py", 
      "imports": [
        "encodings", 
        "mmap", 
        "os", 
        "pprint", 
        "sys", 
        "xlrd.biffh", 
        "xlrd.book", 
        "xlrd.formula", 
        "xlrd.info", 
        "xlrd.licences", 
        "xlrd.sheet", 
        "xlrd.timemachine", 
        "xlrd.xldate", 
        "xlrd.xlsx", 
        "zipfile"
      ]
    }, 
    "xlrd.biffh": {
      "file": "xlrd/biffh.py", 
      "imports": [
        "__future__", 
        "struct", 
        "sys", 
        "xlrd.timemachine"
      ]
    }, 
    "xlrd.book": {
      "file": "xlrd/book.py", 
      "imports": [
        "__future__", 
        "encodings", 
        "gc", 
        "mmap", 
        "struct", 
        "sys", 
        "time", 
        "xlrd.biffh", 
        "xlrd.compdoc", 
        "xlrd.formatting", 
        "xlrd.formula", 
        "xlrd.sheet", 
        "xlrd.timemachine"
      ]
    }, 
    "xlrd.compdoc": {
      "file": "xlrd/compdoc.py", 
      "imports": [
        "__future__", 
        "array", 
        "struct", 
        "sys", 
        "xlrd.timemachine"
      ]
    }, 
    "xlrd.formatting": {
      "file": "xlrd/formatting.py", 
      "imports": [
        "__future__", 
        "re", 
        "struct", 
        "xlrd.biffh", 
        "xlrd.timemachine"
      ]
    }, 
    "xlrd.formula": {
      "file": "xlrd/formula.py", 
      "imports": [
        "__future__", 
        "copy", 
        "operator", 
        "struct", 
        "xlrd.biffh", 
        "xlrd.timemachine"
      ]
    }, 
    "xlrd.info": {
      "file": "xlrd/info.py", 
      "imports": []
    }, 
    "xlrd.licences": {
      "file": "xlrd/licences.py", 
      "imports": []
    }, 
    "xlrd.sheet": {
      "file": "xlrd/sheet.py", 
      "imports": [
        "__future__", 
        "array.array", 
        "struct", 
        "xlrd.biffh", 
        "xlrd.formatting", 
        "xlrd.formula", 
        "xlrd.timemachine"
      ]
    }, 
    "xlrd.timemachine": {
      "file": "xlrd/timemachine.py", 
      "imports": [
        "__future__", 
        "cStringIO", 
        "io", 
        "sys"
      ]
    }, 
    "xlrd.xldate": {
      "file": "xlrd/xldate.py", 
      "imports": [
        "datetime"
      ]
    }, 
    "xlrd.xlsx": {
      "file": "xlrd/xlsx.py", 
      "imports": [
        "__future__", 
        "cElementTree", 
        "elementtree.ElementTree", 
        "lxml.etree", 
        "re", 
        "sys", 
        "xlrd.biffh", 
        "xlrd.book", 
        "xlrd.formatting", 
        "xlrd.sheet", 
        "xlrd.timemachine", 
        "xml.etree.ElementTree", 
        "xml.etree.cElementTree"
      ]
    }, 
    "xlwt": {
      "dir": "xlwt"
    }, 
    "xlwt.BIFFRecords": {
      "file": "xlwt/BIFFRecords.py", 
      "imports": [
        "struct", 
        "xlwt.UnicodeUtils", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.Bitmap": {
      "file": "xlwt/Bitmap.py", 
      "imports": [
        "struct", 
        "xlwt.BIFFRecords"
      ]
    }, 
    "xlwt.Cell": {
      "file": "xlwt/Cell.py", 
      "imports": [
        "struct", 
        "xlwt.BIFFRecords", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.Column": {
      "file": "xlwt/Column.py", 
      "imports": [
        "xlwt.BIFFRecords"
      ]
    }, 
    "xlwt.CompoundDoc": {
      "file": "xlwt/CompoundDoc.py", 
      "imports": [
        "struct", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.ExcelFormula": {
      "file": "xlwt/ExcelFormula.py", 
      "imports": [
        "struct", 
        "xlwt.ExcelFormulaLexer", 
        "xlwt.ExcelFormulaParser", 
        "xlwt.antlr"
      ]
    }, 
    "xlwt.ExcelFormulaLexer": {
      "file": "xlwt/ExcelFormulaLexer.py", 
      "imports": [
        "__future__", 
        "re", 
        "xlwt.ExcelFormulaParser", 
        "xlwt.antlr"
      ]
    }, 
    "xlwt.ExcelFormulaParser": {
      "file": "xlwt/ExcelFormulaParser.py", 
      "imports": [
        "struct", 
        "xlwt.ExcelMagic", 
        "xlwt.UnicodeUtils", 
        "xlwt.Utils", 
        "xlwt.antlr"
      ]
    }, 
    "xlwt.ExcelMagic": {
      "file": "xlwt/ExcelMagic.py", 
      "imports": []
    }, 
    "xlwt.Formatting": {
      "file": "xlwt/Formatting.py", 
      "imports": [
        "xlwt.BIFFRecords"
      ]
    }, 
    "xlwt.Row": {
      "file": "xlwt/Row.py", 
      "imports": [
        "datetime", 
        "decimal", 
        "xlwt.BIFFRecords", 
        "xlwt.Cell", 
        "xlwt.ExcelFormula", 
        "xlwt.Formatting", 
        "xlwt.Style", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.Style": {
      "file": "xlwt/Style.py", 
      "imports": [
        "__future__", 
        "xlwt.BIFFRecords", 
        "xlwt.Formatting", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.UnicodeUtils": {
      "file": "xlwt/UnicodeUtils.py", 
      "imports": [
        "struct", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.Utils": {
      "file": "xlwt/Utils.py", 
      "imports": [
        "re", 
        "xlwt.ExcelMagic", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.Workbook": {
      "file": "xlwt/Workbook.py", 
      "imports": [
        "xlwt.BIFFRecords", 
        "xlwt.CompoundDoc", 
        "xlwt.Style", 
        "xlwt.Utils", 
        "xlwt.Worksheet", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.Worksheet": {
      "file": "xlwt/Worksheet.py", 
      "imports": [
        "tempfile", 
        "xlwt.BIFFRecords", 
        "xlwt.Bitmap", 
        "xlwt.Column", 
        "xlwt.Row", 
        "xlwt.Style", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.__init__": {
      "file": "xlwt/__init__.py", 
      "imports": [
        "xlwt.Column", 
        "xlwt.ExcelFormula", 
        "xlwt.Formatting", 
        "xlwt.Row", 
        "xlwt.Style", 
        "xlwt.Workbook", 
        "xlwt.Worksheet"
      ]
    }, 
    "xlwt.antlr": {
      "file": "xlwt/antlr.py", 
      "imports": [
        "__future__", 
        "sys", 
        "xlwt.compat"
      ]
    }, 
    "xlwt.compat": {
      "file": "xlwt/compat.py", 
      "imports": [
        "sys"
      ]
    }, 
    "xml": {
      "dir": "xml"
    }, 
    "xml.__init__": {
      "file": "xml/__init__.py", 
      "imports": [
        "_xmlplus", 
        "sys"
      ]
    }, 
    "xml.dom": {
      "dir": "xml/dom"
    }, 
    "xml.dom.NodeFilter": {
      "file": "xml/dom/NodeFilter.py", 
      "imports": []
    }, 
    "xml.dom.__init__": {
      "file": "xml/dom/__init__.py", 
      "imports": [
        "xml.dom.domreg"
      ]
    }, 
    "xml.dom.domreg": {
      "file": "xml/dom/domreg.py", 
      "imports": [
        "os", 
        "xml.dom.minicompat"
      ]
    }, 
    "xml.dom.expatbuilder": {
      "file": "xml/dom/expatbuilder.py", 
      "imports": [
        "xml.dom", 
        "xml.dom.NodeFilter", 
        "xml.dom.minicompat", 
        "xml.dom.minidom", 
        "xml.dom.xmlbuilder", 
        "xml.parsers.expat"
      ]
    }, 
    "xml.dom.minicompat": {
      "file": "xml/dom/minicompat.py", 
      "imports": [
        "xml.dom"
      ]
    }, 
    "xml.dom.minidom": {
      "file": "xml/dom/minidom.py", 
      "imports": [
        "StringIO", 
        "codecs", 
        "xml.dom", 
        "xml.dom.domreg", 
        "xml.dom.expatbuilder", 
        "xml.dom.minicompat", 
        "xml.dom.pulldom", 
        "xml.dom.xmlbuilder"
      ]
    }, 
    "xml.dom.pulldom": {
      "file": "xml/dom/pulldom.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "types", 
        "xml.dom", 
        "xml.dom.minidom", 
        "xml.sax", 
        "xml.sax.handler"
      ]
    }, 
    "xml.dom.xmlbuilder": {
      "file": "xml/dom/xmlbuilder.py", 
      "imports": [
        "copy", 
        "posixpath", 
        "urllib2", 
        "urlparse", 
        "xml.dom", 
        "xml.dom.NodeFilter", 
        "xml.dom.expatbuilder"
      ]
    }, 
    "xml.etree": {
      "dir": "xml/etree"
    }, 
    "xml.etree.ElementInclude": {
      "file": "xml/etree/ElementInclude.py", 
      "imports": [
        "copy", 
        "xml.etree.ElementTree"
      ]
    }, 
    "xml.etree.ElementPath": {
      "file": "xml/etree/ElementPath.py", 
      "imports": [
        "re"
      ]
    }, 
    "xml.etree.ElementTree": {
      "file": "xml/etree/ElementTree.py", 
      "imports": [
        "ElementC14N._serialize_c14n", 
        "pyexpat", 
        "re", 
        "sys", 
        "warnings", 
        "xml.etree.ElementPath", 
        "xml.parsers.expat"
      ]
    }, 
    "xml.etree.__init__": {
      "file": "xml/etree/__init__.py", 
      "imports": []
    }, 
    "xml.etree.cElementTree": {
      "file": "xml/etree/cElementTree.py", 
      "imports": [
        "_elementtree"
      ]
    }, 
    "xml.parsers": {
      "dir": "xml/parsers"
    }, 
    "xml.parsers.__init__": {
      "file": "xml/parsers/__init__.py", 
      "imports": []
    }, 
    "xml.parsers.expat": {
      "file": "xml/parsers/expat.py", 
      "imports": [
        "pyexpat.*"
      ]
    }, 
    "xml.sax": {
      "dir": "xml/sax"
    }, 
    "xml.sax.__init__": {
      "file": "xml/sax/__init__.py", 
      "imports": [
        "StringIO", 
        "cStringIO.StringIO", 
        "org.python.core.imp", 
        "os", 
        "sys", 
        "xml.sax._exceptions", 
        "xml.sax.expatreader", 
        "xml.sax.handler", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax._exceptions": {
      "file": "xml/sax/_exceptions.py", 
      "imports": [
        "java.lang.Exception", 
        "sys"
      ]
    }, 
    "xml.sax.expatreader": {
      "file": "xml/sax/expatreader.py", 
      "imports": [
        "_weakref", 
        "sys", 
        "weakref", 
        "xml.parsers.expat", 
        "xml.sax._exceptions", 
        "xml.sax.handler", 
        "xml.sax.saxutils", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax.handler": {
      "file": "xml/sax/handler.py", 
      "imports": []
    }, 
    "xml.sax.saxutils": {
      "file": "xml/sax/saxutils.py", 
      "imports": [
        "io", 
        "os", 
        "sys", 
        "types", 
        "urllib", 
        "urlparse", 
        "xml.sax.handler", 
        "xml.sax.xmlreader"
      ]
    }, 
    "xml.sax.xmlreader": {
      "file": "xml/sax/xmlreader.py", 
      "imports": [
        "xml.sax._exceptions", 
        "xml.sax.handler", 
        "xml.sax.saxutils"
      ]
    }, 
    "xmllib": {
      "file": "xmllib.py", 
      "imports": [
        "getopt", 
        "re", 
        "string", 
        "sys", 
        "time.time", 
        "warnings"
      ]
    }, 
    "xmlrpclib": {
      "file": "xmlrpclib.py", 
      "imports": [
        "StringIO", 
        "_xmlrpclib", 
        "base64", 
        "cStringIO", 
        "errno", 
        "gzip", 
        "httplib", 
        "operator", 
        "re", 
        "socket", 
        "string", 
        "sys.modules", 
        "time", 
        "types", 
        "urllib", 
        "xml.parsers.expat", 
        "xmllib", 
        "datetime"
      ]
    }, 
    "zipfile": {
      "file": "zipfile.py", 
      "imports": [
        "binascii", 
        "cStringIO", 
        "io", 
        "os", 
        "py_compile", 
        "re", 
        "shutil", 
        "stat", 
        "string", 
        "struct", 
        "sys", 
        "textwrap", 
        "time", 
        "warnings", 
        "zlib"
      ]
    }, 
    "zipimport": {
      "file": "zipimport.py", 
      "imports": []
    }, 
    "zope": {
      "dir": "zope"
    }, 
    "zope.__init__": {
      "file": "zope/__init__.py", 
      "imports": []
    }, 
    "zope.fixers": {
      "dir": "zope/fixers"
    }, 
    "zope.fixers.__init__": {
      "file": "zope/fixers/__init__.py", 
      "imports": []
    }, 
    "zope.fixers.base": {
      "file": "zope/fixers/base.py", 
      "imports": [
        "lib2to3.fixer_base", 
        "lib2to3.fixer_util", 
        "lib2to3.patcomp"
      ]
    }, 
    "zope.fixers.fix_class_provides": {
      "file": "zope/fixers/fix_class_provides.py", 
      "imports": [
        "zope.fixers.base"
      ]
    }, 
    "zope.fixers.fix_implements": {
      "file": "zope/fixers/fix_implements.py", 
      "imports": [
        "zope.fixers.base"
      ]
    }, 
    "zope.fixers.fix_implements_only": {
      "file": "zope/fixers/fix_implements_only.py", 
      "imports": [
        "zope.fixers.base"
      ]
    }, 
    "zope.fixers.tests": {
      "file": "zope/fixers/tests.py", 
      "imports": [
        "lib2to3.refactor", 
        "unittest"
      ]
    }, 
    "zope.interface": {
      "dir": "zope/interface"
    }, 
    "zope.interface.__init__": {
      "file": "zope/interface/__init__.py", 
      "imports": [
        "zope.interface.declarations", 
        "zope.interface.exceptions", 
        "zope.interface.interface", 
        "zope.interface.interfaces"
      ]
    }, 
    "zope.interface._compat": {
      "file": "zope/interface/_compat.py", 
      "imports": [
        "sys", 
        "types"
      ]
    }, 
    "zope.interface._flatten": {
      "file": "zope/interface/_flatten.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "zope.interface.adapter": {
      "file": "zope/interface/adapter.py", 
      "imports": [
        "_zope_interface_coptimizations.LookupBase", 
        "_zope_interface_coptimizations.VerifyingBase", 
        "weakref", 
        "zope.interface", 
        "zope.interface._compat", 
        "zope.interface.ro"
      ]
    }, 
    "zope.interface.advice": {
      "file": "zope/interface/advice.py", 
      "imports": [
        "sys", 
        "types"
      ]
    }, 
    "zope.interface.common": {
      "dir": "zope/interface/common"
    }, 
    "zope.interface.common.__init__": {
      "file": "zope/interface/common/__init__.py", 
      "imports": []
    }, 
    "zope.interface.common.idatetime": {
      "file": "zope/interface/common/idatetime.py", 
      "imports": [
        "datetime", 
        "zope.interface"
      ]
    }, 
    "zope.interface.common.interfaces": {
      "file": "zope/interface/common/interfaces.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "zope.interface.common.mapping": {
      "file": "zope/interface/common/mapping.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "zope.interface.common.sequence": {
      "file": "zope/interface/common/sequence.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "zope.interface.common.tests": {
      "dir": "zope/interface/common/tests"
    }, 
    "zope.interface.common.tests.__init__": {
      "file": "zope/interface/common/tests/__init__.py", 
      "imports": []
    }, 
    "zope.interface.common.tests.basemapping": {
      "file": "zope/interface/common/tests/basemapping.py", 
      "imports": [
        "operator.__getitem__"
      ]
    }, 
    "zope.interface.common.tests.test_idatetime": {
      "file": "zope/interface/common/tests/test_idatetime.py", 
      "imports": [
        "datetime", 
        "unittest", 
        "zope.interface.common.idatetime", 
        "zope.interface.verify"
      ]
    }, 
    "zope.interface.common.tests.test_import_interfaces": {
      "file": "zope/interface/common/tests/test_import_interfaces.py", 
      "imports": [
        "doctest", 
        "unittest"
      ]
    }, 
    "zope.interface.declarations": {
      "file": "zope/interface/declarations.py", 
      "imports": [
        "_zope_interface_coptimizations", 
        "_zope_interface_coptimizations.ClassProvidesBase", 
        "_zope_interface_coptimizations.ObjectSpecificationDescriptor", 
        "_zope_interface_coptimizations.getObjectSpecification", 
        "_zope_interface_coptimizations.implementedBy", 
        "_zope_interface_coptimizations.providedBy", 
        "sys", 
        "types", 
        "warnings", 
        "weakref", 
        "zope.interface._compat", 
        "zope.interface.advice", 
        "zope.interface.interface"
      ]
    }, 
    "zope.interface.document": {
      "file": "zope/interface/document.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "zope.interface.exceptions": {
      "file": "zope/interface/exceptions.py", 
      "imports": []
    }, 
    "zope.interface.interface": {
      "file": "zope/interface/interface.py", 
      "imports": [
        "__future__", 
        "_zope_interface_coptimizations.InterfaceBase", 
        "_zope_interface_coptimizations.SpecificationBase", 
        "_zope_interface_coptimizations.adapter_hooks", 
        "sys", 
        "types", 
        "warnings", 
        "weakref", 
        "zope.interface.declarations", 
        "zope.interface.exceptions", 
        "zope.interface.interfaces", 
        "zope.interface.ro"
      ]
    }, 
    "zope.interface.interfaces": {
      "file": "zope/interface/interfaces.py", 
      "imports": [
        "zope.interface._compat", 
        "zope.interface.declarations", 
        "zope.interface.interface"
      ]
    }, 
    "zope.interface.registry": {
      "file": "zope/interface/registry.py", 
      "imports": [
        "zope", 
        "zope.interface._compat", 
        "zope.interface.adapter", 
        "zope.interface.declarations", 
        "zope.interface.interface", 
        "zope.interface.interfaces"
      ]
    }, 
    "zope.interface.ro": {
      "file": "zope/interface/ro.py", 
      "imports": []
    }, 
    "zope.interface.tests": {
      "dir": "zope/interface/tests"
    }, 
    "zope.interface.tests.__init__": {
      "file": "zope/interface/tests/__init__.py", 
      "imports": [
        "os", 
        "unittest"
      ]
    }, 
    "zope.interface.tests.advisory_testing": {
      "file": "zope/interface/tests/advisory_testing.py", 
      "imports": [
        "sys", 
        "types", 
        "zope.interface.advice"
      ]
    }, 
    "zope.interface.tests.dummy": {
      "file": "zope/interface/tests/dummy.py", 
      "imports": [
        "zope.interface", 
        "zope.interface.tests.idummy"
      ]
    }, 
    "zope.interface.tests.idummy": {
      "file": "zope/interface/tests/idummy.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "zope.interface.tests.ifoo": {
      "file": "zope/interface/tests/ifoo.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "zope.interface.tests.ifoo_other": {
      "file": "zope/interface/tests/ifoo_other.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "zope.interface.tests.m1": {
      "file": "zope/interface/tests/m1.py", 
      "imports": [
        "zope.interface"
      ]
    }, 
    "zope.interface.tests.m2": {
      "file": "zope/interface/tests/m2.py", 
      "imports": []
    }, 
    "zope.interface.tests.odd": {
      "file": "zope/interface/tests/odd.py", 
      "imports": [
        "__main__", 
        "doctest"
      ]
    }, 
    "zope.interface.tests.test_adapter": {
      "file": "zope/interface/tests/test_adapter.py", 
      "imports": [
        "sys", 
        "unittest", 
        "zope.interface", 
        "zope.interface._compat", 
        "zope.interface.adapter", 
        "zope.interface.declarations", 
        "zope.interface.interface"
      ]
    }, 
    "zope.interface.tests.test_advice": {
      "file": "zope/interface/tests/test_advice.py", 
      "imports": [
        "sys", 
        "types", 
        "unittest", 
        "zope.interface._compat", 
        "zope.interface.advice", 
        "zope.interface.tests.advisory_testing"
      ]
    }, 
    "zope.interface.tests.test_declarations": {
      "file": "zope/interface/tests/test_declarations.py", 
      "imports": [
        "unittest", 
        "warnings", 
        "zope.interface._compat", 
        "zope.interface.declarations", 
        "zope.interface.interface"
      ]
    }, 
    "zope.interface.tests.test_document": {
      "file": "zope/interface/tests/test_document.py", 
      "imports": [
        "unittest", 
        "zope.interface", 
        "zope.interface.document"
      ]
    }, 
    "zope.interface.tests.test_element": {
      "file": "zope/interface/tests/test_element.py", 
      "imports": [
        "unittest", 
        "zope.interface.interface"
      ]
    }, 
    "zope.interface.tests.test_exceptions": {
      "file": "zope/interface/tests/test_exceptions.py", 
      "imports": [
        "unittest", 
        "zope.interface", 
        "zope.interface.exceptions"
      ]
    }, 
    "zope.interface.tests.test_interface": {
      "file": "zope/interface/tests/test_interface.py", 
      "imports": [
        "doctest", 
        "sys", 
        "unittest", 
        "warnings", 
        "zope.interface", 
        "zope.interface._compat", 
        "zope.interface.declarations", 
        "zope.interface.exceptions", 
        "zope.interface.interface", 
        "zope.interface.verify"
      ]
    }, 
    "zope.interface.tests.test_interfaces": {
      "file": "zope/interface/tests/test_interfaces.py", 
      "imports": [
        "unittest", 
        "zope.interface.interfaces", 
        "zope.interface.verify"
      ]
    }, 
    "zope.interface.tests.test_odd_declarations": {
      "file": "zope/interface/tests/test_odd_declarations.py", 
      "imports": [
        "doctest", 
        "unittest", 
        "zope.interface", 
        "zope.interface._compat", 
        "zope.interface.tests.odd"
      ]
    }, 
    "zope.interface.tests.test_registry": {
      "file": "zope/interface/tests/test_registry.py", 
      "imports": [
        "unittest", 
        "zope.interface._compat", 
        "zope.interface.adapter", 
        "zope.interface.declarations", 
        "zope.interface.interface", 
        "zope.interface.interfaces", 
        "zope.interface.registry", 
        "zope.interface.verify"
      ]
    }, 
    "zope.interface.tests.test_sorting": {
      "file": "zope/interface/tests/test_sorting.py", 
      "imports": [
        "unittest", 
        "unittest.main", 
        "zope.interface", 
        "zope.interface.tests.m1"
      ]
    }, 
    "zope.interface.tests.test_verify": {
      "file": "zope/interface/tests/test_verify.py", 
      "imports": [
        "unittest", 
        "zope.interface", 
        "zope.interface.exceptions", 
        "zope.interface.interface", 
        "zope.interface.tests.dummy", 
        "zope.interface.tests.idummy", 
        "zope.interface.verify"
      ]
    }, 
    "zope.interface.verify": {
      "file": "zope/interface/verify.py", 
      "imports": [
        "sys", 
        "types", 
        "zope.interface.exceptions", 
        "zope.interface.interface"
      ]
    }
  }, 
  "preload": {
    "Queue": "\"\"\"A multi-producer, multi-consumer queue.\"\"\"\n\nfrom time import time as _time\n\nimport dummy_threading as _threading\nfrom collections import deque\nimport heapq\n\n__all__ = ['Empty', 'Full', 'Queue', 'PriorityQueue', 'LifoQueue']\n\nclass Empty(Exception):\n    \"Exception raised by Queue.get(block=0)/get_nowait().\"\n    pass\n\nclass Full(Exception):\n    \"Exception raised by Queue.put(block=0)/put_nowait().\"\n    pass\n\nclass Queue:\n    \"\"\"Create a queue object with a given maximum size.\n\n    If maxsize is <= 0, the queue size is infinite.\n    \"\"\"\n    def __init__(self, maxsize=0):\n        self.maxsize = maxsize\n        self._init(maxsize)\n        # mutex must be held whenever the queue is mutating.  All methods\n        # that acquire mutex must release it before returning.  mutex\n        # is shared between the three conditions, so acquiring and\n        # releasing the conditions also acquires and releases mutex.\n        self.mutex = _threading.Lock()\n        # Notify not_empty whenever an item is added to the queue; a\n        # thread waiting to get is notified then.\n        self.not_empty = _threading.Condition(self.mutex)\n        # Notify not_full whenever an item is removed from the queue;\n        # a thread waiting to put is notified then.\n        self.not_full = _threading.Condition(self.mutex)\n        # Notify all_tasks_done whenever the number of unfinished tasks\n        # drops to zero; thread waiting to join() is notified to resume\n        self.all_tasks_done = _threading.Condition(self.mutex)\n        self.unfinished_tasks = 0\n\n    def task_done(self):\n        \"\"\"Indicate that a formerly enqueued task is complete.\n\n        Used by Queue consumer threads.  For each get() used to fetch a task,\n        a subsequent call to task_done() tells the queue that the processing\n        on the task is complete.\n\n        If a join() is currently blocking, it will resume when all items\n        have been processed (meaning that a task_done() call was received\n        for every item that had been put() into the queue).\n\n        Raises a ValueError if called more times than there were items\n        placed in the queue.\n        \"\"\"\n        self.all_tasks_done.acquire()\n        try:\n            unfinished = self.unfinished_tasks - 1\n            if unfinished <= 0:\n                if unfinished < 0:\n                    raise ValueError('task_done() called too many times')\n                self.all_tasks_done.notify_all()\n            self.unfinished_tasks = unfinished\n        finally:\n            self.all_tasks_done.release()\n\n    def join(self):\n        \"\"\"Blocks until all items in the Queue have been gotten and processed.\n\n        The count of unfinished tasks goes up whenever an item is added to the\n        queue. The count goes down whenever a consumer thread calls task_done()\n        to indicate the item was retrieved and all work on it is complete.\n\n        When the count of unfinished tasks drops to zero, join() unblocks.\n        \"\"\"\n        self.all_tasks_done.acquire()\n        try:\n            while self.unfinished_tasks:\n                self.all_tasks_done.wait()\n        finally:\n            self.all_tasks_done.release()\n\n    def qsize(self):\n        \"\"\"Return the approximate size of the queue (not reliable!).\"\"\"\n        self.mutex.acquire()\n        n = self._qsize()\n        self.mutex.release()\n        return n\n\n    def empty(self):\n        \"\"\"Return True if the queue is empty, False otherwise (not reliable!).\"\"\"\n        self.mutex.acquire()\n        n = not self._qsize()\n        self.mutex.release()\n        return n\n\n    def full(self):\n        \"\"\"Return True if the queue is full, False otherwise (not reliable!).\"\"\"\n        self.mutex.acquire()\n        n = 0 < self.maxsize == self._qsize()\n        self.mutex.release()\n        return n\n\n    def put(self, item, block=True, timeout=None):\n        \"\"\"Put an item into the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until a free slot is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Full exception if no free slot was available within that time.\n        Otherwise ('block' is false), put an item on the queue if a free slot\n        is immediately available, else raise the Full exception ('timeout'\n        is ignored in that case).\n        \"\"\"\n        self.not_full.acquire()\n        try:\n            if self.maxsize > 0:\n                if not block:\n                    if self._qsize() == self.maxsize:\n                        raise Full\n                elif timeout is None:\n                    while self._qsize() == self.maxsize:\n                        self.not_full.wait()\n                elif timeout < 0:\n                    raise ValueError(\"'timeout' must be a non-negative number\")\n                else:\n                    endtime = _time() + timeout\n                    while self._qsize() == self.maxsize:\n                        remaining = endtime - _time()\n                        if remaining <= 0.0:\n                            raise Full\n                        self.not_full.wait(remaining)\n            self._put(item)\n            self.unfinished_tasks += 1\n            self.not_empty.notify()\n        finally:\n            self.not_full.release()\n\n    def put_nowait(self, item):\n        \"\"\"Put an item into the queue without blocking.\n\n        Only enqueue the item if a free slot is immediately available.\n        Otherwise raise the Full exception.\n        \"\"\"\n        return self.put(item, False)\n\n    def get(self, block=True, timeout=None):\n        \"\"\"Remove and return an item from the queue.\n\n        If optional args 'block' is true and 'timeout' is None (the default),\n        block if necessary until an item is available. If 'timeout' is\n        a non-negative number, it blocks at most 'timeout' seconds and raises\n        the Empty exception if no item was available within that time.\n        Otherwise ('block' is false), return an item if one is immediately\n        available, else raise the Empty exception ('timeout' is ignored\n        in that case).\n        \"\"\"\n        self.not_empty.acquire()\n        try:\n            if not block:\n                if not self._qsize():\n                    raise Empty\n            elif timeout is None:\n                while not self._qsize():\n                    self.not_empty.wait()\n            elif timeout < 0:\n                raise ValueError(\"'timeout' must be a non-negative number\")\n            else:\n                endtime = _time() + timeout\n                while not self._qsize():\n                    remaining = endtime - _time()\n                    if remaining <= 0.0:\n                        raise Empty\n                    self.not_empty.wait(remaining)\n            item = self._get()\n            self.not_full.notify()\n            return item\n        finally:\n            self.not_empty.release()\n\n    def get_nowait(self):\n        \"\"\"Remove and return an item from the queue without blocking.\n\n        Only get an item if one is immediately available. Otherwise\n        raise the Empty exception.\n        \"\"\"\n        return self.get(False)\n\n    # Override these methods to implement other queue organizations\n    # (e.g. stack or priority queue).\n    # These will only be called with appropriate locks held\n\n    # Initialize the queue representation\n    def _init(self, maxsize):\n        self.queue = deque()\n\n    def _qsize(self, len=len):\n        return len(self.queue)\n\n    # Put a new item in the queue\n    def _put(self, item):\n        self.queue.append(item)\n\n    # Get an item from the queue\n    def _get(self):\n        return self.queue.popleft()\n\n\nclass PriorityQueue(Queue):\n    '''Variant of Queue that retrieves open entries in priority order (lowest first).\n\n    Entries are typically tuples of the form:  (priority number, data).\n    '''\n\n    def _init(self, maxsize):\n        self.queue = []\n\n    def _qsize(self, len=len):\n        return len(self.queue)\n\n    def _put(self, item, heappush=heapq.heappush):\n        heappush(self.queue, item)\n\n    def _get(self, heappop=heapq.heappop):\n        return heappop(self.queue)\n\n\nclass LifoQueue(Queue):\n    '''Variant of Queue that retrieves most recently added entries first.'''\n\n    def _init(self, maxsize):\n        self.queue = []\n\n    def _qsize(self, len=len):\n        return len(self.queue)\n\n    def _put(self, item):\n        self.queue.append(item)\n\n    def _get(self):\n        return self.queue.pop()\n", 
    "StringIO": "r\"\"\"File-like objects that read from or write to a string buffer.\n\nThis implements (nearly) all stdio methods.\n\nf = StringIO()      # ready for writing\nf = StringIO(buf)   # ready for reading\nf.close()           # explicitly release resources held\nflag = f.isatty()   # always false\npos = f.tell()      # get current position\nf.seek(pos)         # set current position\nf.seek(pos, mode)   # mode 0: absolute; 1: relative; 2: relative to EOF\nbuf = f.read()      # read until EOF\nbuf = f.read(n)     # read up to n bytes\nbuf = f.readline()  # read until end of line ('\\n') or EOF\nlist = f.readlines()# list of f.readline() results until EOF\nf.truncate([size])  # truncate file at to at most size (default: current pos)\nf.write(buf)        # write at current position\nf.writelines(list)  # for line in list: f.write(line)\nf.getvalue()        # return whole file's contents as a string\n\nNotes:\n- Using a real file is often faster (but less convenient).\n- There's also a much faster implementation in C, called cStringIO, but\n  it's not subclassable.\n- fileno() is left unimplemented so that code which uses it triggers\n  an exception early.\n- Seeking far beyond EOF and then writing will insert real null\n  bytes that occupy space in the buffer.\n- There's a simple test set (see end of this file).\n\"\"\"\ntry:\n    from errno import EINVAL\nexcept ImportError:\n    EINVAL = 22\n\n__all__ = [\"StringIO\"]\n\ndef _complain_ifclosed(closed):\n    if closed:\n        raise ValueError, \"I/O operation on closed file\"\n\nclass StringIO:\n    \"\"\"class StringIO([buffer])\n\n    When a StringIO object is created, it can be initialized to an existing\n    string by passing the string to the constructor. If no string is given,\n    the StringIO will start empty.\n\n    The StringIO object can accept either Unicode or 8-bit strings, but\n    mixing the two may take some care. If both are used, 8-bit strings that\n    cannot be interpreted as 7-bit ASCII (that use the 8th bit) will cause\n    a UnicodeError to be raised when getvalue() is called.\n    \"\"\"\n    def __init__(self, buf = ''):\n        # Force self.buf to be a string or unicode\n        if not isinstance(buf, basestring):\n            buf = str(buf)\n        self.buf = buf\n        self.len = len(buf)\n        self.buflist = []\n        self.pos = 0\n        self.closed = False\n        self.softspace = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        \"\"\"A file object is its own iterator, for example iter(f) returns f\n        (unless f is closed). When a file is used as an iterator, typically\n        in a for loop (for example, for line in f: print line), the next()\n        method is called repeatedly. This method returns the next input line,\n        or raises StopIteration when EOF is hit.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        r = self.readline()\n        if not r:\n            raise StopIteration\n        return r\n\n    def close(self):\n        \"\"\"Free the memory buffer.\n        \"\"\"\n        if not self.closed:\n            self.closed = True\n            del self.buf, self.pos\n\n    def isatty(self):\n        \"\"\"Returns False because StringIO objects are not connected to a\n        tty-like device.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        return False\n\n    def seek(self, pos, mode = 0):\n        \"\"\"Set the file's current position.\n\n        The mode argument is optional and defaults to 0 (absolute file\n        positioning); other values are 1 (seek relative to the current\n        position) and 2 (seek relative to the file's end).\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if mode == 1:\n            pos += self.pos\n        elif mode == 2:\n            pos += self.len\n        self.pos = max(0, pos)\n\n    def tell(self):\n        \"\"\"Return the file's current position.\"\"\"\n        _complain_ifclosed(self.closed)\n        return self.pos\n\n    def read(self, n = -1):\n        \"\"\"Read at most size bytes from the file\n        (less if the read hits EOF before obtaining size bytes).\n\n        If the size argument is negative or omitted, read all data until EOF\n        is reached. The bytes are returned as a string object. An empty\n        string is returned when EOF is encountered immediately.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        if n is None or n < 0:\n            newpos = self.len\n        else:\n            newpos = min(self.pos+n, self.len)\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readline(self, length=None):\n        r\"\"\"Read one entire line from the file.\n\n        A trailing newline character is kept in the string (but may be absent\n        when a file ends with an incomplete line). If the size argument is\n        present and non-negative, it is a maximum byte count (including the\n        trailing newline) and an incomplete line may be returned.\n\n        An empty string is returned only when EOF is encountered immediately.\n\n        Note: Unlike stdio's fgets(), the returned string contains null\n        characters ('\\0') if they occurred in the input.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        i = self.buf.find('\\n', self.pos)\n        if i < 0:\n            newpos = self.len\n        else:\n            newpos = i+1\n        if length is not None and length >= 0:\n            if self.pos + length < newpos:\n                newpos = self.pos + length\n        r = self.buf[self.pos:newpos]\n        self.pos = newpos\n        return r\n\n    def readlines(self, sizehint = 0):\n        \"\"\"Read until EOF using readline() and return a list containing the\n        lines thus read.\n\n        If the optional sizehint argument is present, instead of reading up\n        to EOF, whole lines totalling approximately sizehint bytes (or more\n        to accommodate a final whole line).\n        \"\"\"\n        total = 0\n        lines = []\n        line = self.readline()\n        while line:\n            lines.append(line)\n            total += len(line)\n            if 0 < sizehint <= total:\n                break\n            line = self.readline()\n        return lines\n\n    def truncate(self, size=None):\n        \"\"\"Truncate the file's size.\n\n        If the optional size argument is present, the file is truncated to\n        (at most) that size. The size defaults to the current position.\n        The current file position is not changed unless the position\n        is beyond the new file size.\n\n        If the specified size exceeds the file's current size, the\n        file remains unchanged.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if size is None:\n            size = self.pos\n        elif size < 0:\n            raise IOError(EINVAL, \"Negative size not allowed\")\n        elif size < self.pos:\n            self.pos = size\n        self.buf = self.getvalue()[:size]\n        self.len = size\n\n    def write(self, s):\n        \"\"\"Write a string to the file.\n\n        There is no return value.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if not s: return\n        # Force s to be a string or unicode\n        if not isinstance(s, basestring):\n            s = str(s)\n        spos = self.pos\n        slen = self.len\n        if spos == slen:\n            self.buflist.append(s)\n            self.len = self.pos = spos + len(s)\n            return\n        if spos > slen:\n            self.buflist.append('\\0'*(spos - slen))\n            slen = spos\n        newpos = spos + len(s)\n        if spos < slen:\n            if self.buflist:\n                self.buf += ''.join(self.buflist)\n            self.buflist = [self.buf[:spos], s, self.buf[newpos:]]\n            self.buf = ''\n            if newpos > slen:\n                slen = newpos\n        else:\n            self.buflist.append(s)\n            slen = newpos\n        self.len = slen\n        self.pos = newpos\n\n    def writelines(self, iterable):\n        \"\"\"Write a sequence of strings to the file. The sequence can be any\n        iterable object producing strings, typically a list of strings. There\n        is no return value.\n\n        (The name is intended to match readlines(); writelines() does not add\n        line separators.)\n        \"\"\"\n        write = self.write\n        for line in iterable:\n            write(line)\n\n    def flush(self):\n        \"\"\"Flush the internal buffer\n        \"\"\"\n        _complain_ifclosed(self.closed)\n\n    def getvalue(self):\n        \"\"\"\n        Retrieve the entire contents of the \"file\" at any time before\n        the StringIO object's close() method is called.\n\n        The StringIO object can accept either Unicode or 8-bit strings,\n        but mixing the two may take some care. If both are used, 8-bit\n        strings that cannot be interpreted as 7-bit ASCII (that use the\n        8th bit) will cause a UnicodeError to be raised when getvalue()\n        is called.\n        \"\"\"\n        _complain_ifclosed(self.closed)\n        if self.buflist:\n            self.buf += ''.join(self.buflist)\n            self.buflist = []\n        return self.buf\n\n\n# A little test suite\n\ndef test():\n    import sys\n    if sys.argv[1:]:\n        file = sys.argv[1]\n    else:\n        file = '/etc/passwd'\n    lines = open(file, 'r').readlines()\n    text = open(file, 'r').read()\n    f = StringIO()\n    for line in lines[:-2]:\n        f.write(line)\n    f.writelines(lines[-2:])\n    if f.getvalue() != text:\n        raise RuntimeError, 'write failed'\n    length = f.tell()\n    print 'File length =', length\n    f.seek(len(lines[0]))\n    f.write(lines[1])\n    f.seek(0)\n    print 'First line =', repr(f.readline())\n    print 'Position =', f.tell()\n    line = f.readline()\n    print 'Second line =', repr(line)\n    f.seek(-len(line), 1)\n    line2 = f.read(len(line))\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back'\n    f.seek(len(line2), 1)\n    list = f.readlines()\n    line = list[-1]\n    f.seek(f.tell() - len(line))\n    line2 = f.read()\n    if line != line2:\n        raise RuntimeError, 'bad result after seek back from EOF'\n    print 'Read', len(list), 'more lines'\n    print 'File length =', f.tell()\n    if f.tell() != length:\n        raise RuntimeError, 'bad length'\n    f.truncate(length/2)\n    f.seek(0, 2)\n    print 'Truncated length =', f.tell()\n    if f.tell() != length/2:\n        raise RuntimeError, 'truncate did not adjust length'\n    f.close()\n\nif __name__ == '__main__':\n    test()\n", 
    "UserDict": "\"\"\"A more or less complete user-defined wrapper around dictionary objects.\"\"\"\n\nclass UserDict:\n    def __init__(self, dict=None, **kwargs):\n        self.data = {}\n        if dict is not None:\n            self.update(dict)\n        if len(kwargs):\n            self.update(kwargs)\n    def __repr__(self): return repr(self.data)\n    def __cmp__(self, dict):\n        if isinstance(dict, UserDict):\n            return cmp(self.data, dict.data)\n        else:\n            return cmp(self.data, dict)\n    __hash__ = None # Avoid Py3k warning\n    def __len__(self): return len(self.data)\n    def __getitem__(self, key):\n        if key in self.data:\n            return self.data[key]\n        if hasattr(self.__class__, \"__missing__\"):\n            return self.__class__.__missing__(self, key)\n        raise KeyError(key)\n    def __setitem__(self, key, item): self.data[key] = item\n    def __delitem__(self, key): del self.data[key]\n    def clear(self): self.data.clear()\n    def copy(self):\n        if self.__class__ is UserDict:\n            return UserDict(self.data.copy())\n        import copy\n        data = self.data\n        try:\n            self.data = {}\n            c = copy.copy(self)\n        finally:\n            self.data = data\n        c.update(self)\n        return c\n    def keys(self): return self.data.keys()\n    def items(self): return self.data.items()\n    def iteritems(self): return self.data.iteritems()\n    def iterkeys(self): return self.data.iterkeys()\n    def itervalues(self): return self.data.itervalues()\n    def values(self): return self.data.values()\n    def has_key(self, key): return key in self.data\n    def update(self, dict=None, **kwargs):\n        if dict is None:\n            pass\n        elif isinstance(dict, UserDict):\n            self.data.update(dict.data)\n        elif isinstance(dict, type({})) or not hasattr(dict, 'items'):\n            self.data.update(dict)\n        else:\n            for k, v in dict.items():\n                self[k] = v\n        if len(kwargs):\n            self.data.update(kwargs)\n    def get(self, key, failobj=None):\n        if key not in self:\n            return failobj\n        return self[key]\n    def setdefault(self, key, failobj=None):\n        if key not in self:\n            self[key] = failobj\n        return self[key]\n    def pop(self, key, *args):\n        return self.data.pop(key, *args)\n    def popitem(self):\n        return self.data.popitem()\n    def __contains__(self, key):\n        return key in self.data\n    @classmethod\n    def fromkeys(cls, iterable, value=None):\n        d = cls()\n        for key in iterable:\n            d[key] = value\n        return d\n\nclass IterableUserDict(UserDict):\n    def __iter__(self):\n        return iter(self.data)\n\ntry:\n    import _abcoll\nexcept ImportError:\n    pass    # e.g. no '_weakref' module on this pypy\nelse:\n    _abcoll.MutableMapping.register(IterableUserDict)\n\n\nclass DictMixin:\n    # Mixin defining all dictionary methods for classes that already have\n    # a minimum dictionary interface including getitem, setitem, delitem,\n    # and keys. Without knowledge of the subclass constructor, the mixin\n    # does not define __init__() or copy().  In addition to the four base\n    # methods, progressively more efficiency comes with defining\n    # __contains__(), __iter__(), and iteritems().\n\n    # second level definitions support higher levels\n    def __iter__(self):\n        for k in self.keys():\n            yield k\n    def has_key(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        return True\n    def __contains__(self, key):\n        return self.has_key(key)\n\n    # third level takes advantage of second level definitions\n    def iteritems(self):\n        for k in self:\n            yield (k, self[k])\n    def iterkeys(self):\n        return self.__iter__()\n\n    # fourth level uses definitions from lower levels\n    def itervalues(self):\n        for _, v in self.iteritems():\n            yield v\n    def values(self):\n        return [v for _, v in self.iteritems()]\n    def items(self):\n        return list(self.iteritems())\n    def clear(self):\n        for key in self.keys():\n            del self[key]\n    def setdefault(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n    def pop(self, key, *args):\n        if len(args) > 1:\n            raise TypeError, \"pop expected at most 2 arguments, got \"\\\n                              + repr(1 + len(args))\n        try:\n            value = self[key]\n        except KeyError:\n            if args:\n                return args[0]\n            raise\n        del self[key]\n        return value\n    def popitem(self):\n        try:\n            k, v = self.iteritems().next()\n        except StopIteration:\n            raise KeyError, 'container is empty'\n        del self[k]\n        return (k, v)\n    def update(self, other=None, **kwargs):\n        # Make progressively weaker assumptions about \"other\"\n        if other is None:\n            pass\n        elif hasattr(other, 'iteritems'):  # iteritems saves memory and lookups\n            for k, v in other.iteritems():\n                self[k] = v\n        elif hasattr(other, 'keys'):\n            for k in other.keys():\n                self[k] = other[k]\n        else:\n            for k, v in other:\n                self[k] = v\n        if kwargs:\n            self.update(kwargs)\n    def get(self, key, default=None):\n        try:\n            return self[key]\n        except KeyError:\n            return default\n    def __repr__(self):\n        return repr(dict(self.iteritems()))\n    def __cmp__(self, other):\n        if other is None:\n            return 1\n        if isinstance(other, DictMixin):\n            other = dict(other.iteritems())\n        return cmp(dict(self.iteritems()), other)\n    def __len__(self):\n        return len(self.keys())\n", 
    "_LWPCookieJar": "\"\"\"Load / save to libwww-perl (LWP) format files.\n\nActually, the format is slightly extended from that used by LWP's\n(libwww-perl's) HTTP::Cookies, to avoid losing some RFC 2965 information\nnot recorded by LWP.\n\nIt uses the version string \"2.0\", though really there isn't an LWP Cookies\n2.0 format.  This indicates that there is extra information in here\n(domain_dot and # port_spec) while still being compatible with\nlibwww-perl, I hope.\n\n\"\"\"\n\nimport time, re\nfrom cookielib import (_warn_unhandled_exception, FileCookieJar, LoadError,\n                       Cookie, MISSING_FILENAME_TEXT,\n                       join_header_words, split_header_words,\n                       iso2time, time2isoz)\n\ndef lwp_cookie_str(cookie):\n    \"\"\"Return string representation of Cookie in an the LWP cookie file format.\n\n    Actually, the format is extended a bit -- see module docstring.\n\n    \"\"\"\n    h = [(cookie.name, cookie.value),\n         (\"path\", cookie.path),\n         (\"domain\", cookie.domain)]\n    if cookie.port is not None: h.append((\"port\", cookie.port))\n    if cookie.path_specified: h.append((\"path_spec\", None))\n    if cookie.port_specified: h.append((\"port_spec\", None))\n    if cookie.domain_initial_dot: h.append((\"domain_dot\", None))\n    if cookie.secure: h.append((\"secure\", None))\n    if cookie.expires: h.append((\"expires\",\n                               time2isoz(float(cookie.expires))))\n    if cookie.discard: h.append((\"discard\", None))\n    if cookie.comment: h.append((\"comment\", cookie.comment))\n    if cookie.comment_url: h.append((\"commenturl\", cookie.comment_url))\n\n    keys = cookie._rest.keys()\n    keys.sort()\n    for k in keys:\n        h.append((k, str(cookie._rest[k])))\n\n    h.append((\"version\", str(cookie.version)))\n\n    return join_header_words([h])\n\nclass LWPCookieJar(FileCookieJar):\n    \"\"\"\n    The LWPCookieJar saves a sequence of \"Set-Cookie3\" lines.\n    \"Set-Cookie3\" is the format used by the libwww-perl libary, not known\n    to be compatible with any browser, but which is easy to read and\n    doesn't lose information about RFC 2965 cookies.\n\n    Additional methods\n\n    as_lwp_str(ignore_discard=True, ignore_expired=True)\n\n    \"\"\"\n\n    def as_lwp_str(self, ignore_discard=True, ignore_expires=True):\n        \"\"\"Return cookies as a string of \"\\\\n\"-separated \"Set-Cookie3\" headers.\n\n        ignore_discard and ignore_expires: see docstring for FileCookieJar.save\n\n        \"\"\"\n        now = time.time()\n        r = []\n        for cookie in self:\n            if not ignore_discard and cookie.discard:\n                continue\n            if not ignore_expires and cookie.is_expired(now):\n                continue\n            r.append(\"Set-Cookie3: %s\" % lwp_cookie_str(cookie))\n        return \"\\n\".join(r+[\"\"])\n\n    def save(self, filename=None, ignore_discard=False, ignore_expires=False):\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        f = open(filename, \"w\")\n        try:\n            # There really isn't an LWP Cookies 2.0 format, but this indicates\n            # that there is extra information in here (domain_dot and\n            # port_spec) while still being compatible with libwww-perl, I hope.\n            f.write(\"#LWP-Cookies-2.0\\n\")\n            f.write(self.as_lwp_str(ignore_discard, ignore_expires))\n        finally:\n            f.close()\n\n    def _really_load(self, f, filename, ignore_discard, ignore_expires):\n        magic = f.readline()\n        if not re.search(self.magic_re, magic):\n            msg = (\"%r does not look like a Set-Cookie3 (LWP) format \"\n                   \"file\" % filename)\n            raise LoadError(msg)\n\n        now = time.time()\n\n        header = \"Set-Cookie3:\"\n        boolean_attrs = (\"port_spec\", \"path_spec\", \"domain_dot\",\n                         \"secure\", \"discard\")\n        value_attrs = (\"version\",\n                       \"port\", \"path\", \"domain\",\n                       \"expires\",\n                       \"comment\", \"commenturl\")\n\n        try:\n            while 1:\n                line = f.readline()\n                if line == \"\": break\n                if not line.startswith(header):\n                    continue\n                line = line[len(header):].strip()\n\n                for data in split_header_words([line]):\n                    name, value = data[0]\n                    standard = {}\n                    rest = {}\n                    for k in boolean_attrs:\n                        standard[k] = False\n                    for k, v in data[1:]:\n                        if k is not None:\n                            lc = k.lower()\n                        else:\n                            lc = None\n                        # don't lose case distinction for unknown fields\n                        if (lc in value_attrs) or (lc in boolean_attrs):\n                            k = lc\n                        if k in boolean_attrs:\n                            if v is None: v = True\n                            standard[k] = v\n                        elif k in value_attrs:\n                            standard[k] = v\n                        else:\n                            rest[k] = v\n\n                    h = standard.get\n                    expires = h(\"expires\")\n                    discard = h(\"discard\")\n                    if expires is not None:\n                        expires = iso2time(expires)\n                    if expires is None:\n                        discard = True\n                    domain = h(\"domain\")\n                    domain_specified = domain.startswith(\".\")\n                    c = Cookie(h(\"version\"), name, value,\n                               h(\"port\"), h(\"port_spec\"),\n                               domain, domain_specified, h(\"domain_dot\"),\n                               h(\"path\"), h(\"path_spec\"),\n                               h(\"secure\"),\n                               expires,\n                               discard,\n                               h(\"comment\"),\n                               h(\"commenturl\"),\n                               rest)\n                    if not ignore_discard and c.discard:\n                        continue\n                    if not ignore_expires and c.is_expired(now):\n                        continue\n                    self.set_cookie(c)\n\n        except IOError:\n            raise\n        except Exception:\n            _warn_unhandled_exception()\n            raise LoadError(\"invalid Set-Cookie3 format file %r: %r\" %\n                            (filename, line))\n", 
    "_MozillaCookieJar": "\"\"\"Mozilla / Netscape cookie loading / saving.\"\"\"\n\nimport re, time\n\nfrom cookielib import (_warn_unhandled_exception, FileCookieJar, LoadError,\n                       Cookie, MISSING_FILENAME_TEXT)\n\nclass MozillaCookieJar(FileCookieJar):\n    \"\"\"\n\n    WARNING: you may want to backup your browser's cookies file if you use\n    this class to save cookies.  I *think* it works, but there have been\n    bugs in the past!\n\n    This class differs from CookieJar only in the format it uses to save and\n    load cookies to and from a file.  This class uses the Mozilla/Netscape\n    `cookies.txt' format.  lynx uses this file format, too.\n\n    Don't expect cookies saved while the browser is running to be noticed by\n    the browser (in fact, Mozilla on unix will overwrite your saved cookies if\n    you change them on disk while it's running; on Windows, you probably can't\n    save at all while the browser is running).\n\n    Note that the Mozilla/Netscape format will downgrade RFC2965 cookies to\n    Netscape cookies on saving.\n\n    In particular, the cookie version and port number information is lost,\n    together with information about whether or not Path, Port and Discard were\n    specified by the Set-Cookie2 (or Set-Cookie) header, and whether or not the\n    domain as set in the HTTP header started with a dot (yes, I'm aware some\n    domains in Netscape files start with a dot and some don't -- trust me, you\n    really don't want to know any more about this).\n\n    Note that though Mozilla and Netscape use the same format, they use\n    slightly different headers.  The class saves cookies using the Netscape\n    header by default (Mozilla can cope with that).\n\n    \"\"\"\n    magic_re = \"#( Netscape)? HTTP Cookie File\"\n    header = \"\"\"\\\n# Netscape HTTP Cookie File\n# http://curl.haxx.se/rfc/cookie_spec.html\n# This is a generated file!  Do not edit.\n\n\"\"\"\n\n    def _really_load(self, f, filename, ignore_discard, ignore_expires):\n        now = time.time()\n\n        magic = f.readline()\n        if not re.search(self.magic_re, magic):\n            f.close()\n            raise LoadError(\n                \"%r does not look like a Netscape format cookies file\" %\n                filename)\n\n        try:\n            while 1:\n                line = f.readline()\n                if line == \"\": break\n\n                # last field may be absent, so keep any trailing tab\n                if line.endswith(\"\\n\"): line = line[:-1]\n\n                # skip comments and blank lines XXX what is $ for?\n                if (line.strip().startswith((\"#\", \"$\")) or\n                    line.strip() == \"\"):\n                    continue\n\n                domain, domain_specified, path, secure, expires, name, value = \\\n                        line.split(\"\\t\")\n                secure = (secure == \"TRUE\")\n                domain_specified = (domain_specified == \"TRUE\")\n                if name == \"\":\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas cookielib regards it as a\n                    # cookie with no value.\n                    name = value\n                    value = None\n\n                initial_dot = domain.startswith(\".\")\n                assert domain_specified == initial_dot\n\n                discard = False\n                if expires == \"\":\n                    expires = None\n                    discard = True\n\n                # assume path_specified is false\n                c = Cookie(0, name, value,\n                           None, False,\n                           domain, domain_specified, initial_dot,\n                           path, False,\n                           secure,\n                           expires,\n                           discard,\n                           None,\n                           None,\n                           {})\n                if not ignore_discard and c.discard:\n                    continue\n                if not ignore_expires and c.is_expired(now):\n                    continue\n                self.set_cookie(c)\n\n        except IOError:\n            raise\n        except Exception:\n            _warn_unhandled_exception()\n            raise LoadError(\"invalid Netscape format cookies file %r: %r\" %\n                            (filename, line))\n\n    def save(self, filename=None, ignore_discard=False, ignore_expires=False):\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        f = open(filename, \"w\")\n        try:\n            f.write(self.header)\n            now = time.time()\n            for cookie in self:\n                if not ignore_discard and cookie.discard:\n                    continue\n                if not ignore_expires and cookie.is_expired(now):\n                    continue\n                if cookie.secure: secure = \"TRUE\"\n                else: secure = \"FALSE\"\n                if cookie.domain.startswith(\".\"): initial_dot = \"TRUE\"\n                else: initial_dot = \"FALSE\"\n                if cookie.expires is not None:\n                    expires = str(cookie.expires)\n                else:\n                    expires = \"\"\n                if cookie.value is None:\n                    # cookies.txt regards 'Set-Cookie: foo' as a cookie\n                    # with no name, whereas cookielib regards it as a\n                    # cookie with no value.\n                    name = \"\"\n                    value = cookie.name\n                else:\n                    name = cookie.name\n                    value = cookie.value\n                f.write(\n                    \"\\t\".join([cookie.domain, initial_dot, cookie.path,\n                               secure, expires, name, value])+\n                    \"\\n\")\n        finally:\n            f.close()\n", 
    "__future__": "\"\"\"Record of phased-in incompatible language changes.\n\nEach line is of the form:\n\n    FeatureName = \"_Feature(\" OptionalRelease \",\" MandatoryRelease \",\"\n                              CompilerFlag \")\"\n\nwhere, normally, OptionalRelease < MandatoryRelease, and both are 5-tuples\nof the same form as sys.version_info:\n\n    (PY_MAJOR_VERSION, # the 2 in 2.1.0a3; an int\n     PY_MINOR_VERSION, # the 1; an int\n     PY_MICRO_VERSION, # the 0; an int\n     PY_RELEASE_LEVEL, # \"alpha\", \"beta\", \"candidate\" or \"final\"; string\n     PY_RELEASE_SERIAL # the 3; an int\n    )\n\nOptionalRelease records the first release in which\n\n    from __future__ import FeatureName\n\nwas accepted.\n\nIn the case of MandatoryReleases that have not yet occurred,\nMandatoryRelease predicts the release in which the feature will become part\nof the language.\n\nElse MandatoryRelease records when the feature became part of the language;\nin releases at or after that, modules no longer need\n\n    from __future__ import FeatureName\n\nto use the feature in question, but may continue to use such imports.\n\nMandatoryRelease may also be None, meaning that a planned feature got\ndropped.\n\nInstances of class _Feature have two corresponding methods,\n.getOptionalRelease() and .getMandatoryRelease().\n\nCompilerFlag is the (bitfield) flag that should be passed in the fourth\nargument to the builtin function compile() to enable the feature in\ndynamically compiled code.  This flag is stored in the .compiler_flag\nattribute on _Future instances.  These values must match the appropriate\n#defines of CO_xxx flags in Include/compile.h.\n\nNo feature line is ever to be deleted from this file.\n\"\"\"\n\nall_feature_names = [\n    \"nested_scopes\",\n    \"generators\",\n    \"division\",\n    \"absolute_import\",\n    \"with_statement\",\n    \"print_function\",\n    \"unicode_literals\",\n]\n\n__all__ = [\"all_feature_names\"] + all_feature_names\n\n# The CO_xxx symbols are defined here under the same names used by\n# compile.h, so that an editor search will find them here.  However,\n# they're not exported in __all__, because they don't really belong to\n# this module.\nCO_NESTED            = 0x0010   # nested_scopes\nCO_GENERATOR_ALLOWED = 0        # generators (obsolete, was 0x1000)\nCO_FUTURE_DIVISION   = 0x2000   # division\nCO_FUTURE_ABSOLUTE_IMPORT = 0x4000 # perform absolute imports by default\nCO_FUTURE_WITH_STATEMENT  = 0x8000   # with statement\nCO_FUTURE_PRINT_FUNCTION  = 0x10000   # print function\nCO_FUTURE_UNICODE_LITERALS = 0x20000 # unicode string literals\n\nclass _Feature:\n    def __init__(self, optionalRelease, mandatoryRelease, compiler_flag):\n        self.optional = optionalRelease\n        self.mandatory = mandatoryRelease\n        self.compiler_flag = compiler_flag\n\n    def getOptionalRelease(self):\n        \"\"\"Return first release in which this feature was recognized.\n\n        This is a 5-tuple, of the same form as sys.version_info.\n        \"\"\"\n\n        return self.optional\n\n    def getMandatoryRelease(self):\n        \"\"\"Return release in which this feature will become mandatory.\n\n        This is a 5-tuple, of the same form as sys.version_info, or, if\n        the feature was dropped, is None.\n        \"\"\"\n\n        return self.mandatory\n\n    def __repr__(self):\n        return \"_Feature\" + repr((self.optional,\n                                  self.mandatory,\n                                  self.compiler_flag))\n\nnested_scopes = _Feature((2, 1, 0, \"beta\",  1),\n                         (2, 2, 0, \"alpha\", 0),\n                         CO_NESTED)\n\ngenerators = _Feature((2, 2, 0, \"alpha\", 1),\n                      (2, 3, 0, \"final\", 0),\n                      CO_GENERATOR_ALLOWED)\n\ndivision = _Feature((2, 2, 0, \"alpha\", 2),\n                    (3, 0, 0, \"alpha\", 0),\n                    CO_FUTURE_DIVISION)\n\nabsolute_import = _Feature((2, 5, 0, \"alpha\", 1),\n                           (3, 0, 0, \"alpha\", 0),\n                           CO_FUTURE_ABSOLUTE_IMPORT)\n\nwith_statement = _Feature((2, 5, 0, \"alpha\", 1),\n                          (2, 6, 0, \"alpha\", 0),\n                          CO_FUTURE_WITH_STATEMENT)\n\nprint_function = _Feature((2, 6, 0, \"alpha\", 2),\n                          (3, 0, 0, \"alpha\", 0),\n                          CO_FUTURE_PRINT_FUNCTION)\n\nunicode_literals = _Feature((2, 6, 0, \"alpha\", 2),\n                            (3, 0, 0, \"alpha\", 0),\n                            CO_FUTURE_UNICODE_LITERALS)\n", 
    "_abcoll": "# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) for collections, according to PEP 3119.\n\nDON'T USE THIS MODULE DIRECTLY!  The classes here should be imported\nvia collections; they are defined here only to alleviate certain\nbootstrapping issues.  Unit tests are in test_collections.\n\"\"\"\n\nfrom abc import ABCMeta, abstractmethod\nimport sys\n\n__all__ = [\"Hashable\", \"Iterable\", \"Iterator\",\n           \"Sized\", \"Container\", \"Callable\",\n           \"Set\", \"MutableSet\",\n           \"Mapping\", \"MutableMapping\",\n           \"MappingView\", \"KeysView\", \"ItemsView\", \"ValuesView\",\n           \"Sequence\", \"MutableSequence\",\n           ]\n\n### ONE-TRICK PONIES ###\n\ndef _hasattr(C, attr):\n    try:\n        return any(attr in B.__dict__ for B in C.__mro__)\n    except AttributeError:\n        # Old-style class\n        return hasattr(C, attr)\n\n\nclass Hashable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __hash__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Hashable:\n            try:\n                for B in C.__mro__:\n                    if \"__hash__\" in B.__dict__:\n                        if B.__dict__[\"__hash__\"]:\n                            return True\n                        break\n            except AttributeError:\n                # Old-style class\n                if getattr(C, \"__hash__\", None):\n                    return True\n        return NotImplemented\n\n\nclass Iterable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __iter__(self):\n        while False:\n            yield None\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterable:\n            if _hasattr(C, \"__iter__\"):\n                return True\n        return NotImplemented\n\nIterable.register(str)\n\n\nclass Iterator(Iterable):\n\n    @abstractmethod\n    def next(self):\n        'Return the next item from the iterator. When exhausted, raise StopIteration'\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Iterator:\n            if _hasattr(C, \"next\") and _hasattr(C, \"__iter__\"):\n                return True\n        return NotImplemented\n\n\nclass Sized:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __len__(self):\n        return 0\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Sized:\n            if _hasattr(C, \"__len__\"):\n                return True\n        return NotImplemented\n\n\nclass Container:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __contains__(self, x):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Container:\n            if _hasattr(C, \"__contains__\"):\n                return True\n        return NotImplemented\n\n\nclass Callable:\n    __metaclass__ = ABCMeta\n\n    @abstractmethod\n    def __call__(self, *args, **kwds):\n        return False\n\n    @classmethod\n    def __subclasshook__(cls, C):\n        if cls is Callable:\n            if _hasattr(C, \"__call__\"):\n                return True\n        return NotImplemented\n\n\n### SETS ###\n\n\nclass Set(Sized, Iterable, Container):\n    \"\"\"A set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__ and __len__.\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), redefine __le__ and __ge__,\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    def __le__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) > len(other):\n            return False\n        for elem in self:\n            if elem not in other:\n                return False\n        return True\n\n    def __lt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) < len(other) and self.__le__(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) > len(other) and self.__ge__(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        if len(self) < len(other):\n            return False\n        for elem in other:\n            if elem not in self:\n                return False\n        return True\n\n    def __eq__(self, other):\n        if not isinstance(other, Set):\n            return NotImplemented\n        return len(self) == len(other) and self.__le__(other)\n\n    def __ne__(self, other):\n        return not (self == other)\n\n    @classmethod\n    def _from_iterable(cls, it):\n        '''Construct an instance of the class from any iterable input.\n\n        Must override this method if the class constructor signature\n        does not accept an iterable for an input.\n        '''\n        return cls(it)\n\n    def __and__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        return self._from_iterable(value for value in other if value in self)\n\n    __rand__ = __and__\n\n    def isdisjoint(self, other):\n        'Return True if two sets have a null intersection.'\n        for value in other:\n            if value in self:\n                return False\n        return True\n\n    def __or__(self, other):\n        if not isinstance(other, Iterable):\n            return NotImplemented\n        chain = (e for s in (self, other) for e in s)\n        return self._from_iterable(chain)\n\n    __ror__ = __or__\n\n    def __sub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in self\n                                   if value not in other)\n\n    def __rsub__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return self._from_iterable(value for value in other\n                                   if value not in self)\n\n    def __xor__(self, other):\n        if not isinstance(other, Set):\n            if not isinstance(other, Iterable):\n                return NotImplemented\n            other = self._from_iterable(other)\n        return (self - other) | (other - self)\n\n    __rxor__ = __xor__\n\n    # Sets are not hashable by default, but subclasses can change this\n    __hash__ = None\n\n    def _hash(self):\n        \"\"\"Compute the hash value of a set.\n\n        Note that we don't define __hash__: not all sets are hashable.\n        But if you define a hashable set type, its __hash__ should\n        call this function.\n\n        This must be compatible __eq__.\n\n        All sets ought to compare equal if they contain the same\n        elements, regardless of how they are implemented, and\n        regardless of the order of the elements; so there's not much\n        freedom for __eq__ or __hash__.  We match the algorithm used\n        by the built-in frozenset type.\n        \"\"\"\n        MAX = sys.maxint\n        MASK = 2 * MAX + 1\n        n = len(self)\n        h = 1927868237 * (n + 1)\n        h &= MASK\n        for x in self:\n            hx = hash(x)\n            h ^= (hx ^ (hx << 16) ^ 89869747)  * 3644798167\n            h &= MASK\n        h = h * 69069 + 907133923\n        h &= MASK\n        if h > MAX:\n            h -= MASK + 1\n        if h == -1:\n            h = 590923713\n        return h\n\nSet.register(frozenset)\n\n\nclass MutableSet(Set):\n    \"\"\"A mutable set is a finite, iterable container.\n\n    This class provides concrete generic implementations of all\n    methods except for __contains__, __iter__, __len__,\n    add(), and discard().\n\n    To override the comparisons (presumably for speed, as the\n    semantics are fixed), all you have to do is redefine __le__ and\n    then the other operations will automatically follow suit.\n    \"\"\"\n\n    @abstractmethod\n    def add(self, value):\n        \"\"\"Add an element.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def discard(self, value):\n        \"\"\"Remove an element.  Do not raise an exception if absent.\"\"\"\n        raise NotImplementedError\n\n    def remove(self, value):\n        \"\"\"Remove an element. If not a member, raise a KeyError.\"\"\"\n        if value not in self:\n            raise KeyError(value)\n        self.discard(value)\n\n    def pop(self):\n        \"\"\"Return the popped value.  Raise KeyError if empty.\"\"\"\n        it = iter(self)\n        try:\n            value = next(it)\n        except StopIteration:\n            raise KeyError\n        self.discard(value)\n        return value\n\n    def clear(self):\n        \"\"\"This is slow (creates N new iterators!) but effective.\"\"\"\n        try:\n            while True:\n                self.pop()\n        except KeyError:\n            pass\n\n    def __ior__(self, it):\n        for value in it:\n            self.add(value)\n        return self\n\n    def __iand__(self, it):\n        for value in (self - it):\n            self.discard(value)\n        return self\n\n    def __ixor__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            if not isinstance(it, Set):\n                it = self._from_iterable(it)\n            for value in it:\n                if value in self:\n                    self.discard(value)\n                else:\n                    self.add(value)\n        return self\n\n    def __isub__(self, it):\n        if it is self:\n            self.clear()\n        else:\n            for value in it:\n                self.discard(value)\n        return self\n\nMutableSet.register(set)\n\n\n### MAPPINGS ###\n\n\nclass Mapping(Sized, Iterable, Container):\n\n    \"\"\"A Mapping is a generic container for associating key/value\n    pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, key):\n        raise KeyError\n\n    def get(self, key, default=None):\n        'D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.'\n        try:\n            return self[key]\n        except KeyError:\n            return default\n\n    def __contains__(self, key):\n        try:\n            self[key]\n        except KeyError:\n            return False\n        else:\n            return True\n\n    def iterkeys(self):\n        'D.iterkeys() -> an iterator over the keys of D'\n        return iter(self)\n\n    def itervalues(self):\n        'D.itervalues() -> an iterator over the values of D'\n        for key in self:\n            yield self[key]\n\n    def iteritems(self):\n        'D.iteritems() -> an iterator over the (key, value) items of D'\n        for key in self:\n            yield (key, self[key])\n\n    def keys(self):\n        \"D.keys() -> list of D's keys\"\n        return list(self)\n\n    def items(self):\n        \"D.items() -> list of D's (key, value) pairs, as 2-tuples\"\n        return [(key, self[key]) for key in self]\n\n    def values(self):\n        \"D.values() -> list of D's values\"\n        return [self[key] for key in self]\n\n    # Mappings are not hashable by default, but subclasses can change this\n    __hash__ = None\n\n    def __eq__(self, other):\n        if not isinstance(other, Mapping):\n            return NotImplemented\n        return dict(self.items()) == dict(other.items())\n\n    def __ne__(self, other):\n        return not (self == other)\n\nclass MappingView(Sized):\n\n    def __init__(self, mapping):\n        self._mapping = mapping\n\n    def __len__(self):\n        return len(self._mapping)\n\n    def __repr__(self):\n        return '{0.__class__.__name__}({0._mapping!r})'.format(self)\n\n\nclass KeysView(MappingView, Set):\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, key):\n        return key in self._mapping\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield key\n\n\nclass ItemsView(MappingView, Set):\n\n    @classmethod\n    def _from_iterable(self, it):\n        return set(it)\n\n    def __contains__(self, item):\n        key, value = item\n        try:\n            v = self._mapping[key]\n        except KeyError:\n            return False\n        else:\n            return v == value\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield (key, self._mapping[key])\n\n\nclass ValuesView(MappingView):\n\n    def __contains__(self, value):\n        for key in self._mapping:\n            if value == self._mapping[key]:\n                return True\n        return False\n\n    def __iter__(self):\n        for key in self._mapping:\n            yield self._mapping[key]\n\n\nclass MutableMapping(Mapping):\n\n    \"\"\"A MutableMapping is a generic container for associating\n    key/value pairs.\n\n    This class provides concrete generic implementations of all\n    methods except for __getitem__, __setitem__, __delitem__,\n    __iter__, and __len__.\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, key, value):\n        raise KeyError\n\n    @abstractmethod\n    def __delitem__(self, key):\n        raise KeyError\n\n    __marker = object()\n\n    def pop(self, key, default=__marker):\n        '''D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n          If key is not found, d is returned if given, otherwise KeyError is raised.\n        '''\n        try:\n            value = self[key]\n        except KeyError:\n            if default is self.__marker:\n                raise\n            return default\n        else:\n            del self[key]\n            return value\n\n    def popitem(self):\n        '''D.popitem() -> (k, v), remove and return some (key, value) pair\n           as a 2-tuple; but raise KeyError if D is empty.\n        '''\n        try:\n            key = next(iter(self))\n        except StopIteration:\n            raise KeyError\n        value = self[key]\n        del self[key]\n        return key, value\n\n    def clear(self):\n        'D.clear() -> None.  Remove all items from D.'\n        try:\n            while True:\n                self.popitem()\n        except KeyError:\n            pass\n\n    def update(*args, **kwds):\n        ''' D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n            If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n            If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n            In either case, this is followed by: for k, v in F.items(): D[k] = v\n        '''\n        if len(args) > 2:\n            raise TypeError(\"update() takes at most 2 positional \"\n                            \"arguments ({} given)\".format(len(args)))\n        elif not args:\n            raise TypeError(\"update() takes at least 1 argument (0 given)\")\n        self = args[0]\n        other = args[1] if len(args) >= 2 else ()\n\n        if isinstance(other, Mapping):\n            for key in other:\n                self[key] = other[key]\n        elif hasattr(other, \"keys\"):\n            for key in other.keys():\n                self[key] = other[key]\n        else:\n            for key, value in other:\n                self[key] = value\n        for key, value in kwds.items():\n            self[key] = value\n\n    def setdefault(self, key, default=None):\n        'D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D'\n        try:\n            return self[key]\n        except KeyError:\n            self[key] = default\n        return default\n\nMutableMapping.register(dict)\n\n\n### SEQUENCES ###\n\n\nclass Sequence(Sized, Iterable, Container):\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must override __new__ or __init__,\n    __getitem__, and __len__.\n    \"\"\"\n\n    @abstractmethod\n    def __getitem__(self, index):\n        raise IndexError\n\n    def __iter__(self):\n        i = 0\n        try:\n            while True:\n                v = self[i]\n                yield v\n                i += 1\n        except IndexError:\n            return\n\n    def __contains__(self, value):\n        for v in self:\n            if v == value:\n                return True\n        return False\n\n    def __reversed__(self):\n        for i in reversed(range(len(self))):\n            yield self[i]\n\n    def index(self, value):\n        '''S.index(value) -> integer -- return first index of value.\n           Raises ValueError if the value is not present.\n        '''\n        for i, v in enumerate(self):\n            if v == value:\n                return i\n        raise ValueError\n\n    def count(self, value):\n        'S.count(value) -> integer -- return number of occurrences of value'\n        return sum(1 for v in self if v == value)\n\nSequence.register(tuple)\nSequence.register(basestring)\nSequence.register(buffer)\nSequence.register(xrange)\n\n\nclass MutableSequence(Sequence):\n\n    \"\"\"All the operations on a read-only sequence.\n\n    Concrete subclasses must provide __new__ or __init__,\n    __getitem__, __setitem__, __delitem__, __len__, and insert().\n\n    \"\"\"\n\n    @abstractmethod\n    def __setitem__(self, index, value):\n        raise IndexError\n\n    @abstractmethod\n    def __delitem__(self, index):\n        raise IndexError\n\n    @abstractmethod\n    def insert(self, index, value):\n        'S.insert(index, object) -- insert object before index'\n        raise IndexError\n\n    def append(self, value):\n        'S.append(object) -- append object to the end of the sequence'\n        self.insert(len(self), value)\n\n    def reverse(self):\n        'S.reverse() -- reverse *IN PLACE*'\n        n = len(self)\n        for i in range(n//2):\n            self[i], self[n-i-1] = self[n-i-1], self[i]\n\n    def extend(self, values):\n        'S.extend(iterable) -- extend sequence by appending elements from the iterable'\n        for v in values:\n            self.append(v)\n\n    def pop(self, index=-1):\n        '''S.pop([index]) -> item -- remove and return item at index (default last).\n           Raise IndexError if list is empty or index is out of range.\n        '''\n        v = self[index]\n        del self[index]\n        return v\n\n    def remove(self, value):\n        '''S.remove(value) -- remove first occurrence of value.\n           Raise ValueError if the value is not present.\n        '''\n        del self[self.index(value)]\n\n    def __iadd__(self, values):\n        self.extend(values)\n        return self\n\nMutableSequence.register(list)\n", 
    "_functools": "\"\"\" Supplies the internal functions for functools.py in the standard library \"\"\"\n\n# reduce() has moved to _functools in Python 2.6+.\nreduce = reduce\n\nclass partial(object):\n    \"\"\"\n    partial(func, *args, **keywords) - new function with partial application\n    of the given arguments and keywords.\n    \"\"\"\n    def __init__(*args, **keywords):\n        if len(args) < 2:\n            raise TypeError('__init__() takes at least 2 arguments (%d given)'\n                            % len(args))\n        self, func, args = args[0], args[1], args[2:]\n        if not callable(func):\n            raise TypeError(\"the first argument must be callable\")\n        self._func = func\n        self._args = args\n        self._keywords = keywords\n\n    def __delattr__(self, key):\n        if key == '__dict__':\n            raise TypeError(\"a partial object's dictionary may not be deleted\")\n        object.__delattr__(self, key)\n\n    @property\n    def func(self):\n        return self._func\n\n    @property\n    def args(self):\n        return self._args\n\n    @property\n    def keywords(self):\n        return self._keywords\n\n    def __call__(self, *fargs, **fkeywords):\n        if self._keywords:\n            fkeywords = dict(self._keywords, **fkeywords)\n        return self._func(*(self._args + fargs), **fkeywords)\n\n    def __reduce__(self):\n        d = dict((k, v) for k, v in self.__dict__.iteritems() if k not in\n                ('_func', '_args', '_keywords'))\n        if len(d) == 0:\n            d = None\n        return (type(self), (self._func,),\n                (self._func, self._args, self._keywords, d))\n\n    def __setstate__(self, state):\n        func, args, keywords, d = state\n        if d is not None:\n            self.__dict__.update(d)\n        self._func = func\n        self._args = args\n        self._keywords = keywords\n", 
    "_marshal": "\"\"\"Internal Python object serialization\n\nThis module contains functions that can read and write Python values in a binary format. The format is specific to Python, but independent of machine architecture issues (e.g., you can write a Python value to a file on a PC, transport the file to a Sun, and read it back there). Details of the format may change between Python versions.\n\"\"\"\n\n# NOTE: This module is used in the Python3 interpreter, but also by\n# the \"sandboxed\" process.  It must work for Python2 as well.\n\nimport types\n\ntry:\n    intern\nexcept NameError:\n    from sys import intern\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nTYPE_NULL     = '0'\nTYPE_NONE     = 'N'\nTYPE_FALSE    = 'F'\nTYPE_TRUE     = 'T'\nTYPE_STOPITER = 'S'\nTYPE_ELLIPSIS = '.'\nTYPE_INT      = 'i'\nTYPE_INT64    = 'I'\nTYPE_FLOAT    = 'f'\nTYPE_COMPLEX  = 'x'\nTYPE_LONG     = 'l'\nTYPE_STRING   = 's'\nTYPE_INTERNED = 't'\nTYPE_STRINGREF= 'R'\nTYPE_TUPLE    = '('\nTYPE_LIST     = '['\nTYPE_DICT     = '{'\nTYPE_CODE     = 'c'\nTYPE_UNICODE  = 'u'\nTYPE_UNKNOWN  = '?'\nTYPE_SET      = '<'\nTYPE_FROZENSET= '>'\n\nclass _Marshaller:\n\n    dispatch = {}\n\n    def __init__(self, writefunc):\n        self._write = writefunc\n\n    def dump(self, x):\n        try:\n            self.dispatch[type(x)](self, x)\n        except KeyError:\n            for tp in type(x).mro():\n                func = self.dispatch.get(tp)\n                if func:\n                    break\n            else:\n                raise ValueError(\"unmarshallable object\")\n            func(self, x)\n\n    def w_long64(self, x):\n        self.w_long(x)\n        self.w_long(x>>32)\n\n    def w_long(self, x):\n        a = chr(x & 0xff)\n        x >>= 8\n        b = chr(x & 0xff)\n        x >>= 8\n        c = chr(x & 0xff)\n        x >>= 8\n        d = chr(x & 0xff)\n        self._write(a + b + c + d)\n\n    def w_short(self, x):\n        self._write(chr((x)     & 0xff))\n        self._write(chr((x>> 8) & 0xff))\n\n    def dump_none(self, x):\n        self._write(TYPE_NONE)\n    dispatch[type(None)] = dump_none\n\n    def dump_bool(self, x):\n        if x:\n            self._write(TYPE_TRUE)\n        else:\n            self._write(TYPE_FALSE)\n    dispatch[bool] = dump_bool\n\n    def dump_stopiter(self, x):\n        if x is not StopIteration:\n            raise ValueError(\"unmarshallable object\")\n        self._write(TYPE_STOPITER)\n    dispatch[type(StopIteration)] = dump_stopiter\n\n    def dump_ellipsis(self, x):\n        self._write(TYPE_ELLIPSIS)\n    \n    try:\n        dispatch[type(Ellipsis)] = dump_ellipsis\n    except NameError:\n        pass\n\n    # In Python3, this function is not used; see dump_long() below.\n    def dump_int(self, x):\n        y = x>>31\n        if y and y != -1:\n            self._write(TYPE_INT64)\n            self.w_long64(x)\n        else:\n            self._write(TYPE_INT)\n            self.w_long(x)\n    dispatch[int] = dump_int\n\n    def dump_long(self, x):\n        self._write(TYPE_LONG)\n        sign = 1\n        if x < 0:\n            sign = -1\n            x = -x\n        digits = []\n        while x:\n            digits.append(x & 0x7FFF)\n            x = x>>15\n        self.w_long(len(digits) * sign)\n        for d in digits:\n            self.w_short(d)\n    try:\n        long\n    except NameError:\n        dispatch[int] = dump_long\n    else:\n        dispatch[long] = dump_long\n\n    def dump_float(self, x):\n        write = self._write\n        write(TYPE_FLOAT)\n        s = repr(x)\n        write(chr(len(s)))\n        write(s)\n    dispatch[float] = dump_float\n\n    def dump_complex(self, x):\n        write = self._write\n        write(TYPE_COMPLEX)\n        s = repr(x.real)\n        write(chr(len(s)))\n        write(s)\n        s = repr(x.imag)\n        write(chr(len(s)))\n        write(s)\n    try:\n        dispatch[complex] = dump_complex\n    except NameError:\n        pass\n\n    def dump_string(self, x):\n        # XXX we can't check for interned strings, yet,\n        # so we (for now) never create TYPE_INTERNED or TYPE_STRINGREF\n        self._write(TYPE_STRING)\n        self.w_long(len(x))\n        self._write(x)\n    dispatch[bytes] = dump_string\n\n    def dump_unicode(self, x):\n        self._write(TYPE_UNICODE)\n        s = x.encode('utf8')\n        self.w_long(len(s))\n        self._write(s)\n    try:\n        unicode\n    except NameError:\n        dispatch[str] = dump_unicode\n    else:\n        dispatch[unicode] = dump_unicode\n\n    def dump_tuple(self, x):\n        self._write(TYPE_TUPLE)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[tuple] = dump_tuple\n\n    def dump_list(self, x):\n        self._write(TYPE_LIST)\n        self.w_long(len(x))\n        for item in x:\n            self.dump(item)\n    dispatch[list] = dump_list\n\n    def dump_dict(self, x):\n        self._write(TYPE_DICT)\n        for key, value in x.items():\n            self.dump(key)\n            self.dump(value)\n        self._write(TYPE_NULL)\n    dispatch[dict] = dump_dict\n\n    def dump_code(self, x):\n        self._write(TYPE_CODE)\n        self.w_long(x.co_argcount)\n        self.w_long(x.co_nlocals)\n        self.w_long(x.co_stacksize)\n        self.w_long(x.co_flags)\n        self.dump(x.co_code)\n        self.dump(x.co_consts)\n        self.dump(x.co_names)\n        self.dump(x.co_varnames)\n        self.dump(x.co_freevars)\n        self.dump(x.co_cellvars)\n        self.dump(x.co_filename)\n        self.dump(x.co_name)\n        self.w_long(x.co_firstlineno)\n        self.dump(x.co_lnotab)\n    try:\n        dispatch[types.CodeType] = dump_code\n    except NameError:\n        pass\n\n    def dump_set(self, x):\n        self._write(TYPE_SET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[set] = dump_set\n    except NameError:\n        pass\n\n    def dump_frozenset(self, x):\n        self._write(TYPE_FROZENSET)\n        self.w_long(len(x))\n        for each in x:\n            self.dump(each)\n    try:\n        dispatch[frozenset] = dump_frozenset\n    except NameError:\n        pass\n\nclass _NULL:\n    pass\n\nclass _StringBuffer:\n    def __init__(self, value):\n        self.bufstr = value\n        self.bufpos = 0\n\n    def read(self, n):\n        pos = self.bufpos\n        newpos = pos + n\n        ret = self.bufstr[pos : newpos]\n        self.bufpos = newpos\n        return ret\n\n\nclass _Unmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, readfunc):\n        self._read = readfunc\n        self._stringtable = []\n\n    def load(self):\n        c = self._read(1)\n        if not c:\n            raise EOFError\n        try:\n            return self.dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n\n    def r_short(self):\n        lo = ord(self._read(1))\n        hi = ord(self._read(1))\n        x = lo | (hi<<8)\n        if x & 0x8000:\n            x = x - 0x10000\n        return x\n\n    def r_long(self):\n        s = self._read(4)\n        a = ord(s[0])\n        b = ord(s[1])\n        c = ord(s[2])\n        d = ord(s[3])\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        if d & 0x80 and x > 0:\n            x = -((1<<32) - x)\n            return int(x)\n        else:\n            return x\n\n    def r_long64(self):\n        a = ord(self._read(1))\n        b = ord(self._read(1))\n        c = ord(self._read(1))\n        d = ord(self._read(1))\n        e = ord(self._read(1))\n        f = ord(self._read(1))\n        g = ord(self._read(1))\n        h = ord(self._read(1))\n        x = a | (b<<8) | (c<<16) | (d<<24)\n        x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n        if h & 0x80 and x > 0:\n            x = -((1<<64) - x)\n        return x\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    dispatch[TYPE_INT] = r_long\n\n    dispatch[TYPE_INT64] = r_long64\n\n    def load_long(self):\n        size = self.r_long()\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = self.r_short()\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(self._read(1))\n        s = self._read(n)\n        real = float(s)\n        n = ord(self._read(1))\n        s = self._read(n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = self.r_long()\n        return self._read(n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = self.r_long()\n        ret = intern(self._read(n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = self.r_long()\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = self.r_long()\n        s = self._read(n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = self.r_long()\n        list = [self.load() for i in range(n)]\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = self.r_long()\n        nlocals = self.r_long()\n        stacksize = self.r_long()\n        flags = self.r_long()\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = self.r_long()\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = self.r_long()\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n# ________________________________________________________________\n\ndef _read(self, n):\n    pos = self.bufpos\n    newpos = pos + n\n    if newpos > len(self.bufstr): raise EOFError\n    ret = self.bufstr[pos : newpos]\n    self.bufpos = newpos\n    return ret\n\ndef _read1(self):\n    ret = self.bufstr[self.bufpos]\n    self.bufpos += 1\n    return ret\n\ndef _r_short(self):\n    lo = ord(_read1(self))\n    hi = ord(_read1(self))\n    x = lo | (hi<<8)\n    if x & 0x8000:\n        x = x - 0x10000\n    return x\n\ndef _r_long(self):\n    # inlined this most common case\n    p = self.bufpos\n    s = self.bufstr\n    a = ord(s[p])\n    b = ord(s[p+1])\n    c = ord(s[p+2])\n    d = ord(s[p+3])\n    self.bufpos += 4\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    if d & 0x80 and x > 0:\n        x = -((1<<32) - x)\n        return int(x)\n    else:\n        return x\n\ndef _r_long64(self):\n    a = ord(_read1(self))\n    b = ord(_read1(self))\n    c = ord(_read1(self))\n    d = ord(_read1(self))\n    e = ord(_read1(self))\n    f = ord(_read1(self))\n    g = ord(_read1(self))\n    h = ord(_read1(self))\n    x = a | (b<<8) | (c<<16) | (d<<24)\n    x = x | (e<<32) | (f<<40) | (g<<48) | (h<<56)\n    if h & 0x80 and x > 0:\n        x = -((1<<64) - x)\n    return x\n\n_load_dispatch = {}\n\nclass _FastUnmarshaller:\n\n    dispatch = {}\n\n    def __init__(self, buffer):\n        self.bufstr = buffer\n        self.bufpos = 0\n        self._stringtable = []\n\n    def load(self):\n        # make flow space happy\n        c = '?'\n        try:\n            c = self.bufstr[self.bufpos]\n            self.bufpos += 1\n            return _load_dispatch[c](self)\n        except KeyError:\n            raise ValueError(\"bad marshal code: %c (%d)\" % (c, ord(c)))\n        except IndexError:\n            raise EOFError\n\n    def load_null(self):\n        return _NULL\n    dispatch[TYPE_NULL] = load_null\n\n    def load_none(self):\n        return None\n    dispatch[TYPE_NONE] = load_none\n\n    def load_true(self):\n        return True\n    dispatch[TYPE_TRUE] = load_true\n\n    def load_false(self):\n        return False\n    dispatch[TYPE_FALSE] = load_false\n\n    def load_stopiter(self):\n        return StopIteration\n    dispatch[TYPE_STOPITER] = load_stopiter\n\n    def load_ellipsis(self):\n        return Ellipsis\n    dispatch[TYPE_ELLIPSIS] = load_ellipsis\n\n    def load_int(self):\n        return _r_long(self)\n    dispatch[TYPE_INT] = load_int\n\n    def load_int64(self):\n        return _r_long64(self)\n    dispatch[TYPE_INT64] = load_int64\n\n    def load_long(self):\n        size = _r_long(self)\n        sign = 1\n        if size < 0:\n            sign = -1\n            size = -size\n        x = 0\n        for i in range(size):\n            d = _r_short(self)\n            x = x | (d<<(i*15))\n        return x * sign\n    dispatch[TYPE_LONG] = load_long\n\n    def load_float(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        return float(s)\n    dispatch[TYPE_FLOAT] = load_float\n\n    def load_complex(self):\n        n = ord(_read1(self))\n        s = _read(self, n)\n        real = float(s)\n        n = ord(_read1(self))\n        s = _read(self, n)\n        imag = float(s)\n        return complex(real, imag)\n    dispatch[TYPE_COMPLEX] = load_complex\n\n    def load_string(self):\n        n = _r_long(self)\n        return _read(self, n)\n    dispatch[TYPE_STRING] = load_string\n\n    def load_interned(self):\n        n = _r_long(self)\n        ret = intern(_read(self, n))\n        self._stringtable.append(ret)\n        return ret\n    dispatch[TYPE_INTERNED] = load_interned\n\n    def load_stringref(self):\n        n = _r_long(self)\n        return self._stringtable[n]\n    dispatch[TYPE_STRINGREF] = load_stringref\n\n    def load_unicode(self):\n        n = _r_long(self)\n        s = _read(self, n)\n        ret = s.decode('utf8')\n        return ret\n    dispatch[TYPE_UNICODE] = load_unicode\n\n    def load_tuple(self):\n        return tuple(self.load_list())\n    dispatch[TYPE_TUPLE] = load_tuple\n\n    def load_list(self):\n        n = _r_long(self)\n        list = []\n        for i in range(n):\n            list.append(self.load())\n        return list\n    dispatch[TYPE_LIST] = load_list\n\n    def load_dict(self):\n        d = {}\n        while 1:\n            key = self.load()\n            if key is _NULL:\n                break\n            value = self.load()\n            d[key] = value\n        return d\n    dispatch[TYPE_DICT] = load_dict\n\n    def load_code(self):\n        argcount = _r_long(self)\n        nlocals = _r_long(self)\n        stacksize = _r_long(self)\n        flags = _r_long(self)\n        code = self.load()\n        consts = self.load()\n        names = self.load()\n        varnames = self.load()\n        freevars = self.load()\n        cellvars = self.load()\n        filename = self.load()\n        name = self.load()\n        firstlineno = _r_long(self)\n        lnotab = self.load()\n        return types.CodeType(argcount, nlocals, stacksize, flags, code, consts,\n                              names, varnames, filename, name, firstlineno,\n                              lnotab, freevars, cellvars)\n    dispatch[TYPE_CODE] = load_code\n\n    def load_set(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return set(args)\n    dispatch[TYPE_SET] = load_set\n\n    def load_frozenset(self):\n        n = _r_long(self)\n        args = [self.load() for i in range(n)]\n        return frozenset(args)\n    dispatch[TYPE_FROZENSET] = load_frozenset\n\n_load_dispatch = _FastUnmarshaller.dispatch\n\n# _________________________________________________________________\n#\n# user interface\n\nversion = 1\n\n@builtinify\ndef dump(x, f, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    m = _Marshaller(f.write)\n    m.dump(x)\n\n@builtinify\ndef load(f):\n    um = _Unmarshaller(f.read)\n    return um.load()\n\n@builtinify\ndef dumps(x, version=version):\n    # XXX 'version' is ignored, we always dump in a version-0-compatible format\n    buffer = []\n    m = _Marshaller(buffer.append)\n    m.dump(x)\n    return ''.join(buffer)\n\n@builtinify\ndef loads(s):\n    um = _FastUnmarshaller(s)\n    return um.load()\n", 
    "_md5": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'md5' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of MD5 in pure Python.\n\nThis is an implementation of the MD5 hash function, as specified by\nRFC 1321, in pure Python. It was implemented using Bruce Schneier's\nexcellent book \"Applied Cryptography\", 2nd ed., 1996.\n\nSurely this is not meant to compete with the existing implementation\nof the Python standard library (written in C). Rather, it should be\nseen as a Python complement that is more readable than C and can be\nused more conveniently for learning and experimenting purposes in\nthe field of cryptography.\n\nThis module tries very hard to follow the API of the existing Python\nstandard library's \"md5\" module, but although it seems to work fine,\nit has not been extensively tested! (But note that there is a test\nmodule, test_md5py.py, that compares this Python implementation with\nthe C one of the Python standard library.\n\nBEWARE: this comes with no guarantee whatsoever about fitness and/or\nother properties! Specifically, do not use this in any production\ncode! License is Python License!\n\nSpecial thanks to Aurelian Coman who fixed some nasty bugs!\n\nDinu C. Gherman\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n__metaclass__ = type # or genrpy won't work\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n# ======================================================================\n\ndef _bytelist2long(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j])\n        b1 = ord(list[j+1]) << 8\n        b2 = ord(list[j+2]) << 16\n        b3 = ord(list[j+3]) << 24\n        hl[i] = b0 | b1 |b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The real MD5 meat...\n#\n#   Implemented after \"Applied Cryptography\", 2nd ed., 1996,\n#   pp. 436-441 by Bruce Schneier.\n# ======================================================================\n\n# F, G, H and I are basic MD5 functions.\n\ndef F(x, y, z):\n    return (x & y) | ((~x) & z)\n\ndef G(x, y, z):\n    return (x & z) | (y & (~z))\n\ndef H(x, y, z):\n    return x ^ y ^ z\n\ndef I(x, y, z):\n    return y ^ (x | (~z))\n\n\ndef XX(func, a, b, c, d, x, s, ac):\n    \"\"\"Wrapper for call distribution to functions F, G, H and I.\n\n    This replaces functions FF, GG, HH and II from \"Appl. Crypto.\"\n    Rotation is separate from addition to prevent recomputation\n    (now summed-up in one function).\n    \"\"\"\n\n    res = 0\n    res = res + a + func(b, c, d)\n    res = res + x \n    res = res + ac\n    res = res & 0xffffffff\n    res = _rotateLeft(res, s)\n    res = res & 0xffffffff\n    res = res + b\n\n    return res & 0xffffffff\n\n\nclass MD5Type:\n    \"An implementation of the MD5 hash function in pure Python.\"\n\n    digest_size = digestsize = 16\n    block_size = 64\n\n    def __init__(self):\n        \"Initialisation.\"\n        \n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.count = [0, 0]\n        self.input = []\n\n        # Load magic initialization constants.\n        self.A = 0x67452301\n        self.B = 0xefcdab89\n        self.C = 0x98badcfe\n        self.D = 0x10325476\n\n\n    def _transform(self, inp):\n        \"\"\"Basic MD5 step transforming the digest based on the input.\n\n        Note that if the Mysterious Constants are arranged backwards\n        in little-endian order and decrypted with the DES they produce\n        OCCULT MESSAGES!\n        \"\"\"\n\n        a, b, c, d = A, B, C, D = self.A, self.B, self.C, self.D\n\n        # Round 1.\n\n        S11, S12, S13, S14 = 7, 12, 17, 22\n\n        a = XX(F, a, b, c, d, inp[ 0], S11, 0xD76AA478) # 1 \n        d = XX(F, d, a, b, c, inp[ 1], S12, 0xE8C7B756) # 2 \n        c = XX(F, c, d, a, b, inp[ 2], S13, 0x242070DB) # 3 \n        b = XX(F, b, c, d, a, inp[ 3], S14, 0xC1BDCEEE) # 4 \n        a = XX(F, a, b, c, d, inp[ 4], S11, 0xF57C0FAF) # 5 \n        d = XX(F, d, a, b, c, inp[ 5], S12, 0x4787C62A) # 6 \n        c = XX(F, c, d, a, b, inp[ 6], S13, 0xA8304613) # 7 \n        b = XX(F, b, c, d, a, inp[ 7], S14, 0xFD469501) # 8 \n        a = XX(F, a, b, c, d, inp[ 8], S11, 0x698098D8) # 9 \n        d = XX(F, d, a, b, c, inp[ 9], S12, 0x8B44F7AF) # 10 \n        c = XX(F, c, d, a, b, inp[10], S13, 0xFFFF5BB1) # 11 \n        b = XX(F, b, c, d, a, inp[11], S14, 0x895CD7BE) # 12 \n        a = XX(F, a, b, c, d, inp[12], S11, 0x6B901122) # 13 \n        d = XX(F, d, a, b, c, inp[13], S12, 0xFD987193) # 14 \n        c = XX(F, c, d, a, b, inp[14], S13, 0xA679438E) # 15 \n        b = XX(F, b, c, d, a, inp[15], S14, 0x49B40821) # 16 \n\n        # Round 2.\n\n        S21, S22, S23, S24 = 5, 9, 14, 20\n\n        a = XX(G, a, b, c, d, inp[ 1], S21, 0xF61E2562) # 17 \n        d = XX(G, d, a, b, c, inp[ 6], S22, 0xC040B340) # 18 \n        c = XX(G, c, d, a, b, inp[11], S23, 0x265E5A51) # 19 \n        b = XX(G, b, c, d, a, inp[ 0], S24, 0xE9B6C7AA) # 20 \n        a = XX(G, a, b, c, d, inp[ 5], S21, 0xD62F105D) # 21 \n        d = XX(G, d, a, b, c, inp[10], S22, 0x02441453) # 22 \n        c = XX(G, c, d, a, b, inp[15], S23, 0xD8A1E681) # 23 \n        b = XX(G, b, c, d, a, inp[ 4], S24, 0xE7D3FBC8) # 24 \n        a = XX(G, a, b, c, d, inp[ 9], S21, 0x21E1CDE6) # 25 \n        d = XX(G, d, a, b, c, inp[14], S22, 0xC33707D6) # 26 \n        c = XX(G, c, d, a, b, inp[ 3], S23, 0xF4D50D87) # 27 \n        b = XX(G, b, c, d, a, inp[ 8], S24, 0x455A14ED) # 28 \n        a = XX(G, a, b, c, d, inp[13], S21, 0xA9E3E905) # 29 \n        d = XX(G, d, a, b, c, inp[ 2], S22, 0xFCEFA3F8) # 30 \n        c = XX(G, c, d, a, b, inp[ 7], S23, 0x676F02D9) # 31 \n        b = XX(G, b, c, d, a, inp[12], S24, 0x8D2A4C8A) # 32 \n\n        # Round 3.\n\n        S31, S32, S33, S34 = 4, 11, 16, 23\n\n        a = XX(H, a, b, c, d, inp[ 5], S31, 0xFFFA3942) # 33 \n        d = XX(H, d, a, b, c, inp[ 8], S32, 0x8771F681) # 34 \n        c = XX(H, c, d, a, b, inp[11], S33, 0x6D9D6122) # 35 \n        b = XX(H, b, c, d, a, inp[14], S34, 0xFDE5380C) # 36 \n        a = XX(H, a, b, c, d, inp[ 1], S31, 0xA4BEEA44) # 37 \n        d = XX(H, d, a, b, c, inp[ 4], S32, 0x4BDECFA9) # 38 \n        c = XX(H, c, d, a, b, inp[ 7], S33, 0xF6BB4B60) # 39 \n        b = XX(H, b, c, d, a, inp[10], S34, 0xBEBFBC70) # 40 \n        a = XX(H, a, b, c, d, inp[13], S31, 0x289B7EC6) # 41 \n        d = XX(H, d, a, b, c, inp[ 0], S32, 0xEAA127FA) # 42 \n        c = XX(H, c, d, a, b, inp[ 3], S33, 0xD4EF3085) # 43 \n        b = XX(H, b, c, d, a, inp[ 6], S34, 0x04881D05) # 44 \n        a = XX(H, a, b, c, d, inp[ 9], S31, 0xD9D4D039) # 45 \n        d = XX(H, d, a, b, c, inp[12], S32, 0xE6DB99E5) # 46 \n        c = XX(H, c, d, a, b, inp[15], S33, 0x1FA27CF8) # 47 \n        b = XX(H, b, c, d, a, inp[ 2], S34, 0xC4AC5665) # 48 \n\n        # Round 4.\n\n        S41, S42, S43, S44 = 6, 10, 15, 21\n\n        a = XX(I, a, b, c, d, inp[ 0], S41, 0xF4292244) # 49 \n        d = XX(I, d, a, b, c, inp[ 7], S42, 0x432AFF97) # 50 \n        c = XX(I, c, d, a, b, inp[14], S43, 0xAB9423A7) # 51 \n        b = XX(I, b, c, d, a, inp[ 5], S44, 0xFC93A039) # 52 \n        a = XX(I, a, b, c, d, inp[12], S41, 0x655B59C3) # 53 \n        d = XX(I, d, a, b, c, inp[ 3], S42, 0x8F0CCC92) # 54 \n        c = XX(I, c, d, a, b, inp[10], S43, 0xFFEFF47D) # 55 \n        b = XX(I, b, c, d, a, inp[ 1], S44, 0x85845DD1) # 56 \n        a = XX(I, a, b, c, d, inp[ 8], S41, 0x6FA87E4F) # 57 \n        d = XX(I, d, a, b, c, inp[15], S42, 0xFE2CE6E0) # 58 \n        c = XX(I, c, d, a, b, inp[ 6], S43, 0xA3014314) # 59 \n        b = XX(I, b, c, d, a, inp[13], S44, 0x4E0811A1) # 60 \n        a = XX(I, a, b, c, d, inp[ 4], S41, 0xF7537E82) # 61 \n        d = XX(I, d, a, b, c, inp[11], S42, 0xBD3AF235) # 62 \n        c = XX(I, c, d, a, b, inp[ 2], S43, 0x2AD7D2BB) # 63 \n        b = XX(I, b, c, d, a, inp[ 9], S44, 0xEB86D391) # 64 \n\n        A = (A + a) & 0xffffffff\n        B = (B + b) & 0xffffffff\n        C = (C + c) & 0xffffffff\n        D = (D + d) & 0xffffffff\n\n        self.A, self.B, self.C, self.D = A, B, C, D\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the md5 module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). This allows us to keep an\n        intermediate value for the hash, so that we only need to make\n        minimal recalculation if we call update() to add moredata to\n        the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[0] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[0] = self.count[0] + (leninBuf << 3)\n        if self.count[0] < (leninBuf << 3):\n            self.count[1] = self.count[1] + 1\n        self.count[1] = self.count[1] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2long(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2long(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        A = self.A\n        B = self.B\n        C = self.C\n        D = self.D\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[0] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = [b'\\200'] + [b'\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2long(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = struct.pack(\"<IIII\", self.A, self.B, self.C, self.D)\n\n        self.A = A \n        self.B = B\n        self.C = C\n        self.D = D\n        self.input = input \n        self.count = count \n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n        if 0: # set this to 1 to make the flow space crash\n            return copy.deepcopy(self)\n        clone = self.__class__()\n        clone.length = self.length\n        clone.count  = [] + self.count[:]\n        clone.input  = [] + self.input\n        clone.A = self.A\n        clone.B = self.B\n        clone.C = self.C\n        clone.D = self.D\n        return clone\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _md5 module of the standard library.\n# ======================================================================\n\ndigest_size = 16\n\ndef new(arg=None):\n    \"\"\"Return a new md5 crypto object.\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = MD5Type()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n\n", 
    "_multiprocessing": "import os\nimport pickle\nclass Connection(object):\n    def __init__(self, fno, writeable=True, readable=True):\n        self.fno=fno\n        self.writeable=writeable\n        self.readable=readable\n        self._buffer=''\n    def send(self, msg):\n        if not self.writeable:\n            raise OSError('not writable')\n        return os.write(self.fno, pickle.dumps(pickle.dumps(msg)))\n    def recv(self):\n        if not self.readable:\n            raise OSError('not readable')\n        if os.fileNumbers[self.fno].buffer.buffer!='':\n            self._buffer+=os.read(self.fno, len(os.fileNumbers[self.fno].buffer.buffer))\n        if \"\\n.\" in self._buffer:\n            obj, self._buffer = self._buffer.split('\\n.',1)\n            return pickle.loads(pickle.loads(obj+'\\n.'))\n        raise EOFError()\n    def poll(self):\n        if self.readable:\n            import select\n            s=select.select([self.fno],[],[],0)[0]\n            return bool(s)\n        return False\n    def close(self):\n        pass\n", 
    "_osx_support": "\"\"\"Shared OS X support functions.\"\"\"\n\nimport os\nimport re\nimport sys\n\n__all__ = [\n    'compiler_fixup',\n    'customize_config_vars',\n    'customize_compiler',\n    'get_platform_osx',\n]\n\n# configuration variables that may contain universal build flags,\n# like \"-arch\" or \"-isdkroot\", that may need customization for\n# the user environment\n_UNIVERSAL_CONFIG_VARS = ('CFLAGS', 'LDFLAGS', 'CPPFLAGS', 'BASECFLAGS',\n                            'BLDSHARED', 'LDSHARED', 'CC', 'CXX',\n                            'PY_CFLAGS', 'PY_LDFLAGS', 'PY_CPPFLAGS',\n                            'PY_CORE_CFLAGS')\n\n# configuration variables that may contain compiler calls\n_COMPILER_CONFIG_VARS = ('BLDSHARED', 'LDSHARED', 'CC', 'CXX')\n\n# prefix added to original configuration variable names\n_INITPRE = '_OSX_SUPPORT_INITIAL_'\n\n\ndef _find_executable(executable, path=None):\n    \"\"\"Tries to find 'executable' in the directories listed in 'path'.\n\n    A string listing directories separated by 'os.pathsep'; defaults to\n    os.environ['PATH'].  Returns the complete filename or None if not found.\n    \"\"\"\n    if path is None:\n        path = os.environ['PATH']\n\n    paths = path.split(os.pathsep)\n    base, ext = os.path.splitext(executable)\n\n    if (sys.platform == 'win32' or os.name == 'os2') and (ext != '.exe'):\n        executable = executable + '.exe'\n\n    if not os.path.isfile(executable):\n        for p in paths:\n            f = os.path.join(p, executable)\n            if os.path.isfile(f):\n                # the file exists, we have a shot at spawn working\n                return f\n        return None\n    else:\n        return executable\n\n\ndef _read_output(commandstring):\n    \"\"\"Output from successful command execution or None\"\"\"\n    # Similar to os.popen(commandstring, \"r\").read(),\n    # but without actually using os.popen because that\n    # function is not usable during python bootstrap.\n    # tempfile is also not available then.\n    import contextlib\n    try:\n        import tempfile\n        fp = tempfile.NamedTemporaryFile()\n    except ImportError:\n        fp = open(\"/tmp/_osx_support.%s\"%(\n            os.getpid(),), \"w+b\")\n\n    with contextlib.closing(fp) as fp:\n        cmd = \"%s 2>/dev/null >'%s'\" % (commandstring, fp.name)\n        return fp.read().strip() if not os.system(cmd) else None\n\n\ndef _find_build_tool(toolname):\n    \"\"\"Find a build tool on current path or using xcrun\"\"\"\n    return (_find_executable(toolname)\n                or _read_output(\"/usr/bin/xcrun -find %s\" % (toolname,))\n                or ''\n            )\n\n_SYSTEM_VERSION = None\n\ndef _get_system_version():\n    \"\"\"Return the OS X system version as a string\"\"\"\n    # Reading this plist is a documented way to get the system\n    # version (see the documentation for the Gestalt Manager)\n    # We avoid using platform.mac_ver to avoid possible bootstrap issues during\n    # the build of Python itself (distutils is used to build standard library\n    # extensions).\n\n    global _SYSTEM_VERSION\n\n    if _SYSTEM_VERSION is None:\n        _SYSTEM_VERSION = ''\n        try:\n            f = open('/System/Library/CoreServices/SystemVersion.plist')\n        except IOError:\n            # We're on a plain darwin box, fall back to the default\n            # behaviour.\n            pass\n        else:\n            try:\n                m = re.search(r'<key>ProductUserVisibleVersion</key>\\s*'\n                              r'<string>(.*?)</string>', f.read())\n            finally:\n                f.close()\n            if m is not None:\n                _SYSTEM_VERSION = '.'.join(m.group(1).split('.')[:2])\n            # else: fall back to the default behaviour\n\n    return _SYSTEM_VERSION\n\ndef _remove_original_values(_config_vars):\n    \"\"\"Remove original unmodified values for testing\"\"\"\n    # This is needed for higher-level cross-platform tests of get_platform.\n    for k in list(_config_vars):\n        if k.startswith(_INITPRE):\n            del _config_vars[k]\n\ndef _save_modified_value(_config_vars, cv, newvalue):\n    \"\"\"Save modified and original unmodified value of configuration var\"\"\"\n\n    oldvalue = _config_vars.get(cv, '')\n    if (oldvalue != newvalue) and (_INITPRE + cv not in _config_vars):\n        _config_vars[_INITPRE + cv] = oldvalue\n    _config_vars[cv] = newvalue\n\ndef _supports_universal_builds():\n    \"\"\"Returns True if universal builds are supported on this system\"\"\"\n    # As an approximation, we assume that if we are running on 10.4 or above,\n    # then we are running with an Xcode environment that supports universal\n    # builds, in particular -isysroot and -arch arguments to the compiler. This\n    # is in support of allowing 10.4 universal builds to run on 10.3.x systems.\n\n    osx_version = _get_system_version()\n    if osx_version:\n        try:\n            osx_version = tuple(int(i) for i in osx_version.split('.'))\n        except ValueError:\n            osx_version = ''\n    return bool(osx_version >= (10, 4)) if osx_version else False\n\n\ndef _find_appropriate_compiler(_config_vars):\n    \"\"\"Find appropriate C compiler for extension module builds\"\"\"\n\n    # Issue #13590:\n    #    The OSX location for the compiler varies between OSX\n    #    (or rather Xcode) releases.  With older releases (up-to 10.5)\n    #    the compiler is in /usr/bin, with newer releases the compiler\n    #    can only be found inside Xcode.app if the \"Command Line Tools\"\n    #    are not installed.\n    #\n    #    Futhermore, the compiler that can be used varies between\n    #    Xcode releases. Up to Xcode 4 it was possible to use 'gcc-4.2'\n    #    as the compiler, after that 'clang' should be used because\n    #    gcc-4.2 is either not present, or a copy of 'llvm-gcc' that\n    #    miscompiles Python.\n\n    # skip checks if the compiler was overriden with a CC env variable\n    if 'CC' in os.environ:\n        return _config_vars\n\n    # The CC config var might contain additional arguments.\n    # Ignore them while searching.\n    cc = oldcc = _config_vars['CC'].split()[0]\n    if not _find_executable(cc):\n        # Compiler is not found on the shell search PATH.\n        # Now search for clang, first on PATH (if the Command LIne\n        # Tools have been installed in / or if the user has provided\n        # another location via CC).  If not found, try using xcrun\n        # to find an uninstalled clang (within a selected Xcode).\n\n        # NOTE: Cannot use subprocess here because of bootstrap\n        # issues when building Python itself (and os.popen is\n        # implemented on top of subprocess and is therefore not\n        # usable as well)\n\n        cc = _find_build_tool('clang')\n\n    elif os.path.basename(cc).startswith('gcc'):\n        # Compiler is GCC, check if it is LLVM-GCC\n        data = _read_output(\"'%s' --version\"\n                             % (cc.replace(\"'\", \"'\\\"'\\\"'\"),))\n        if data and 'llvm-gcc' in data:\n            # Found LLVM-GCC, fall back to clang\n            cc = _find_build_tool('clang')\n\n    if not cc:\n        raise SystemError(\n               \"Cannot locate working compiler\")\n\n    if cc != oldcc:\n        # Found a replacement compiler.\n        # Modify config vars using new compiler, if not already explicitly\n        # overriden by an env variable, preserving additional arguments.\n        for cv in _COMPILER_CONFIG_VARS:\n            if cv in _config_vars and cv not in os.environ:\n                cv_split = _config_vars[cv].split()\n                cv_split[0] = cc if cv != 'CXX' else cc + '++'\n                _save_modified_value(_config_vars, cv, ' '.join(cv_split))\n\n    return _config_vars\n\n\ndef _remove_universal_flags(_config_vars):\n    \"\"\"Remove all universal build arguments from config vars\"\"\"\n\n    for cv in _UNIVERSAL_CONFIG_VARS:\n        # Do not alter a config var explicitly overriden by env var\n        if cv in _config_vars and cv not in os.environ:\n            flags = _config_vars[cv]\n            flags = re.sub('-arch\\s+\\w+\\s', ' ', flags)\n            flags = re.sub('-isysroot [^ \\t]*', ' ', flags)\n            _save_modified_value(_config_vars, cv, flags)\n\n    return _config_vars\n\n\ndef _remove_unsupported_archs(_config_vars):\n    \"\"\"Remove any unsupported archs from config vars\"\"\"\n    # Different Xcode releases support different sets for '-arch'\n    # flags. In particular, Xcode 4.x no longer supports the\n    # PPC architectures.\n    #\n    # This code automatically removes '-arch ppc' and '-arch ppc64'\n    # when these are not supported. That makes it possible to\n    # build extensions on OSX 10.7 and later with the prebuilt\n    # 32-bit installer on the python.org website.\n\n    # skip checks if the compiler was overriden with a CC env variable\n    if 'CC' in os.environ:\n        return _config_vars\n\n    if re.search('-arch\\s+ppc', _config_vars['CFLAGS']) is not None:\n        # NOTE: Cannot use subprocess here because of bootstrap\n        # issues when building Python itself\n        status = os.system(\n            \"\"\"echo 'int main{};' | \"\"\"\n            \"\"\"'%s' -c -arch ppc -x c -o /dev/null /dev/null 2>/dev/null\"\"\"\n            %(_config_vars['CC'].replace(\"'\", \"'\\\"'\\\"'\"),))\n        if status:\n            # The compile failed for some reason.  Because of differences\n            # across Xcode and compiler versions, there is no reliable way\n            # to be sure why it failed.  Assume here it was due to lack of\n            # PPC support and remove the related '-arch' flags from each\n            # config variables not explicitly overriden by an environment\n            # variable.  If the error was for some other reason, we hope the\n            # failure will show up again when trying to compile an extension\n            # module.\n            for cv in _UNIVERSAL_CONFIG_VARS:\n                if cv in _config_vars and cv not in os.environ:\n                    flags = _config_vars[cv]\n                    flags = re.sub('-arch\\s+ppc\\w*\\s', ' ', flags)\n                    _save_modified_value(_config_vars, cv, flags)\n\n    return _config_vars\n\n\ndef _override_all_archs(_config_vars):\n    \"\"\"Allow override of all archs with ARCHFLAGS env var\"\"\"\n    # NOTE: This name was introduced by Apple in OSX 10.5 and\n    # is used by several scripting languages distributed with\n    # that OS release.\n    if 'ARCHFLAGS' in os.environ:\n        arch = os.environ['ARCHFLAGS']\n        for cv in _UNIVERSAL_CONFIG_VARS:\n            if cv in _config_vars and '-arch' in _config_vars[cv]:\n                flags = _config_vars[cv]\n                flags = re.sub('-arch\\s+\\w+\\s', ' ', flags)\n                flags = flags + ' ' + arch\n                _save_modified_value(_config_vars, cv, flags)\n\n    return _config_vars\n\n\ndef _check_for_unavailable_sdk(_config_vars):\n    \"\"\"Remove references to any SDKs not available\"\"\"\n    # If we're on OSX 10.5 or later and the user tries to\n    # compile an extension using an SDK that is not present\n    # on the current machine it is better to not use an SDK\n    # than to fail.  This is particularly important with\n    # the standalone Command Line Tools alternative to a\n    # full-blown Xcode install since the CLT packages do not\n    # provide SDKs.  If the SDK is not present, it is assumed\n    # that the header files and dev libs have been installed\n    # to /usr and /System/Library by either a standalone CLT\n    # package or the CLT component within Xcode.\n    cflags = _config_vars.get('CFLAGS', '')\n    m = re.search(r'-isysroot\\s+(\\S+)', cflags)\n    if m is not None:\n        sdk = m.group(1)\n        if not os.path.exists(sdk):\n            for cv in _UNIVERSAL_CONFIG_VARS:\n                # Do not alter a config var explicitly overriden by env var\n                if cv in _config_vars and cv not in os.environ:\n                    flags = _config_vars[cv]\n                    flags = re.sub(r'-isysroot\\s+\\S+(?:\\s|$)', ' ', flags)\n                    _save_modified_value(_config_vars, cv, flags)\n\n    return _config_vars\n\n\ndef compiler_fixup(compiler_so, cc_args):\n    \"\"\"\n    This function will strip '-isysroot PATH' and '-arch ARCH' from the\n    compile flags if the user has specified one them in extra_compile_flags.\n\n    This is needed because '-arch ARCH' adds another architecture to the\n    build, without a way to remove an architecture. Furthermore GCC will\n    barf if multiple '-isysroot' arguments are present.\n    \"\"\"\n    stripArch = stripSysroot = False\n\n    compiler_so = list(compiler_so)\n\n    if not _supports_universal_builds():\n        # OSX before 10.4.0, these don't support -arch and -isysroot at\n        # all.\n        stripArch = stripSysroot = True\n    else:\n        stripArch = '-arch' in cc_args\n        stripSysroot = '-isysroot' in cc_args\n\n    if stripArch or 'ARCHFLAGS' in os.environ:\n        while True:\n            try:\n                index = compiler_so.index('-arch')\n                # Strip this argument and the next one:\n                del compiler_so[index:index+2]\n            except ValueError:\n                break\n\n    if 'ARCHFLAGS' in os.environ and not stripArch:\n        # User specified different -arch flags in the environ,\n        # see also distutils.sysconfig\n        compiler_so = compiler_so + os.environ['ARCHFLAGS'].split()\n\n    if stripSysroot:\n        while True:\n            try:\n                index = compiler_so.index('-isysroot')\n                # Strip this argument and the next one:\n                del compiler_so[index:index+2]\n            except ValueError:\n                break\n\n    # Check if the SDK that is used during compilation actually exists,\n    # the universal build requires the usage of a universal SDK and not all\n    # users have that installed by default.\n    sysroot = None\n    if '-isysroot' in cc_args:\n        idx = cc_args.index('-isysroot')\n        sysroot = cc_args[idx+1]\n    elif '-isysroot' in compiler_so:\n        idx = compiler_so.index('-isysroot')\n        sysroot = compiler_so[idx+1]\n\n    if sysroot and not os.path.isdir(sysroot):\n        from distutils import log\n        log.warn(\"Compiling with an SDK that doesn't seem to exist: %s\",\n                sysroot)\n        log.warn(\"Please check your Xcode installation\")\n\n    return compiler_so\n\n\ndef customize_config_vars(_config_vars):\n    \"\"\"Customize Python build configuration variables.\n\n    Called internally from sysconfig with a mutable mapping\n    containing name/value pairs parsed from the configured\n    makefile used to build this interpreter.  Returns\n    the mapping updated as needed to reflect the environment\n    in which the interpreter is running; in the case of\n    a Python from a binary installer, the installed\n    environment may be very different from the build\n    environment, i.e. different OS levels, different\n    built tools, different available CPU architectures.\n\n    This customization is performed whenever\n    distutils.sysconfig.get_config_vars() is first\n    called.  It may be used in environments where no\n    compilers are present, i.e. when installing pure\n    Python dists.  Customization of compiler paths\n    and detection of unavailable archs is deferred\n    until the first extension module build is\n    requested (in distutils.sysconfig.customize_compiler).\n\n    Currently called from distutils.sysconfig\n    \"\"\"\n\n    if not _supports_universal_builds():\n        # On Mac OS X before 10.4, check if -arch and -isysroot\n        # are in CFLAGS or LDFLAGS and remove them if they are.\n        # This is needed when building extensions on a 10.3 system\n        # using a universal build of python.\n        _remove_universal_flags(_config_vars)\n\n    # Allow user to override all archs with ARCHFLAGS env var\n    _override_all_archs(_config_vars)\n\n    # Remove references to sdks that are not found\n    _check_for_unavailable_sdk(_config_vars)\n\n    return _config_vars\n\n\ndef customize_compiler(_config_vars):\n    \"\"\"Customize compiler path and configuration variables.\n\n    This customization is performed when the first\n    extension module build is requested\n    in distutils.sysconfig.customize_compiler).\n    \"\"\"\n\n    # Find a compiler to use for extension module builds\n    _find_appropriate_compiler(_config_vars)\n\n    # Remove ppc arch flags if not supported here\n    _remove_unsupported_archs(_config_vars)\n\n    # Allow user to override all archs with ARCHFLAGS env var\n    _override_all_archs(_config_vars)\n\n    return _config_vars\n\n\ndef get_platform_osx(_config_vars, osname, release, machine):\n    \"\"\"Filter values for get_platform()\"\"\"\n    # called from get_platform() in sysconfig and distutils.util\n    #\n    # For our purposes, we'll assume that the system version from\n    # distutils' perspective is what MACOSX_DEPLOYMENT_TARGET is set\n    # to. This makes the compatibility story a bit more sane because the\n    # machine is going to compile and link as if it were\n    # MACOSX_DEPLOYMENT_TARGET.\n\n    macver = _config_vars.get('MACOSX_DEPLOYMENT_TARGET', '')\n    macrelease = _get_system_version() or macver\n    macver = macver or macrelease\n\n    if macver:\n        release = macver\n        osname = \"macosx\"\n\n        # Use the original CFLAGS value, if available, so that we\n        # return the same machine type for the platform string.\n        # Otherwise, distutils may consider this a cross-compiling\n        # case and disallow installs.\n        cflags = _config_vars.get(_INITPRE+'CFLAGS',\n                                    _config_vars.get('CFLAGS', ''))\n        if macrelease:\n            try:\n                macrelease = tuple(int(i) for i in macrelease.split('.')[0:2])\n            except ValueError:\n                macrelease = (10, 0)\n        else:\n            # assume no universal support\n            macrelease = (10, 0)\n\n        if (macrelease >= (10, 4)) and '-arch' in cflags.strip():\n            # The universal build will build fat binaries, but not on\n            # systems before 10.4\n\n            machine = 'fat'\n\n            archs = re.findall('-arch\\s+(\\S+)', cflags)\n            archs = tuple(sorted(set(archs)))\n\n            if len(archs) == 1:\n                machine = archs[0]\n            elif archs == ('i386', 'ppc'):\n                machine = 'fat'\n            elif archs == ('i386', 'x86_64'):\n                machine = 'intel'\n            elif archs == ('i386', 'ppc', 'x86_64'):\n                machine = 'fat3'\n            elif archs == ('ppc64', 'x86_64'):\n                machine = 'fat64'\n            elif archs == ('i386', 'ppc', 'ppc64', 'x86_64'):\n                machine = 'universal'\n            else:\n                raise ValueError(\n                   \"Don't know machine value for archs=%r\" % (archs,))\n\n        elif machine == 'i386':\n            # On OSX the machine type returned by uname is always the\n            # 32-bit variant, even if the executable architecture is\n            # the 64-bit variant\n            if sys.maxint >= 2**32:\n                machine = 'x86_64'\n\n        elif machine in ('PowerPC', 'Power_Macintosh'):\n            # Pick a sane name for the PPC architecture.\n            # See 'i386' case\n            if sys.maxint >= 2**32:\n                machine = 'ppc64'\n            else:\n                machine = 'ppc'\n\n    return (osname, release, machine)\n", 
    "_scproxy": "\"\"\"Helper methods for urllib to fetch the proxy configuration settings using\nthe SystemConfiguration framework.\n\n\"\"\"\nimport sys\nif sys.platform != 'darwin':\n    raise ImportError('Requires Mac OS X')\n\nfrom ctypes import c_int32, c_int64, c_void_p, c_char_p, c_int, cdll\nfrom ctypes import pointer, create_string_buffer\nfrom ctypes.util import find_library\n\nkCFNumberSInt32Type = 3\nkCFStringEncodingUTF8 = 134217984\n\ndef _CFSetup():\n    sc = cdll.LoadLibrary(find_library(\"SystemConfiguration\"))\n    cf = cdll.LoadLibrary(find_library(\"CoreFoundation\"))\n    sctable = [\n        ('SCDynamicStoreCopyProxies', [c_void_p], c_void_p),\n    ]\n    cftable = [\n        ('CFArrayGetCount', [c_void_p], c_int64),\n        ('CFArrayGetValueAtIndex', [c_void_p, c_int64], c_void_p),\n        ('CFDictionaryGetValue', [c_void_p, c_void_p], c_void_p),\n        ('CFStringCreateWithCString', [c_void_p, c_char_p, c_int32], c_void_p),\n        ('CFStringGetLength', [c_void_p], c_int32),\n        ('CFStringGetCString', [c_void_p, c_char_p, c_int32, c_int32], c_int32),\n        ('CFNumberGetValue', [c_void_p, c_int, c_void_p], c_int32),\n        ('CFRelease', [c_void_p], None),\n    ]\n    scconst = [\n        'kSCPropNetProxiesExceptionsList',\n        'kSCPropNetProxiesExcludeSimpleHostnames',\n        'kSCPropNetProxiesHTTPEnable',\n        'kSCPropNetProxiesHTTPProxy',\n        'kSCPropNetProxiesHTTPPort',\n        'kSCPropNetProxiesHTTPSEnable',\n        'kSCPropNetProxiesHTTPSProxy',\n        'kSCPropNetProxiesHTTPSPort',\n        'kSCPropNetProxiesFTPEnable',\n        'kSCPropNetProxiesFTPProxy',\n        'kSCPropNetProxiesFTPPort',\n        'kSCPropNetProxiesGopherEnable',\n        'kSCPropNetProxiesGopherProxy',\n        'kSCPropNetProxiesGopherPort',\n    ]\n    class CFProxy(object):\n        def __init__(self):\n            for mod, table in [(sc, sctable), (cf, cftable)]:\n                for fname, argtypes, restype in table:\n                    func = getattr(mod, fname)\n                    func.argtypes = argtypes\n                    func.restype = restype\n                    setattr(self, fname, func)\n            for k in scconst:\n                v = None\n                try:\n                    v = c_void_p.in_dll(sc, k)\n                except ValueError:\n                    v = None\n                setattr(self, k, v)\n    return CFProxy()\nffi = _CFSetup()\n\ndef cfstring_to_pystring(value):\n    length = (ffi.CFStringGetLength(value) * 4) + 1\n    buff = create_string_buffer(length)\n    ffi.CFStringGetCString(value, buff, length * 4, kCFStringEncodingUTF8)\n    return unicode(buff.value, 'utf8')\n\ndef cfnum_to_int32(num):\n    result_ptr = pointer(c_int32(0))\n    ffi.CFNumberGetValue(num, kCFNumberSInt32Type, result_ptr)\n    return result_ptr[0]\n\ndef _get_proxy_settings():\n    result = {'exclude_simple': False}\n    cfdct = ffi.SCDynamicStoreCopyProxies(None)\n    if not cfdct:\n        return result\n    try:\n        k = ffi.kSCPropNetProxiesExcludeSimpleHostnames\n        if k:\n            cfnum = ffi.CFDictionaryGetValue(cfdct, k)\n            if cfnum:\n                result['exclude_simple'] = bool(cfnum_to_int32(cfnum))\n        k = ffi.kSCPropNetProxiesExceptionsList\n        if k:\n            cfarr = ffi.CFDictionaryGetValue(cfdct, k)\n            if cfarr:\n                lst = []\n                for i in range(ffi.CFArrayGetCount(cfarr)):\n                    cfstr = ffi.CFArrayGetValueAtIndex(cfarr, i)\n                    if cfstr:\n                        v = cfstring_to_pystring(cfstr)\n                    else:\n                        v = None\n                    lst.append(v)\n                result['exceptions'] = lst\n        return result\n    finally:\n        ffi.CFRelease(cfdct)\n\ndef _get_proxies():\n    result = {}\n    cfdct = ffi.SCDynamicStoreCopyProxies(None)\n    if not cfdct:\n        return result\n    try:\n        for proto in 'HTTP', 'HTTPS', 'FTP', 'Gopher':\n            enabled_key = getattr(ffi, 'kSCPropNetProxies' + proto + 'Enable')\n            proxy_key = getattr(ffi, 'kSCPropNetProxies' + proto + 'Proxy')\n            port_key = getattr(ffi, 'kSCPropNetProxies' + proto + 'Port')\n            cfnum = ffi.CFDictionaryGetValue(cfdct, enabled_key)\n            if cfnum and cfnum_to_int32(cfnum):\n                cfhoststr = ffi.CFDictionaryGetValue(cfdct, proxy_key)\n                cfportnum = ffi.CFDictionaryGetValue(cfdct, port_key)\n                if cfhoststr:\n                    host = cfstring_to_pystring(cfhoststr)\n                    if host:\n                        if cfportnum:\n                            port = cfnum_to_int32(cfportnum)\n                            v = u'http://%s:%d' % (host, port)\n                        else:\n                            v = u'http://%s' % (host,)\n                        result[proto.lower()] = v\n        return result\n    finally:\n        ffi.CFRelease(cfdct)\n", 
    "_sha": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n# Note that PyPy contains also a built-in module 'sha' which will hide\n# this one if compiled in.\n\n\"\"\"A sample implementation of SHA-1 in pure Python.\n\n   Framework adapted from Dinu Gherman's MD5 implementation by\n   J. Hall\u00e9n and L. Creighton. SHA-1 implementation based directly on\n   the text of the NIST standard FIPS PUB 180-1.\n\"\"\"\n\n\n__date__    = '2004-11-17'\n__version__ = 0.91 # Modernised by J. Hall\u00e9n and L. Creighton for Pypy\n\n\nimport struct, copy\n\n\n# ======================================================================\n# Bit-Manipulation helpers\n#\n#   _long2bytes() was contributed by Barry Warsaw\n#   and is reused here with tiny modifications.\n# ======================================================================\n\ndef _long2bytesBigEndian(n, blocksize=0):\n    \"\"\"Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front\n    of the byte string with binary zeros so that the length is a multiple\n    of blocksize.\n    \"\"\"\n\n    # After much testing, this algorithm was deemed to be the fastest.\n    s = b''\n    pack = struct.pack\n    while n > 0:\n        s = pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n\n    # Strip off leading zeros.\n    for i in range(len(s)):\n        if s[i] != '\\000':\n            break\n    else:\n        # Only happens when n == 0.\n        s = '\\000'\n        i = 0\n\n    s = s[i:]\n\n    # Add back some pad bytes. This could be done more efficiently\n    # w.r.t. the de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * '\\000' + s\n\n    return s\n\n\ndef _bytelist2longBigEndian(list):\n    \"Transform a list of characters into a list of longs.\"\n\n    imax = len(list) // 4\n    hl = [0] * imax\n\n    j = 0\n    i = 0\n    while i < imax:\n        b0 = ord(list[j]) << 24\n        b1 = ord(list[j+1]) << 16\n        b2 = ord(list[j+2]) << 8\n        b3 = ord(list[j+3])\n        hl[i] = b0 | b1 | b2 | b3\n        i = i+1\n        j = j+4\n\n    return hl\n\n\ndef _rotateLeft(x, n):\n    \"Rotate x (32 bit) left n bits circularly.\"\n\n    return (x << n) | (x >> (32-n))\n\n\n# ======================================================================\n# The SHA transformation functions\n#\n# ======================================================================\n\ndef f0_19(B, C, D):\n    return (B & C) | ((~ B) & D)\n\ndef f20_39(B, C, D):\n    return B ^ C ^ D\n\ndef f40_59(B, C, D):\n    return (B & C) | (B & D) | (C & D)\n\ndef f60_79(B, C, D):\n    return B ^ C ^ D\n\n\nf = [f0_19, f20_39, f40_59, f60_79]\n\n# Constants to be used\nK = [\n    0x5A827999, # ( 0 <= t <= 19)\n    0x6ED9EBA1, # (20 <= t <= 39)\n    0x8F1BBCDC, # (40 <= t <= 59)\n    0xCA62C1D6  # (60 <= t <= 79)\n    ]\n\nclass sha:\n    \"An implementation of the SHA hash function in pure Python.\"\n\n    digest_size = digestsize = 20\n    block_size = 512 // 8\n\n    def __init__(self):\n        \"Initialisation.\"\n\n        # Initial message length in bits(!).\n        self.length = 0\n        self.count = [0, 0]\n\n        # Initial empty message as a sequence of bytes (8 bit characters).\n        self.input = []\n\n        # Call a separate init function, that can be used repeatedly\n        # to start from scratch on the same object.\n        self.init()\n\n\n    def init(self):\n        \"Initialize the message-digest and set all fields to zero.\"\n\n        self.length = 0\n        self.input = []\n\n        # Initial 160 bit message digest (5 times 32 bit).\n        self.H0 = 0x67452301\n        self.H1 = 0xEFCDAB89\n        self.H2 = 0x98BADCFE\n        self.H3 = 0x10325476\n        self.H4 = 0xC3D2E1F0\n\n    def _transform(self, W):\n\n        for t in range(16, 80):\n            W.append(_rotateLeft(\n                W[t-3] ^ W[t-8] ^ W[t-14] ^ W[t-16], 1) & 0xffffffff)\n\n        A = self.H0\n        B = self.H1\n        C = self.H2\n        D = self.H3\n        E = self.H4\n\n        \"\"\"\n        This loop was unrolled to gain about 10% in speed\n        for t in range(0, 80):\n            TEMP = _rotateLeft(A, 5) + f[t/20] + E + W[t] + K[t/20]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n        \"\"\"\n\n        for t in range(0, 20):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | ((~ B) & D)) + E + W[t] + K[0]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(20, 40):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D) + E + W[t] + K[1]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(40, 60):\n            TEMP = _rotateLeft(A, 5) + ((B & C) | (B & D) | (C & D)) + E + W[t] + K[2]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n        for t in range(60, 80):\n            TEMP = _rotateLeft(A, 5) + (B ^ C ^ D)  + E + W[t] + K[3]\n            E = D\n            D = C\n            C = _rotateLeft(B, 30) & 0xffffffff\n            B = A\n            A = TEMP & 0xffffffff\n\n\n        self.H0 = (self.H0 + A) & 0xffffffff\n        self.H1 = (self.H1 + B) & 0xffffffff\n        self.H2 = (self.H2 + C) & 0xffffffff\n        self.H3 = (self.H3 + D) & 0xffffffff\n        self.H4 = (self.H4 + E) & 0xffffffff\n\n\n    # Down from here all methods follow the Python Standard Library\n    # API of the sha module.\n\n    def update(self, inBuf):\n        \"\"\"Add to the current message.\n\n        Update the md5 object with the string arg. Repeated calls\n        are equivalent to a single call with the concatenation of all\n        the arguments, i.e. m.update(a); m.update(b) is equivalent\n        to m.update(a+b).\n\n        The hash is immediately calculated for all full blocks. The final\n        calculation is made in digest(). It will calculate 1-2 blocks,\n        depending on how much padding we have to add. This allows us to\n        keep an intermediate value for the hash, so that we only need to\n        make minimal recalculation if we call update() to add more data\n        to the hashed string.\n        \"\"\"\n\n        leninBuf = len(inBuf)\n\n        # Compute number of bytes mod 64.\n        index = (self.count[1] >> 3) & 0x3F\n\n        # Update number of bits.\n        self.count[1] = self.count[1] + (leninBuf << 3)\n        if self.count[1] < (leninBuf << 3):\n            self.count[0] = self.count[0] + 1\n        self.count[0] = self.count[0] + (leninBuf >> 29)\n\n        partLen = 64 - index\n\n        if leninBuf >= partLen:\n            self.input[index:] = list(inBuf[:partLen])\n            self._transform(_bytelist2longBigEndian(self.input))\n            i = partLen\n            while i + 63 < leninBuf:\n                self._transform(_bytelist2longBigEndian(list(inBuf[i:i+64])))\n                i = i + 64\n            else:\n                self.input = list(inBuf[i:leninBuf])\n        else:\n            i = 0\n            self.input = self.input + list(inBuf)\n\n\n    def digest(self):\n        \"\"\"Terminate the message-digest computation and return digest.\n\n        Return the digest of the strings passed to the update()\n        method so far. This is a 16-byte string which may contain\n        non-ASCII characters, including null bytes.\n        \"\"\"\n\n        H0 = self.H0\n        H1 = self.H1\n        H2 = self.H2\n        H3 = self.H3\n        H4 = self.H4\n        input = [] + self.input\n        count = [] + self.count\n\n        index = (self.count[1] >> 3) & 0x3f\n\n        if index < 56:\n            padLen = 56 - index\n        else:\n            padLen = 120 - index\n\n        padding = ['\\200'] + ['\\000'] * 63\n        self.update(padding[:padLen])\n\n        # Append length (before padding).\n        bits = _bytelist2longBigEndian(self.input[:56]) + count\n\n        self._transform(bits)\n\n        # Store state in digest.\n        digest = _long2bytesBigEndian(self.H0, 4) + \\\n                 _long2bytesBigEndian(self.H1, 4) + \\\n                 _long2bytesBigEndian(self.H2, 4) + \\\n                 _long2bytesBigEndian(self.H3, 4) + \\\n                 _long2bytesBigEndian(self.H4, 4)\n\n        self.H0 = H0\n        self.H1 = H1\n        self.H2 = H2\n        self.H3 = H3\n        self.H4 = H4\n        self.input = input\n        self.count = count\n\n        return digest\n\n\n    def hexdigest(self):\n        \"\"\"Terminate and return digest in HEX form.\n\n        Like digest() except the digest is returned as a string of\n        length 32, containing only hexadecimal digits. This may be\n        used to exchange the value safely in email or other non-\n        binary environments.\n        \"\"\"\n        return ''.join(['%02x' % ord(c) for c in self.digest()])\n\n    def copy(self):\n        \"\"\"Return a clone object.\n\n        Return a copy ('clone') of the md5 object. This can be used\n        to efficiently compute the digests of strings that share\n        a common initial substring.\n        \"\"\"\n\n        return copy.deepcopy(self)\n\n\n# ======================================================================\n# Mimic Python top-level functions from standard library API\n# for consistency with the _sha module of the standard library.\n# ======================================================================\n\n# These are mandatory variables in the module. They have constant values\n# in the SHA standard.\n\ndigest_size = 20\ndigestsize = 20\nblocksize = 1\n\ndef new(arg=None):\n    \"\"\"Return a new sha crypto object.\n\n    If arg is present, the method call update(arg) is made.\n    \"\"\"\n\n    crypto = sha()\n    if arg:\n        crypto.update(arg)\n\n    return crypto\n", 
    "_sha256": "import struct\n\nSHA_BLOCKSIZE = 64\nSHA_DIGESTSIZE = 32\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR = lambda x, y: (((x & 0xffffffff) >> (y & 31)) | (x << (32 - (y & 31)))) & 0xffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR(x, n)\nR = lambda x, n: (x & 0xffffffff) >> n\nSigma0 = lambda x: (S(x, 2) ^ S(x, 13) ^ S(x, 22))\nSigma1 = lambda x: (S(x, 6) ^ S(x, 11) ^ S(x, 25))\nGamma0 = lambda x: (S(x, 7) ^ S(x, 18) ^ R(x, 3))\nGamma1 = lambda x: (S(x, 17) ^ S(x, 19) ^ R(x, 10))\n\ndef sha_transform(sha_info):\n    W = []\n    \n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[4*i]<<24) + (d[4*i+1]<<16) + (d[4*i+2]<<8) + d[4*i+3])\n    \n    for i in xrange(16,64):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffff )\n    \n    ss = sha_info['digest'][:]\n    \n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = h + Sigma1(e) + Ch(e, f, g) + ki + W[i];\n        t1 = Sigma0(a) + Maj(a, b, c);\n        d += t0;\n        h  = t0 + t1;\n        return d & 0xffffffff, h & 0xffffffff\n    \n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x71374491);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcf);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25b);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b01);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc6);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dc);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c8);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x14292967);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a85);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b2138);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d13);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a7354);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c85);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a1);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664b);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a3);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd6990624);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e3585);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa070);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c08);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774c);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4a);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3);\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee);\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f);\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814);\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc70208);\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa);\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506ceb);\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7);\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2);\n    \n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0x6A09E667, 0xBB67AE85, 0x3C6EF372, 0xA54FF53A, 0x510E527F, 0x9B05688C, 0x1F83D9AB, 0x5BE0CD19]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 32\n    return sha_info\n\ndef sha224_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [0xc1059ed8, 0x367cd507, 0x3070dd17, 0xf70e5939, 0xffc00b31, 0x68581511, 0x64f98fa7, 0xbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 28\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n    \n    sha_info['count_hi'] += (count >> 29)\n    \n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n        \n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n        \n        count -= i\n        buffer_idx += i\n        \n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n    \n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n        \n    \n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x3f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 8:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n    \n    sha_info['data'][56] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][57] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][58] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][59] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][60] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][61] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][62] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][63] = (lo_bit_count >>  0) & 0xff\n    \n    sha_transform(sha_info)\n    \n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha256(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n    \n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n    \n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n    \n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha256.__new__(sha256)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha224(sha256):\n    digest_size = digestsize = 28\n\n    def __init__(self, s=None):\n        self._sha = sha224_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha224.__new__(sha224)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    a_str = \"just a test string\"\n    \n    assert 'e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855' == sha256().hexdigest()\n    assert 'd7b553c6f09ac85d142415f857c5310f3bbbe7cdd787cce4b985acedd585266f' == sha256(a_str).hexdigest()\n    assert '8113ebf33c97daa9998762aacafe750c7cefc2b2f173c90c59663a57fe626f21' == sha256(a_str*7).hexdigest()\n    \n    s = sha256(a_str)\n    s.update(a_str)\n    assert '03d9963e05a094593190b6fc794cb1a3e1ac7d7883f0b5855268afeccc70d461' == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n\n\n", 
    "_sha512": "\"\"\"\nThis code was Ported from CPython's sha512module.c\n\"\"\"\n\nimport struct\n\nSHA_BLOCKSIZE = 128\nSHA_DIGESTSIZE = 64\n\n\ndef new_shaobject():\n    return {\n        'digest': [0]*8,\n        'count_lo': 0,\n        'count_hi': 0,\n        'data': [0]* SHA_BLOCKSIZE,\n        'local': 0,\n        'digestsize': 0\n    }\n\nROR64 = lambda x, y: (((x & 0xffffffffffffffff) >> (y & 63)) | (x << (64 - (y & 63)))) & 0xffffffffffffffff\nCh = lambda x, y, z: (z ^ (x & (y ^ z)))\nMaj = lambda x, y, z: (((x | y) & z) | (x & y))\nS = lambda x, n: ROR64(x, n)\nR = lambda x, n: (x & 0xffffffffffffffff) >> n\nSigma0 = lambda x: (S(x, 28) ^ S(x, 34) ^ S(x, 39))\nSigma1 = lambda x: (S(x, 14) ^ S(x, 18) ^ S(x, 41))\nGamma0 = lambda x: (S(x, 1) ^ S(x, 8) ^ R(x, 7))\nGamma1 = lambda x: (S(x, 19) ^ S(x, 61) ^ R(x, 6))\n\ndef sha_transform(sha_info):\n    W = []\n\n    d = sha_info['data']\n    for i in xrange(0,16):\n        W.append( (d[8*i]<<56) + (d[8*i+1]<<48) + (d[8*i+2]<<40) + (d[8*i+3]<<32) + (d[8*i+4]<<24) + (d[8*i+5]<<16) + (d[8*i+6]<<8) + d[8*i+7])\n\n    for i in xrange(16,80):\n        W.append( (Gamma1(W[i - 2]) + W[i - 7] + Gamma0(W[i - 15]) + W[i - 16]) & 0xffffffffffffffff )\n\n    ss = sha_info['digest'][:]\n\n    def RND(a,b,c,d,e,f,g,h,i,ki):\n        t0 = (h + Sigma1(e) + Ch(e, f, g) + ki + W[i]) & 0xffffffffffffffff\n        t1 = (Sigma0(a) + Maj(a, b, c)) & 0xffffffffffffffff\n        d = (d + t0) & 0xffffffffffffffff\n        h = (t0 + t1) & 0xffffffffffffffff\n        return d & 0xffffffffffffffff, h & 0xffffffffffffffff\n\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],0,0x428a2f98d728ae22)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],1,0x7137449123ef65cd)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],2,0xb5c0fbcfec4d3b2f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],3,0xe9b5dba58189dbbc)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],4,0x3956c25bf348b538)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],5,0x59f111f1b605d019)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],6,0x923f82a4af194f9b)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],7,0xab1c5ed5da6d8118)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],8,0xd807aa98a3030242)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],9,0x12835b0145706fbe)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],10,0x243185be4ee4b28c)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],11,0x550c7dc3d5ffb4e2)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],12,0x72be5d74f27b896f)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],13,0x80deb1fe3b1696b1)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],14,0x9bdc06a725c71235)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],15,0xc19bf174cf692694)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],16,0xe49b69c19ef14ad2)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],17,0xefbe4786384f25e3)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],18,0x0fc19dc68b8cd5b5)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],19,0x240ca1cc77ac9c65)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],20,0x2de92c6f592b0275)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],21,0x4a7484aa6ea6e483)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],22,0x5cb0a9dcbd41fbd4)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],23,0x76f988da831153b5)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],24,0x983e5152ee66dfab)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],25,0xa831c66d2db43210)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],26,0xb00327c898fb213f)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],27,0xbf597fc7beef0ee4)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],28,0xc6e00bf33da88fc2)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],29,0xd5a79147930aa725)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],30,0x06ca6351e003826f)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],31,0x142929670a0e6e70)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],32,0x27b70a8546d22ffc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],33,0x2e1b21385c26c926)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],34,0x4d2c6dfc5ac42aed)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],35,0x53380d139d95b3df)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],36,0x650a73548baf63de)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],37,0x766a0abb3c77b2a8)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],38,0x81c2c92e47edaee6)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],39,0x92722c851482353b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],40,0xa2bfe8a14cf10364)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],41,0xa81a664bbc423001)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],42,0xc24b8b70d0f89791)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],43,0xc76c51a30654be30)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],44,0xd192e819d6ef5218)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],45,0xd69906245565a910)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],46,0xf40e35855771202a)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],47,0x106aa07032bbd1b8)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],48,0x19a4c116b8d2d0c8)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],49,0x1e376c085141ab53)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],50,0x2748774cdf8eeb99)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],51,0x34b0bcb5e19b48a8)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],52,0x391c0cb3c5c95a63)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],53,0x4ed8aa4ae3418acb)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],54,0x5b9cca4f7763e373)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],55,0x682e6ff3d6b2b8a3)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],56,0x748f82ee5defb2fc)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],57,0x78a5636f43172f60)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],58,0x84c87814a1f0ab72)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],59,0x8cc702081a6439ec)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],60,0x90befffa23631e28)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],61,0xa4506cebde82bde9)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],62,0xbef9a3f7b2c67915)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],63,0xc67178f2e372532b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],64,0xca273eceea26619c)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],65,0xd186b8c721c0c207)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],66,0xeada7dd6cde0eb1e)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],67,0xf57d4f7fee6ed178)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],68,0x06f067aa72176fba)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],69,0x0a637dc5a2c898a6)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],70,0x113f9804bef90dae)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],71,0x1b710b35131c471b)\n    ss[3], ss[7] = RND(ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],72,0x28db77f523047d84)\n    ss[2], ss[6] = RND(ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],73,0x32caab7b40c72493)\n    ss[1], ss[5] = RND(ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],ss[5],74,0x3c9ebe0a15c9bebc)\n    ss[0], ss[4] = RND(ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],ss[4],75,0x431d67c49c100d4c)\n    ss[7], ss[3] = RND(ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],ss[3],76,0x4cc5d4becb3e42b6)\n    ss[6], ss[2] = RND(ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],ss[2],77,0x597f299cfc657e2a)\n    ss[5], ss[1] = RND(ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],ss[1],78,0x5fcb6fab3ad6faec)\n    ss[4], ss[0] = RND(ss[1],ss[2],ss[3],ss[4],ss[5],ss[6],ss[7],ss[0],79,0x6c44198c4a475817)\n\n    dig = []\n    for i, x in enumerate(sha_info['digest']):\n        dig.append( (x + ss[i]) & 0xffffffffffffffff )\n    sha_info['digest'] = dig\n\ndef sha_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0x6a09e667f3bcc908, 0xbb67ae8584caa73b, 0x3c6ef372fe94f82b, 0xa54ff53a5f1d36f1, 0x510e527fade682d1, 0x9b05688c2b3e6c1f, 0x1f83d9abfb41bd6b, 0x5be0cd19137e2179]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 64\n    return sha_info\n\ndef sha384_init():\n    sha_info = new_shaobject()\n    sha_info['digest'] = [ 0xcbbb9d5dc1059ed8, 0x629a292a367cd507, 0x9159015a3070dd17, 0x152fecd8f70e5939, 0x67332667ffc00b31, 0x8eb44a8768581511, 0xdb0c2e0d64f98fa7, 0x47b5481dbefa4fa4]\n    sha_info['count_lo'] = 0\n    sha_info['count_hi'] = 0\n    sha_info['local'] = 0\n    sha_info['digestsize'] = 48\n    return sha_info\n\ndef getbuf(s):\n    if isinstance(s, str):\n        return s\n    elif isinstance(s, unicode):\n        return str(s)\n    else:\n        return buffer(s)\n\ndef sha_update(sha_info, buffer):\n    count = len(buffer)\n    buffer_idx = 0\n    clo = (sha_info['count_lo'] + (count << 3)) & 0xffffffff\n    if clo < sha_info['count_lo']:\n        sha_info['count_hi'] += 1\n    sha_info['count_lo'] = clo\n\n    sha_info['count_hi'] += (count >> 29)\n\n    if sha_info['local']:\n        i = SHA_BLOCKSIZE - sha_info['local']\n        if i > count:\n            i = count\n\n        # copy buffer\n        for x in enumerate(buffer[buffer_idx:buffer_idx+i]):\n            sha_info['data'][sha_info['local']+x[0]] = struct.unpack('B', x[1])[0]\n\n        count -= i\n        buffer_idx += i\n\n        sha_info['local'] += i\n        if sha_info['local'] == SHA_BLOCKSIZE:\n            sha_transform(sha_info)\n            sha_info['local'] = 0\n        else:\n            return\n\n    while count >= SHA_BLOCKSIZE:\n        # copy buffer\n        sha_info['data'] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + SHA_BLOCKSIZE]]\n        count -= SHA_BLOCKSIZE\n        buffer_idx += SHA_BLOCKSIZE\n        sha_transform(sha_info)\n\n    # copy buffer\n    pos = sha_info['local']\n    sha_info['data'][pos:pos+count] = [struct.unpack('B',c)[0] for c in buffer[buffer_idx:buffer_idx + count]]\n    sha_info['local'] = count\n\ndef sha_final(sha_info):\n    lo_bit_count = sha_info['count_lo']\n    hi_bit_count = sha_info['count_hi']\n    count = (lo_bit_count >> 3) & 0x7f\n    sha_info['data'][count] = 0x80;\n    count += 1\n    if count > SHA_BLOCKSIZE - 16:\n        # zero the bytes in data after the count\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n        sha_transform(sha_info)\n        # zero bytes in data\n        sha_info['data'] = [0] * SHA_BLOCKSIZE\n    else:\n        sha_info['data'] = sha_info['data'][:count] + ([0] * (SHA_BLOCKSIZE - count))\n\n    sha_info['data'][112] = 0;\n    sha_info['data'][113] = 0;\n    sha_info['data'][114] = 0;\n    sha_info['data'][115] = 0;\n    sha_info['data'][116] = 0;\n    sha_info['data'][117] = 0;\n    sha_info['data'][118] = 0;\n    sha_info['data'][119] = 0;\n\n    sha_info['data'][120] = (hi_bit_count >> 24) & 0xff\n    sha_info['data'][121] = (hi_bit_count >> 16) & 0xff\n    sha_info['data'][122] = (hi_bit_count >>  8) & 0xff\n    sha_info['data'][123] = (hi_bit_count >>  0) & 0xff\n    sha_info['data'][124] = (lo_bit_count >> 24) & 0xff\n    sha_info['data'][125] = (lo_bit_count >> 16) & 0xff\n    sha_info['data'][126] = (lo_bit_count >>  8) & 0xff\n    sha_info['data'][127] = (lo_bit_count >>  0) & 0xff\n\n    sha_transform(sha_info)\n\n    dig = []\n    for i in sha_info['digest']:\n        dig.extend([ ((i>>56) & 0xff), ((i>>48) & 0xff), ((i>>40) & 0xff), ((i>>32) & 0xff), ((i>>24) & 0xff), ((i>>16) & 0xff), ((i>>8) & 0xff), (i & 0xff) ])\n    return ''.join([chr(i) for i in dig])\n\nclass sha512(object):\n    digest_size = digestsize = SHA_DIGESTSIZE\n    block_size = SHA_BLOCKSIZE\n\n    def __init__(self, s=None):\n        self._sha = sha_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def update(self, s):\n        sha_update(self._sha, getbuf(s))\n\n    def digest(self):\n        return sha_final(self._sha.copy())[:self._sha['digestsize']]\n\n    def hexdigest(self):\n        return ''.join(['%.2x' % ord(i) for i in self.digest()])\n\n    def copy(self):\n        new = sha512.__new__(sha512)\n        new._sha = self._sha.copy()\n        return new\n\nclass sha384(sha512):\n    digest_size = digestsize = 48\n\n    def __init__(self, s=None):\n        self._sha = sha384_init()\n        if s:\n            sha_update(self._sha, getbuf(s))\n\n    def copy(self):\n        new = sha384.__new__(sha384)\n        new._sha = self._sha.copy()\n        return new\n\ndef test():\n    import _sha512\n\n    a_str = \"just a test string\"\n\n    assert _sha512.sha512().hexdigest() == sha512().hexdigest()\n    assert _sha512.sha512(a_str).hexdigest() == sha512(a_str).hexdigest()\n    assert _sha512.sha512(a_str*7).hexdigest() == sha512(a_str*7).hexdigest()\n\n    s = sha512(a_str)\n    s.update(a_str)\n    assert _sha512.sha512(a_str+a_str).hexdigest() == s.hexdigest()\n\nif __name__ == \"__main__\":\n    test()\n", 
    "_socket": "from __future__ import unicode_literals\nAF_APPLETALK = AF_ASH = AF_ATMPVC = AF_ATMSVC = AF_AX25 = AF_BLUETOOTH = AF_BRIDGE = AF_ECONET = AF_INET = AF_INET6 = AF_IPX = AF_IRDA = AF_KEY = AF_LLC = AF_NETBEUI = AF_NETLINK = AF_NETROM = AF_PACKET = AF_PPPOX = AF_ROSE = AF_ROUTE = AF_SECURITY = AF_SNA = AF_UNIX = AF_UNSPEC = AF_WANPIPE = AF_X25 = AI_ADDRCONFIG = AI_ALL = AI_CANONNAME = AI_NUMERICHOST = AI_NUMERICSERV = AI_PASSIVE = AI_V4MAPPED = BDADDR_ANY = BDADDR_LOCAL = EAI_ADDRFAMILY = EAI_AGAIN = EAI_BADFLAGS = EAI_FAIL = EAI_FAMILY = EAI_MEMORY = EAI_NODATA = EAI_NONAME = EAI_OVERFLOW = EAI_SERVICE = EAI_SOCKTYPE = EAI_SYSTEM = FD_SETSIZE = INADDR_ALLHOSTS_GROUP = INADDR_ANY = INADDR_BROADCAST = INADDR_LOOPBACK = INADDR_MAX_LOCAL_GROUP = INADDR_NONE = INADDR_UNSPEC_GROUP = IPPORT_RESERVED = IPPORT_USERRESERVED = IPPROTO_AH = IPPROTO_DSTOPTS = IPPROTO_EGP = IPPROTO_ESP = IPPROTO_FRAGMENT = IPPROTO_GRE = IPPROTO_HOPOPTS = IPPROTO_ICMP = IPPROTO_ICMPV6 = IPPROTO_IDP = IPPROTO_IGMP = IPPROTO_IP = IPPROTO_IPIP = IPPROTO_IPV6 = IPPROTO_NONE = IPPROTO_PIM = IPPROTO_PUP = IPPROTO_RAW = IPPROTO_ROUTING = IPPROTO_RSVP = IPPROTO_TCP = IPPROTO_TP = IPPROTO_UDP = IPV6_CHECKSUM = IPV6_DSTOPTS = IPV6_HOPLIMIT = IPV6_HOPOPTS = IPV6_JOIN_GROUP = IPV6_LEAVE_GROUP = IPV6_MULTICAST_HOPS = IPV6_MULTICAST_IF = IPV6_MULTICAST_LOOP = IPV6_NEXTHOP = IPV6_PKTINFO = IPV6_RECVDSTOPTS = IPV6_RECVHOPLIMIT = IPV6_RECVHOPOPTS = IPV6_RECVPKTINFO = IPV6_RECVRTHDR = IPV6_RECVTCLASS = IPV6_RTHDR = IPV6_RTHDRDSTOPTS = IPV6_RTHDR_TYPE_0 = IPV6_TCLASS = IPV6_UNICAST_HOPS = IPV6_V6ONLY = IP_ADD_MEMBERSHIP = IP_DEFAULT_MULTICAST_LOOP = IP_DEFAULT_MULTICAST_TTL = IP_DROP_MEMBERSHIP = IP_HDRINCL = IP_MAX_MEMBERSHIPS = IP_MULTICAST_IF = IP_MULTICAST_LOOP = IP_MULTICAST_TTL = IP_OPTIONS = IP_RECVOPTS = IP_RECVRETOPTS = IP_RETOPTS = IP_TOS = IP_TTL = MSG_CTRUNC = MSG_DONTROUTE = MSG_DONTWAIT = MSG_EOR = MSG_OOB = MSG_PEEK = MSG_TRUNC = MSG_WAITALL = NETLINK_DNRTMSG = NETLINK_FIREWALL = NETLINK_IP6_FW = NETLINK_NFLOG = NETLINK_ROUTE = NETLINK_USERSOCK = NETLINK_XFRM = NI_DGRAM = NI_MAXHOST = NI_MAXSERV = NI_NAMEREQD = NI_NOFQDN = NI_NUMERICHOST = NI_NUMERICSERV = PACKET_BROADCAST = PACKET_FASTROUTE = PACKET_HOST = PACKET_LOOPBACK = PACKET_MULTICAST = PACKET_OTHERHOST = PACKET_OUTGOING = POLLERR = POLLHUP = POLLIN = POLLMSG = POLLNVAL = POLLOUT = POLLPRI = POLLRDBAND = POLLRDNORM = POLLWRNORM = SHUT_RD = SHUT_RDWR = SHUT_WR = SIOCGIFINDEX = SIOCGIFNAME = SOCK_DGRAM = SOCK_RAW = SOCK_RDM = SOCK_SEQPACKET = SOCK_STREAM = SOL_IP = SOL_SOCKET = SOL_TCP = SOL_UDP = SOMAXCONN = SO_ACCEPTCONN = SO_BROADCAST = SO_DEBUG = SO_DONTROUTE = SO_ERROR = SO_KEEPALIVE = SO_LINGER = SO_OOBINLINE = SO_RCVBUF = SO_RCVLOWAT = SO_RCVTIMEO = SO_REUSEADDR = SO_REUSEPORT = SO_SNDBUF = SO_SNDLOWAT = SO_SNDTIMEO = SO_TYPE = 1\n\nSocketType = TCP_CORK = TCP_DEFER_ACCEPT = TCP_INFO = TCP_KEEPCNT = TCP_KEEPIDLE = TCP_KEEPINTVL = TCP_LINGER2 = TCP_MAXSEG = TCP_NODELAY = TCP_QUICKACK = TCP_SYNCNT = TCP_WINDOW_CLAMP = error = fromfd = gaierror = getaddrinfo = getdefaulttimeout = gethostbyaddr = gethostbyname = gethostbyname_ex = gethostname = getnameinfo = getprotobyname = getservbyname = getservbyport = has_ipv6 = herror = htonl = htons = inet_aton = inet_ntoa = inet_ntop = inet_pton = ntohl = ntohs = setdefaulttimeout = socket = socketpair = timeout = None\n\nimport js\nimport os\nimport StringIO\nimport time\n\n\n\nsockets=os.fileNumbers\nclass error(Exception): pass\n\n\nclass _fileobject(file):\n    def __init__(self, sock):\n        self._sock=sock\n    def read(self, length):\n        return self._sock.recv(length)\n    def write(self, msg):\n        return self._sock.send(msg)\n    def pullFromJS(self):\n        return self._sock.pullFromJS()\n\nclass _socket(object):\n    '''socket implementation using websockets'''\n    def __init__(self, *_):\n        self._fileno=len(sockets)\n        os.fileNumbers.append(self.makefile())\n        self.timeout=None\n        self.closed=False\n    @property\n    def ready(self):\n        return hasattr(self, '_sock') and self._sock.ready\n    def fileno(self):\n        return self._fileno\n    def accept(self):\n        raise Exception(\"sockets in server mode not yet implemented\")\n    def bind(self, (address, port)):\n        raise Exception(\"Bind not supported\")\n    def listen(self):\n        raise Exception(\"Listen not supported\")\n    def close(self, *_):\n        self._sock.close()\n        self.closed=True\n    def connect(self, arg):\n        self._stringIO = StringIO.StringIO()\n        if isinstance(arg, (str, unicode)):\n            self.connection_info=arg.split(':')\n            self._sock = js.globals['socketConnect'](arg)\n        else:\n            address,port=arg\n            self.connection_info=address,port\n            self._sock = js.globals['socketConnect'](\"ws://\" + address + ':' + str(port))\n        def onmsg(msg):\n            self._stringIO.write(msg)\n        self.onmsgHandler=js.Function(onmsg),onmsg\n        self._sock.onrecv=self.onmsgHandler[0]\n    def settimeout(self, t):\n        self.timeout=t\n    def setsockopt(self, *a):\n        pass\n    def getsockopt(self, *a):\n        return 0\n    def gettimeout(self):\n        return self.timeout\n    def makefile(self):\n        return _fileobject(self)\n    proto=0\n    def pullFromJS(self):\n        #while self._sock.queue.length:\n        #    self._stringIO.write(self._sock.queue.shift());\n        return len(self._stringIO.getvalue())\n    def recv(self, length, *_):\n        self.pullFromJS()\n        if self.timeout is None:\n            while True:\n                r = self._stringIO.getvalue()\n                self._stringIO.buf=r[length:]\n                r=r[:length]\n                if r:\n                    return r\n                time.sleep(0.1)\n        else:\n            r = self._stringIO.getvalue()\n            self._stringIO.buf=r[length:]\n            r=r[:length]\n            if r:\n                return r\n            time.sleep(self.timeout)\n            r = self._stringIO.getvalue()\n            self._stringIO.buf=r[length:]\n            r=r[:length]\n            if r:\n                return r\n        raise error('Timed out')\n    def send(self, msg, *a):\n        self._sock.send(str(msg))\n        return len(msg)\n    def recv_into(self, buff):\n        buff.write(self.recv(1024))\n    def recvfrom(self, num):\n        return self.recv(num), 'server'\n    def recvfrom_into(self, buff):\n        return self.recv_into(num), 'server'\n    def sendto(self, data, flags, address):\n        raise Exception(\"not implemented\")\n    def connect_ex(self, addr):\n        return self.connect(addr)\n    def getpeername(self):\n        return self.connection_info\n    def getsockname(self):\n        return '127.0.0.1',0\n    def sendall(self, data):\n        self.send(data)\n    def setblocking(self, v):\n        self.blocking=v\n    def shutdown(self, side):\n        self.close()\n    def _drop(self, *_):\n        self.close()\n\ndef getservbyname(a,b):\n    return a\n\nsocket=_socket\n", 
    "_strptime": "\"\"\"Strptime-related classes and functions.\n\nCLASSES:\n    LocaleTime -- Discovers and stores locale-specific time information\n    TimeRE -- Creates regexes for pattern matching a string of text containing\n                time information\n\nFUNCTIONS:\n    _getlang -- Figure out what language is being used for the locale\n    strptime -- Calculates the time struct represented by the passed-in string\n\n\"\"\"\nimport time\nimport locale\nimport calendar\nfrom re import compile as re_compile\nfrom re import IGNORECASE\nfrom re import escape as re_escape\nfrom datetime import date as datetime_date\ntry:\n    from thread import allocate_lock as _thread_allocate_lock\nexcept:\n    from dummy_thread import allocate_lock as _thread_allocate_lock\n\n__all__ = []\n\ndef _getlang():\n    # Figure out what the current language is set to.\n    return locale.getlocale(locale.LC_TIME)\n\nclass LocaleTime(object):\n    \"\"\"Stores and handles locale-specific information related to time.\n\n    ATTRIBUTES:\n        f_weekday -- full weekday names (7-item list)\n        a_weekday -- abbreviated weekday names (7-item list)\n        f_month -- full month names (13-item list; dummy value in [0], which\n                    is added by code)\n        a_month -- abbreviated month names (13-item list, dummy value in\n                    [0], which is added by code)\n        am_pm -- AM/PM representation (2-item list)\n        LC_date_time -- format string for date/time representation (string)\n        LC_date -- format string for date representation (string)\n        LC_time -- format string for time representation (string)\n        timezone -- daylight- and non-daylight-savings timezone representation\n                    (2-item list of sets)\n        lang -- Language used by instance (2-item tuple)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Set all attributes.\n\n        Order of methods called matters for dependency reasons.\n\n        The locale language is set at the offset and then checked again before\n        exiting.  This is to make sure that the attributes were not set with a\n        mix of information from more than one locale.  This would most likely\n        happen when using threads where one thread calls a locale-dependent\n        function while another thread changes the locale while the function in\n        the other thread is still running.  Proper coding would call for\n        locks to prevent changing the locale while locale-dependent code is\n        running.  The check here is done in case someone does not think about\n        doing this.\n\n        Only other possible issue is if someone changed the timezone and did\n        not call tz.tzset .  That is an issue for the programmer, though,\n        since changing the timezone is worthless without that call.\n\n        \"\"\"\n        self.lang = _getlang()\n        self.__calc_weekday()\n        self.__calc_month()\n        self.__calc_am_pm()\n        self.__calc_timezone()\n        self.__calc_date_time()\n        if _getlang() != self.lang:\n            raise ValueError(\"locale changed during initialization\")\n\n    def __pad(self, seq, front):\n        # Add '' to seq to either the front (is True), else the back.\n        seq = list(seq)\n        if front:\n            seq.insert(0, '')\n        else:\n            seq.append('')\n        return seq\n\n    def __calc_weekday(self):\n        # Set self.a_weekday and self.f_weekday using the calendar\n        # module.\n        a_weekday = [calendar.day_abbr[i].lower() for i in range(7)]\n        f_weekday = [calendar.day_name[i].lower() for i in range(7)]\n        self.a_weekday = a_weekday\n        self.f_weekday = f_weekday\n\n    def __calc_month(self):\n        # Set self.f_month and self.a_month using the calendar module.\n        a_month = [calendar.month_abbr[i].lower() for i in range(13)]\n        f_month = [calendar.month_name[i].lower() for i in range(13)]\n        self.a_month = a_month\n        self.f_month = f_month\n\n    def __calc_am_pm(self):\n        # Set self.am_pm by using time.strftime().\n\n        # The magic date (1999,3,17,hour,44,55,2,76,0) is not really that\n        # magical; just happened to have used it everywhere else where a\n        # static date was needed.\n        am_pm = []\n        for hour in (01,22):\n            time_tuple = time.struct_time((1999,3,17,hour,44,55,2,76,0))\n            am_pm.append(time.strftime(\"%p\", time_tuple).lower())\n        self.am_pm = am_pm\n\n    def __calc_date_time(self):\n        # Set self.date_time, self.date, & self.time by using\n        # time.strftime().\n\n        # Use (1999,3,17,22,44,55,2,76,0) for magic date because the amount of\n        # overloaded numbers is minimized.  The order in which searches for\n        # values within the format string is very important; it eliminates\n        # possible ambiguity for what something represents.\n        time_tuple = time.struct_time((1999,3,17,22,44,55,2,76,0))\n        date_time = [None, None, None]\n        date_time[0] = time.strftime(\"%c\", time_tuple).lower()\n        date_time[1] = time.strftime(\"%x\", time_tuple).lower()\n        date_time[2] = time.strftime(\"%X\", time_tuple).lower()\n        replacement_pairs = [('%', '%%'), (self.f_weekday[2], '%A'),\n                    (self.f_month[3], '%B'), (self.a_weekday[2], '%a'),\n                    (self.a_month[3], '%b'), (self.am_pm[1], '%p'),\n                    ('1999', '%Y'), ('99', '%y'), ('22', '%H'),\n                    ('44', '%M'), ('55', '%S'), ('76', '%j'),\n                    ('17', '%d'), ('03', '%m'), ('3', '%m'),\n                    # '3' needed for when no leading zero.\n                    ('2', '%w'), ('10', '%I')]\n        replacement_pairs.extend([(tz, \"%Z\") for tz_values in self.timezone\n                                                for tz in tz_values])\n        for offset,directive in ((0,'%c'), (1,'%x'), (2,'%X')):\n            current_format = date_time[offset]\n            for old, new in replacement_pairs:\n                # Must deal with possible lack of locale info\n                # manifesting itself as the empty string (e.g., Swedish's\n                # lack of AM/PM info) or a platform returning a tuple of empty\n                # strings (e.g., MacOS 9 having timezone as ('','')).\n                if old:\n                    current_format = current_format.replace(old, new)\n            # If %W is used, then Sunday, 2005-01-03 will fall on week 0 since\n            # 2005-01-03 occurs before the first Monday of the year.  Otherwise\n            # %U is used.\n            time_tuple = time.struct_time((1999,1,3,1,1,1,6,3,0))\n            if '00' in time.strftime(directive, time_tuple):\n                U_W = '%W'\n            else:\n                U_W = '%U'\n            date_time[offset] = current_format.replace('11', U_W)\n        self.LC_date_time = date_time[0]\n        self.LC_date = date_time[1]\n        self.LC_time = date_time[2]\n\n    def __calc_timezone(self):\n        # Set self.timezone by using time.tzname.\n        # Do not worry about possibility of time.tzname[0] == timetzname[1]\n        # and time.daylight; handle that in strptime .\n        try:\n            time.tzset()\n        except AttributeError:\n            pass\n        no_saving = frozenset([\"utc\", \"gmt\", time.tzname[0].lower()])\n        if time.daylight:\n            has_saving = frozenset([time.tzname[1].lower()])\n        else:\n            has_saving = frozenset()\n        self.timezone = (no_saving, has_saving)\n\n\nclass TimeRE(dict):\n    \"\"\"Handle conversion from format directives to regexes.\"\"\"\n\n    def __init__(self, locale_time=None):\n        \"\"\"Create keys/values.\n\n        Order of execution is important for dependency reasons.\n\n        \"\"\"\n        if locale_time:\n            self.locale_time = locale_time\n        else:\n            self.locale_time = LocaleTime()\n        base = super(TimeRE, self)\n        base.__init__({\n            # The \" \\d\" part of the regex is to make %c from ANSI C work\n            'd': r\"(?P<d>3[0-1]|[1-2]\\d|0[1-9]|[1-9]| [1-9])\",\n            'f': r\"(?P<f>[0-9]{1,6})\",\n            'H': r\"(?P<H>2[0-3]|[0-1]\\d|\\d)\",\n            'I': r\"(?P<I>1[0-2]|0[1-9]|[1-9])\",\n            'j': r\"(?P<j>36[0-6]|3[0-5]\\d|[1-2]\\d\\d|0[1-9]\\d|00[1-9]|[1-9]\\d|0[1-9]|[1-9])\",\n            'm': r\"(?P<m>1[0-2]|0[1-9]|[1-9])\",\n            'M': r\"(?P<M>[0-5]\\d|\\d)\",\n            'S': r\"(?P<S>6[0-1]|[0-5]\\d|\\d)\",\n            'U': r\"(?P<U>5[0-3]|[0-4]\\d|\\d)\",\n            'w': r\"(?P<w>[0-6])\",\n            # W is set below by using 'U'\n            'y': r\"(?P<y>\\d\\d)\",\n            #XXX: Does 'Y' need to worry about having less or more than\n            #     4 digits?\n            'Y': r\"(?P<Y>\\d\\d\\d\\d)\",\n            'A': self.__seqToRE(self.locale_time.f_weekday, 'A'),\n            'a': self.__seqToRE(self.locale_time.a_weekday, 'a'),\n            'B': self.__seqToRE(self.locale_time.f_month[1:], 'B'),\n            'b': self.__seqToRE(self.locale_time.a_month[1:], 'b'),\n            'p': self.__seqToRE(self.locale_time.am_pm, 'p'),\n            'Z': self.__seqToRE((tz for tz_names in self.locale_time.timezone\n                                        for tz in tz_names),\n                                'Z'),\n            '%': '%'})\n        base.__setitem__('W', base.__getitem__('U').replace('U', 'W'))\n        base.__setitem__('c', self.pattern(self.locale_time.LC_date_time))\n        base.__setitem__('x', self.pattern(self.locale_time.LC_date))\n        base.__setitem__('X', self.pattern(self.locale_time.LC_time))\n\n    def __seqToRE(self, to_convert, directive):\n        \"\"\"Convert a list to a regex string for matching a directive.\n\n        Want possible matching values to be from longest to shortest.  This\n        prevents the possibility of a match occurring for a value that also\n        a substring of a larger value that should have matched (e.g., 'abc'\n        matching when 'abcdef' should have been the match).\n\n        \"\"\"\n        to_convert = sorted(to_convert, key=len, reverse=True)\n        for value in to_convert:\n            if value != '':\n                break\n        else:\n            return ''\n        regex = '|'.join(re_escape(stuff) for stuff in to_convert)\n        regex = '(?P<%s>%s' % (directive, regex)\n        return '%s)' % regex\n\n    def pattern(self, format):\n        \"\"\"Return regex pattern for the format string.\n\n        Need to make sure that any characters that might be interpreted as\n        regex syntax are escaped.\n\n        \"\"\"\n        processed_format = ''\n        # The sub() call escapes all characters that might be misconstrued\n        # as regex syntax.  Cannot use re.escape since we have to deal with\n        # format directives (%m, etc.).\n        regex_chars = re_compile(r\"([\\\\.^$*+?\\(\\){}\\[\\]|])\")\n        format = regex_chars.sub(r\"\\\\\\1\", format)\n        whitespace_replacement = re_compile('\\s+')\n        format = whitespace_replacement.sub('\\s+', format)\n        while '%' in format:\n            directive_index = format.index('%')+1\n            processed_format = \"%s%s%s\" % (processed_format,\n                                           format[:directive_index-1],\n                                           self[format[directive_index]])\n            format = format[directive_index+1:]\n        return \"%s%s\" % (processed_format, format)\n\n    def compile(self, format):\n        \"\"\"Return a compiled re object for the format string.\"\"\"\n        return re_compile(self.pattern(format), IGNORECASE)\n\n_cache_lock = _thread_allocate_lock()\n# DO NOT modify _TimeRE_cache or _regex_cache without acquiring the cache lock\n# first!\n_TimeRE_cache = TimeRE()\n_CACHE_MAX_SIZE = 5 # Max number of regexes stored in _regex_cache\n_regex_cache = {}\n\ndef _calc_julian_from_U_or_W(year, week_of_year, day_of_week, week_starts_Mon):\n    \"\"\"Calculate the Julian day based on the year, week of the year, and day of\n    the week, with week_start_day representing whether the week of the year\n    assumes the week starts on Sunday or Monday (6 or 0).\"\"\"\n    first_weekday = datetime_date(year, 1, 1).weekday()\n    # If we are dealing with the %U directive (week starts on Sunday), it's\n    # easier to just shift the view to Sunday being the first day of the\n    # week.\n    if not week_starts_Mon:\n        first_weekday = (first_weekday + 1) % 7\n        day_of_week = (day_of_week + 1) % 7\n    # Need to watch out for a week 0 (when the first day of the year is not\n    # the same as that specified by %U or %W).\n    week_0_length = (7 - first_weekday) % 7\n    if week_of_year == 0:\n        return 1 + day_of_week - first_weekday\n    else:\n        days_to_week = week_0_length + (7 * (week_of_year - 1))\n        return 1 + days_to_week + day_of_week\n\n\ndef _strptime(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\n    \"\"\"Return a time struct based on the input string and the format string.\"\"\"\n    global _TimeRE_cache, _regex_cache\n    with _cache_lock:\n        if _getlang() != _TimeRE_cache.locale_time.lang:\n            _TimeRE_cache = TimeRE()\n            _regex_cache.clear()\n        if len(_regex_cache) > _CACHE_MAX_SIZE:\n            _regex_cache.clear()\n        locale_time = _TimeRE_cache.locale_time\n        format_regex = _regex_cache.get(format)\n        if not format_regex:\n            try:\n                format_regex = _TimeRE_cache.compile(format)\n            # KeyError raised when a bad format is found; can be specified as\n            # \\\\, in which case it was a stray % but with a space after it\n            except KeyError, err:\n                bad_directive = err.args[0]\n                if bad_directive == \"\\\\\":\n                    bad_directive = \"%\"\n                del err\n                raise ValueError(\"'%s' is a bad directive in format '%s'\" %\n                                    (bad_directive, format))\n            # IndexError only occurs when the format string is \"%\"\n            except IndexError:\n                raise ValueError(\"stray %% in format '%s'\" % format)\n            _regex_cache[format] = format_regex\n    found = format_regex.match(data_string)\n    if not found:\n        raise ValueError(\"time data %r does not match format %r\" %\n                         (data_string, format))\n    if len(data_string) != found.end():\n        raise ValueError(\"unconverted data remains: %s\" %\n                          data_string[found.end():])\n\n    year = None\n    month = day = 1\n    hour = minute = second = fraction = 0\n    tz = -1\n    # Default to -1 to signify that values not known; not critical to have,\n    # though\n    week_of_year = -1\n    week_of_year_start = -1\n    # weekday and julian defaulted to -1 so as to signal need to calculate\n    # values\n    weekday = julian = -1\n    found_dict = found.groupdict()\n    for group_key in found_dict.iterkeys():\n        # Directives not explicitly handled below:\n        #   c, x, X\n        #      handled by making out of other directives\n        #   U, W\n        #      worthless without day of the week\n        if group_key == 'y':\n            year = int(found_dict['y'])\n            # Open Group specification for strptime() states that a %y\n            #value in the range of [00, 68] is in the century 2000, while\n            #[69,99] is in the century 1900\n            if year <= 68:\n                year += 2000\n            else:\n                year += 1900\n        elif group_key == 'Y':\n            year = int(found_dict['Y'])\n        elif group_key == 'm':\n            month = int(found_dict['m'])\n        elif group_key == 'B':\n            month = locale_time.f_month.index(found_dict['B'].lower())\n        elif group_key == 'b':\n            month = locale_time.a_month.index(found_dict['b'].lower())\n        elif group_key == 'd':\n            day = int(found_dict['d'])\n        elif group_key == 'H':\n            hour = int(found_dict['H'])\n        elif group_key == 'I':\n            hour = int(found_dict['I'])\n            ampm = found_dict.get('p', '').lower()\n            # If there was no AM/PM indicator, we'll treat this like AM\n            if ampm in ('', locale_time.am_pm[0]):\n                # We're in AM so the hour is correct unless we're\n                # looking at 12 midnight.\n                # 12 midnight == 12 AM == hour 0\n                if hour == 12:\n                    hour = 0\n            elif ampm == locale_time.am_pm[1]:\n                # We're in PM so we need to add 12 to the hour unless\n                # we're looking at 12 noon.\n                # 12 noon == 12 PM == hour 12\n                if hour != 12:\n                    hour += 12\n        elif group_key == 'M':\n            minute = int(found_dict['M'])\n        elif group_key == 'S':\n            second = int(found_dict['S'])\n        elif group_key == 'f':\n            s = found_dict['f']\n            # Pad to always return microseconds.\n            s += \"0\" * (6 - len(s))\n            fraction = int(s)\n        elif group_key == 'A':\n            weekday = locale_time.f_weekday.index(found_dict['A'].lower())\n        elif group_key == 'a':\n            weekday = locale_time.a_weekday.index(found_dict['a'].lower())\n        elif group_key == 'w':\n            weekday = int(found_dict['w'])\n            if weekday == 0:\n                weekday = 6\n            else:\n                weekday -= 1\n        elif group_key == 'j':\n            julian = int(found_dict['j'])\n        elif group_key in ('U', 'W'):\n            week_of_year = int(found_dict[group_key])\n            if group_key == 'U':\n                # U starts week on Sunday.\n                week_of_year_start = 6\n            else:\n                # W starts week on Monday.\n                week_of_year_start = 0\n        elif group_key == 'Z':\n            # Since -1 is default value only need to worry about setting tz if\n            # it can be something other than -1.\n            found_zone = found_dict['Z'].lower()\n            for value, tz_values in enumerate(locale_time.timezone):\n                if found_zone in tz_values:\n                    # Deal with bad locale setup where timezone names are the\n                    # same and yet time.daylight is true; too ambiguous to\n                    # be able to tell what timezone has daylight savings\n                    if (time.tzname[0] == time.tzname[1] and\n                       time.daylight and found_zone not in (\"utc\", \"gmt\")):\n                        break\n                    else:\n                        tz = value\n                        break\n    leap_year_fix = False\n    if year is None and month == 2 and day == 29:\n        year = 1904  # 1904 is first leap year of 20th century\n        leap_year_fix = True\n    elif year is None:\n        year = 1900\n    # If we know the week of the year and what day of that week, we can figure\n    # out the Julian day of the year.\n    if julian == -1 and week_of_year != -1 and weekday != -1:\n        week_starts_Mon = True if week_of_year_start == 0 else False\n        julian = _calc_julian_from_U_or_W(year, week_of_year, weekday,\n                                            week_starts_Mon)\n    # Cannot pre-calculate datetime_date() since can change in Julian\n    # calculation and thus could have different value for the day of the week\n    # calculation.\n    if julian == -1:\n        # Need to add 1 to result since first day of the year is 1, not 0.\n        julian = datetime_date(year, month, day).toordinal() - \\\n                  datetime_date(year, 1, 1).toordinal() + 1\n    else:  # Assume that if they bothered to include Julian day it will\n           # be accurate.\n        datetime_result = datetime_date.fromordinal((julian - 1) + datetime_date(year, 1, 1).toordinal())\n        year = datetime_result.year\n        month = datetime_result.month\n        day = datetime_result.day\n    if weekday == -1:\n        weekday = datetime_date(year, month, day).weekday()\n    if leap_year_fix:\n        # the caller didn't supply a year but asked for Feb 29th. We couldn't\n        # use the default of 1900 for computations. We set it back to ensure\n        # that February 29th is smaller than March 1st.\n        year = 1900\n\n    return (time.struct_time((year, month, day,\n                              hour, minute, second,\n                              weekday, julian, tz)), fraction)\n\ndef _strptime_time(data_string, format=\"%a %b %d %H:%M:%S %Y\"):\n    return _strptime(data_string, format)[0]\n", 
    "_structseq": "\"\"\"\nImplementation helper: a struct that looks like a tuple.  See timemodule\nand posixmodule for example uses.\n\"\"\"\n\nclass structseqfield(object):\n    \"\"\"Definition of field of a structseq.  The 'index' is for positional\n    tuple-like indexing.  Fields whose index is after a gap in the numbers\n    cannot be accessed like this, but only by name.\n    \"\"\"\n    def __init__(self, index, doc=None, default=lambda self: None):\n        self.__name__ = '?'\n        self.index    = index    # patched to None if not positional\n        self._index   = index\n        self.__doc__  = doc\n        self._default = default\n\n    def __repr__(self):\n        return '<field %s (%s)>' % (self.__name__,\n                                    self.__doc__ or 'undocumented')\n\n    def __get__(self, obj, typ=None):\n        if obj is None:\n            return self\n        if self.index is None:\n            return obj.__dict__[self.__name__]\n        else:\n            return obj[self.index]\n\n    def __set__(self, obj, value):\n        raise TypeError(\"readonly attribute\")\n\n\nclass structseqtype(type):\n\n    def __new__(metacls, classname, bases, dict):\n        assert not bases\n        fields_by_index = {}\n        for name, field in dict.items():\n            if isinstance(field, structseqfield):\n                assert field._index not in fields_by_index\n                fields_by_index[field._index] = field\n                field.__name__ = name\n        dict['n_fields'] = len(fields_by_index)\n\n        extra_fields = sorted(fields_by_index.iteritems())\n        n_sequence_fields = 0\n        while extra_fields and extra_fields[0][0] == n_sequence_fields:\n            extra_fields.pop(0)\n            n_sequence_fields += 1\n        dict['n_sequence_fields'] = n_sequence_fields\n        dict['n_unnamed_fields'] = 0     # no fully anonymous fields in PyPy\n\n        extra_fields = [field for index, field in extra_fields]\n        for field in extra_fields:\n            field.index = None     # no longer relevant\n\n        assert '__new__' not in dict\n        dict['_extra_fields'] = tuple(extra_fields)\n        dict['__new__'] = structseq_new\n        dict['__reduce__'] = structseq_reduce\n        dict['__setattr__'] = structseq_setattr\n        dict['__repr__'] = structseq_repr\n        dict['_name'] = dict.get('name', '')\n        return type.__new__(metacls, classname, (tuple,), dict)\n\n\nbuiltin_dict = dict\n\ndef structseq_new(cls, sequence, dict={}):\n    sequence = tuple(sequence)\n    dict = builtin_dict(dict)\n    N = cls.n_sequence_fields\n    if len(sequence) < N:\n        if N < cls.n_fields:\n            msg = \"at least\"\n        else:\n            msg = \"exactly\"\n        raise TypeError(\"expected a sequence with %s %d items\" % (\n            msg, N))\n    if len(sequence) > N:\n        if len(sequence) > cls.n_fields:\n            if N < cls.n_fields:\n                msg = \"at most\"\n            else:\n                msg = \"exactly\"\n            raise TypeError(\"expected a sequence with %s %d items\" % (\n                msg, cls.n_fields))\n        for field, value in zip(cls._extra_fields, sequence[N:]):\n            name = field.__name__\n            if name in dict:\n                raise TypeError(\"duplicate value for %r\" % (name,))\n            dict[name] = value\n        sequence = sequence[:N]\n    result = tuple.__new__(cls, sequence)\n    object.__setattr__(result, '__dict__', dict)\n    for field in cls._extra_fields:\n        name = field.__name__\n        if name not in dict:\n            dict[name] = field._default(result)\n    return result\n\ndef structseq_reduce(self):\n    return type(self), (tuple(self), self.__dict__)\n\ndef structseq_setattr(self, attr, value):\n    raise AttributeError(\"%r object has no attribute %r\" % (\n        self.__class__.__name__, attr))\n\ndef structseq_repr(self):\n    fields = {}\n    for field in type(self).__dict__.values():\n        if isinstance(field, structseqfield):\n            fields[field._index] = field\n    parts = [\"%s=%r\" % (fields[index].__name__, value)\n             for index, value in enumerate(self)]\n    return \"%s(%s)\" % (self._name, \", \".join(parts))\n", 
    "_threading_local": "\"\"\"Thread-local objects.\n\n(Note that this module provides a Python version of the threading.local\n class.  Depending on the version of Python you're using, there may be a\n faster one available.  You should always import the `local` class from\n `threading`.)\n\nThread-local objects support the management of thread-local data.\nIf you have data that you want to be local to a thread, simply create\na thread-local object and use its attributes:\n\n  >>> mydata = local()\n  >>> mydata.number = 42\n  >>> mydata.number\n  42\n\nYou can also access the local-object's dictionary:\n\n  >>> mydata.__dict__\n  {'number': 42}\n  >>> mydata.__dict__.setdefault('widgets', [])\n  []\n  >>> mydata.widgets\n  []\n\nWhat's important about thread-local objects is that their data are\nlocal to a thread. If we access the data in a different thread:\n\n  >>> log = []\n  >>> def f():\n  ...     items = mydata.__dict__.items()\n  ...     items.sort()\n  ...     log.append(items)\n  ...     mydata.number = 11\n  ...     log.append(mydata.number)\n\n  >>> import threading\n  >>> thread = threading.Thread(target=f)\n  >>> thread.start()\n  >>> thread.join()\n  >>> log\n  [[], 11]\n\nwe get different data.  Furthermore, changes made in the other thread\ndon't affect data seen in this thread:\n\n  >>> mydata.number\n  42\n\nOf course, values you get from a local object, including a __dict__\nattribute, are for whatever thread was current at the time the\nattribute was read.  For that reason, you generally don't want to save\nthese values across threads, as they apply only to the thread they\ncame from.\n\nYou can create custom local objects by subclassing the local class:\n\n  >>> class MyLocal(local):\n  ...     number = 2\n  ...     initialized = False\n  ...     def __init__(self, **kw):\n  ...         if self.initialized:\n  ...             raise SystemError('__init__ called too many times')\n  ...         self.initialized = True\n  ...         self.__dict__.update(kw)\n  ...     def squared(self):\n  ...         return self.number ** 2\n\nThis can be useful to support default values, methods and\ninitialization.  Note that if you define an __init__ method, it will be\ncalled each time the local object is used in a separate thread.  This\nis necessary to initialize each thread's dictionary.\n\nNow if we create a local object:\n\n  >>> mydata = MyLocal(color='red')\n\nNow we have a default number:\n\n  >>> mydata.number\n  2\n\nan initial color:\n\n  >>> mydata.color\n  'red'\n  >>> del mydata.color\n\nAnd a method that operates on the data:\n\n  >>> mydata.squared()\n  4\n\nAs before, we can access the data in a separate thread:\n\n  >>> log = []\n  >>> thread = threading.Thread(target=f)\n  >>> thread.start()\n  >>> thread.join()\n  >>> log\n  [[('color', 'red'), ('initialized', True)], 11]\n\nwithout affecting this thread's data:\n\n  >>> mydata.number\n  2\n  >>> mydata.color\n  Traceback (most recent call last):\n  ...\n  AttributeError: 'MyLocal' object has no attribute 'color'\n\nNote that subclasses can define slots, but they are not thread\nlocal. They are shared across threads:\n\n  >>> class MyLocal(local):\n  ...     __slots__ = 'number'\n\n  >>> mydata = MyLocal()\n  >>> mydata.number = 42\n  >>> mydata.color = 'red'\n\nSo, the separate thread:\n\n  >>> thread = threading.Thread(target=f)\n  >>> thread.start()\n  >>> thread.join()\n\naffects what we see:\n\n  >>> mydata.number\n  11\n\n>>> del mydata\n\"\"\"\n\n__all__ = [\"local\"]\n\n# We need to use objects from the threading module, but the threading\n# module may also want to use our `local` class, if support for locals\n# isn't compiled in to the `thread` module.  This creates potential problems\n# with circular imports.  For that reason, we don't import `threading`\n# until the bottom of this file (a hack sufficient to worm around the\n# potential problems).  Note that almost all platforms do have support for\n# locals in the `thread` module, and there is no circular import problem\n# then, so problems introduced by fiddling the order of imports here won't\n# manifest on most boxes.\n\nclass _localbase(object):\n    __slots__ = '_local__key', '_local__args', '_local__lock'\n\n    def __new__(cls, *args, **kw):\n        self = object.__new__(cls)\n        key = '_local__key', 'thread.local.' + str(id(self))\n        object.__setattr__(self, '_local__key', key)\n        object.__setattr__(self, '_local__args', (args, kw))\n        object.__setattr__(self, '_local__lock', RLock())\n\n        if (args or kw) and (cls.__init__ is object.__init__):\n            raise TypeError(\"Initialization arguments are not supported\")\n\n        # We need to create the thread dict in anticipation of\n        # __init__ being called, to make sure we don't call it\n        # again ourselves.\n        dict = object.__getattribute__(self, '__dict__')\n        current_thread().__dict__[key] = dict\n\n        return self\n\ndef _patch(self):\n    key = object.__getattribute__(self, '_local__key')\n    d = current_thread().__dict__.get(key)\n    if d is None:\n        d = {}\n        current_thread().__dict__[key] = d\n        object.__setattr__(self, '__dict__', d)\n\n        # we have a new instance dict, so call out __init__ if we have\n        # one\n        cls = type(self)\n        if cls.__init__ is not object.__init__:\n            args, kw = object.__getattribute__(self, '_local__args')\n            cls.__init__(self, *args, **kw)\n    else:\n        object.__setattr__(self, '__dict__', d)\n\nclass local(_localbase):\n\n    def __getattribute__(self, name):\n        lock = object.__getattribute__(self, '_local__lock')\n        lock.acquire()\n        try:\n            _patch(self)\n            return object.__getattribute__(self, name)\n        finally:\n            lock.release()\n\n    def __setattr__(self, name, value):\n        if name == '__dict__':\n            raise AttributeError(\n                \"%r object attribute '__dict__' is read-only\"\n                % self.__class__.__name__)\n        lock = object.__getattribute__(self, '_local__lock')\n        lock.acquire()\n        try:\n            _patch(self)\n            return object.__setattr__(self, name, value)\n        finally:\n            lock.release()\n\n    def __delattr__(self, name):\n        if name == '__dict__':\n            raise AttributeError(\n                \"%r object attribute '__dict__' is read-only\"\n                % self.__class__.__name__)\n        lock = object.__getattribute__(self, '_local__lock')\n        lock.acquire()\n        try:\n            _patch(self)\n            return object.__delattr__(self, name)\n        finally:\n            lock.release()\n\n    def __del__(self):\n        import threading\n\n        key = object.__getattribute__(self, '_local__key')\n\n        try:\n            # We use the non-locking API since we might already hold the lock\n            # (__del__ can be called at any point by the cyclic GC).\n            threads = threading._enumerate()\n        except:\n            # If enumerating the current threads fails, as it seems to do\n            # during shutdown, we'll skip cleanup under the assumption\n            # that there is nothing to clean up.\n            return\n\n        for thread in threads:\n            try:\n                __dict__ = thread.__dict__\n            except AttributeError:\n                # Thread is dying, rest in peace.\n                continue\n\n            if key in __dict__:\n                try:\n                    del __dict__[key]\n                except KeyError:\n                    pass # didn't have anything in this thread\n\n#from threading import current_thread, RLock\nclass current_thread(object):\n    pass\n\nclass RLock(object):\n    def acquire(self):\n        return \n    def release(self):\n        return\n", 
    "_weakrefset": "# Access WeakSet through the weakref module.\n# This code is separated-out because it is needed\n# by abc.py to load everything else at startup.\n\nfrom _weakref import ref\n\n__all__ = ['WeakSet']\n\n\nclass _IterationGuard(object):\n    # This context manager registers itself in the current iterators of the\n    # weak container, such as to delay all removals until the context manager\n    # exits.\n    # This technique should be relatively thread-safe (since sets are).\n\n    def __init__(self, weakcontainer):\n        # Don't create cycles\n        self.weakcontainer = ref(weakcontainer)\n\n    def __enter__(self):\n        w = self.weakcontainer()\n        if w is not None:\n            w._iterating.add(self)\n        return self\n\n    def __exit__(self, e, t, b):\n        w = self.weakcontainer()\n        if w is not None:\n            s = w._iterating\n            s.remove(self)\n            if not s:\n                w._commit_removals()\n\n\nclass WeakSet(object):\n    def __init__(self, data=None):\n        self.data = set()\n        def _remove(item, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(item)\n                else:\n                    self.data.discard(item)\n        self._remove = _remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        if data is not None:\n            self.update(data)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        discard = self.data.discard\n        while l:\n            discard(l.pop())\n\n    def __iter__(self):\n        with _IterationGuard(self):\n            for itemref in self.data:\n                item = itemref()\n                if item is not None:\n                    # Caveat: the iterator will keep a strong reference to\n                    # `item` until it is resumed or closed.\n                    yield item\n\n    def __len__(self):\n        return len(self.data) - len(self._pending_removals)\n\n    def __contains__(self, item):\n        try:\n            wr = ref(item)\n        except TypeError:\n            return False\n        return wr in self.data\n\n    def __reduce__(self):\n        return (self.__class__, (list(self),),\n                getattr(self, '__dict__', None))\n\n    __hash__ = None\n\n    def add(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.add(ref(item, self._remove))\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        return self.__class__(self)\n\n    def pop(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while True:\n            try:\n                itemref = self.data.pop()\n            except KeyError:\n                raise KeyError('pop from empty WeakSet')\n            item = itemref()\n            if item is not None:\n                return item\n\n    def remove(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.remove(ref(item))\n\n    def discard(self, item):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.discard(ref(item))\n\n    def update(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        for element in other:\n            self.add(element)\n\n    def __ior__(self, other):\n        self.update(other)\n        return self\n\n    def difference(self, other):\n        newset = self.copy()\n        newset.difference_update(other)\n        return newset\n    __sub__ = difference\n\n    def difference_update(self, other):\n        self.__isub__(other)\n    def __isub__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.difference_update(ref(item) for item in other)\n        return self\n\n    def intersection(self, other):\n        return self.__class__(item for item in other if item in self)\n    __and__ = intersection\n\n    def intersection_update(self, other):\n        self.__iand__(other)\n    def __iand__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.intersection_update(ref(item) for item in other)\n        return self\n\n    def issubset(self, other):\n        return self.data.issubset(ref(item) for item in other)\n    __le__ = issubset\n\n    def __lt__(self, other):\n        return self.data < set(ref(item) for item in other)\n\n    def issuperset(self, other):\n        return self.data.issuperset(ref(item) for item in other)\n    __ge__ = issuperset\n\n    def __gt__(self, other):\n        return self.data > set(ref(item) for item in other)\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return self.data == set(ref(item) for item in other)\n\n    def __ne__(self, other):\n        opposite = self.__eq__(other)\n        if opposite is NotImplemented:\n            return NotImplemented\n        return not opposite\n\n    def symmetric_difference(self, other):\n        newset = self.copy()\n        newset.symmetric_difference_update(other)\n        return newset\n    __xor__ = symmetric_difference\n\n    def symmetric_difference_update(self, other):\n        self.__ixor__(other)\n    def __ixor__(self, other):\n        if self._pending_removals:\n            self._commit_removals()\n        if self is other:\n            self.data.clear()\n        else:\n            self.data.symmetric_difference_update(ref(item, self._remove) for item in other)\n        return self\n\n    def union(self, other):\n        return self.__class__(e for s in (self, other) for e in s)\n    __or__ = union\n\n    def isdisjoint(self, other):\n        return len(self.intersection(other)) == 0\n", 
    "abc": "# Copyright 2007 Google, Inc. All Rights Reserved.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Abstract Base Classes (ABCs) according to PEP 3119.\"\"\"\n\nimport types\n\nfrom _weakrefset import WeakSet\n\n# Instance of old-style class\nclass _C: pass\n_InstanceType = type(_C())\n\n\ndef abstractmethod(funcobj):\n    \"\"\"A decorator indicating abstract methods.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract methods are overridden.\n    The abstract methods can be called using any of the normal\n    'super' call mechanisms.\n\n    Usage:\n\n        class C:\n            __metaclass__ = ABCMeta\n            @abstractmethod\n            def my_abstract_method(self, ...):\n                ...\n    \"\"\"\n    funcobj.__isabstractmethod__ = True\n    return funcobj\n\n\nclass abstractproperty(property):\n    \"\"\"A decorator indicating abstract properties.\n\n    Requires that the metaclass is ABCMeta or derived from it.  A\n    class that has a metaclass derived from ABCMeta cannot be\n    instantiated unless all of its abstract properties are overridden.\n    The abstract properties can be called using any of the normal\n    'super' call mechanisms.\n\n    Usage:\n\n        class C:\n            __metaclass__ = ABCMeta\n            @abstractproperty\n            def my_abstract_property(self):\n                ...\n\n    This defines a read-only property; you can also define a read-write\n    abstract property using the 'long' form of property declaration:\n\n        class C:\n            __metaclass__ = ABCMeta\n            def getx(self): ...\n            def setx(self, value): ...\n            x = abstractproperty(getx, setx)\n    \"\"\"\n    __isabstractmethod__ = True\n\n\nclass ABCMeta(type):\n\n    \"\"\"Metaclass for defining Abstract Base Classes (ABCs).\n\n    Use this metaclass to create an ABC.  An ABC can be subclassed\n    directly, and then acts as a mix-in class.  You can also register\n    unrelated concrete classes (even built-in classes) and unrelated\n    ABCs as 'virtual subclasses' -- these and their descendants will\n    be considered subclasses of the registering ABC by the built-in\n    issubclass() function, but the registering ABC won't show up in\n    their MRO (Method Resolution Order) nor will method\n    implementations defined by the registering ABC be callable (not\n    even via super()).\n\n    \"\"\"\n\n    # A global counter that is incremented each time a class is\n    # registered as a virtual subclass of anything.  It forces the\n    # negative cache to be cleared before its next use.\n    _abc_invalidation_counter = 0\n\n    def __new__(mcls, name, bases, namespace):\n        cls = super(ABCMeta, mcls).__new__(mcls, name, bases, namespace)\n        # Compute set of abstract method names\n        abstracts = set(name\n                     for name, value in namespace.items()\n                     if getattr(value, \"__isabstractmethod__\", False))\n        for base in bases:\n            for name in getattr(base, \"__abstractmethods__\", set()):\n                value = getattr(cls, name, None)\n                if getattr(value, \"__isabstractmethod__\", False):\n                    abstracts.add(name)\n        cls.__abstractmethods__ = frozenset(abstracts)\n        # Set up inheritance registry\n        cls._abc_registry = WeakSet()\n        cls._abc_cache = WeakSet()\n        cls._abc_negative_cache = WeakSet()\n        cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n        return cls\n\n    def register(cls, subclass):\n        \"\"\"Register a virtual subclass of an ABC.\"\"\"\n        if not isinstance(subclass, (type, types.ClassType)):\n            raise TypeError(\"Can only register classes\")\n        if issubclass(subclass, cls):\n            return  # Already a subclass\n        # Subtle: test for cycles *after* testing for \"already a subclass\";\n        # this means we allow X.register(X) and interpret it as a no-op.\n        if issubclass(cls, subclass):\n            # This would create a cycle, which is bad for the algorithm below\n            raise RuntimeError(\"Refusing to create an inheritance cycle\")\n        cls._abc_registry.add(subclass)\n        ABCMeta._abc_invalidation_counter += 1  # Invalidate negative cache\n\n    def _dump_registry(cls, file=None):\n        \"\"\"Debug helper to print the ABC registry.\"\"\"\n        print >> file, \"Class: %s.%s\" % (cls.__module__, cls.__name__)\n        print >> file, \"Inv.counter: %s\" % ABCMeta._abc_invalidation_counter\n        for name in sorted(cls.__dict__.keys()):\n            if name.startswith(\"_abc_\"):\n                value = getattr(cls, name)\n                print >> file, \"%s: %r\" % (name, value)\n\n    def __instancecheck__(cls, instance):\n        \"\"\"Override for isinstance(instance, cls).\"\"\"\n        # Inline the cache checking when it's simple.\n        subclass = getattr(instance, '__class__', None)\n        if subclass is not None and subclass in cls._abc_cache:\n            return True\n        subtype = type(instance)\n        # Old-style instances\n        if subtype is _InstanceType:\n            subtype = subclass\n        if subtype is subclass or subclass is None:\n            if (cls._abc_negative_cache_version ==\n                ABCMeta._abc_invalidation_counter and\n                subtype in cls._abc_negative_cache):\n                return False\n            # Fall back to the subclass check.\n            return cls.__subclasscheck__(subtype)\n        return (cls.__subclasscheck__(subclass) or\n                cls.__subclasscheck__(subtype))\n\n    def __subclasscheck__(cls, subclass):\n        \"\"\"Override for issubclass(subclass, cls).\"\"\"\n        # Check cache\n        if subclass in cls._abc_cache:\n            return True\n        # Check negative cache; may have to invalidate\n        if cls._abc_negative_cache_version < ABCMeta._abc_invalidation_counter:\n            # Invalidate the negative cache\n            cls._abc_negative_cache = WeakSet()\n            cls._abc_negative_cache_version = ABCMeta._abc_invalidation_counter\n        elif subclass in cls._abc_negative_cache:\n            return False\n        # Check the subclass hook\n        ok = cls.__subclasshook__(subclass)\n        if ok is not NotImplemented:\n            assert isinstance(ok, bool)\n            if ok:\n                cls._abc_cache.add(subclass)\n            else:\n                cls._abc_negative_cache.add(subclass)\n            return ok\n        # Check if it's a direct subclass\n        if cls in getattr(subclass, '__mro__', ()):\n            cls._abc_cache.add(subclass)\n            return True\n        # Check if it's a subclass of a registered class (recursive)\n        for rcls in cls._abc_registry:\n            if issubclass(subclass, rcls):\n                cls._abc_cache.add(subclass)\n                return True\n        # Check if it's a subclass of a subclass (recursive)\n        for scls in cls.__subclasses__():\n            if issubclass(subclass, scls):\n                cls._abc_cache.add(subclass)\n                return True\n        # No dice; update negative cache\n        cls._abc_negative_cache.add(subclass)\n        return False\n", 
    "atexit": "\"\"\"\natexit.py - allow programmer to define multiple exit functions to be executed\nupon normal program termination.\n\nOne public function, register, is defined.\n\"\"\"\n\n__all__ = [\"register\"]\n\nimport sys\n\n_exithandlers = []\ndef _run_exitfuncs():\n    \"\"\"run any registered exit functions\n\n    _exithandlers is traversed in reverse order so functions are executed\n    last in, first out.\n    \"\"\"\n\n    exc_info = None\n    while _exithandlers:\n        func, targs, kargs = _exithandlers.pop()\n        try:\n            func(*targs, **kargs)\n        except SystemExit:\n            exc_info = sys.exc_info()\n        except:\n            import traceback\n            print >> sys.stderr, \"Error in atexit._run_exitfuncs:\"\n            traceback.print_exc()\n            exc_info = sys.exc_info()\n\n    if exc_info is not None:\n        raise exc_info[0], exc_info[1], exc_info[2]\n\n\ndef register(func, *targs, **kargs):\n    \"\"\"register a function to be executed upon normal program termination\n\n    func - function to be called at exit\n    targs - optional arguments to pass to func\n    kargs - optional keyword arguments to pass to func\n\n    func is returned to facilitate usage as a decorator.\n    \"\"\"\n    _exithandlers.append((func, targs, kargs))\n    return func\n\nif hasattr(sys, \"exitfunc\"):\n    # Assume it's another registered exit function - append it to our list\n    register(sys.exitfunc)\nsys.exitfunc = _run_exitfuncs\n\nif __name__ == \"__main__\":\n    def x1():\n        print \"running x1\"\n    def x2(n):\n        print \"running x2(%r)\" % (n,)\n    def x3(n, kwd=None):\n        print \"running x3(%r, kwd=%r)\" % (n, kwd)\n\n    register(x1)\n    register(x2, 12)\n    register(x3, 5, \"bar\")\n    register(x3, \"no kwd args\")\n", 
    "base64": "#! /usr/bin/env python\n\n\"\"\"RFC 3548: Base16, Base32, Base64 Data Encodings\"\"\"\n\n# Modified 04-Oct-1995 by Jack Jansen to use binascii module\n# Modified 30-Dec-2003 by Barry Warsaw to add full RFC 3548 support\n\nimport re\nimport struct\nimport binascii\n\n\n__all__ = [\n    # Legacy interface exports traditional RFC 1521 Base64 encodings\n    'encode', 'decode', 'encodestring', 'decodestring',\n    # Generalized interface for other encodings\n    'b64encode', 'b64decode', 'b32encode', 'b32decode',\n    'b16encode', 'b16decode',\n    # Standard Base64 encoding\n    'standard_b64encode', 'standard_b64decode',\n    # Some common Base64 alternatives.  As referenced by RFC 3458, see thread\n    # starting at:\n    #\n    # http://zgp.org/pipermail/p2p-hackers/2001-September/000316.html\n    'urlsafe_b64encode', 'urlsafe_b64decode',\n    ]\n\n_translation = [chr(_x) for _x in range(256)]\nEMPTYSTRING = ''\n\n\ndef _translate(s, altchars):\n    translation = _translation[:]\n    for k, v in altchars.items():\n        translation[ord(k)] = v\n    return s.translate(''.join(translation))\n\n\n\f\n# Base64 encoding/decoding uses binascii\n\ndef b64encode(s, altchars=None):\n    \"\"\"Encode a string using Base64.\n\n    s is the string to encode.  Optional altchars must be a string of at least\n    length 2 (additional characters are ignored) which specifies an\n    alternative alphabet for the '+' and '/' characters.  This allows an\n    application to e.g. generate url or filesystem safe Base64 strings.\n\n    The encoded string is returned.\n    \"\"\"\n    # Strip off the trailing newline\n    encoded = binascii.b2a_base64(s)[:-1]\n    if altchars is not None:\n        return _translate(encoded, {'+': altchars[0], '/': altchars[1]})\n    return encoded\n\n\ndef b64decode(s, altchars=None):\n    \"\"\"Decode a Base64 encoded string.\n\n    s is the string to decode.  Optional altchars must be a string of at least\n    length 2 (additional characters are ignored) which specifies the\n    alternative alphabet used instead of the '+' and '/' characters.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    if altchars is not None:\n        s = _translate(s, {altchars[0]: '+', altchars[1]: '/'})\n    try:\n        return binascii.a2b_base64(s)\n    except binascii.Error, msg:\n        # Transform this exception for consistency\n        raise TypeError(msg)\n\n\ndef standard_b64encode(s):\n    \"\"\"Encode a string using the standard Base64 alphabet.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    return b64encode(s)\n\ndef standard_b64decode(s):\n    \"\"\"Decode a string encoded with the standard Base64 alphabet.\n\n    s is the string to decode.  The decoded string is returned.  A TypeError\n    is raised if the string is incorrectly padded or if there are non-alphabet\n    characters present in the string.\n    \"\"\"\n    return b64decode(s)\n\ndef urlsafe_b64encode(s):\n    \"\"\"Encode a string using a url-safe Base64 alphabet.\n\n    s is the string to encode.  The encoded string is returned.  The alphabet\n    uses '-' instead of '+' and '_' instead of '/'.\n    \"\"\"\n    return b64encode(s, '-_')\n\ndef urlsafe_b64decode(s):\n    \"\"\"Decode a string encoded with the standard Base64 alphabet.\n\n    s is the string to decode.  The decoded string is returned.  A TypeError\n    is raised if the string is incorrectly padded or if there are non-alphabet\n    characters present in the string.\n\n    The alphabet uses '-' instead of '+' and '_' instead of '/'.\n    \"\"\"\n    return b64decode(s, '-_')\n\n\n\f\n# Base32 encoding/decoding must be done in Python\n_b32alphabet = {\n    0: 'A',  9: 'J', 18: 'S', 27: '3',\n    1: 'B', 10: 'K', 19: 'T', 28: '4',\n    2: 'C', 11: 'L', 20: 'U', 29: '5',\n    3: 'D', 12: 'M', 21: 'V', 30: '6',\n    4: 'E', 13: 'N', 22: 'W', 31: '7',\n    5: 'F', 14: 'O', 23: 'X',\n    6: 'G', 15: 'P', 24: 'Y',\n    7: 'H', 16: 'Q', 25: 'Z',\n    8: 'I', 17: 'R', 26: '2',\n    }\n\n_b32tab = _b32alphabet.items()\n_b32tab.sort()\n_b32tab = [v for k, v in _b32tab]\n_b32rev = dict([(v, long(k)) for k, v in _b32alphabet.items()])\n\n\ndef b32encode(s):\n    \"\"\"Encode a string using Base32.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    parts = []\n    quanta, leftover = divmod(len(s), 5)\n    # Pad the last quantum with zero bits if necessary\n    if leftover:\n        s += ('\\0' * (5 - leftover))\n        quanta += 1\n    for i in range(quanta):\n        # c1 and c2 are 16 bits wide, c3 is 8 bits wide.  The intent of this\n        # code is to process the 40 bits in units of 5 bits.  So we take the 1\n        # leftover bit of c1 and tack it onto c2.  Then we take the 2 leftover\n        # bits of c2 and tack them onto c3.  The shifts and masks are intended\n        # to give us values of exactly 5 bits in width.\n        c1, c2, c3 = struct.unpack('!HHB', s[i*5:(i+1)*5])\n        c2 += (c1 & 1) << 16 # 17 bits wide\n        c3 += (c2 & 3) << 8  # 10 bits wide\n        parts.extend([_b32tab[c1 >> 11],         # bits 1 - 5\n                      _b32tab[(c1 >> 6) & 0x1f], # bits 6 - 10\n                      _b32tab[(c1 >> 1) & 0x1f], # bits 11 - 15\n                      _b32tab[c2 >> 12],         # bits 16 - 20 (1 - 5)\n                      _b32tab[(c2 >> 7) & 0x1f], # bits 21 - 25 (6 - 10)\n                      _b32tab[(c2 >> 2) & 0x1f], # bits 26 - 30 (11 - 15)\n                      _b32tab[c3 >> 5],          # bits 31 - 35 (1 - 5)\n                      _b32tab[c3 & 0x1f],        # bits 36 - 40 (1 - 5)\n                      ])\n    encoded = EMPTYSTRING.join(parts)\n    # Adjust for any leftover partial quanta\n    if leftover == 1:\n        return encoded[:-6] + '======'\n    elif leftover == 2:\n        return encoded[:-4] + '===='\n    elif leftover == 3:\n        return encoded[:-3] + '==='\n    elif leftover == 4:\n        return encoded[:-1] + '='\n    return encoded\n\n\ndef b32decode(s, casefold=False, map01=None):\n    \"\"\"Decode a Base32 encoded string.\n\n    s is the string to decode.  Optional casefold is a flag specifying whether\n    a lowercase alphabet is acceptable as input.  For security purposes, the\n    default is False.\n\n    RFC 3548 allows for optional mapping of the digit 0 (zero) to the letter O\n    (oh), and for optional mapping of the digit 1 (one) to either the letter I\n    (eye) or letter L (el).  The optional argument map01 when not None,\n    specifies which letter the digit 1 should be mapped to (when map01 is not\n    None, the digit 0 is always mapped to the letter O).  For security\n    purposes the default is None, so that 0 and 1 are not allowed in the\n    input.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    quanta, leftover = divmod(len(s), 8)\n    if leftover:\n        raise TypeError('Incorrect padding')\n    # Handle section 2.4 zero and one mapping.  The flag map01 will be either\n    # False, or the character to map the digit 1 (one) to.  It should be\n    # either L (el) or I (eye).\n    if map01:\n        s = _translate(s, {'0': 'O', '1': map01})\n    if casefold:\n        s = s.upper()\n    # Strip off pad characters from the right.  We need to count the pad\n    # characters because this will tell us how many null bytes to remove from\n    # the end of the decoded string.\n    padchars = 0\n    mo = re.search('(?P<pad>[=]*)$', s)\n    if mo:\n        padchars = len(mo.group('pad'))\n        if padchars > 0:\n            s = s[:-padchars]\n    # Now decode the full quanta\n    parts = []\n    acc = 0\n    shift = 35\n    for c in s:\n        val = _b32rev.get(c)\n        if val is None:\n            raise TypeError('Non-base32 digit found')\n        acc += _b32rev[c] << shift\n        shift -= 5\n        if shift < 0:\n            parts.append(binascii.unhexlify('%010x' % acc))\n            acc = 0\n            shift = 35\n    # Process the last, partial quanta\n    last = binascii.unhexlify('%010x' % acc)\n    if padchars == 0:\n        last = ''                       # No characters\n    elif padchars == 1:\n        last = last[:-1]\n    elif padchars == 3:\n        last = last[:-2]\n    elif padchars == 4:\n        last = last[:-3]\n    elif padchars == 6:\n        last = last[:-4]\n    else:\n        raise TypeError('Incorrect padding')\n    parts.append(last)\n    return EMPTYSTRING.join(parts)\n\n\n\f\n# RFC 3548, Base 16 Alphabet specifies uppercase, but hexlify() returns\n# lowercase.  The RFC also recommends against accepting input case\n# insensitively.\ndef b16encode(s):\n    \"\"\"Encode a string using Base16.\n\n    s is the string to encode.  The encoded string is returned.\n    \"\"\"\n    return binascii.hexlify(s).upper()\n\n\ndef b16decode(s, casefold=False):\n    \"\"\"Decode a Base16 encoded string.\n\n    s is the string to decode.  Optional casefold is a flag specifying whether\n    a lowercase alphabet is acceptable as input.  For security purposes, the\n    default is False.\n\n    The decoded string is returned.  A TypeError is raised if s were\n    incorrectly padded or if there are non-alphabet characters present in the\n    string.\n    \"\"\"\n    if casefold:\n        s = s.upper()\n    if re.search('[^0-9A-F]', s):\n        raise TypeError('Non-base16 digit found')\n    return binascii.unhexlify(s)\n\n\n\f\n# Legacy interface.  This code could be cleaned up since I don't believe\n# binascii has any line length limitations.  It just doesn't seem worth it\n# though.\n\nMAXLINESIZE = 76 # Excluding the CRLF\nMAXBINSIZE = (MAXLINESIZE//4)*3\n\ndef encode(input, output):\n    \"\"\"Encode a file.\"\"\"\n    while True:\n        s = input.read(MAXBINSIZE)\n        if not s:\n            break\n        while len(s) < MAXBINSIZE:\n            ns = input.read(MAXBINSIZE-len(s))\n            if not ns:\n                break\n            s += ns\n        line = binascii.b2a_base64(s)\n        output.write(line)\n\n\ndef decode(input, output):\n    \"\"\"Decode a file.\"\"\"\n    while True:\n        line = input.readline()\n        if not line:\n            break\n        s = binascii.a2b_base64(line)\n        output.write(s)\n\n\ndef encodestring(s):\n    \"\"\"Encode a string into multiple lines of base-64 data.\"\"\"\n    pieces = []\n    for i in range(0, len(s), MAXBINSIZE):\n        chunk = s[i : i + MAXBINSIZE]\n        pieces.append(binascii.b2a_base64(chunk))\n    return \"\".join(pieces)\n\n\ndef decodestring(s):\n    \"\"\"Decode a string.\"\"\"\n    return binascii.a2b_base64(s)\n\n\n\f\n# Useable as a script...\ndef test():\n    \"\"\"Small test program\"\"\"\n    import sys, getopt\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], 'deut')\n    except getopt.error, msg:\n        sys.stdout = sys.stderr\n        print msg\n        print \"\"\"usage: %s [-d|-e|-u|-t] [file|-]\n        -d, -u: decode\n        -e: encode (default)\n        -t: encode and decode string 'Aladdin:open sesame'\"\"\"%sys.argv[0]\n        sys.exit(2)\n    func = encode\n    for o, a in opts:\n        if o == '-e': func = encode\n        if o == '-d': func = decode\n        if o == '-u': func = decode\n        if o == '-t': test1(); return\n    if args and args[0] != '-':\n        with open(args[0], 'rb') as f:\n            func(f, sys.stdout)\n    else:\n        func(sys.stdin, sys.stdout)\n\n\ndef test1():\n    s0 = \"Aladdin:open sesame\"\n    s1 = encodestring(s0)\n    s2 = decodestring(s1)\n    print s0, repr(s1), s2\n\n\nif __name__ == '__main__':\n    test()\n", 
    "bdb": "\"\"\"Debugger basics\"\"\"\n\nimport fnmatch\nimport sys\nimport os\nimport types\n\n__all__ = [\"BdbQuit\",\"Bdb\",\"Breakpoint\"]\n\nclass BdbQuit(Exception):\n    \"\"\"Exception to give up completely\"\"\"\n\n\nclass Bdb:\n\n    \"\"\"Generic Python debugger base class.\n\n    This class takes care of details of the trace facility;\n    a derived class should implement user interaction.\n    The standard debugger class (pdb.Pdb) is an example.\n    \"\"\"\n\n    def __init__(self, skip=None):\n        self.skip = set(skip) if skip else None\n        self.breaks = {}\n        self.fncache = {}\n        self.frame_returning = None\n\n    def canonic(self, filename):\n        if filename == \"<\" + filename[1:-1] + \">\":\n            return filename\n        canonic = self.fncache.get(filename)\n        if not canonic:\n            canonic = os.path.abspath(filename)\n            canonic = os.path.normcase(canonic)\n            self.fncache[filename] = canonic\n        return canonic\n\n    def reset(self):\n        import linecache\n        linecache.checkcache()\n        self.botframe = None\n        self._set_stopinfo(None, None)\n\n    def trace_dispatch(self, frame, event, arg):\n        if self.quitting:\n            return # None\n        if event == 'line':\n            return self.dispatch_line(frame)\n        if event == 'call':\n            return self.dispatch_call(frame, arg)\n        if event == 'return':\n            return self.dispatch_return(frame, arg)\n        if event == 'exception':\n            return self.dispatch_exception(frame, arg)\n        if event == 'c_call':\n            return self.trace_dispatch\n        if event == 'c_exception':\n            return self.trace_dispatch\n        if event == 'c_return':\n            return self.trace_dispatch\n        print 'bdb.Bdb.dispatch: unknown debugging event:', repr(event)\n        return self.trace_dispatch\n\n    def dispatch_line(self, frame):\n        if self.stop_here(frame) or self.break_here(frame):\n            self.user_line(frame)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_call(self, frame, arg):\n        # XXX 'arg' is no longer used\n        if self.botframe is None:\n            # First call of dispatch since reset()\n            self.botframe = frame.f_back # (CT) Note that this may also be None!\n            return self.trace_dispatch\n        if not (self.stop_here(frame) or self.break_anywhere(frame)):\n            # No need to trace this function\n            return # None\n        self.user_call(frame, arg)\n        if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_return(self, frame, arg):\n        if self.stop_here(frame) or frame == self.returnframe:\n            try:\n                self.frame_returning = frame\n                self.user_return(frame, arg)\n            finally:\n                self.frame_returning = None\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    def dispatch_exception(self, frame, arg):\n        if self.stop_here(frame):\n            self.user_exception(frame, arg)\n            if self.quitting: raise BdbQuit\n        return self.trace_dispatch\n\n    # Normally derived classes don't override the following\n    # methods, but they may if they want to redefine the\n    # definition of stopping and breakpoints.\n\n    def is_skipped_module(self, module_name):\n        for pattern in self.skip:\n            if fnmatch.fnmatch(module_name, pattern):\n                return True\n        return False\n\n    def stop_here(self, frame):\n        # (CT) stopframe may now also be None, see dispatch_call.\n        # (CT) the former test for None is therefore removed from here.\n        if self.skip and \\\n               self.is_skipped_module(frame.f_globals.get('__name__')):\n            return False\n        if frame is self.stopframe:\n            if self.stoplineno == -1:\n                return False\n            return frame.f_lineno >= self.stoplineno\n        while frame is not None and frame is not self.stopframe:\n            if frame is self.botframe:\n                return True\n            frame = frame.f_back\n        return False\n\n    def break_here(self, frame):\n        filename = self.canonic(frame.f_code.co_filename)\n        if not filename in self.breaks:\n            return False\n        lineno = frame.f_lineno\n        if not lineno in self.breaks[filename]:\n            # The line itself has no breakpoint, but maybe the line is the\n            # first line of a function with breakpoint set by function name.\n            lineno = frame.f_code.co_firstlineno\n            if not lineno in self.breaks[filename]:\n                return False\n\n        # flag says ok to delete temp. bp\n        (bp, flag) = effective(filename, lineno, frame)\n        if bp:\n            self.currentbp = bp.number\n            if (flag and bp.temporary):\n                self.do_clear(str(bp.number))\n            return True\n        else:\n            return False\n\n    def do_clear(self, arg):\n        raise NotImplementedError, \"subclass of bdb must implement do_clear()\"\n\n    def break_anywhere(self, frame):\n        return self.canonic(frame.f_code.co_filename) in self.breaks\n\n    # Derived classes should override the user_* methods\n    # to gain control.\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        pass\n\n    def user_line(self, frame):\n        \"\"\"This method is called when we stop or break at this line.\"\"\"\n        pass\n\n    def user_return(self, frame, return_value):\n        \"\"\"This method is called when a return trap is set here.\"\"\"\n        pass\n\n    def user_exception(self, frame, exc_info):\n        exc_type, exc_value, exc_traceback = exc_info\n        \"\"\"This method is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        pass\n\n    def _set_stopinfo(self, stopframe, returnframe, stoplineno=0):\n        self.stopframe = stopframe\n        self.returnframe = returnframe\n        self.quitting = 0\n        # stoplineno >= 0 means: stop at line >= the stoplineno\n        # stoplineno -1 means: don't stop at all\n        self.stoplineno = stoplineno\n\n    # Derived classes and clients can call the following methods\n    # to affect the stepping state.\n\n    def set_until(self, frame): #the name \"until\" is borrowed from gdb\n        \"\"\"Stop when the line with the line no greater than the current one is\n        reached or when returning from current frame\"\"\"\n        self._set_stopinfo(frame, frame, frame.f_lineno+1)\n\n    def set_step(self):\n        \"\"\"Stop after one line of code.\"\"\"\n        # Issue #13183: pdb skips frames after hitting a breakpoint and running\n        # step commands.\n        # Restore the trace function in the caller (that may not have been set\n        # for performance reasons) when returning from the current frame.\n        if self.frame_returning:\n            caller_frame = self.frame_returning.f_back\n            if caller_frame and not caller_frame.f_trace:\n                caller_frame.f_trace = self.trace_dispatch\n        self._set_stopinfo(None, None)\n\n    def set_next(self, frame):\n        \"\"\"Stop on the next line in or below the given frame.\"\"\"\n        self._set_stopinfo(frame, None)\n\n    def set_return(self, frame):\n        \"\"\"Stop when returning from the given frame.\"\"\"\n        self._set_stopinfo(frame.f_back, frame)\n\n    def set_trace(self, frame=None):\n        \"\"\"Start debugging from `frame`.\n\n        If frame is not specified, debugging starts from caller's frame.\n        \"\"\"\n        if frame is None:\n            frame = sys._getframe().f_back\n        self.reset()\n        while frame:\n            frame.f_trace = self.trace_dispatch\n            self.botframe = frame\n            frame = frame.f_back\n        self.set_step()\n        sys.settrace(self.trace_dispatch)\n\n    def set_continue(self):\n        # Don't stop except at breakpoints or when finished\n        self._set_stopinfo(self.botframe, None, -1)\n        if not self.breaks:\n            # no breakpoints; run without debugger overhead\n            sys.settrace(None)\n            frame = sys._getframe().f_back\n            while frame and frame is not self.botframe:\n                del frame.f_trace\n                frame = frame.f_back\n\n    def set_quit(self):\n        self.stopframe = self.botframe\n        self.returnframe = None\n        self.quitting = 1\n        sys.settrace(None)\n\n    # Derived classes and clients can call the following methods\n    # to manipulate breakpoints.  These methods return an\n    # error message is something went wrong, None if all is well.\n    # Set_break prints out the breakpoint line and file:lineno.\n    # Call self.get_*break*() to see the breakpoints or better\n    # for bp in Breakpoint.bpbynumber: if bp: bp.bpprint().\n\n    def set_break(self, filename, lineno, temporary=0, cond = None,\n                  funcname=None):\n        filename = self.canonic(filename)\n        import linecache # Import as late as possible\n        line = linecache.getline(filename, lineno)\n        if not line:\n            return 'Line %s:%d does not exist' % (filename,\n                                   lineno)\n        if not filename in self.breaks:\n            self.breaks[filename] = []\n        list = self.breaks[filename]\n        if not lineno in list:\n            list.append(lineno)\n        bp = Breakpoint(filename, lineno, temporary, cond, funcname)\n\n    def _prune_breaks(self, filename, lineno):\n        if (filename, lineno) not in Breakpoint.bplist:\n            self.breaks[filename].remove(lineno)\n        if not self.breaks[filename]:\n            del self.breaks[filename]\n\n    def clear_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        if lineno not in self.breaks[filename]:\n            return 'There is no breakpoint at %s:%d' % (filename,\n                                    lineno)\n        # If there's only one bp in the list for that file,line\n        # pair, then remove the breaks entry\n        for bp in Breakpoint.bplist[filename, lineno][:]:\n            bp.deleteMe()\n        self._prune_breaks(filename, lineno)\n\n    def clear_bpbynumber(self, arg):\n        try:\n            number = int(arg)\n        except:\n            return 'Non-numeric breakpoint number (%s)' % arg\n        try:\n            bp = Breakpoint.bpbynumber[number]\n        except IndexError:\n            return 'Breakpoint number (%d) out of range' % number\n        if not bp:\n            return 'Breakpoint (%d) already deleted' % number\n        bp.deleteMe()\n        self._prune_breaks(bp.file, bp.line)\n\n    def clear_all_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if not filename in self.breaks:\n            return 'There are no breakpoints in %s' % filename\n        for line in self.breaks[filename]:\n            blist = Breakpoint.bplist[filename, line]\n            for bp in blist:\n                bp.deleteMe()\n        del self.breaks[filename]\n\n    def clear_all_breaks(self):\n        if not self.breaks:\n            return 'There are no breakpoints'\n        for bp in Breakpoint.bpbynumber:\n            if bp:\n                bp.deleteMe()\n        self.breaks = {}\n\n    def get_break(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename]\n\n    def get_breaks(self, filename, lineno):\n        filename = self.canonic(filename)\n        return filename in self.breaks and \\\n            lineno in self.breaks[filename] and \\\n            Breakpoint.bplist[filename, lineno] or []\n\n    def get_file_breaks(self, filename):\n        filename = self.canonic(filename)\n        if filename in self.breaks:\n            return self.breaks[filename]\n        else:\n            return []\n\n    def get_all_breaks(self):\n        return self.breaks\n\n    # Derived classes and clients can call the following method\n    # to get a data structure representing a stack trace.\n\n    def get_stack(self, f, t):\n        stack = []\n        if t and t.tb_frame is f:\n            t = t.tb_next\n        while f is not None:\n            stack.append((f, f.f_lineno))\n            if f is self.botframe:\n                break\n            f = f.f_back\n        stack.reverse()\n        i = max(0, len(stack) - 1)\n        while t is not None:\n            stack.append((t.tb_frame, t.tb_lineno))\n            t = t.tb_next\n        if f is None:\n            i = max(0, len(stack) - 1)\n        return stack, i\n\n    #\n\n    def format_stack_entry(self, frame_lineno, lprefix=': '):\n        import linecache, repr\n        frame, lineno = frame_lineno\n        filename = self.canonic(frame.f_code.co_filename)\n        s = '%s(%r)' % (filename, lineno)\n        if frame.f_code.co_name:\n            s = s + frame.f_code.co_name\n        else:\n            s = s + \"<lambda>\"\n        if '__args__' in frame.f_locals:\n            args = frame.f_locals['__args__']\n        else:\n            args = None\n        if args:\n            s = s + repr.repr(args)\n        else:\n            s = s + '()'\n        if '__return__' in frame.f_locals:\n            rv = frame.f_locals['__return__']\n            s = s + '->'\n            s = s + repr.repr(rv)\n        line = linecache.getline(filename, lineno, frame.f_globals)\n        if line: s = s + lprefix + line.strip()\n        return s\n\n    # The following two methods can be called by clients to use\n    # a debugger to debug a statement, given as a string.\n\n    def run(self, cmd, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(cmd, types.CodeType):\n            cmd = cmd+'\\n'\n        try:\n            exec cmd in globals, locals\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runeval(self, expr, globals=None, locals=None):\n        if globals is None:\n            import __main__\n            globals = __main__.__dict__\n        if locals is None:\n            locals = globals\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        if not isinstance(expr, types.CodeType):\n            expr = expr+'\\n'\n        try:\n            return eval(expr, globals, locals)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n\n    def runctx(self, cmd, globals, locals):\n        # B/W compatibility\n        self.run(cmd, globals, locals)\n\n    # This method is more useful to debug a single function call.\n\n    def runcall(self, func, *args, **kwds):\n        self.reset()\n        sys.settrace(self.trace_dispatch)\n        res = None\n        try:\n            res = func(*args, **kwds)\n        except BdbQuit:\n            pass\n        finally:\n            self.quitting = 1\n            sys.settrace(None)\n        return res\n\n\ndef set_trace():\n    Bdb().set_trace()\n\n\nclass Breakpoint:\n\n    \"\"\"Breakpoint class\n\n    Implements temporary breakpoints, ignore counts, disabling and\n    (re)-enabling, and conditionals.\n\n    Breakpoints are indexed by number through bpbynumber and by\n    the file,line tuple using bplist.  The former points to a\n    single instance of class Breakpoint.  The latter points to a\n    list of such instances since there may be more than one\n    breakpoint per line.\n\n    \"\"\"\n\n    # XXX Keeping state in the class is a mistake -- this means\n    # you cannot have more than one active Bdb instance.\n\n    next = 1        # Next bp to be assigned\n    bplist = {}     # indexed by (file, lineno) tuple\n    bpbynumber = [None] # Each entry is None or an instance of Bpt\n                # index 0 is unused, except for marking an\n                # effective break .... see effective()\n\n    def __init__(self, file, line, temporary=0, cond=None, funcname=None):\n        self.funcname = funcname\n        # Needed if funcname is not None.\n        self.func_first_executable_line = None\n        self.file = file    # This better be in canonical form!\n        self.line = line\n        self.temporary = temporary\n        self.cond = cond\n        self.enabled = 1\n        self.ignore = 0\n        self.hits = 0\n        self.number = Breakpoint.next\n        Breakpoint.next = Breakpoint.next + 1\n        # Build the two lists\n        self.bpbynumber.append(self)\n        if (file, line) in self.bplist:\n            self.bplist[file, line].append(self)\n        else:\n            self.bplist[file, line] = [self]\n\n\n    def deleteMe(self):\n        index = (self.file, self.line)\n        self.bpbynumber[self.number] = None   # No longer in list\n        self.bplist[index].remove(self)\n        if not self.bplist[index]:\n            # No more bp for this f:l combo\n            del self.bplist[index]\n\n    def enable(self):\n        self.enabled = 1\n\n    def disable(self):\n        self.enabled = 0\n\n    def bpprint(self, out=None):\n        if out is None:\n            out = sys.stdout\n        if self.temporary:\n            disp = 'del  '\n        else:\n            disp = 'keep '\n        if self.enabled:\n            disp = disp + 'yes  '\n        else:\n            disp = disp + 'no   '\n        print >>out, '%-4dbreakpoint   %s at %s:%d' % (self.number, disp,\n                                                       self.file, self.line)\n        if self.cond:\n            print >>out, '\\tstop only if %s' % (self.cond,)\n        if self.ignore:\n            print >>out, '\\tignore next %d hits' % (self.ignore)\n        if (self.hits):\n            if (self.hits > 1): ss = 's'\n            else: ss = ''\n            print >>out, ('\\tbreakpoint already hit %d time%s' %\n                          (self.hits, ss))\n\n# -----------end of Breakpoint class----------\n\ndef checkfuncname(b, frame):\n    \"\"\"Check whether we should break here because of `b.funcname`.\"\"\"\n    if not b.funcname:\n        # Breakpoint was set via line number.\n        if b.line != frame.f_lineno:\n            # Breakpoint was set at a line with a def statement and the function\n            # defined is called: don't break.\n            return False\n        return True\n\n    # Breakpoint set via function name.\n\n    if frame.f_code.co_name != b.funcname:\n        # It's not a function call, but rather execution of def statement.\n        return False\n\n    # We are in the right frame.\n    if not b.func_first_executable_line:\n        # The function is entered for the 1st time.\n        b.func_first_executable_line = frame.f_lineno\n\n    if  b.func_first_executable_line != frame.f_lineno:\n        # But we are not at the first line number: don't break.\n        return False\n    return True\n\n# Determines if there is an effective (active) breakpoint at this\n# line of code.  Returns breakpoint number or 0 if none\ndef effective(file, line, frame):\n    \"\"\"Determine which breakpoint for this file:line is to be acted upon.\n\n    Called only if we know there is a bpt at this\n    location.  Returns breakpoint that was triggered and a flag\n    that indicates if it is ok to delete a temporary bp.\n\n    \"\"\"\n    possibles = Breakpoint.bplist[file,line]\n    for i in range(0, len(possibles)):\n        b = possibles[i]\n        if b.enabled == 0:\n            continue\n        if not checkfuncname(b, frame):\n            continue\n        # Count every hit when bp is enabled\n        b.hits = b.hits + 1\n        if not b.cond:\n            # If unconditional, and ignoring,\n            # go on to next, else break\n            if b.ignore > 0:\n                b.ignore = b.ignore -1\n                continue\n            else:\n                # breakpoint and marker that's ok\n                # to delete if temporary\n                return (b,1)\n        else:\n            # Conditional bp.\n            # Ignore count applies only to those bpt hits where the\n            # condition evaluates to true.\n            try:\n                val = eval(b.cond, frame.f_globals,\n                       frame.f_locals)\n                if val:\n                    if b.ignore > 0:\n                        b.ignore = b.ignore -1\n                        # continue\n                    else:\n                        return (b,1)\n                # else:\n                #   continue\n            except:\n                # if eval fails, most conservative\n                # thing is to stop on breakpoint\n                # regardless of ignore count.\n                # Don't delete temporary,\n                # as another hint to user.\n                return (b,0)\n    return (None, None)\n\n# -------------------- testing --------------------\n\nclass Tdb(Bdb):\n    def user_call(self, frame, args):\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        print '+++ call', name, args\n    def user_line(self, frame):\n        import linecache\n        name = frame.f_code.co_name\n        if not name: name = '???'\n        fn = self.canonic(frame.f_code.co_filename)\n        line = linecache.getline(fn, frame.f_lineno, frame.f_globals)\n        print '+++', fn, frame.f_lineno, name, ':', line.strip()\n    def user_return(self, frame, retval):\n        print '+++ return', retval\n    def user_exception(self, frame, exc_stuff):\n        print '+++ exception', exc_stuff\n        self.set_continue()\n\ndef foo(n):\n    print 'foo(', n, ')'\n    x = bar(n*10)\n    print 'bar returned', x\n\ndef bar(a):\n    print 'bar(', a, ')'\n    return a/2\n\ndef test():\n    t = Tdb()\n    t.run('import bdb; bdb.foo(10)')\n\n# end\n", 
    "bisect": "\"\"\"Bisection algorithms.\"\"\"\n\ndef insort_right(a, x, lo=0, hi=None):\n    \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted.\n\n    If x is already in a, insert it to the right of the rightmost x.\n\n    Optional args lo (default 0) and hi (default len(a)) bound the\n    slice of a to be searched.\n    \"\"\"\n\n    if lo < 0:\n        raise ValueError('lo must be non-negative')\n    if hi is None:\n        hi = len(a)\n    while lo < hi:\n        mid = (lo+hi)//2\n        if x < a[mid]: hi = mid\n        else: lo = mid+1\n    a.insert(lo, x)\n\ninsort = insort_right   # backward compatibility\n\ndef bisect_right(a, x, lo=0, hi=None):\n    \"\"\"Return the index where to insert item x in list a, assuming a is sorted.\n\n    The return value i is such that all e in a[:i] have e <= x, and all e in\n    a[i:] have e > x.  So if x already appears in the list, a.insert(x) will\n    insert just after the rightmost x already there.\n\n    Optional args lo (default 0) and hi (default len(a)) bound the\n    slice of a to be searched.\n    \"\"\"\n\n    if lo < 0:\n        raise ValueError('lo must be non-negative')\n    if hi is None:\n        hi = len(a)\n    while lo < hi:\n        mid = (lo+hi)//2\n        if x < a[mid]: hi = mid\n        else: lo = mid+1\n    return lo\n\nbisect = bisect_right   # backward compatibility\n\ndef insort_left(a, x, lo=0, hi=None):\n    \"\"\"Insert item x in list a, and keep it sorted assuming a is sorted.\n\n    If x is already in a, insert it to the left of the leftmost x.\n\n    Optional args lo (default 0) and hi (default len(a)) bound the\n    slice of a to be searched.\n    \"\"\"\n\n    if lo < 0:\n        raise ValueError('lo must be non-negative')\n    if hi is None:\n        hi = len(a)\n    while lo < hi:\n        mid = (lo+hi)//2\n        if a[mid] < x: lo = mid+1\n        else: hi = mid\n    a.insert(lo, x)\n\n\ndef bisect_left(a, x, lo=0, hi=None):\n    \"\"\"Return the index where to insert item x in list a, assuming a is sorted.\n\n    The return value i is such that all e in a[:i] have e < x, and all e in\n    a[i:] have e >= x.  So if x already appears in the list, a.insert(x) will\n    insert just before the leftmost x already there.\n\n    Optional args lo (default 0) and hi (default len(a)) bound the\n    slice of a to be searched.\n    \"\"\"\n\n    if lo < 0:\n        raise ValueError('lo must be non-negative')\n    if hi is None:\n        hi = len(a)\n    while lo < hi:\n        mid = (lo+hi)//2\n        if a[mid] < x: lo = mid+1\n        else: hi = mid\n    return lo\n\n# Overwrite above definitions with a fast C implementation\ntry:\n    from _bisect import *\nexcept ImportError:\n    pass\n", 
    "cPickle": "#\n# Reimplementation of cPickle, mostly as a copy of pickle.py\n#\n\nfrom pickle import Pickler, dump, dumps, PickleError, PicklingError, UnpicklingError, _EmptyClass\nfrom pickle import __doc__, __version__, format_version, compatible_formats\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal, struct, sys\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\nBadPickleGet = KeyError\nUnpickleableError = PicklingError\n\nMARK            = ord('(')   # push special markobject on stack\nSTOP            = ord('.')   # every pickle ends with STOP\nPOP             = ord('0')   # discard topmost stack item\nPOP_MARK        = ord('1')   # discard stack top through topmost markobject\nDUP             = ord('2')   # duplicate top stack item\nFLOAT           = ord('F')   # push float object; decimal string argument\nINT             = ord('I')   # push integer or bool; decimal string argument\nBININT          = ord('J')   # push four-byte signed int\nBININT1         = ord('K')   # push 1-byte unsigned int\nLONG            = ord('L')   # push long; decimal string argument\nBININT2         = ord('M')   # push 2-byte unsigned int\nNONE            = ord('N')   # push None\nPERSID          = ord('P')   # push persistent object; id is taken from string arg\nBINPERSID       = ord('Q')   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = ord('R')   # apply callable to argtuple, both on stack\nSTRING          = ord('S')   # push string; NL-terminated string argument\nBINSTRING       = ord('T')   # push string; counted binary string argument\nSHORT_BINSTRING = ord('U')   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = ord('V')   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = ord('X')   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = ord('a')   # append stack top to list below it\nBUILD           = ord('b')   # call __setstate__ or __dict__.update()\nGLOBAL          = ord('c')   # push self.find_class(modname, name); 2 string args\nDICT            = ord('d')   # build a dict from stack items\nEMPTY_DICT      = ord('}')   # push empty dict\nAPPENDS         = ord('e')   # extend list on stack by topmost stack slice\nGET             = ord('g')   # push item from memo on stack; index is string arg\nBINGET          = ord('h')   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = ord('i')   # build & push class instance\nLONG_BINGET     = ord('j')   # push item from memo on stack; index is 4-byte arg\nLIST            = ord('l')   # build list from topmost stack items\nEMPTY_LIST      = ord(']')   # push empty list\nOBJ             = ord('o')   # build & push class instance\nPUT             = ord('p')   # store stack top in memo; index is string arg\nBINPUT          = ord('q')   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = ord('r')   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = ord('s')   # add key+value pair to dict\nTUPLE           = ord('t')   # build tuple from topmost stack items\nEMPTY_TUPLE     = ord(')')   # push empty tuple\nSETITEMS        = ord('u')   # modify dict by adding topmost key+value pairs\nBINFLOAT        = ord('G')   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = ord('\\x80')  # identify pickle protocol\nNEWOBJ          = ord('\\x81')  # build object by applying cls.__new__ to argtuple\nEXT1            = ord('\\x82')  # push object from extension registry; 1-byte index\nEXT2            = ord('\\x83')  # ditto, but 2-byte index\nEXT4            = ord('\\x84')  # ditto, but 4-byte index\nTUPLE1          = ord('\\x85')  # build 1-tuple from stack top\nTUPLE2          = ord('\\x86')  # build 2-tuple from two topmost stack items\nTUPLE3          = ord('\\x87')  # build 3-tuple from three topmost stack items\nNEWTRUE         = ord('\\x88')  # push True\nNEWFALSE        = ord('\\x89')  # push False\nLONG1           = ord('\\x8a')  # push long from < 256 bytes\nLONG4           = ord('\\x8b')  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n# ____________________________________________________________\n# XXX some temporary dark magic to produce pickled dumps that are\n#     closer to the ones produced by cPickle in CPython\n\nfrom pickle import StringIO\n\nPythonPickler = Pickler\nclass Pickler(PythonPickler):\n    def __init__(self, *args, **kw):\n        self.__f = None\n        if len(args) == 1 and isinstance(args[0], int):\n            self.__f = StringIO()\n            PythonPickler.__init__(self, self.__f, args[0], **kw)\n        else:\n            PythonPickler.__init__(self, *args, **kw)\n\n    def memoize(self, obj):\n        self.memo[id(None)] = None   # cPickle starts counting at one\n        return PythonPickler.memoize(self, obj)\n\n    def getvalue(self):\n        return self.__f and self.__f.getvalue()\n\n@builtinify\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\n@builtinify\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\n# Unpickling machinery\n\nclass _Stack(list):\n    def pop(self, index=-1):\n        try:\n            return list.pop(self, index)\n        except IndexError:\n            raise UnpicklingError(\"unpickling stack underflow\")\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = _Stack()\n        self.append = self.stack.append\n        try:\n            key = ord(self.read(1))\n            while key != STOP:\n                self.dispatch[key](self)\n                key = ord(self.read(1))\n        except TypeError:\n            if self.read(1) == '':\n                raise EOFError\n            raise\n        return self.stack.pop()\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        k = len(self.stack)-1\n        while self.stack[k] is not self.mark: k -= 1\n        return k\n\n    dispatch = {}\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            val = int(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()\n        if len(rep) < 3:\n            raise ValueError, \"insecure string pickle\"\n        if rep[0] == \"'\" == rep[-2]:\n            rep = rep[1:-2]\n        elif rep[0] == '\"' == rep[-2]:\n            rep = rep[1:-2]\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        L = mloads('i' + self.read(4))\n        self.append(self.read(L))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        L = mloads('i' + self.read(4))\n        self.append(unicode(self.read(L),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        L = ord(self.read(1))\n        self.append(self.read(L))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        args = self.stack.pop()\n        func = self.stack[-1]\n        value = self.stack[-1](*args)\n        self.stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        value = self.stack.pop()\n        self.stack[-1].append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        lst = stack[mark - 1]\n        lst.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n#from pickle import decode_long\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ind = nbytes - 1\n    while ind and ord(data[ind]) == 0:\n        ind -= 1\n    n = ord(data[ind])\n    while ind:\n        n <<= 8\n        ind -= 1\n        if ord(data[ind]):\n            n += ord(data[ind])\n    if ord(data[nbytes - 1]) >= 128:\n        n -= 1L << (nbytes << 3)\n    return n\n\ndef load(f):\n    return Unpickler(f).load()\n\ndef loads(str):\n    f = StringIO(str)\n    return Unpickler(f).load()\n", 
    "cStringIO": "#\n# StringIO-based cStringIO implementation.\n#\n\n# Note that PyPy also contains a built-in module 'cStringIO' which will hide\n# this one if compiled in.\n\nfrom StringIO import *\nfrom StringIO import __doc__\n\nclass StringIO(StringIO):\n    def reset(self):\n        \"\"\"\n        reset() -- Reset the file position to the beginning\n        \"\"\"\n        self.seek(0, 0)\n", 
    "calendar": "\"\"\"Calendar printing functions\n\nNote when comparing these calendars to the ones printed by cal(1): By\ndefault, these calendars have Monday as the first day of the week, and\nSunday as the last (the European convention). Use setfirstweekday() to\nset the first day of the week (0=Monday, 6=Sunday).\"\"\"\n\nimport sys\nimport datetime\nimport locale as _locale\n\n__all__ = [\"IllegalMonthError\", \"IllegalWeekdayError\", \"setfirstweekday\",\n           \"firstweekday\", \"isleap\", \"leapdays\", \"weekday\", \"monthrange\",\n           \"monthcalendar\", \"prmonth\", \"month\", \"prcal\", \"calendar\",\n           \"timegm\", \"month_name\", \"month_abbr\", \"day_name\", \"day_abbr\"]\n\n# Exception raised for bad input (with string parameter for details)\nerror = ValueError\n\n# Exceptions raised for bad input\nclass IllegalMonthError(ValueError):\n    def __init__(self, month):\n        self.month = month\n    def __str__(self):\n        return \"bad month number %r; must be 1-12\" % self.month\n\n\nclass IllegalWeekdayError(ValueError):\n    def __init__(self, weekday):\n        self.weekday = weekday\n    def __str__(self):\n        return \"bad weekday number %r; must be 0 (Monday) to 6 (Sunday)\" % self.weekday\n\n\n# Constants for months referenced later\nJanuary = 1\nFebruary = 2\n\n# Number of days per month (except for February in leap years)\nmdays = [0, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\n# This module used to have hard-coded lists of day and month names, as\n# English strings.  The classes following emulate a read-only version of\n# that, but supply localized names.  Note that the values are computed\n# fresh on each call, in case the user changes locale between calls.\n\nclass _localized_month:\n\n    _months = [datetime.date(2001, i+1, 1).strftime for i in range(12)]\n    _months.insert(0, lambda x: \"\")\n\n    def __init__(self, format):\n        self.format = format\n\n    def __getitem__(self, i):\n        funcs = self._months[i]\n        if isinstance(i, slice):\n            return [f(self.format) for f in funcs]\n        else:\n            return funcs(self.format)\n\n    def __len__(self):\n        return 13\n\n\nclass _localized_day:\n\n    # January 1, 2001, was a Monday.\n    _days = [datetime.date(2001, 1, i+1).strftime for i in range(7)]\n\n    def __init__(self, format):\n        self.format = format\n\n    def __getitem__(self, i):\n        funcs = self._days[i]\n        if isinstance(i, slice):\n            return [f(self.format) for f in funcs]\n        else:\n            return funcs(self.format)\n\n    def __len__(self):\n        return 7\n\n\n# Full and abbreviated names of weekdays\nday_name = _localized_day('%A')\nday_abbr = _localized_day('%a')\n\n# Full and abbreviated names of months (1-based arrays!!!)\nmonth_name = _localized_month('%B')\nmonth_abbr = _localized_month('%b')\n\n# Constants for weekdays\n(MONDAY, TUESDAY, WEDNESDAY, THURSDAY, FRIDAY, SATURDAY, SUNDAY) = range(7)\n\n\ndef isleap(year):\n    \"\"\"Return True for leap years, False for non-leap years.\"\"\"\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n\n\ndef leapdays(y1, y2):\n    \"\"\"Return number of leap years in range [y1, y2).\n       Assume y1 <= y2.\"\"\"\n    y1 -= 1\n    y2 -= 1\n    return (y2//4 - y1//4) - (y2//100 - y1//100) + (y2//400 - y1//400)\n\n\ndef weekday(year, month, day):\n    \"\"\"Return weekday (0-6 ~ Mon-Sun) for year (1970-...), month (1-12),\n       day (1-31).\"\"\"\n    return datetime.date(year, month, day).weekday()\n\n\ndef monthrange(year, month):\n    \"\"\"Return weekday (0-6 ~ Mon-Sun) and number of days (28-31) for\n       year, month.\"\"\"\n    if not 1 <= month <= 12:\n        raise IllegalMonthError(month)\n    day1 = weekday(year, month, 1)\n    ndays = mdays[month] + (month == February and isleap(year))\n    return day1, ndays\n\n\nclass Calendar(object):\n    \"\"\"\n    Base calendar class. This class doesn't do any formatting. It simply\n    provides data to subclasses.\n    \"\"\"\n\n    def __init__(self, firstweekday=0):\n        self.firstweekday = firstweekday # 0 = Monday, 6 = Sunday\n\n    def getfirstweekday(self):\n        return self._firstweekday % 7\n\n    def setfirstweekday(self, firstweekday):\n        self._firstweekday = firstweekday\n\n    firstweekday = property(getfirstweekday, setfirstweekday)\n\n    def iterweekdays(self):\n        \"\"\"\n        Return a iterator for one week of weekday numbers starting with the\n        configured first one.\n        \"\"\"\n        for i in range(self.firstweekday, self.firstweekday + 7):\n            yield i%7\n\n    def itermonthdates(self, year, month):\n        \"\"\"\n        Return an iterator for one month. The iterator will yield datetime.date\n        values and will always iterate through complete weeks, so it will yield\n        dates outside the specified month.\n        \"\"\"\n        date = datetime.date(year, month, 1)\n        # Go back to the beginning of the week\n        days = (date.weekday() - self.firstweekday) % 7\n        date -= datetime.timedelta(days=days)\n        oneday = datetime.timedelta(days=1)\n        while True:\n            yield date\n            try:\n                date += oneday\n            except OverflowError:\n                # Adding one day could fail after datetime.MAXYEAR\n                break\n            if date.month != month and date.weekday() == self.firstweekday:\n                break\n\n    def itermonthdays2(self, year, month):\n        \"\"\"\n        Like itermonthdates(), but will yield (day number, weekday number)\n        tuples. For days outside the specified month the day number is 0.\n        \"\"\"\n        for date in self.itermonthdates(year, month):\n            if date.month != month:\n                yield (0, date.weekday())\n            else:\n                yield (date.day, date.weekday())\n\n    def itermonthdays(self, year, month):\n        \"\"\"\n        Like itermonthdates(), but will yield day numbers. For days outside\n        the specified month the day number is 0.\n        \"\"\"\n        for date in self.itermonthdates(year, month):\n            if date.month != month:\n                yield 0\n            else:\n                yield date.day\n\n    def monthdatescalendar(self, year, month):\n        \"\"\"\n        Return a matrix (list of lists) representing a month's calendar.\n        Each row represents a week; week entries are datetime.date values.\n        \"\"\"\n        dates = list(self.itermonthdates(year, month))\n        return [ dates[i:i+7] for i in range(0, len(dates), 7) ]\n\n    def monthdays2calendar(self, year, month):\n        \"\"\"\n        Return a matrix representing a month's calendar.\n        Each row represents a week; week entries are\n        (day number, weekday number) tuples. Day numbers outside this month\n        are zero.\n        \"\"\"\n        days = list(self.itermonthdays2(year, month))\n        return [ days[i:i+7] for i in range(0, len(days), 7) ]\n\n    def monthdayscalendar(self, year, month):\n        \"\"\"\n        Return a matrix representing a month's calendar.\n        Each row represents a week; days outside this month are zero.\n        \"\"\"\n        days = list(self.itermonthdays(year, month))\n        return [ days[i:i+7] for i in range(0, len(days), 7) ]\n\n    def yeardatescalendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting. The return\n        value is a list of month rows. Each month row contains up to width months.\n        Each month contains between 4 and 6 weeks and each week contains 1-7\n        days. Days are datetime.date objects.\n        \"\"\"\n        months = [\n            self.monthdatescalendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n    def yeardays2calendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting (similar to\n        yeardatescalendar()). Entries in the week lists are\n        (day number, weekday number) tuples. Day numbers outside this month are\n        zero.\n        \"\"\"\n        months = [\n            self.monthdays2calendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n    def yeardayscalendar(self, year, width=3):\n        \"\"\"\n        Return the data for the specified year ready for formatting (similar to\n        yeardatescalendar()). Entries in the week lists are day numbers.\n        Day numbers outside this month are zero.\n        \"\"\"\n        months = [\n            self.monthdayscalendar(year, i)\n            for i in range(January, January+12)\n        ]\n        return [months[i:i+width] for i in range(0, len(months), width) ]\n\n\nclass TextCalendar(Calendar):\n    \"\"\"\n    Subclass of Calendar that outputs a calendar as a simple plain text\n    similar to the UNIX program cal.\n    \"\"\"\n\n    def prweek(self, theweek, width):\n        \"\"\"\n        Print a single week (no newline).\n        \"\"\"\n        print self.formatweek(theweek, width),\n\n    def formatday(self, day, weekday, width):\n        \"\"\"\n        Returns a formatted day.\n        \"\"\"\n        if day == 0:\n            s = ''\n        else:\n            s = '%2i' % day             # right-align single-digit days\n        return s.center(width)\n\n    def formatweek(self, theweek, width):\n        \"\"\"\n        Returns a single week in a string (no newline).\n        \"\"\"\n        return ' '.join(self.formatday(d, wd, width) for (d, wd) in theweek)\n\n    def formatweekday(self, day, width):\n        \"\"\"\n        Returns a formatted week day name.\n        \"\"\"\n        if width >= 9:\n            names = day_name\n        else:\n            names = day_abbr\n        return names[day][:width].center(width)\n\n    def formatweekheader(self, width):\n        \"\"\"\n        Return a header for a week.\n        \"\"\"\n        return ' '.join(self.formatweekday(i, width) for i in self.iterweekdays())\n\n    def formatmonthname(self, theyear, themonth, width, withyear=True):\n        \"\"\"\n        Return a formatted month name.\n        \"\"\"\n        s = month_name[themonth]\n        if withyear:\n            s = \"%s %r\" % (s, theyear)\n        return s.center(width)\n\n    def prmonth(self, theyear, themonth, w=0, l=0):\n        \"\"\"\n        Print a month's calendar.\n        \"\"\"\n        print self.formatmonth(theyear, themonth, w, l),\n\n    def formatmonth(self, theyear, themonth, w=0, l=0):\n        \"\"\"\n        Return a month's calendar string (multi-line).\n        \"\"\"\n        w = max(2, w)\n        l = max(1, l)\n        s = self.formatmonthname(theyear, themonth, 7 * (w + 1) - 1)\n        s = s.rstrip()\n        s += '\\n' * l\n        s += self.formatweekheader(w).rstrip()\n        s += '\\n' * l\n        for week in self.monthdays2calendar(theyear, themonth):\n            s += self.formatweek(week, w).rstrip()\n            s += '\\n' * l\n        return s\n\n    def formatyear(self, theyear, w=2, l=1, c=6, m=3):\n        \"\"\"\n        Returns a year's calendar as a multi-line string.\n        \"\"\"\n        w = max(2, w)\n        l = max(1, l)\n        c = max(2, c)\n        colwidth = (w + 1) * 7 - 1\n        v = []\n        a = v.append\n        a(repr(theyear).center(colwidth*m+c*(m-1)).rstrip())\n        a('\\n'*l)\n        header = self.formatweekheader(w)\n        for (i, row) in enumerate(self.yeardays2calendar(theyear, m)):\n            # months in this row\n            months = range(m*i+1, min(m*(i+1)+1, 13))\n            a('\\n'*l)\n            names = (self.formatmonthname(theyear, k, colwidth, False)\n                     for k in months)\n            a(formatstring(names, colwidth, c).rstrip())\n            a('\\n'*l)\n            headers = (header for k in months)\n            a(formatstring(headers, colwidth, c).rstrip())\n            a('\\n'*l)\n            # max number of weeks for this row\n            height = max(len(cal) for cal in row)\n            for j in range(height):\n                weeks = []\n                for cal in row:\n                    if j >= len(cal):\n                        weeks.append('')\n                    else:\n                        weeks.append(self.formatweek(cal[j], w))\n                a(formatstring(weeks, colwidth, c).rstrip())\n                a('\\n' * l)\n        return ''.join(v)\n\n    def pryear(self, theyear, w=0, l=0, c=6, m=3):\n        \"\"\"Print a year's calendar.\"\"\"\n        print self.formatyear(theyear, w, l, c, m)\n\n\nclass HTMLCalendar(Calendar):\n    \"\"\"\n    This calendar returns complete HTML pages.\n    \"\"\"\n\n    # CSS classes for the day <td>s\n    cssclasses = [\"mon\", \"tue\", \"wed\", \"thu\", \"fri\", \"sat\", \"sun\"]\n\n    def formatday(self, day, weekday):\n        \"\"\"\n        Return a day as a table cell.\n        \"\"\"\n        if day == 0:\n            return '<td class=\"noday\">&nbsp;</td>' # day outside month\n        else:\n            return '<td class=\"%s\">%d</td>' % (self.cssclasses[weekday], day)\n\n    def formatweek(self, theweek):\n        \"\"\"\n        Return a complete week as a table row.\n        \"\"\"\n        s = ''.join(self.formatday(d, wd) for (d, wd) in theweek)\n        return '<tr>%s</tr>' % s\n\n    def formatweekday(self, day):\n        \"\"\"\n        Return a weekday name as a table header.\n        \"\"\"\n        return '<th class=\"%s\">%s</th>' % (self.cssclasses[day], day_abbr[day])\n\n    def formatweekheader(self):\n        \"\"\"\n        Return a header for a week as a table row.\n        \"\"\"\n        s = ''.join(self.formatweekday(i) for i in self.iterweekdays())\n        return '<tr>%s</tr>' % s\n\n    def formatmonthname(self, theyear, themonth, withyear=True):\n        \"\"\"\n        Return a month name as a table row.\n        \"\"\"\n        if withyear:\n            s = '%s %s' % (month_name[themonth], theyear)\n        else:\n            s = '%s' % month_name[themonth]\n        return '<tr><th colspan=\"7\" class=\"month\">%s</th></tr>' % s\n\n    def formatmonth(self, theyear, themonth, withyear=True):\n        \"\"\"\n        Return a formatted month as a table.\n        \"\"\"\n        v = []\n        a = v.append\n        a('<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"month\">')\n        a('\\n')\n        a(self.formatmonthname(theyear, themonth, withyear=withyear))\n        a('\\n')\n        a(self.formatweekheader())\n        a('\\n')\n        for week in self.monthdays2calendar(theyear, themonth):\n            a(self.formatweek(week))\n            a('\\n')\n        a('</table>')\n        a('\\n')\n        return ''.join(v)\n\n    def formatyear(self, theyear, width=3):\n        \"\"\"\n        Return a formatted year as a table of tables.\n        \"\"\"\n        v = []\n        a = v.append\n        width = max(width, 1)\n        a('<table border=\"0\" cellpadding=\"0\" cellspacing=\"0\" class=\"year\">')\n        a('\\n')\n        a('<tr><th colspan=\"%d\" class=\"year\">%s</th></tr>' % (width, theyear))\n        for i in range(January, January+12, width):\n            # months in this row\n            months = range(i, min(i+width, 13))\n            a('<tr>')\n            for m in months:\n                a('<td>')\n                a(self.formatmonth(theyear, m, withyear=False))\n                a('</td>')\n            a('</tr>')\n        a('</table>')\n        return ''.join(v)\n\n    def formatyearpage(self, theyear, width=3, css='calendar.css', encoding=None):\n        \"\"\"\n        Return a formatted year as a complete HTML page.\n        \"\"\"\n        if encoding is None:\n            encoding = sys.getdefaultencoding()\n        v = []\n        a = v.append\n        a('<?xml version=\"1.0\" encoding=\"%s\"?>\\n' % encoding)\n        a('<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Strict//EN\" \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd\">\\n')\n        a('<html>\\n')\n        a('<head>\\n')\n        a('<meta http-equiv=\"Content-Type\" content=\"text/html; charset=%s\" />\\n' % encoding)\n        if css is not None:\n            a('<link rel=\"stylesheet\" type=\"text/css\" href=\"%s\" />\\n' % css)\n        a('<title>Calendar for %d</title>\\n' % theyear)\n        a('</head>\\n')\n        a('<body>\\n')\n        a(self.formatyear(theyear, width))\n        a('</body>\\n')\n        a('</html>\\n')\n        return ''.join(v).encode(encoding, \"xmlcharrefreplace\")\n\n\nclass TimeEncoding:\n    def __init__(self, locale):\n        self.locale = locale\n\n    def __enter__(self):\n        self.oldlocale = _locale.getlocale(_locale.LC_TIME)\n        _locale.setlocale(_locale.LC_TIME, self.locale)\n        return _locale.getlocale(_locale.LC_TIME)[1]\n\n    def __exit__(self, *args):\n        _locale.setlocale(_locale.LC_TIME, self.oldlocale)\n\n\nclass LocaleTextCalendar(TextCalendar):\n    \"\"\"\n    This class can be passed a locale name in the constructor and will return\n    month and weekday names in the specified locale. If this locale includes\n    an encoding all strings containing month and weekday names will be returned\n    as unicode.\n    \"\"\"\n\n    def __init__(self, firstweekday=0, locale=None):\n        TextCalendar.__init__(self, firstweekday)\n        if locale is None:\n            locale = _locale.getdefaultlocale()\n        self.locale = locale\n\n    def formatweekday(self, day, width):\n        with TimeEncoding(self.locale) as encoding:\n            if width >= 9:\n                names = day_name\n            else:\n                names = day_abbr\n            name = names[day]\n            if encoding is not None:\n                name = name.decode(encoding)\n            return name[:width].center(width)\n\n    def formatmonthname(self, theyear, themonth, width, withyear=True):\n        with TimeEncoding(self.locale) as encoding:\n            s = month_name[themonth]\n            if encoding is not None:\n                s = s.decode(encoding)\n            if withyear:\n                s = \"%s %r\" % (s, theyear)\n            return s.center(width)\n\n\nclass LocaleHTMLCalendar(HTMLCalendar):\n    \"\"\"\n    This class can be passed a locale name in the constructor and will return\n    month and weekday names in the specified locale. If this locale includes\n    an encoding all strings containing month and weekday names will be returned\n    as unicode.\n    \"\"\"\n    def __init__(self, firstweekday=0, locale=None):\n        HTMLCalendar.__init__(self, firstweekday)\n        if locale is None:\n            locale = _locale.getdefaultlocale()\n        self.locale = locale\n\n    def formatweekday(self, day):\n        with TimeEncoding(self.locale) as encoding:\n            s = day_abbr[day]\n            if encoding is not None:\n                s = s.decode(encoding)\n            return '<th class=\"%s\">%s</th>' % (self.cssclasses[day], s)\n\n    def formatmonthname(self, theyear, themonth, withyear=True):\n        with TimeEncoding(self.locale) as encoding:\n            s = month_name[themonth]\n            if encoding is not None:\n                s = s.decode(encoding)\n            if withyear:\n                s = '%s %s' % (s, theyear)\n            return '<tr><th colspan=\"7\" class=\"month\">%s</th></tr>' % s\n\n\n# Support for old module level interface\nc = TextCalendar()\n\nfirstweekday = c.getfirstweekday\n\ndef setfirstweekday(firstweekday):\n    try:\n        firstweekday.__index__\n    except AttributeError:\n        raise IllegalWeekdayError(firstweekday)\n    if not MONDAY <= firstweekday <= SUNDAY:\n        raise IllegalWeekdayError(firstweekday)\n    c.firstweekday = firstweekday\n\nmonthcalendar = c.monthdayscalendar\nprweek = c.prweek\nweek = c.formatweek\nweekheader = c.formatweekheader\nprmonth = c.prmonth\nmonth = c.formatmonth\ncalendar = c.formatyear\nprcal = c.pryear\n\n\n# Spacing of month columns for multi-column year calendar\n_colwidth = 7*3 - 1         # Amount printed by prweek()\n_spacing = 6                # Number of spaces between columns\n\n\ndef format(cols, colwidth=_colwidth, spacing=_spacing):\n    \"\"\"Prints multi-column formatting for year calendars\"\"\"\n    print formatstring(cols, colwidth, spacing)\n\n\ndef formatstring(cols, colwidth=_colwidth, spacing=_spacing):\n    \"\"\"Returns a string formatted from n strings, centered within n columns.\"\"\"\n    spacing *= ' '\n    return spacing.join(c.center(colwidth) for c in cols)\n\n\nEPOCH = 1970\n_EPOCH_ORD = datetime.date(EPOCH, 1, 1).toordinal()\n\n\ndef timegm(tuple):\n    \"\"\"Unrelated but handy function to calculate Unix timestamp from GMT.\"\"\"\n    year, month, day, hour, minute, second = tuple[:6]\n    days = datetime.date(year, month, 1).toordinal() - _EPOCH_ORD + day - 1\n    hours = days*24 + hour\n    minutes = hours*60 + minute\n    seconds = minutes*60 + second\n    return seconds\n\n\ndef main(args):\n    import optparse\n    parser = optparse.OptionParser(usage=\"usage: %prog [options] [year [month]]\")\n    parser.add_option(\n        \"-w\", \"--width\",\n        dest=\"width\", type=\"int\", default=2,\n        help=\"width of date column (default 2, text only)\"\n    )\n    parser.add_option(\n        \"-l\", \"--lines\",\n        dest=\"lines\", type=\"int\", default=1,\n        help=\"number of lines for each week (default 1, text only)\"\n    )\n    parser.add_option(\n        \"-s\", \"--spacing\",\n        dest=\"spacing\", type=\"int\", default=6,\n        help=\"spacing between months (default 6, text only)\"\n    )\n    parser.add_option(\n        \"-m\", \"--months\",\n        dest=\"months\", type=\"int\", default=3,\n        help=\"months per row (default 3, text only)\"\n    )\n    parser.add_option(\n        \"-c\", \"--css\",\n        dest=\"css\", default=\"calendar.css\",\n        help=\"CSS to use for page (html only)\"\n    )\n    parser.add_option(\n        \"-L\", \"--locale\",\n        dest=\"locale\", default=None,\n        help=\"locale to be used from month and weekday names\"\n    )\n    parser.add_option(\n        \"-e\", \"--encoding\",\n        dest=\"encoding\", default=None,\n        help=\"Encoding to use for output\"\n    )\n    parser.add_option(\n        \"-t\", \"--type\",\n        dest=\"type\", default=\"text\",\n        choices=(\"text\", \"html\"),\n        help=\"output type (text or html)\"\n    )\n\n    (options, args) = parser.parse_args(args)\n\n    if options.locale and not options.encoding:\n        parser.error(\"if --locale is specified --encoding is required\")\n        sys.exit(1)\n\n    locale = options.locale, options.encoding\n\n    if options.type == \"html\":\n        if options.locale:\n            cal = LocaleHTMLCalendar(locale=locale)\n        else:\n            cal = HTMLCalendar()\n        encoding = options.encoding\n        if encoding is None:\n            encoding = sys.getdefaultencoding()\n        optdict = dict(encoding=encoding, css=options.css)\n        if len(args) == 1:\n            print cal.formatyearpage(datetime.date.today().year, **optdict)\n        elif len(args) == 2:\n            print cal.formatyearpage(int(args[1]), **optdict)\n        else:\n            parser.error(\"incorrect number of arguments\")\n            sys.exit(1)\n    else:\n        if options.locale:\n            cal = LocaleTextCalendar(locale=locale)\n        else:\n            cal = TextCalendar()\n        optdict = dict(w=options.width, l=options.lines)\n        if len(args) != 3:\n            optdict[\"c\"] = options.spacing\n            optdict[\"m\"] = options.months\n        if len(args) == 1:\n            result = cal.formatyear(datetime.date.today().year, **optdict)\n        elif len(args) == 2:\n            result = cal.formatyear(int(args[1]), **optdict)\n        elif len(args) == 3:\n            result = cal.formatmonth(int(args[1]), int(args[2]), **optdict)\n        else:\n            parser.error(\"incorrect number of arguments\")\n            sys.exit(1)\n        if options.encoding:\n            result = result.encode(options.encoding)\n        print result\n\n\nif __name__ == \"__main__\":\n    main(sys.argv)\n", 
    "cmd": "\"\"\"A generic class to build line-oriented command interpreters.\n\nInterpreters constructed with this class obey the following conventions:\n\n1. End of file on input is processed as the command 'EOF'.\n2. A command is parsed out of each line by collecting the prefix composed\n   of characters in the identchars member.\n3. A command `foo' is dispatched to a method 'do_foo()'; the do_ method\n   is passed a single argument consisting of the remainder of the line.\n4. Typing an empty line repeats the last command.  (Actually, it calls the\n   method `emptyline', which may be overridden in a subclass.)\n5. There is a predefined `help' method.  Given an argument `topic', it\n   calls the command `help_topic'.  With no arguments, it lists all topics\n   with defined help_ functions, broken into up to three topics; documented\n   commands, miscellaneous help topics, and undocumented commands.\n6. The command '?' is a synonym for `help'.  The command '!' is a synonym\n   for `shell', if a do_shell method exists.\n7. If completion is enabled, completing commands will be done automatically,\n   and completing of commands args is done by calling complete_foo() with\n   arguments text, line, begidx, endidx.  text is string we are matching\n   against, all returned matches must begin with it.  line is the current\n   input line (lstripped), begidx and endidx are the beginning and end\n   indexes of the text being matched, which could be used to provide\n   different completion depending upon which position the argument is in.\n\nThe `default' method may be overridden to intercept commands for which there\nis no do_ method.\n\nThe `completedefault' method may be overridden to intercept completions for\ncommands that have no complete_ method.\n\nThe data member `self.ruler' sets the character used to draw separator lines\nin the help messages.  If empty, no ruler line is drawn.  It defaults to \"=\".\n\nIf the value of `self.intro' is nonempty when the cmdloop method is called,\nit is printed out on interpreter startup.  This value may be overridden\nvia an optional argument to the cmdloop() method.\n\nThe data members `self.doc_header', `self.misc_header', and\n`self.undoc_header' set the headers used for the help function's\nlistings of documented functions, miscellaneous topics, and undocumented\nfunctions respectively.\n\nThese interpreters use raw_input; thus, if the readline module is loaded,\nthey automatically support Emacs-like command history and editing features.\n\"\"\"\n\nimport string\n\n__all__ = [\"Cmd\"]\n\nPROMPT = '(Cmd) '\nIDENTCHARS = string.ascii_letters + string.digits + '_'\n\nclass Cmd:\n    \"\"\"A simple framework for writing line-oriented command interpreters.\n\n    These are often useful for test harnesses, administrative tools, and\n    prototypes that will later be wrapped in a more sophisticated interface.\n\n    A Cmd instance or subclass instance is a line-oriented interpreter\n    framework.  There is no good reason to instantiate Cmd itself; rather,\n    it's useful as a superclass of an interpreter class you define yourself\n    in order to inherit Cmd's methods and encapsulate action methods.\n\n    \"\"\"\n    prompt = PROMPT\n    identchars = IDENTCHARS\n    ruler = '='\n    lastcmd = ''\n    intro = None\n    doc_leader = \"\"\n    doc_header = \"Documented commands (type help <topic>):\"\n    misc_header = \"Miscellaneous help topics:\"\n    undoc_header = \"Undocumented commands:\"\n    nohelp = \"*** No help on %s\"\n    use_rawinput = 1\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None):\n        \"\"\"Instantiate a line-oriented interpreter framework.\n\n        The optional argument 'completekey' is the readline name of a\n        completion key; it defaults to the Tab key. If completekey is\n        not None and the readline module is available, command completion\n        is done automatically. The optional arguments stdin and stdout\n        specify alternate input and output file objects; if not specified,\n        sys.stdin and sys.stdout are used.\n\n        \"\"\"\n        import sys\n        if stdin is not None:\n            self.stdin = stdin\n        else:\n            self.stdin = sys.stdin\n        if stdout is not None:\n            self.stdout = stdout\n        else:\n            self.stdout = sys.stdout\n        self.cmdqueue = []\n        self.completekey = completekey\n\n    def cmdloop(self, intro=None):\n        \"\"\"Repeatedly issue a prompt, accept input, parse an initial prefix\n        off the received input, and dispatch to action methods, passing them\n        the remainder of the line as argument.\n\n        \"\"\"\n\n        self.preloop()\n        if self.use_rawinput and self.completekey:\n            try:\n                import readline\n                self.old_completer = readline.get_completer()\n                readline.set_completer(self.complete)\n                readline.parse_and_bind(self.completekey+\": complete\")\n            except ImportError:\n                pass\n        try:\n            if intro is not None:\n                self.intro = intro\n            if self.intro:\n                self.stdout.write(str(self.intro)+\"\\n\")\n            stop = None\n            while not stop:\n                if self.cmdqueue:\n                    line = self.cmdqueue.pop(0)\n                else:\n                    if self.use_rawinput:\n                        try:\n                            line = raw_input(self.prompt)\n                        except EOFError:\n                            line = 'EOF'\n                    else:\n                        self.stdout.write(self.prompt)\n                        self.stdout.flush()\n                        line = self.stdin.readline()\n                        if not len(line):\n                            line = 'EOF'\n                        else:\n                            line = line.rstrip('\\r\\n')\n                line = self.precmd(line)\n                stop = self.onecmd(line)\n                stop = self.postcmd(stop, line)\n            self.postloop()\n        finally:\n            if self.use_rawinput and self.completekey:\n                try:\n                    import readline\n                    readline.set_completer(self.old_completer)\n                except ImportError:\n                    pass\n\n\n    def precmd(self, line):\n        \"\"\"Hook method executed just before the command line is\n        interpreted, but after the input prompt is generated and issued.\n\n        \"\"\"\n        return line\n\n    def postcmd(self, stop, line):\n        \"\"\"Hook method executed just after a command dispatch is finished.\"\"\"\n        return stop\n\n    def preloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is called.\"\"\"\n        pass\n\n    def postloop(self):\n        \"\"\"Hook method executed once when the cmdloop() method is about to\n        return.\n\n        \"\"\"\n        pass\n\n    def parseline(self, line):\n        \"\"\"Parse the line into a command name and a string containing\n        the arguments.  Returns a tuple containing (command, args, line).\n        'command' and 'args' may be None if the line couldn't be parsed.\n        \"\"\"\n        line = line.strip()\n        if not line:\n            return None, None, line\n        elif line[0] == '?':\n            line = 'help ' + line[1:]\n        elif line[0] == '!':\n            if hasattr(self, 'do_shell'):\n                line = 'shell ' + line[1:]\n            else:\n                return None, None, line\n        i, n = 0, len(line)\n        while i < n and line[i] in self.identchars: i = i+1\n        cmd, arg = line[:i], line[i:].strip()\n        return cmd, arg, line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        This may be overridden, but should not normally need to be;\n        see the precmd() and postcmd() methods for useful execution hooks.\n        The return value is a flag indicating whether interpretation of\n        commands by the interpreter should stop.\n\n        \"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not line:\n            return self.emptyline()\n        if cmd is None:\n            return self.default(line)\n        self.lastcmd = line\n        if line == 'EOF' :\n            self.lastcmd = ''\n        if cmd == '':\n            return self.default(line)\n        else:\n            try:\n                func = getattr(self, 'do_' + cmd)\n            except AttributeError:\n                return self.default(line)\n            return func(arg)\n\n    def emptyline(self):\n        \"\"\"Called when an empty line is entered in response to the prompt.\n\n        If this method is not overridden, it repeats the last nonempty\n        command entered.\n\n        \"\"\"\n        if self.lastcmd:\n            return self.onecmd(self.lastcmd)\n\n    def default(self, line):\n        \"\"\"Called on an input line when the command prefix is not recognized.\n\n        If this method is not overridden, it prints an error message and\n        returns.\n\n        \"\"\"\n        self.stdout.write('*** Unknown syntax: %s\\n'%line)\n\n    def completedefault(self, *ignored):\n        \"\"\"Method called to complete an input line when no command-specific\n        complete_*() method is available.\n\n        By default, it returns an empty list.\n\n        \"\"\"\n        return []\n\n    def completenames(self, text, *ignored):\n        dotext = 'do_'+text\n        return [a[3:] for a in self.get_names() if a.startswith(dotext)]\n\n    def complete(self, text, state):\n        \"\"\"Return the next possible completion for 'text'.\n\n        If a command has not been entered, then complete against command list.\n        Otherwise try to call complete_<command> to get list of completions.\n        \"\"\"\n        if state == 0:\n            import readline\n            origline = readline.get_line_buffer()\n            line = origline.lstrip()\n            stripped = len(origline) - len(line)\n            begidx = readline.get_begidx() - stripped\n            endidx = readline.get_endidx() - stripped\n            if begidx>0:\n                cmd, args, foo = self.parseline(line)\n                if cmd == '':\n                    compfunc = self.completedefault\n                else:\n                    try:\n                        compfunc = getattr(self, 'complete_' + cmd)\n                    except AttributeError:\n                        compfunc = self.completedefault\n            else:\n                compfunc = self.completenames\n            self.completion_matches = compfunc(text, line, begidx, endidx)\n        try:\n            return self.completion_matches[state]\n        except IndexError:\n            return None\n\n    def get_names(self):\n        # This method used to pull in base class attributes\n        # at a time dir() didn't do it yet.\n        return dir(self.__class__)\n\n    def complete_help(self, *args):\n        commands = set(self.completenames(*args))\n        topics = set(a[5:] for a in self.get_names()\n                     if a.startswith('help_' + args[0]))\n        return list(commands | topics)\n\n    def do_help(self, arg):\n        'List available commands with \"help\" or detailed help with \"help cmd\".'\n        if arg:\n            # XXX check arg syntax\n            try:\n                func = getattr(self, 'help_' + arg)\n            except AttributeError:\n                try:\n                    doc=getattr(self, 'do_' + arg).__doc__\n                    if doc:\n                        self.stdout.write(\"%s\\n\"%str(doc))\n                        return\n                except AttributeError:\n                    pass\n                self.stdout.write(\"%s\\n\"%str(self.nohelp % (arg,)))\n                return\n            func()\n        else:\n            names = self.get_names()\n            cmds_doc = []\n            cmds_undoc = []\n            help = {}\n            for name in names:\n                if name[:5] == 'help_':\n                    help[name[5:]]=1\n            names.sort()\n            # There can be duplicates if routines overridden\n            prevname = ''\n            for name in names:\n                if name[:3] == 'do_':\n                    if name == prevname:\n                        continue\n                    prevname = name\n                    cmd=name[3:]\n                    if cmd in help:\n                        cmds_doc.append(cmd)\n                        del help[cmd]\n                    elif getattr(self, name).__doc__:\n                        cmds_doc.append(cmd)\n                    else:\n                        cmds_undoc.append(cmd)\n            self.stdout.write(\"%s\\n\"%str(self.doc_leader))\n            self.print_topics(self.doc_header,   cmds_doc,   15,80)\n            self.print_topics(self.misc_header,  help.keys(),15,80)\n            self.print_topics(self.undoc_header, cmds_undoc, 15,80)\n\n    def print_topics(self, header, cmds, cmdlen, maxcol):\n        if cmds:\n            self.stdout.write(\"%s\\n\"%str(header))\n            if self.ruler:\n                self.stdout.write(\"%s\\n\"%str(self.ruler * len(header)))\n            self.columnize(cmds, maxcol-1)\n            self.stdout.write(\"\\n\")\n\n    def columnize(self, list, displaywidth=80):\n        \"\"\"Display a list of strings as a compact set of columns.\n\n        Each column is only as wide as necessary.\n        Columns are separated by two spaces (one was not legible enough).\n        \"\"\"\n        if not list:\n            self.stdout.write(\"<empty>\\n\")\n            return\n        nonstrings = [i for i in range(len(list))\n                        if not isinstance(list[i], str)]\n        if nonstrings:\n            raise TypeError, (\"list[i] not a string for i in %s\" %\n                              \", \".join(map(str, nonstrings)))\n        size = len(list)\n        if size == 1:\n            self.stdout.write('%s\\n'%str(list[0]))\n            return\n        # Try every row count from 1 upwards\n        for nrows in range(1, len(list)):\n            ncols = (size+nrows-1) // nrows\n            colwidths = []\n            totwidth = -2\n            for col in range(ncols):\n                colwidth = 0\n                for row in range(nrows):\n                    i = row + nrows*col\n                    if i >= size:\n                        break\n                    x = list[i]\n                    colwidth = max(colwidth, len(x))\n                colwidths.append(colwidth)\n                totwidth += colwidth + 2\n                if totwidth > displaywidth:\n                    break\n            if totwidth <= displaywidth:\n                break\n        else:\n            nrows = len(list)\n            ncols = 1\n            colwidths = [0]\n        for row in range(nrows):\n            texts = []\n            for col in range(ncols):\n                i = row + nrows*col\n                if i >= size:\n                    x = \"\"\n                else:\n                    x = list[i]\n                texts.append(x)\n            while texts and not texts[-1]:\n                del texts[-1]\n            for col in range(len(texts)):\n                texts[col] = texts[col].ljust(colwidths[col])\n            self.stdout.write(\"%s\\n\"%str(\"  \".join(texts)))\n", 
    "code": "\"\"\"Utilities needed to emulate Python's interactive interpreter.\n\n\"\"\"\n\n# Inspired by similar code by Jeff Epler and Fredrik Lundh.\n\n\nimport sys\nimport traceback\nfrom codeop import CommandCompiler, compile_command\n\n__all__ = [\"InteractiveInterpreter\", \"InteractiveConsole\", \"interact\",\n           \"compile_command\"]\n\ndef softspace(file, newvalue):\n    oldvalue = 0\n    try:\n        oldvalue = file.softspace\n    except AttributeError:\n        pass\n    try:\n        file.softspace = newvalue\n    except (AttributeError, TypeError):\n        # \"attribute-less object\" or \"read-only attributes\"\n        pass\n    return oldvalue\n\nclass InteractiveInterpreter:\n    \"\"\"Base class for InteractiveConsole.\n\n    This class deals with parsing and interpreter state (the user's\n    namespace); it doesn't deal with input buffering or prompting or\n    input file naming (the filename is always passed in explicitly).\n\n    \"\"\"\n\n    def __init__(self, locals=None):\n        \"\"\"Constructor.\n\n        The optional 'locals' argument specifies the dictionary in\n        which code will be executed; it defaults to a newly created\n        dictionary with key \"__name__\" set to \"__console__\" and key\n        \"__doc__\" set to None.\n\n        \"\"\"\n        if locals is None:\n            locals = {\"__name__\": \"__console__\", \"__doc__\": None}\n        self.locals = locals\n        self.compile = CommandCompiler()\n\n    def runsource(self, source, filename=\"<input>\", symbol=\"single\"):\n        \"\"\"Compile and run some source in the interpreter.\n\n        Arguments are as for compile_command().\n\n        One several things can happen:\n\n        1) The input is incorrect; compile_command() raised an\n        exception (SyntaxError or OverflowError).  A syntax traceback\n        will be printed by calling the showsyntaxerror() method.\n\n        2) The input is incomplete, and more input is required;\n        compile_command() returned None.  Nothing happens.\n\n        3) The input is complete; compile_command() returned a code\n        object.  The code is executed by calling self.runcode() (which\n        also handles run-time exceptions, except for SystemExit).\n\n        The return value is True in case 2, False in the other cases (unless\n        an exception is raised).  The return value can be used to\n        decide whether to use sys.ps1 or sys.ps2 to prompt the next\n        line.\n\n        \"\"\"\n        try:\n            code = self.compile(source, filename, symbol)\n        except (OverflowError, SyntaxError, ValueError):\n            # Case 1\n            self.showsyntaxerror(filename)\n            return False\n\n        if code is None:\n            # Case 2\n            return True\n\n        # Case 3\n        self.runcode(code)\n        return False\n\n    def runcode(self, code):\n        \"\"\"Execute a code object.\n\n        When an exception occurs, self.showtraceback() is called to\n        display a traceback.  All exceptions are caught except\n        SystemExit, which is reraised.\n\n        A note about KeyboardInterrupt: this exception may occur\n        elsewhere in this code, and may not always be caught.  The\n        caller should be prepared to deal with it.\n\n        \"\"\"\n        try:\n            exec code in self.locals\n        except SystemExit:\n            raise\n        except:\n            self.showtraceback()\n        else:\n            if softspace(sys.stdout, 0):\n                print\n\n    def showsyntaxerror(self, filename=None):\n        \"\"\"Display the syntax error that just occurred.\n\n        This doesn't display a stack trace because there isn't one.\n\n        If a filename is given, it is stuffed in the exception instead\n        of what was there before (because Python's parser always uses\n        \"<string>\" when reading from a string).\n\n        The output is written by self.write(), below.\n\n        \"\"\"\n        type, value, sys.last_traceback = sys.exc_info()\n        sys.last_type = type\n        sys.last_value = value\n        if filename and type is SyntaxError:\n            # Work hard to stuff the correct filename in the exception\n            try:\n                msg, (dummy_filename, lineno, offset, line) = value\n            except:\n                # Not the format we expect; leave it alone\n                pass\n            else:\n                # Stuff in the right filename\n                value = SyntaxError(msg, (filename, lineno, offset, line))\n                sys.last_value = value\n        list = traceback.format_exception_only(type, value)\n        map(self.write, list)\n\n    def showtraceback(self):\n        \"\"\"Display the exception that just occurred.\n\n        We remove the first stack item because it is our own code.\n\n        The output is written by self.write(), below.\n\n        \"\"\"\n        try:\n            type, value, tb = sys.exc_info()\n            sys.last_type = type\n            sys.last_value = value\n            sys.last_traceback = tb\n            tblist = traceback.extract_tb(tb)\n            del tblist[:1]\n            list = traceback.format_list(tblist)\n            if list:\n                list.insert(0, \"Traceback (most recent call last):\\n\")\n            list[len(list):] = traceback.format_exception_only(type, value)\n        finally:\n            tblist = tb = None\n        map(self.write, list)\n\n    def write(self, data):\n        \"\"\"Write a string.\n\n        The base implementation writes to sys.stderr; a subclass may\n        replace this with a different implementation.\n\n        \"\"\"\n        sys.stderr.write(data)\n\n\nclass InteractiveConsole(InteractiveInterpreter):\n    \"\"\"Closely emulate the behavior of the interactive Python interpreter.\n\n    This class builds on InteractiveInterpreter and adds prompting\n    using the familiar sys.ps1 and sys.ps2, and input buffering.\n\n    \"\"\"\n\n    def __init__(self, locals=None, filename=\"<console>\"):\n        \"\"\"Constructor.\n\n        The optional locals argument will be passed to the\n        InteractiveInterpreter base class.\n\n        The optional filename argument should specify the (file)name\n        of the input stream; it will show up in tracebacks.\n\n        \"\"\"\n        InteractiveInterpreter.__init__(self, locals)\n        self.filename = filename\n        self.resetbuffer()\n\n    def resetbuffer(self):\n        \"\"\"Reset the input buffer.\"\"\"\n        self.buffer = []\n\n    def interact(self, banner=None):\n        \"\"\"Closely emulate the interactive Python console.\n\n        The optional banner argument specify the banner to print\n        before the first interaction; by default it prints a banner\n        similar to the one printed by the real Python interpreter,\n        followed by the current class name in parentheses (so as not\n        to confuse this with the real interpreter -- since it's so\n        close!).\n\n        \"\"\"\n        try:\n            sys.ps1\n        except AttributeError:\n            sys.ps1 = \">>> \"\n        try:\n            sys.ps2\n        except AttributeError:\n            sys.ps2 = \"... \"\n        cprt = 'Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.'\n        if banner is None:\n            self.write(\"Python %s on %s\\n%s\\n(%s)\\n\" %\n                       (sys.version, sys.platform, cprt,\n                        self.__class__.__name__))\n        else:\n            self.write(\"%s\\n\" % str(banner))\n        more = 0\n        while 1:\n            try:\n                if more:\n                    prompt = sys.ps2\n                else:\n                    prompt = sys.ps1\n                try:\n                    line = self.raw_input(prompt)\n                    # Can be None if sys.stdin was redefined\n                    encoding = getattr(sys.stdin, \"encoding\", None)\n                    if encoding and not isinstance(line, unicode):\n                        line = line.decode(encoding)\n                except EOFError:\n                    self.write(\"\\n\")\n                    break\n                else:\n                    more = self.push(line)\n            except KeyboardInterrupt:\n                self.write(\"\\nKeyboardInterrupt\\n\")\n                self.resetbuffer()\n                more = 0\n\n    def push(self, line):\n        \"\"\"Push a line to the interpreter.\n\n        The line should not have a trailing newline; it may have\n        internal newlines.  The line is appended to a buffer and the\n        interpreter's runsource() method is called with the\n        concatenated contents of the buffer as source.  If this\n        indicates that the command was executed or invalid, the buffer\n        is reset; otherwise, the command is incomplete, and the buffer\n        is left as it was after the line was appended.  The return\n        value is 1 if more input is required, 0 if the line was dealt\n        with in some way (this is the same as runsource()).\n\n        \"\"\"\n        self.buffer.append(line)\n        source = \"\\n\".join(self.buffer)\n        more = self.runsource(source, self.filename)\n        if not more:\n            self.resetbuffer()\n        return more\n\n    def raw_input(self, prompt=\"\"):\n        \"\"\"Write a prompt and read a line.\n\n        The returned line does not include the trailing newline.\n        When the user enters the EOF key sequence, EOFError is raised.\n\n        The base implementation uses the built-in function\n        raw_input(); a subclass may replace this with a different\n        implementation.\n\n        \"\"\"\n        return raw_input(prompt)\n\n\ndef interact(banner=None, readfunc=None, local=None):\n    \"\"\"Closely emulate the interactive Python interpreter.\n\n    This is a backwards compatible interface to the InteractiveConsole\n    class.  When readfunc is not specified, it attempts to import the\n    readline module to enable GNU readline if it is available.\n\n    Arguments (all optional, all default to None):\n\n    banner -- passed to InteractiveConsole.interact()\n    readfunc -- if not None, replaces InteractiveConsole.raw_input()\n    local -- passed to InteractiveInterpreter.__init__()\n\n    \"\"\"\n    console = InteractiveConsole(local)\n    if readfunc is not None:\n        console.raw_input = readfunc\n    else:\n        try:\n            import readline\n        except ImportError:\n            pass\n    console.interact(banner)\n\n\nif __name__ == \"__main__\":\n    interact()\n", 
    "codecs": "\"\"\" codecs -- Python Codec Registry, API and helpers.\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"#\"\n\nimport __builtin__, sys\n\n### Registry and builtin stateless codec functions\n\ntry:\n    from _codecs import *\nexcept ImportError, why:\n    raise SystemError('Failed to load the builtin codecs: %s' % why)\n\n__all__ = [\"register\", \"lookup\", \"open\", \"EncodedFile\", \"BOM\", \"BOM_BE\",\n           \"BOM_LE\", \"BOM32_BE\", \"BOM32_LE\", \"BOM64_BE\", \"BOM64_LE\",\n           \"BOM_UTF8\", \"BOM_UTF16\", \"BOM_UTF16_LE\", \"BOM_UTF16_BE\",\n           \"BOM_UTF32\", \"BOM_UTF32_LE\", \"BOM_UTF32_BE\",\n           \"strict_errors\", \"ignore_errors\", \"replace_errors\",\n           \"xmlcharrefreplace_errors\",\n           \"register_error\", \"lookup_error\"]\n\n### Constants\n\n#\n# Byte Order Mark (BOM = ZERO WIDTH NO-BREAK SPACE = U+FEFF)\n# and its possible byte string values\n# for UTF8/UTF16/UTF32 output and little/big endian machines\n#\n\n# UTF-8\nBOM_UTF8 = '\\xef\\xbb\\xbf'\n\n# UTF-16, little endian\nBOM_LE = BOM_UTF16_LE = '\\xff\\xfe'\n\n# UTF-16, big endian\nBOM_BE = BOM_UTF16_BE = '\\xfe\\xff'\n\n# UTF-32, little endian\nBOM_UTF32_LE = '\\xff\\xfe\\x00\\x00'\n\n# UTF-32, big endian\nBOM_UTF32_BE = '\\x00\\x00\\xfe\\xff'\n\nif sys.byteorder == 'little':\n\n    # UTF-16, native endianness\n    BOM = BOM_UTF16 = BOM_UTF16_LE\n\n    # UTF-32, native endianness\n    BOM_UTF32 = BOM_UTF32_LE\n\nelse:\n\n    # UTF-16, native endianness\n    BOM = BOM_UTF16 = BOM_UTF16_BE\n\n    # UTF-32, native endianness\n    BOM_UTF32 = BOM_UTF32_BE\n\n# Old broken names (don't use in new code)\nBOM32_LE = BOM_UTF16_LE\nBOM32_BE = BOM_UTF16_BE\nBOM64_LE = BOM_UTF32_LE\nBOM64_BE = BOM_UTF32_BE\n\n\n### Codec base classes (defining the API)\n\nclass CodecInfo(tuple):\n\n    def __new__(cls, encode, decode, streamreader=None, streamwriter=None,\n        incrementalencoder=None, incrementaldecoder=None, name=None):\n        self = tuple.__new__(cls, (encode, decode, streamreader, streamwriter))\n        self.name = name\n        self.encode = encode\n        self.decode = decode\n        self.incrementalencoder = incrementalencoder\n        self.incrementaldecoder = incrementaldecoder\n        self.streamwriter = streamwriter\n        self.streamreader = streamreader\n        return self\n\n    def __repr__(self):\n        return \"<%s.%s object for encoding %s at 0x%x>\" % (self.__class__.__module__, self.__class__.__name__, self.name, id(self))\n\nclass Codec:\n\n    \"\"\" Defines the interface for stateless encoders/decoders.\n\n        The .encode()/.decode() methods may use different error\n        handling schemes by providing the errors argument. These\n        string values are predefined:\n\n         'strict' - raise a ValueError error (or a subclass)\n         'ignore' - ignore the character and continue with the next\n         'replace' - replace with a suitable replacement character;\n                    Python will use the official U+FFFD REPLACEMENT\n                    CHARACTER for the builtin Unicode codecs on\n                    decoding and '?' on encoding.\n         'xmlcharrefreplace' - Replace with the appropriate XML\n                               character reference (only for encoding).\n         'backslashreplace'  - Replace with backslashed escape sequences\n                               (only for encoding).\n\n        The set of allowed values can be extended via register_error.\n\n    \"\"\"\n    def encode(self, input, errors='strict'):\n\n        \"\"\" Encodes the object input and returns a tuple (output\n            object, length consumed).\n\n            errors defines the error handling to apply. It defaults to\n            'strict' handling.\n\n            The method may not store state in the Codec instance. Use\n            StreamCodec for codecs which have to keep state in order to\n            make encoding/decoding efficient.\n\n            The encoder must be able to handle zero length input and\n            return an empty object of the output object type in this\n            situation.\n\n        \"\"\"\n        raise NotImplementedError\n\n    def decode(self, input, errors='strict'):\n\n        \"\"\" Decodes the object input and returns a tuple (output\n            object, length consumed).\n\n            input must be an object which provides the bf_getreadbuf\n            buffer slot. Python strings, buffer objects and memory\n            mapped files are examples of objects providing this slot.\n\n            errors defines the error handling to apply. It defaults to\n            'strict' handling.\n\n            The method may not store state in the Codec instance. Use\n            StreamCodec for codecs which have to keep state in order to\n            make encoding/decoding efficient.\n\n            The decoder must be able to handle zero length input and\n            return an empty object of the output object type in this\n            situation.\n\n        \"\"\"\n        raise NotImplementedError\n\nclass IncrementalEncoder(object):\n    \"\"\"\n    An IncrementalEncoder encodes an input in multiple steps. The input can be\n    passed piece by piece to the encode() method. The IncrementalEncoder remembers\n    the state of the Encoding process between calls to encode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        \"\"\"\n        Creates an IncrementalEncoder instance.\n\n        The IncrementalEncoder may use different error handling schemes by\n        providing the errors keyword argument. See the module docstring\n        for a list of possible values.\n        \"\"\"\n        self.errors = errors\n        self.buffer = \"\"\n\n    def encode(self, input, final=False):\n        \"\"\"\n        Encodes input and returns the resulting object.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"\n        Resets the encoder to the initial state.\n        \"\"\"\n\n    def getstate(self):\n        \"\"\"\n        Return the current state of the encoder.\n        \"\"\"\n        return 0\n\n    def setstate(self, state):\n        \"\"\"\n        Set the current state of the encoder. state must have been\n        returned by getstate().\n        \"\"\"\n\nclass BufferedIncrementalEncoder(IncrementalEncoder):\n    \"\"\"\n    This subclass of IncrementalEncoder can be used as the baseclass for an\n    incremental encoder if the encoder must keep some of the output in a\n    buffer between calls to encode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        IncrementalEncoder.__init__(self, errors)\n        self.buffer = \"\" # unencoded input that is kept between calls to encode()\n\n    def _buffer_encode(self, input, errors, final):\n        # Overwrite this method in subclasses: It must encode input\n        # and return an (output, length consumed) tuple\n        raise NotImplementedError\n\n    def encode(self, input, final=False):\n        # encode input (taking the buffer into account)\n        data = self.buffer + input\n        (result, consumed) = self._buffer_encode(data, self.errors, final)\n        # keep unencoded input until the next call\n        self.buffer = data[consumed:]\n        return result\n\n    def reset(self):\n        IncrementalEncoder.reset(self)\n        self.buffer = \"\"\n\n    def getstate(self):\n        return self.buffer or 0\n\n    def setstate(self, state):\n        self.buffer = state or \"\"\n\nclass IncrementalDecoder(object):\n    \"\"\"\n    An IncrementalDecoder decodes an input in multiple steps. The input can be\n    passed piece by piece to the decode() method. The IncrementalDecoder\n    remembers the state of the decoding process between calls to decode().\n    \"\"\"\n    def __init__(self, errors='strict'):\n        \"\"\"\n        Creates a IncrementalDecoder instance.\n\n        The IncrementalDecoder may use different error handling schemes by\n        providing the errors keyword argument. See the module docstring\n        for a list of possible values.\n        \"\"\"\n        self.errors = errors\n\n    def decode(self, input, final=False):\n        \"\"\"\n        Decodes input and returns the resulting object.\n        \"\"\"\n        raise NotImplementedError\n\n    def reset(self):\n        \"\"\"\n        Resets the decoder to the initial state.\n        \"\"\"\n\n    def getstate(self):\n        \"\"\"\n        Return the current state of the decoder.\n\n        This must be a (buffered_input, additional_state_info) tuple.\n        buffered_input must be a bytes object containing bytes that\n        were passed to decode() that have not yet been converted.\n        additional_state_info must be a non-negative integer\n        representing the state of the decoder WITHOUT yet having\n        processed the contents of buffered_input.  In the initial state\n        and after reset(), getstate() must return (b\"\", 0).\n        \"\"\"\n        return (b\"\", 0)\n\n    def setstate(self, state):\n        \"\"\"\n        Set the current state of the decoder.\n\n        state must have been returned by getstate().  The effect of\n        setstate((b\"\", 0)) must be equivalent to reset().\n        \"\"\"\n\nclass BufferedIncrementalDecoder(IncrementalDecoder):\n    \"\"\"\n    This subclass of IncrementalDecoder can be used as the baseclass for an\n    incremental decoder if the decoder must be able to handle incomplete byte\n    sequences.\n    \"\"\"\n    def __init__(self, errors='strict'):\n        IncrementalDecoder.__init__(self, errors)\n        self.buffer = \"\" # undecoded input that is kept between calls to decode()\n\n    def _buffer_decode(self, input, errors, final):\n        # Overwrite this method in subclasses: It must decode input\n        # and return an (output, length consumed) tuple\n        raise NotImplementedError\n\n    def decode(self, input, final=False):\n        # decode input (taking the buffer into account)\n        data = self.buffer + input\n        (result, consumed) = self._buffer_decode(data, self.errors, final)\n        # keep undecoded input until the next call\n        self.buffer = data[consumed:]\n        return result\n\n    def reset(self):\n        IncrementalDecoder.reset(self)\n        self.buffer = \"\"\n\n    def getstate(self):\n        # additional state info is always 0\n        return (self.buffer, 0)\n\n    def setstate(self, state):\n        # ignore additional state info\n        self.buffer = state[0]\n\n#\n# The StreamWriter and StreamReader class provide generic working\n# interfaces which can be used to implement new encoding submodules\n# very easily. See encodings/utf_8.py for an example on how this is\n# done.\n#\n\nclass StreamWriter(Codec):\n\n    def __init__(self, stream, errors='strict'):\n\n        \"\"\" Creates a StreamWriter instance.\n\n            stream must be a file-like object open for writing\n            (binary) data.\n\n            The StreamWriter may use different error handling\n            schemes by providing the errors keyword argument. These\n            parameters are predefined:\n\n             'strict' - raise a ValueError (or a subclass)\n             'ignore' - ignore the character and continue with the next\n             'replace'- replace with a suitable replacement character\n             'xmlcharrefreplace' - Replace with the appropriate XML\n                                   character reference.\n             'backslashreplace'  - Replace with backslashed escape\n                                   sequences (only for encoding).\n\n            The set of allowed parameter values can be extended via\n            register_error.\n        \"\"\"\n        self.stream = stream\n        self.errors = errors\n\n    def write(self, object):\n\n        \"\"\" Writes the object's contents encoded to self.stream.\n        \"\"\"\n        data, consumed = self.encode(object, self.errors)\n        self.stream.write(data)\n\n    def writelines(self, list):\n\n        \"\"\" Writes the concatenated list of strings to the stream\n            using .write().\n        \"\"\"\n        self.write(''.join(list))\n\n    def reset(self):\n\n        \"\"\" Flushes and resets the codec buffers used for keeping state.\n\n            Calling this method should ensure that the data on the\n            output is put into a clean state, that allows appending\n            of new fresh data without having to rescan the whole\n            stream to recover state.\n\n        \"\"\"\n        pass\n\n    def seek(self, offset, whence=0):\n        self.stream.seek(offset, whence)\n        if whence == 0 and offset == 0:\n            self.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamReader(Codec):\n\n    def __init__(self, stream, errors='strict'):\n\n        \"\"\" Creates a StreamReader instance.\n\n            stream must be a file-like object open for reading\n            (binary) data.\n\n            The StreamReader may use different error handling\n            schemes by providing the errors keyword argument. These\n            parameters are predefined:\n\n             'strict' - raise a ValueError (or a subclass)\n             'ignore' - ignore the character and continue with the next\n             'replace'- replace with a suitable replacement character;\n\n            The set of allowed parameter values can be extended via\n            register_error.\n        \"\"\"\n        self.stream = stream\n        self.errors = errors\n        self.bytebuffer = \"\"\n        # For str->str decoding this will stay a str\n        # For str->unicode decoding the first read will promote it to unicode\n        self.charbuffer = \"\"\n        self.linebuffer = None\n\n    def decode(self, input, errors='strict'):\n        raise NotImplementedError\n\n    def read(self, size=-1, chars=-1, firstline=False):\n\n        \"\"\" Decodes data from the stream self.stream and returns the\n            resulting object.\n\n            chars indicates the number of characters to read from the\n            stream. read() will never return more than chars\n            characters, but it might return less, if there are not enough\n            characters available.\n\n            size indicates the approximate maximum number of bytes to\n            read from the stream for decoding purposes. The decoder\n            can modify this setting as appropriate. The default value\n            -1 indicates to read and decode as much as possible.  size\n            is intended to prevent having to decode huge files in one\n            step.\n\n            If firstline is true, and a UnicodeDecodeError happens\n            after the first line terminator in the input only the first line\n            will be returned, the rest of the input will be kept until the\n            next call to read().\n\n            The method should use a greedy read strategy meaning that\n            it should read as much data as is allowed within the\n            definition of the encoding and the given size, e.g.  if\n            optional encoding endings or state markers are available\n            on the stream, these should be read too.\n        \"\"\"\n        # If we have lines cached, first merge them back into characters\n        if self.linebuffer:\n            self.charbuffer = \"\".join(self.linebuffer)\n            self.linebuffer = None\n\n        # read until we get the required number of characters (if available)\n        while True:\n            # can the request be satisfied from the character buffer?\n            if chars >= 0:\n                if len(self.charbuffer) >= chars:\n                    break\n            elif size >= 0:\n                if len(self.charbuffer) >= size:\n                    break\n            # we need more data\n            if size < 0:\n                newdata = self.stream.read()\n            else:\n                newdata = self.stream.read(size)\n            # decode bytes (those remaining from the last call included)\n            data = self.bytebuffer + newdata\n            try:\n                newchars, decodedbytes = self.decode(data, self.errors)\n            except UnicodeDecodeError, exc:\n                if firstline:\n                    newchars, decodedbytes = self.decode(data[:exc.start], self.errors)\n                    lines = newchars.splitlines(True)\n                    if len(lines)<=1:\n                        raise\n                else:\n                    raise\n            # keep undecoded bytes until the next call\n            self.bytebuffer = data[decodedbytes:]\n            # put new characters in the character buffer\n            self.charbuffer += newchars\n            # there was no data available\n            if not newdata:\n                break\n        if chars < 0:\n            # Return everything we've got\n            result = self.charbuffer\n            self.charbuffer = \"\"\n        else:\n            # Return the first chars characters\n            result = self.charbuffer[:chars]\n            self.charbuffer = self.charbuffer[chars:]\n        return result\n\n    def readline(self, size=None, keepends=True):\n\n        \"\"\" Read one line from the input stream and return the\n            decoded data.\n\n            size, if given, is passed as size argument to the\n            read() method.\n\n        \"\"\"\n        # If we have lines cached from an earlier read, return\n        # them unconditionally\n        if self.linebuffer:\n            line = self.linebuffer[0]\n            del self.linebuffer[0]\n            if len(self.linebuffer) == 1:\n                # revert to charbuffer mode; we might need more data\n                # next time\n                self.charbuffer = self.linebuffer[0]\n                self.linebuffer = None\n            if not keepends:\n                line = line.splitlines(False)[0]\n            return line\n\n        readsize = size or 72\n        line = \"\"\n        # If size is given, we call read() only once\n        while True:\n            data = self.read(readsize, firstline=True)\n            if data:\n                # If we're at a \"\\r\" read one extra character (which might\n                # be a \"\\n\") to get a proper line ending. If the stream is\n                # temporarily exhausted we return the wrong line ending.\n                if data.endswith(\"\\r\"):\n                    data += self.read(size=1, chars=1)\n\n            line += data\n            lines = line.splitlines(True)\n            if lines:\n                if len(lines) > 1:\n                    # More than one line result; the first line is a full line\n                    # to return\n                    line = lines[0]\n                    del lines[0]\n                    if len(lines) > 1:\n                        # cache the remaining lines\n                        lines[-1] += self.charbuffer\n                        self.linebuffer = lines\n                        self.charbuffer = None\n                    else:\n                        # only one remaining line, put it back into charbuffer\n                        self.charbuffer = lines[0] + self.charbuffer\n                    if not keepends:\n                        line = line.splitlines(False)[0]\n                    break\n                line0withend = lines[0]\n                line0withoutend = lines[0].splitlines(False)[0]\n                if line0withend != line0withoutend: # We really have a line end\n                    # Put the rest back together and keep it until the next call\n                    self.charbuffer = \"\".join(lines[1:]) + self.charbuffer\n                    if keepends:\n                        line = line0withend\n                    else:\n                        line = line0withoutend\n                    break\n            # we didn't get anything or this was our only try\n            if not data or size is not None:\n                if line and not keepends:\n                    line = line.splitlines(False)[0]\n                break\n            if readsize<8000:\n                readsize *= 2\n        return line\n\n    def readlines(self, sizehint=None, keepends=True):\n\n        \"\"\" Read all lines available on the input stream\n            and return them as list of lines.\n\n            Line breaks are implemented using the codec's decoder\n            method and are included in the list entries.\n\n            sizehint, if given, is ignored since there is no efficient\n            way to finding the true end-of-line.\n\n        \"\"\"\n        data = self.read()\n        return data.splitlines(keepends)\n\n    def reset(self):\n\n        \"\"\" Resets the codec buffers used for keeping state.\n\n            Note that no stream repositioning should take place.\n            This method is primarily intended to be able to recover\n            from decoding errors.\n\n        \"\"\"\n        self.bytebuffer = \"\"\n        self.charbuffer = u\"\"\n        self.linebuffer = None\n\n    def seek(self, offset, whence=0):\n        \"\"\" Set the input stream's current position.\n\n            Resets the codec buffers used for keeping state.\n        \"\"\"\n        self.stream.seek(offset, whence)\n        self.reset()\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        line = self.readline()\n        if line:\n            return line\n        raise StopIteration\n\n    def __iter__(self):\n        return self\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamReaderWriter:\n\n    \"\"\" StreamReaderWriter instances allow wrapping streams which\n        work in both read and write modes.\n\n        The design is such that one can use the factory functions\n        returned by the codec.lookup() function to construct the\n        instance.\n\n    \"\"\"\n    # Optional attributes set by the file wrappers below\n    encoding = 'unknown'\n\n    def __init__(self, stream, Reader, Writer, errors='strict'):\n\n        \"\"\" Creates a StreamReaderWriter instance.\n\n            stream must be a Stream-like object.\n\n            Reader, Writer must be factory functions or classes\n            providing the StreamReader, StreamWriter interface resp.\n\n            Error handling is done in the same way as defined for the\n            StreamWriter/Readers.\n\n        \"\"\"\n        self.stream = stream\n        self.reader = Reader(stream, errors)\n        self.writer = Writer(stream, errors)\n        self.errors = errors\n\n    def read(self, size=-1):\n\n        return self.reader.read(size)\n\n    def readline(self, size=None):\n\n        return self.reader.readline(size)\n\n    def readlines(self, sizehint=None):\n\n        return self.reader.readlines(sizehint)\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        return self.reader.next()\n\n    def __iter__(self):\n        return self\n\n    def write(self, data):\n\n        return self.writer.write(data)\n\n    def writelines(self, list):\n\n        return self.writer.writelines(list)\n\n    def reset(self):\n\n        self.reader.reset()\n        self.writer.reset()\n\n    def seek(self, offset, whence=0):\n        self.stream.seek(offset, whence)\n        self.reader.reset()\n        if whence == 0 and offset == 0:\n            self.writer.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    # these are needed to make \"with codecs.open(...)\" work properly\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n###\n\nclass StreamRecoder:\n\n    \"\"\" StreamRecoder instances provide a frontend - backend\n        view of encoding data.\n\n        They use the complete set of APIs returned by the\n        codecs.lookup() function to implement their task.\n\n        Data written to the stream is first decoded into an\n        intermediate format (which is dependent on the given codec\n        combination) and then written to the stream using an instance\n        of the provided Writer class.\n\n        In the other direction, data is read from the stream using a\n        Reader instance and then return encoded data to the caller.\n\n    \"\"\"\n    # Optional attributes set by the file wrappers below\n    data_encoding = 'unknown'\n    file_encoding = 'unknown'\n\n    def __init__(self, stream, encode, decode, Reader, Writer,\n                 errors='strict'):\n\n        \"\"\" Creates a StreamRecoder instance which implements a two-way\n            conversion: encode and decode work on the frontend (the\n            input to .read() and output of .write()) while\n            Reader and Writer work on the backend (reading and\n            writing to the stream).\n\n            You can use these objects to do transparent direct\n            recodings from e.g. latin-1 to utf-8 and back.\n\n            stream must be a file-like object.\n\n            encode, decode must adhere to the Codec interface, Reader,\n            Writer must be factory functions or classes providing the\n            StreamReader, StreamWriter interface resp.\n\n            encode and decode are needed for the frontend translation,\n            Reader and Writer for the backend translation. Unicode is\n            used as intermediate encoding.\n\n            Error handling is done in the same way as defined for the\n            StreamWriter/Readers.\n\n        \"\"\"\n        self.stream = stream\n        self.encode = encode\n        self.decode = decode\n        self.reader = Reader(stream, errors)\n        self.writer = Writer(stream, errors)\n        self.errors = errors\n\n    def read(self, size=-1):\n\n        data = self.reader.read(size)\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def readline(self, size=None):\n\n        if size is None:\n            data = self.reader.readline()\n        else:\n            data = self.reader.readline(size)\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def readlines(self, sizehint=None):\n\n        data = self.reader.read()\n        data, bytesencoded = self.encode(data, self.errors)\n        return data.splitlines(1)\n\n    def next(self):\n\n        \"\"\" Return the next decoded line from the input stream.\"\"\"\n        data = self.reader.next()\n        data, bytesencoded = self.encode(data, self.errors)\n        return data\n\n    def __iter__(self):\n        return self\n\n    def write(self, data):\n\n        data, bytesdecoded = self.decode(data, self.errors)\n        return self.writer.write(data)\n\n    def writelines(self, list):\n\n        data = ''.join(list)\n        data, bytesdecoded = self.decode(data, self.errors)\n        return self.writer.write(data)\n\n    def reset(self):\n\n        self.reader.reset()\n        self.writer.reset()\n\n    def __getattr__(self, name,\n                    getattr=getattr):\n\n        \"\"\" Inherit all other methods from the underlying stream.\n        \"\"\"\n        return getattr(self.stream, name)\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, tb):\n        self.stream.close()\n\n### Shortcuts\n\ndef open(filename, mode='rb', encoding=None, errors='strict', buffering=1):\n\n    \"\"\" Open an encoded file using the given mode and return\n        a wrapped version providing transparent encoding/decoding.\n\n        Note: The wrapped version will only accept the object format\n        defined by the codecs, i.e. Unicode objects for most builtin\n        codecs. Output is also codec dependent and will usually be\n        Unicode as well.\n\n        Files are always opened in binary mode, even if no binary mode\n        was specified. This is done to avoid data loss due to encodings\n        using 8-bit values. The default file mode is 'rb' meaning to\n        open the file in binary read mode.\n\n        encoding specifies the encoding which is to be used for the\n        file.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n\n        buffering has the same meaning as for the builtin open() API.\n        It defaults to line buffered.\n\n        The returned wrapped file object provides an extra attribute\n        .encoding which allows querying the used encoding. This\n        attribute is only available if an encoding was specified as\n        parameter.\n\n    \"\"\"\n    if encoding is not None:\n        if 'U' in mode:\n            # No automatic conversion of '\\n' is done on reading and writing\n            mode = mode.strip().replace('U', '')\n            if mode[:1] not in set('rwa'):\n                mode = 'r' + mode\n        if 'b' not in mode:\n            # Force opening of the file in binary mode\n            mode = mode + 'b'\n    file = __builtin__.open(filename, mode, buffering)\n    if encoding is None:\n        return file\n    info = lookup(encoding)\n    srw = StreamReaderWriter(file, info.streamreader, info.streamwriter, errors)\n    # Add attributes to simplify introspection\n    srw.encoding = encoding\n    return srw\n\ndef EncodedFile(file, data_encoding, file_encoding=None, errors='strict'):\n\n    \"\"\" Return a wrapped version of file which provides transparent\n        encoding translation.\n\n        Strings written to the wrapped file are interpreted according\n        to the given data_encoding and then written to the original\n        file as string using file_encoding. The intermediate encoding\n        will usually be Unicode but depends on the specified codecs.\n\n        Strings are read from the file using file_encoding and then\n        passed back to the caller as string using data_encoding.\n\n        If file_encoding is not given, it defaults to data_encoding.\n\n        errors may be given to define the error handling. It defaults\n        to 'strict' which causes ValueErrors to be raised in case an\n        encoding error occurs.\n\n        The returned wrapped file object provides two extra attributes\n        .data_encoding and .file_encoding which reflect the given\n        parameters of the same name. The attributes can be used for\n        introspection by Python programs.\n\n    \"\"\"\n    if file_encoding is None:\n        file_encoding = data_encoding\n    data_info = lookup(data_encoding)\n    file_info = lookup(file_encoding)\n    sr = StreamRecoder(file, data_info.encode, data_info.decode,\n                       file_info.streamreader, file_info.streamwriter, errors)\n    # Add attributes to simplify introspection\n    sr.data_encoding = data_encoding\n    sr.file_encoding = file_encoding\n    return sr\n\n### Helpers for codec lookup\n\ndef getencoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its encoder function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).encode\n\ndef getdecoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its decoder function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).decode\n\ndef getincrementalencoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its IncrementalEncoder class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found\n        or the codecs doesn't provide an incremental encoder.\n\n    \"\"\"\n    encoder = lookup(encoding).incrementalencoder\n    if encoder is None:\n        raise LookupError(encoding)\n    return encoder\n\ndef getincrementaldecoder(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its IncrementalDecoder class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found\n        or the codecs doesn't provide an incremental decoder.\n\n    \"\"\"\n    decoder = lookup(encoding).incrementaldecoder\n    if decoder is None:\n        raise LookupError(encoding)\n    return decoder\n\ndef getreader(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its StreamReader class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).streamreader\n\ndef getwriter(encoding):\n\n    \"\"\" Lookup up the codec for the given encoding and return\n        its StreamWriter class or factory function.\n\n        Raises a LookupError in case the encoding cannot be found.\n\n    \"\"\"\n    return lookup(encoding).streamwriter\n\ndef iterencode(iterator, encoding, errors='strict', **kwargs):\n    \"\"\"\n    Encoding iterator.\n\n    Encodes the input strings from the iterator using a IncrementalEncoder.\n\n    errors and kwargs are passed through to the IncrementalEncoder\n    constructor.\n    \"\"\"\n    encoder = getincrementalencoder(encoding)(errors, **kwargs)\n    for input in iterator:\n        output = encoder.encode(input)\n        if output:\n            yield output\n    output = encoder.encode(\"\", True)\n    if output:\n        yield output\n\ndef iterdecode(iterator, encoding, errors='strict', **kwargs):\n    \"\"\"\n    Decoding iterator.\n\n    Decodes the input strings from the iterator using a IncrementalDecoder.\n\n    errors and kwargs are passed through to the IncrementalDecoder\n    constructor.\n    \"\"\"\n    decoder = getincrementaldecoder(encoding)(errors, **kwargs)\n    for input in iterator:\n        output = decoder.decode(input)\n        if output:\n            yield output\n    output = decoder.decode(\"\", True)\n    if output:\n        yield output\n\n### Helpers for charmap-based codecs\n\ndef make_identity_dict(rng):\n\n    \"\"\" make_identity_dict(rng) -> dict\n\n        Return a dictionary where elements of the rng sequence are\n        mapped to themselves.\n\n    \"\"\"\n    res = {}\n    for i in rng:\n        res[i]=i\n    return res\n\ndef make_encoding_map(decoding_map):\n\n    \"\"\" Creates an encoding map from a decoding map.\n\n        If a target mapping in the decoding map occurs multiple\n        times, then that target is mapped to None (undefined mapping),\n        causing an exception when encountered by the charmap codec\n        during translation.\n\n        One example where this happens is cp875.py which decodes\n        multiple character to \\u001a.\n\n    \"\"\"\n    m = {}\n    for k,v in decoding_map.items():\n        if not v in m:\n            m[v] = k\n        else:\n            m[v] = None\n    return m\n\n### error handlers\n\ntry:\n    strict_errors = lookup_error(\"strict\")\n    ignore_errors = lookup_error(\"ignore\")\n    replace_errors = lookup_error(\"replace\")\n    xmlcharrefreplace_errors = lookup_error(\"xmlcharrefreplace\")\n    backslashreplace_errors = lookup_error(\"backslashreplace\")\nexcept LookupError:\n    # In --disable-unicode builds, these error handler are missing\n    strict_errors = None\n    ignore_errors = None\n    replace_errors = None\n    xmlcharrefreplace_errors = None\n    backslashreplace_errors = None\n\n# Tell modulefinder that using codecs probably needs the encodings\n# package\n_false = 0\nif _false:\n    import encodings\n\n### Tests\n\nif __name__ == '__main__':\n\n    # Make stdout translate Latin-1 output into UTF-8 output\n    sys.stdout = EncodedFile(sys.stdout, 'latin-1', 'utf-8')\n\n    # Have stdin translate Latin-1 input into UTF-8 input\n    sys.stdin = EncodedFile(sys.stdin, 'utf-8', 'latin-1')\n", 
    "codeop": "r\"\"\"Utilities to compile possibly incomplete Python source code.\n\nThis module provides two interfaces, broadly similar to the builtin\nfunction compile(), which take program text, a filename and a 'mode'\nand:\n\n- Return code object if the command is complete and valid\n- Return None if the command is incomplete\n- Raise SyntaxError, ValueError or OverflowError if the command is a\n  syntax error (OverflowError and ValueError can be produced by\n  malformed literals).\n\nApproach:\n\nFirst, check if the source consists entirely of blank lines and\ncomments; if so, replace it with 'pass', because the built-in\nparser doesn't always do the right thing for these.\n\nCompile three times: as is, with \\n, and with \\n\\n appended.  If it\ncompiles as is, it's complete.  If it compiles with one \\n appended,\nwe expect more.  If it doesn't compile either way, we compare the\nerror we get when compiling with \\n or \\n\\n appended.  If the errors\nare the same, the code is broken.  But if the errors are different, we\nexpect more.  Not intuitive; not even guaranteed to hold in future\nreleases; but this matches the compiler's behavior from Python 1.4\nthrough 2.2, at least.\n\nCaveat:\n\nIt is possible (but not likely) that the parser stops parsing with a\nsuccessful outcome before reaching the end of the source; in this\ncase, trailing symbols may be ignored instead of causing an error.\nFor example, a backslash followed by two newlines may be followed by\narbitrary garbage.  This will be fixed once the API for the parser is\nbetter.\n\nThe two interfaces are:\n\ncompile_command(source, filename, symbol):\n\n    Compiles a single command in the manner described above.\n\nCommandCompiler():\n\n    Instances of this class have __call__ methods identical in\n    signature to compile_command; the difference is that if the\n    instance compiles program text containing a __future__ statement,\n    the instance 'remembers' and compiles all subsequent program texts\n    with the statement in force.\n\nThe module also provides another class:\n\nCompile():\n\n    Instances of this class act like the built-in function compile,\n    but with 'memory' in the sense described above.\n\"\"\"\n\nimport __future__\n\n_features = [getattr(__future__, fname)\n             for fname in __future__.all_feature_names]\n\n__all__ = [\"compile_command\", \"Compile\", \"CommandCompiler\"]\n\nPyCF_DONT_IMPLY_DEDENT = 0x200          # Matches pythonrun.h\n\ndef _maybe_compile(compiler, source, filename, symbol):\n    # Check for source consisting of only blank lines and comments\n    for line in source.split(\"\\n\"):\n        line = line.strip()\n        if line and line[0] != '#':\n            break               # Leave it alone\n    else:\n        if symbol != \"eval\":\n            source = \"pass\"     # Replace it with a 'pass' statement\n\n    err = err1 = err2 = None\n    code = code1 = code2 = None\n\n    try:\n        code = compiler(source, filename, symbol)\n    except SyntaxError, err:\n        pass\n\n    try:\n        code1 = compiler(source + \"\\n\", filename, symbol)\n    except SyntaxError, err1:\n        pass\n\n    try:\n        code2 = compiler(source + \"\\n\\n\", filename, symbol)\n    except SyntaxError, err2:\n        pass\n\n    if code:\n        return code\n    if not code1 and repr(err1) == repr(err2):\n        raise SyntaxError, err1\n\ndef _compile(source, filename, symbol):\n    return compile(source, filename, symbol, PyCF_DONT_IMPLY_DEDENT)\n\ndef compile_command(source, filename=\"<input>\", symbol=\"single\"):\n    r\"\"\"Compile a command and determine whether it is incomplete.\n\n    Arguments:\n\n    source -- the source string; may contain \\n characters\n    filename -- optional filename from which source was read; default\n                \"<input>\"\n    symbol -- optional grammar start symbol; \"single\" (default) or \"eval\"\n\n    Return value / exceptions raised:\n\n    - Return a code object if the command is complete and valid\n    - Return None if the command is incomplete\n    - Raise SyntaxError, ValueError or OverflowError if the command is a\n      syntax error (OverflowError and ValueError can be produced by\n      malformed literals).\n    \"\"\"\n    return _maybe_compile(_compile, source, filename, symbol)\n\nclass Compile:\n    \"\"\"Instances of this class behave much like the built-in compile\n    function, but if one is used to compile text containing a future\n    statement, it \"remembers\" and compiles all subsequent program texts\n    with the statement in force.\"\"\"\n    def __init__(self):\n        self.flags = PyCF_DONT_IMPLY_DEDENT\n\n    def __call__(self, source, filename, symbol):\n        codeob = compile(source, filename, symbol, self.flags, 1)\n        for feature in _features:\n            if codeob.co_flags & feature.compiler_flag:\n                self.flags |= feature.compiler_flag\n        return codeob\n\nclass CommandCompiler:\n    \"\"\"Instances of this class have __call__ methods identical in\n    signature to compile_command; the difference is that if the\n    instance compiles program text containing a __future__ statement,\n    the instance 'remembers' and compiles all subsequent program texts\n    with the statement in force.\"\"\"\n\n    def __init__(self,):\n        self.compiler = Compile()\n\n    def __call__(self, source, filename=\"<input>\", symbol=\"single\"):\n        r\"\"\"Compile a command and determine whether it is incomplete.\n\n        Arguments:\n\n        source -- the source string; may contain \\n characters\n        filename -- optional filename from which source was read;\n                    default \"<input>\"\n        symbol -- optional grammar start symbol; \"single\" (default) or\n                  \"eval\"\n\n        Return value / exceptions raised:\n\n        - Return a code object if the command is complete and valid\n        - Return None if the command is incomplete\n        - Raise SyntaxError, ValueError or OverflowError if the command is a\n          syntax error (OverflowError and ValueError can be produced by\n          malformed literals).\n        \"\"\"\n        return _maybe_compile(self.compiler, source, filename, symbol)\n", 
    "collections": "__all__ = ['Counter', 'deque', 'defaultdict', 'namedtuple', 'OrderedDict']\n# For bootstrapping reasons, the collection ABCs are defined in _abcoll.py.\n# They should however be considered an integral part of collections.py.\nfrom _abcoll import *\nimport _abcoll\n__all__ += _abcoll.__all__\n\nfrom _collections import deque, defaultdict\nfrom operator import itemgetter as _itemgetter, eq as _eq\nfrom keyword import iskeyword as _iskeyword\nimport sys as _sys\nimport heapq as _heapq\nfrom itertools import repeat as _repeat, chain as _chain, starmap as _starmap\nfrom itertools import imap as _imap\ntry:\n    from __pypy__ import newdict\nexcept ImportError:\n    assert '__pypy__' not in _sys.builtin_module_names\n    newdict = lambda _ : {}\ntry:\n    from __pypy__ import reversed_dict\nexcept ImportError:\n    reversed_dict = lambda d: reversed(d.keys())\n\ntry:\n    from thread import get_ident as _get_ident\nexcept ImportError:\n    from dummy_thread import get_ident as _get_ident\n\n\n################################################################################\n### OrderedDict\n################################################################################\n\nclass OrderedDict(dict):\n    '''Dictionary that remembers insertion order.\n\n    In PyPy all dicts are ordered anyway.  This is mostly useful as a\n    placeholder to mean \"this dict must be ordered even on CPython\".\n\n    Known difference: iterating over an OrderedDict which is being\n    concurrently modified raises RuntimeError in PyPy.  In CPython\n    instead we get some behavior that appears reasonable in some\n    cases but is nonsensical in other cases.  This is officially\n    forbidden by the CPython docs, so we forbid it explicitly for now.\n    '''\n\n    def __reversed__(self):\n        return reversed_dict(self)\n\n    def popitem(self, last=True):\n        '''od.popitem() -> (k, v), return and remove a (key, value) pair.\n        Pairs are returned in LIFO order if last is true or FIFO order if false.\n\n        '''\n        if last:\n            return dict.popitem(self)\n        else:\n            it = dict.__iter__(self)\n            try:\n                k = it.next()\n            except StopIteration:\n                raise KeyError('dictionary is empty')\n            return (k, self.pop(k))\n\n    def __repr__(self, _repr_running={}):\n        'od.__repr__() <==> repr(od)'\n        call_key = id(self), _get_ident()\n        if call_key in _repr_running:\n            return '...'\n        _repr_running[call_key] = 1\n        try:\n            if not self:\n                return '%s()' % (self.__class__.__name__,)\n            return '%s(%r)' % (self.__class__.__name__, self.items())\n        finally:\n            del _repr_running[call_key]\n\n    def __reduce__(self):\n        'Return state information for pickling'\n        items = [[k, self[k]] for k in self]\n        inst_dict = vars(self).copy()\n        if inst_dict:\n            return (self.__class__, (items,), inst_dict)\n        return self.__class__, (items,)\n\n    def copy(self):\n        'od.copy() -> a shallow copy of od'\n        return self.__class__(self)\n\n    def __eq__(self, other):\n        '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive\n        while comparison to a regular mapping is order-insensitive.\n\n        '''\n        if isinstance(other, OrderedDict):\n            return dict.__eq__(self, other) and all(_imap(_eq, self, other))\n        return dict.__eq__(self, other)\n\n    def __ne__(self, other):\n        'od.__ne__(y) <==> od!=y'\n        return not self == other\n\n    # -- the following methods support python 3.x style dictionary views --\n\n    def viewkeys(self):\n        \"od.viewkeys() -> a set-like object providing a view on od's keys\"\n        return KeysView(self)\n\n    def viewvalues(self):\n        \"od.viewvalues() -> an object providing a view on od's values\"\n        return ValuesView(self)\n\n    def viewitems(self):\n        \"od.viewitems() -> a set-like object providing a view on od's items\"\n        return ItemsView(self)\n\n\n################################################################################\n### namedtuple\n################################################################################\n\n_class_template = '''\\\nclass {typename}(tuple):\n    '{typename}({arg_list})'\n\n    __slots__ = ()\n\n    _fields = {field_names!r}\n\n    def __new__(_cls, {arg_list}):\n        'Create new instance of {typename}({arg_list})'\n        return _tuple.__new__(_cls, ({arg_list}))\n\n    @classmethod\n    def _make(cls, iterable, new=tuple.__new__, len=len):\n        'Make a new {typename} object from a sequence or iterable'\n        result = new(cls, iterable)\n        if len(result) != {num_fields:d}:\n            raise TypeError('Expected {num_fields:d} arguments, got %d' % len(result))\n        return result\n\n    def __repr__(self):\n        'Return a nicely formatted representation string'\n        return '{typename}({repr_fmt})' % self\n\n    def _asdict(self):\n        'Return a new OrderedDict which maps field names to their values'\n        return OrderedDict(zip(self._fields, self))\n\n    def _replace(_self, **kwds):\n        'Return a new {typename} object replacing specified fields with new values'\n        result = _self._make(map(kwds.pop, {field_names!r}, _self))\n        if kwds:\n            raise ValueError('Got unexpected field names: %r' % kwds.keys())\n        return result\n\n    def __getnewargs__(self):\n        'Return self as a plain tuple.  Used by copy and pickle.'\n        return tuple(self)\n\n    __dict__ = _property(_asdict)\n\n    def __getstate__(self):\n        'Exclude the OrderedDict from pickling'\n        pass\n\n{field_defs}\n'''\n\n_repr_template = '{name}=%r'\n\n_field_template = '''\\\n    {name} = _property(lambda self: self[{index:d}], doc='Alias for field number {index:d}')\n'''\n\ndef namedtuple(typename, field_names, verbose=False, rename=False):\n    \"\"\"Returns a new subclass of tuple with named fields.\n\n    >>> Point = namedtuple('Point', ['x', 'y'])\n    >>> Point.__doc__                   # docstring for the new class\n    'Point(x, y)'\n    >>> p = Point(11, y=22)             # instantiate with positional args or keywords\n    >>> p[0] + p[1]                     # indexable like a plain tuple\n    33\n    >>> x, y = p                        # unpack like a regular tuple\n    >>> x, y\n    (11, 22)\n    >>> p.x + p.y                       # fields also accessable by name\n    33\n    >>> d = p._asdict()                 # convert to a dictionary\n    >>> d['x']\n    11\n    >>> Point(**d)                      # convert from a dictionary\n    Point(x=11, y=22)\n    >>> p._replace(x=100)               # _replace() is like str.replace() but targets named fields\n    Point(x=100, y=22)\n\n    \"\"\"\n\n    # Validate the field names.  At the user's option, either generate an error\n    # message or automatically replace the field name with a valid name.\n    if isinstance(field_names, basestring):\n        field_names = field_names.replace(',', ' ').split()\n    field_names = map(str, field_names)\n    typename = str(typename)\n    if rename:\n        seen = set()\n        for index, name in enumerate(field_names):\n            if (not all(c.isalnum() or c=='_' for c in name)\n                or _iskeyword(name)\n                or not name\n                or name[0].isdigit()\n                or name.startswith('_')\n                or name in seen):\n                field_names[index] = '_%d' % index\n            seen.add(name)\n    for name in [typename] + field_names:\n        if type(name) != str:\n            raise TypeError('Type names and field names must be strings')\n        if not all(c.isalnum() or c=='_' for c in name):\n            raise ValueError('Type names and field names can only contain '\n                             'alphanumeric characters and underscores: %r' % name)\n        if _iskeyword(name):\n            raise ValueError('Type names and field names cannot be a '\n                             'keyword: %r' % name)\n        if name[0].isdigit():\n            raise ValueError('Type names and field names cannot start with '\n                             'a number: %r' % name)\n    seen = set()\n    for name in field_names:\n        if name.startswith('_') and not rename:\n            raise ValueError('Field names cannot start with an underscore: '\n                             '%r' % name)\n        if name in seen:\n            raise ValueError('Encountered duplicate field name: %r' % name)\n        seen.add(name)\n\n    # Fill-in the class template\n    class_definition = _class_template.format(\n        typename = typename,\n        field_names = tuple(field_names),\n        num_fields = len(field_names),\n        arg_list = repr(tuple(field_names)).replace(\"'\", \"\")[1:-1],\n        repr_fmt = ', '.join(_repr_template.format(name=name)\n                             for name in field_names),\n        field_defs = '\\n'.join(_field_template.format(index=index, name=name)\n                               for index, name in enumerate(field_names))\n    )\n    if verbose:\n        print class_definition\n\n    # Execute the template string in a temporary namespace and support\n    # tracing utilities by setting a value for frame.f_globals['__name__']\n    namespace = newdict('module')\n    namespace['__name__'] = 'namedtuple_%s' % typename\n    namespace['OrderedDict'] = OrderedDict\n    namespace['_property'] = property\n    namespace['_tuple'] = tuple\n    try:\n        exec class_definition in namespace\n    except SyntaxError as e:\n        raise SyntaxError(e.message + ':\\n' + class_definition)\n    result = namespace[typename]\n\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the named tuple is created.  Bypass this step in environments where\n    # sys._getframe is not defined (Jython for example) or sys._getframe is not\n    # defined for arguments greater than 0 (IronPython).\n    try:\n        result.__module__ = _sys._getframe(1).f_globals.get('__name__', '__main__')\n    except (AttributeError, ValueError):\n        pass\n\n    return result\n\n\n########################################################################\n###  Counter\n########################################################################\n\nclass Counter(dict):\n    '''Dict subclass for counting hashable items.  Sometimes called a bag\n    or multiset.  Elements are stored as dictionary keys and their counts\n    are stored as dictionary values.\n\n    >>> c = Counter('abcdeabcdabcaba')  # count elements from a string\n\n    >>> c.most_common(3)                # three most common elements\n    [('a', 5), ('b', 4), ('c', 3)]\n    >>> sorted(c)                       # list all unique elements\n    ['a', 'b', 'c', 'd', 'e']\n    >>> ''.join(sorted(c.elements()))   # list elements with repetitions\n    'aaaaabbbbcccdde'\n    >>> sum(c.values())                 # total of all counts\n    15\n\n    >>> c['a']                          # count of letter 'a'\n    5\n    >>> for elem in 'shazam':           # update counts from an iterable\n    ...     c[elem] += 1                # by adding 1 to each element's count\n    >>> c['a']                          # now there are seven 'a'\n    7\n    >>> del c['b']                      # remove all 'b'\n    >>> c['b']                          # now there are zero 'b'\n    0\n\n    >>> d = Counter('simsalabim')       # make another counter\n    >>> c.update(d)                     # add in the second counter\n    >>> c['a']                          # now there are nine 'a'\n    9\n\n    >>> c.clear()                       # empty the counter\n    >>> c\n    Counter()\n\n    Note:  If a count is set to zero or reduced to zero, it will remain\n    in the counter until the entry is deleted or the counter is cleared:\n\n    >>> c = Counter('aaabbc')\n    >>> c['b'] -= 2                     # reduce the count of 'b' by two\n    >>> c.most_common()                 # 'b' is still in, but its count is zero\n    [('a', 3), ('c', 1), ('b', 0)]\n\n    '''\n    # References:\n    #   http://en.wikipedia.org/wiki/Multiset\n    #   http://www.gnu.org/software/smalltalk/manual-base/html_node/Bag.html\n    #   http://www.demo2s.com/Tutorial/Cpp/0380__set-multiset/Catalog0380__set-multiset.htm\n    #   http://code.activestate.com/recipes/259174/\n    #   Knuth, TAOCP Vol. II section 4.6.3\n\n    def __init__(self, iterable=None, **kwds):\n        '''Create a new, empty Counter object.  And if given, count elements\n        from an input iterable.  Or, initialize the count from another mapping\n        of elements to their counts.\n\n        >>> c = Counter()                           # a new, empty counter\n        >>> c = Counter('gallahad')                 # a new counter from an iterable\n        >>> c = Counter({'a': 4, 'b': 2})           # a new counter from a mapping\n        >>> c = Counter(a=4, b=2)                   # a new counter from keyword args\n\n        '''\n        super(Counter, self).__init__()\n        self.update(iterable, **kwds)\n\n    def __missing__(self, key):\n        'The count of elements not in the Counter is zero.'\n        # Needed so that self[missing_item] does not raise KeyError\n        return 0\n\n    def most_common(self, n=None):\n        '''List the n most common elements and their counts from the most\n        common to the least.  If n is None, then list all element counts.\n\n        >>> Counter('abcdeabcdabcaba').most_common(3)\n        [('a', 5), ('b', 4), ('c', 3)]\n\n        '''\n        # Emulate Bag.sortedByCount from Smalltalk\n        if n is None:\n            return sorted(self.iteritems(), key=_itemgetter(1), reverse=True)\n        return _heapq.nlargest(n, self.iteritems(), key=_itemgetter(1))\n\n    def elements(self):\n        '''Iterator over elements repeating each as many times as its count.\n\n        >>> c = Counter('ABCABC')\n        >>> sorted(c.elements())\n        ['A', 'A', 'B', 'B', 'C', 'C']\n\n        # Knuth's example for prime factors of 1836:  2**2 * 3**3 * 17**1\n        >>> prime_factors = Counter({2: 2, 3: 3, 17: 1})\n        >>> product = 1\n        >>> for factor in prime_factors.elements():     # loop over factors\n        ...     product *= factor                       # and multiply them\n        >>> product\n        1836\n\n        Note, if an element's count has been set to zero or is a negative\n        number, elements() will ignore it.\n\n        '''\n        # Emulate Bag.do from Smalltalk and Multiset.begin from C++.\n        return _chain.from_iterable(_starmap(_repeat, self.iteritems()))\n\n    # Override dict methods where necessary\n\n    @classmethod\n    def fromkeys(cls, iterable, v=None):\n        # There is no equivalent method for counters because setting v=1\n        # means that no element can have a count greater than one.\n        raise NotImplementedError(\n            'Counter.fromkeys() is undefined.  Use Counter(iterable) instead.')\n\n    def update(self, iterable=None, **kwds):\n        '''Like dict.update() but add counts instead of replacing them.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.update('witch')           # add elements from another iterable\n        >>> d = Counter('watch')\n        >>> c.update(d)                 # add elements from another counter\n        >>> c['h']                      # four 'h' in which, witch, and watch\n        4\n\n        '''\n        # The regular dict.update() operation makes no sense here because the\n        # replace behavior results in the some of original untouched counts\n        # being mixed-in with all of the other counts for a mismash that\n        # doesn't have a straight-forward interpretation in most counting\n        # contexts.  Instead, we implement straight-addition.  Both the inputs\n        # and outputs are allowed to contain zero and negative counts.\n\n        if iterable is not None:\n            if isinstance(iterable, Mapping):\n                if self:\n                    self_get = self.get\n                    for elem, count in iterable.iteritems():\n                        self[elem] = self_get(elem, 0) + count\n                else:\n                    super(Counter, self).update(iterable) # fast path when counter is empty\n            else:\n                self_get = self.get\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) + 1\n        if kwds:\n            self.update(kwds)\n\n    def subtract(self, iterable=None, **kwds):\n        '''Like dict.update() but subtracts counts instead of replacing them.\n        Counts can be reduced below zero.  Both the inputs and outputs are\n        allowed to contain zero and negative counts.\n\n        Source can be an iterable, a dictionary, or another Counter instance.\n\n        >>> c = Counter('which')\n        >>> c.subtract('witch')             # subtract elements from another iterable\n        >>> c.subtract(Counter('watch'))    # subtract elements from another counter\n        >>> c['h']                          # 2 in which, minus 1 in witch, minus 1 in watch\n        0\n        >>> c['w']                          # 1 in which, minus 1 in witch, minus 1 in watch\n        -1\n\n        '''\n        if iterable is not None:\n            self_get = self.get\n            if isinstance(iterable, Mapping):\n                for elem, count in iterable.items():\n                    self[elem] = self_get(elem, 0) - count\n            else:\n                for elem in iterable:\n                    self[elem] = self_get(elem, 0) - 1\n        if kwds:\n            self.subtract(kwds)\n\n    def copy(self):\n        'Return a shallow copy.'\n        return self.__class__(self)\n\n    def __reduce__(self):\n        return self.__class__, (dict(self),)\n\n    def __delitem__(self, elem):\n        'Like dict.__delitem__() but does not raise KeyError for missing values.'\n        if elem in self:\n            super(Counter, self).__delitem__(elem)\n\n    def __repr__(self):\n        if not self:\n            return '%s()' % self.__class__.__name__\n        items = ', '.join(map('%r: %r'.__mod__, self.most_common()))\n        return '%s({%s})' % (self.__class__.__name__, items)\n\n    # Multiset-style mathematical operations discussed in:\n    #       Knuth TAOCP Volume II section 4.6.3 exercise 19\n    #       and at http://en.wikipedia.org/wiki/Multiset\n    #\n    # Outputs guaranteed to only include positive counts.\n    #\n    # To strip negative and zero counts, add-in an empty counter:\n    #       c += Counter()\n\n    def __add__(self, other):\n        '''Add counts from two counters.\n\n        >>> Counter('abbb') + Counter('bcc')\n        Counter({'b': 4, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count + other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __sub__(self, other):\n        ''' Subtract count, but keep only results with positive counts.\n\n        >>> Counter('abbbc') - Counter('bccd')\n        Counter({'b': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            newcount = count - other[elem]\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count < 0:\n                result[elem] = 0 - count\n        return result\n\n    def __or__(self, other):\n        '''Union is the maximum of value in either of the input counters.\n\n        >>> Counter('abbb') | Counter('bcc')\n        Counter({'b': 3, 'c': 2, 'a': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = other_count if count < other_count else count\n            if newcount > 0:\n                result[elem] = newcount\n        for elem, count in other.items():\n            if elem not in self and count > 0:\n                result[elem] = count\n        return result\n\n    def __and__(self, other):\n        ''' Intersection is the minimum of corresponding counts.\n\n        >>> Counter('abbb') & Counter('bcc')\n        Counter({'b': 1})\n\n        '''\n        if not isinstance(other, Counter):\n            return NotImplemented\n        result = Counter()\n        for elem, count in self.items():\n            other_count = other[elem]\n            newcount = count if count < other_count else other_count\n            if newcount > 0:\n                result[elem] = newcount\n        return result\n\n\nif __name__ == '__main__':\n    # verify that instances can be pickled\n    from cPickle import loads, dumps\n    Point = namedtuple('Point', 'x, y', True)\n    p = Point(x=10, y=20)\n    assert p == loads(dumps(p))\n\n    # test and demonstrate ability to override methods\n    class Point(namedtuple('Point', 'x y')):\n        __slots__ = ()\n        @property\n        def hypot(self):\n            return (self.x ** 2 + self.y ** 2) ** 0.5\n        def __str__(self):\n            return 'Point: x=%6.3f  y=%6.3f  hypot=%6.3f' % (self.x, self.y, self.hypot)\n\n    for p in Point(3, 4), Point(14, 5/7.):\n        print p\n\n    class Point(namedtuple('Point', 'x y')):\n        'Point class with optimized _make() and _replace() without error-checking'\n        __slots__ = ()\n        _make = classmethod(tuple.__new__)\n        def _replace(self, _map=map, **kwds):\n            return self._make(_map(kwds.get, ('x', 'y'), self))\n\n    print Point(11, 22)._replace(x=100)\n\n    Point3D = namedtuple('Point3D', Point._fields + ('z',))\n    print Point3D.__doc__\n\n    import doctest\n    TestResults = namedtuple('TestResults', 'failed attempted')\n    print TestResults(*doctest.testmod())\n", 
    "contextlib": "\"\"\"Utilities for with-statement contexts.  See PEP 343.\"\"\"\n\nimport sys\nfrom functools import wraps\nfrom warnings import warn\n\n__all__ = [\"contextmanager\", \"nested\", \"closing\"]\n\nclass GeneratorContextManager(object):\n    \"\"\"Helper for @contextmanager decorator.\"\"\"\n\n    def __init__(self, gen):\n        self.gen = gen\n\n    def __enter__(self):\n        try:\n            return self.gen.next()\n        except StopIteration:\n            raise RuntimeError(\"generator didn't yield\")\n\n    def __exit__(self, type, value, traceback):\n        if type is None:\n            try:\n                self.gen.next()\n            except StopIteration:\n                return\n            else:\n                raise RuntimeError(\"generator didn't stop\")\n        else:\n            if value is None:\n                # Need to force instantiation so we can reliably\n                # tell if we get the same exception back\n                value = type()\n            try:\n                self.gen.throw(type, value, traceback)\n                raise RuntimeError(\"generator didn't stop after throw()\")\n            except StopIteration, exc:\n                # Suppress the exception *unless* it's the same exception that\n                # was passed to throw().  This prevents a StopIteration\n                # raised inside the \"with\" statement from being suppressed\n                return exc is not value\n            except:\n                # only re-raise if it's *not* the exception that was\n                # passed to throw(), because __exit__() must not raise\n                # an exception unless __exit__() itself failed.  But throw()\n                # has to raise the exception to signal propagation, so this\n                # fixes the impedance mismatch between the throw() protocol\n                # and the __exit__() protocol.\n                #\n                if sys.exc_info()[1] is not value:\n                    raise\n\n\ndef contextmanager(func):\n    \"\"\"@contextmanager decorator.\n\n    Typical usage:\n\n        @contextmanager\n        def some_generator(<arguments>):\n            <setup>\n            try:\n                yield <value>\n            finally:\n                <cleanup>\n\n    This makes this:\n\n        with some_generator(<arguments>) as <variable>:\n            <body>\n\n    equivalent to this:\n\n        <setup>\n        try:\n            <variable> = <value>\n            <body>\n        finally:\n            <cleanup>\n\n    \"\"\"\n    @wraps(func)\n    def helper(*args, **kwds):\n        return GeneratorContextManager(func(*args, **kwds))\n    return helper\n\n\n@contextmanager\ndef nested(*managers):\n    \"\"\"Combine multiple context managers into a single nested context manager.\n\n   This function has been deprecated in favour of the multiple manager form\n   of the with statement.\n\n   The one advantage of this function over the multiple manager form of the\n   with statement is that argument unpacking allows it to be\n   used with a variable number of context managers as follows:\n\n      with nested(*managers):\n          do_something()\n\n    \"\"\"\n    warn(\"With-statements now directly support multiple context managers\",\n         DeprecationWarning, 3)\n    exits = []\n    vars = []\n    exc = (None, None, None)\n    try:\n        for mgr in managers:\n            exit = mgr.__exit__\n            enter = mgr.__enter__\n            vars.append(enter())\n            exits.append(exit)\n        yield vars\n    except:\n        exc = sys.exc_info()\n    finally:\n        while exits:\n            exit = exits.pop()\n            try:\n                if exit(*exc):\n                    exc = (None, None, None)\n            except:\n                exc = sys.exc_info()\n        if exc != (None, None, None):\n            # Don't rely on sys.exc_info() still containing\n            # the right information. Another exception may\n            # have been raised and caught by an exit method\n            raise exc[0], exc[1], exc[2]\n\n\nclass closing(object):\n    \"\"\"Context to automatically close something at the end of a block.\n\n    Code like this:\n\n        with closing(<module>.open(<arguments>)) as f:\n            <block>\n\n    is equivalent to this:\n\n        f = <module>.open(<arguments>)\n        try:\n            <block>\n        finally:\n            f.close()\n\n    \"\"\"\n    def __init__(self, thing):\n        self.thing = thing\n    def __enter__(self):\n        return self.thing\n    def __exit__(self, *exc_info):\n        self.thing.close()\n", 
    "cookielib": "r\"\"\"HTTP cookie handling for web clients.\n\nThis module has (now fairly distant) origins in Gisle Aas' Perl module\nHTTP::Cookies, from the libwww-perl library.\n\nDocstrings, comments and debug strings in this code refer to the\nattributes of the HTTP cookie system as cookie-attributes, to distinguish\nthem clearly from Python attributes.\n\nClass diagram (note that BSDDBCookieJar and the MSIE* classes are not\ndistributed with the Python standard library, but are available from\nhttp://wwwsearch.sf.net/):\n\n                        CookieJar____\n                        /     \\      \\\n            FileCookieJar      \\      \\\n             /    |   \\         \\      \\\n MozillaCookieJar | LWPCookieJar \\      \\\n                  |               |      \\\n                  |   ---MSIEBase |       \\\n                  |  /      |     |        \\\n                  | /   MSIEDBCookieJar BSDDBCookieJar\n                  |/\n               MSIECookieJar\n\n\"\"\"\n\n__all__ = ['Cookie', 'CookieJar', 'CookiePolicy', 'DefaultCookiePolicy',\n           'FileCookieJar', 'LWPCookieJar', 'lwp_cookie_str', 'LoadError',\n           'MozillaCookieJar']\n\nimport re, urlparse, copy, time, urllib\ntry:\n    import threading as _threading\nexcept ImportError:\n    import dummy_threading as _threading\nimport httplib  # only for the default HTTP port\nfrom calendar import timegm\n\ndebug = False   # set to True to enable debugging via the logging module\nlogger = None\n\ndef _debug(*args):\n    if not debug:\n        return\n    global logger\n    if not logger:\n        import logging\n        logger = logging.getLogger(\"cookielib\")\n    return logger.debug(*args)\n\n\nDEFAULT_HTTP_PORT = str(httplib.HTTP_PORT)\nMISSING_FILENAME_TEXT = (\"a filename was not supplied (nor was the CookieJar \"\n                         \"instance initialised with one)\")\n\ndef _warn_unhandled_exception():\n    # There are a few catch-all except: statements in this module, for\n    # catching input that's bad in unexpected ways.  Warn if any\n    # exceptions are caught there.\n    import warnings, traceback, StringIO\n    f = StringIO.StringIO()\n    traceback.print_exc(None, f)\n    msg = f.getvalue()\n    warnings.warn(\"cookielib bug!\\n%s\" % msg, stacklevel=2)\n\n\n# Date/time conversion\n# -----------------------------------------------------------------------------\n\nEPOCH_YEAR = 1970\ndef _timegm(tt):\n    year, month, mday, hour, min, sec = tt[:6]\n    if ((year >= EPOCH_YEAR) and (1 <= month <= 12) and (1 <= mday <= 31) and\n        (0 <= hour <= 24) and (0 <= min <= 59) and (0 <= sec <= 61)):\n        return timegm(tt)\n    else:\n        return None\n\nDAYS = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\nMONTHS = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n          \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\nMONTHS_LOWER = []\nfor month in MONTHS: MONTHS_LOWER.append(month.lower())\n\ndef time2isoz(t=None):\n    \"\"\"Return a string representing time in seconds since epoch, t.\n\n    If the function is called without an argument, it will use the current\n    time.\n\n    The format of the returned string is like \"YYYY-MM-DD hh:mm:ssZ\",\n    representing Universal Time (UTC, aka GMT).  An example of this format is:\n\n    1994-11-24 08:49:37Z\n\n    \"\"\"\n    if t is None: t = time.time()\n    year, mon, mday, hour, min, sec = time.gmtime(t)[:6]\n    return \"%04d-%02d-%02d %02d:%02d:%02dZ\" % (\n        year, mon, mday, hour, min, sec)\n\ndef time2netscape(t=None):\n    \"\"\"Return a string representing time in seconds since epoch, t.\n\n    If the function is called without an argument, it will use the current\n    time.\n\n    The format of the returned string is like this:\n\n    Wed, DD-Mon-YYYY HH:MM:SS GMT\n\n    \"\"\"\n    if t is None: t = time.time()\n    year, mon, mday, hour, min, sec, wday = time.gmtime(t)[:7]\n    return \"%s %02d-%s-%04d %02d:%02d:%02d GMT\" % (\n        DAYS[wday], mday, MONTHS[mon-1], year, hour, min, sec)\n\n\nUTC_ZONES = {\"GMT\": None, \"UTC\": None, \"UT\": None, \"Z\": None}\n\nTIMEZONE_RE = re.compile(r\"^([-+])?(\\d\\d?):?(\\d\\d)?$\")\ndef offset_from_tz_string(tz):\n    offset = None\n    if tz in UTC_ZONES:\n        offset = 0\n    else:\n        m = TIMEZONE_RE.search(tz)\n        if m:\n            offset = 3600 * int(m.group(2))\n            if m.group(3):\n                offset = offset + 60 * int(m.group(3))\n            if m.group(1) == '-':\n                offset = -offset\n    return offset\n\ndef _str2time(day, mon, yr, hr, min, sec, tz):\n    # translate month name to number\n    # month numbers start with 1 (January)\n    try:\n        mon = MONTHS_LOWER.index(mon.lower())+1\n    except ValueError:\n        # maybe it's already a number\n        try:\n            imon = int(mon)\n        except ValueError:\n            return None\n        if 1 <= imon <= 12:\n            mon = imon\n        else:\n            return None\n\n    # make sure clock elements are defined\n    if hr is None: hr = 0\n    if min is None: min = 0\n    if sec is None: sec = 0\n\n    yr = int(yr)\n    day = int(day)\n    hr = int(hr)\n    min = int(min)\n    sec = int(sec)\n\n    if yr < 1000:\n        # find \"obvious\" year\n        cur_yr = time.localtime(time.time())[0]\n        m = cur_yr % 100\n        tmp = yr\n        yr = yr + cur_yr - m\n        m = m - tmp\n        if abs(m) > 50:\n            if m > 0: yr = yr + 100\n            else: yr = yr - 100\n\n    # convert UTC time tuple to seconds since epoch (not timezone-adjusted)\n    t = _timegm((yr, mon, day, hr, min, sec, tz))\n\n    if t is not None:\n        # adjust time using timezone string, to get absolute time since epoch\n        if tz is None:\n            tz = \"UTC\"\n        tz = tz.upper()\n        offset = offset_from_tz_string(tz)\n        if offset is None:\n            return None\n        t = t - offset\n\n    return t\n\nSTRICT_DATE_RE = re.compile(\n    r\"^[SMTWF][a-z][a-z], (\\d\\d) ([JFMASOND][a-z][a-z]) \"\n    \"(\\d\\d\\d\\d) (\\d\\d):(\\d\\d):(\\d\\d) GMT$\")\nWEEKDAY_RE = re.compile(\n    r\"^(?:Sun|Mon|Tue|Wed|Thu|Fri|Sat)[a-z]*,?\\s*\", re.I)\nLOOSE_HTTP_DATE_RE = re.compile(\n    r\"\"\"^\n    (\\d\\d?)            # day\n       (?:\\s+|[-\\/])\n    (\\w+)              # month\n        (?:\\s+|[-\\/])\n    (\\d+)              # year\n    (?:\n          (?:\\s+|:)    # separator before clock\n       (\\d\\d?):(\\d\\d)  # hour:min\n       (?::(\\d\\d))?    # optional seconds\n    )?                 # optional clock\n       \\s*\n    ([-+]?\\d{2,4}|(?![APap][Mm]\\b)[A-Za-z]+)? # timezone\n       \\s*\n    (?:\\(\\w+\\))?       # ASCII representation of timezone in parens.\n       \\s*$\"\"\", re.X)\ndef http2time(text):\n    \"\"\"Returns time in seconds since epoch of time represented by a string.\n\n    Return value is an integer.\n\n    None is returned if the format of str is unrecognized, the time is outside\n    the representable range, or the timezone string is not recognized.  If the\n    string contains no timezone, UTC is assumed.\n\n    The timezone in the string may be numerical (like \"-0800\" or \"+0100\") or a\n    string timezone (like \"UTC\", \"GMT\", \"BST\" or \"EST\").  Currently, only the\n    timezone strings equivalent to UTC (zero offset) are known to the function.\n\n    The function loosely parses the following formats:\n\n    Wed, 09 Feb 1994 22:23:32 GMT       -- HTTP format\n    Tuesday, 08-Feb-94 14:15:29 GMT     -- old rfc850 HTTP format\n    Tuesday, 08-Feb-1994 14:15:29 GMT   -- broken rfc850 HTTP format\n    09 Feb 1994 22:23:32 GMT            -- HTTP format (no weekday)\n    08-Feb-94 14:15:29 GMT              -- rfc850 format (no weekday)\n    08-Feb-1994 14:15:29 GMT            -- broken rfc850 format (no weekday)\n\n    The parser ignores leading and trailing whitespace.  The time may be\n    absent.\n\n    If the year is given with only 2 digits, the function will select the\n    century that makes the year closest to the current date.\n\n    \"\"\"\n    # fast exit for strictly conforming string\n    m = STRICT_DATE_RE.search(text)\n    if m:\n        g = m.groups()\n        mon = MONTHS_LOWER.index(g[1].lower()) + 1\n        tt = (int(g[2]), mon, int(g[0]),\n              int(g[3]), int(g[4]), float(g[5]))\n        return _timegm(tt)\n\n    # No, we need some messy parsing...\n\n    # clean up\n    text = text.lstrip()\n    text = WEEKDAY_RE.sub(\"\", text, 1)  # Useless weekday\n\n    # tz is time zone specifier string\n    day, mon, yr, hr, min, sec, tz = [None]*7\n\n    # loose regexp parse\n    m = LOOSE_HTTP_DATE_RE.search(text)\n    if m is not None:\n        day, mon, yr, hr, min, sec, tz = m.groups()\n    else:\n        return None  # bad format\n\n    return _str2time(day, mon, yr, hr, min, sec, tz)\n\nISO_DATE_RE = re.compile(\n    \"\"\"^\n    (\\d{4})              # year\n       [-\\/]?\n    (\\d\\d?)              # numerical month\n       [-\\/]?\n    (\\d\\d?)              # day\n   (?:\n         (?:\\s+|[-:Tt])  # separator before clock\n      (\\d\\d?):?(\\d\\d)    # hour:min\n      (?::?(\\d\\d(?:\\.\\d*)?))?  # optional seconds (and fractional)\n   )?                    # optional clock\n      \\s*\n   ([-+]?\\d\\d?:?(:?\\d\\d)?\n    |Z|z)?               # timezone  (Z is \"zero meridian\", i.e. GMT)\n      \\s*$\"\"\", re.X)\ndef iso2time(text):\n    \"\"\"\n    As for http2time, but parses the ISO 8601 formats:\n\n    1994-02-03 14:15:29 -0100    -- ISO 8601 format\n    1994-02-03 14:15:29          -- zone is optional\n    1994-02-03                   -- only date\n    1994-02-03T14:15:29          -- Use T as separator\n    19940203T141529Z             -- ISO 8601 compact format\n    19940203                     -- only date\n\n    \"\"\"\n    # clean up\n    text = text.lstrip()\n\n    # tz is time zone specifier string\n    day, mon, yr, hr, min, sec, tz = [None]*7\n\n    # loose regexp parse\n    m = ISO_DATE_RE.search(text)\n    if m is not None:\n        # XXX there's an extra bit of the timezone I'm ignoring here: is\n        #   this the right thing to do?\n        yr, mon, day, hr, min, sec, tz, _ = m.groups()\n    else:\n        return None  # bad format\n\n    return _str2time(day, mon, yr, hr, min, sec, tz)\n\n\n# Header parsing\n# -----------------------------------------------------------------------------\n\ndef unmatched(match):\n    \"\"\"Return unmatched part of re.Match object.\"\"\"\n    start, end = match.span(0)\n    return match.string[:start]+match.string[end:]\n\nHEADER_TOKEN_RE =        re.compile(r\"^\\s*([^=\\s;,]+)\")\nHEADER_QUOTED_VALUE_RE = re.compile(r\"^\\s*=\\s*\\\"([^\\\"\\\\]*(?:\\\\.[^\\\"\\\\]*)*)\\\"\")\nHEADER_VALUE_RE =        re.compile(r\"^\\s*=\\s*([^\\s;,]*)\")\nHEADER_ESCAPE_RE = re.compile(r\"\\\\(.)\")\ndef split_header_words(header_values):\n    r\"\"\"Parse header values into a list of lists containing key,value pairs.\n\n    The function knows how to deal with \",\", \";\" and \"=\" as well as quoted\n    values after \"=\".  A list of space separated tokens are parsed as if they\n    were separated by \";\".\n\n    If the header_values passed as argument contains multiple values, then they\n    are treated as if they were a single value separated by comma \",\".\n\n    This means that this function is useful for parsing header fields that\n    follow this syntax (BNF as from the HTTP/1.1 specification, but we relax\n    the requirement for tokens).\n\n      headers           = #header\n      header            = (token | parameter) *( [\";\"] (token | parameter))\n\n      token             = 1*<any CHAR except CTLs or separators>\n      separators        = \"(\" | \")\" | \"<\" | \">\" | \"@\"\n                        | \",\" | \";\" | \":\" | \"\\\" | <\">\n                        | \"/\" | \"[\" | \"]\" | \"?\" | \"=\"\n                        | \"{\" | \"}\" | SP | HT\n\n      quoted-string     = ( <\"> *(qdtext | quoted-pair ) <\"> )\n      qdtext            = <any TEXT except <\">>\n      quoted-pair       = \"\\\" CHAR\n\n      parameter         = attribute \"=\" value\n      attribute         = token\n      value             = token | quoted-string\n\n    Each header is represented by a list of key/value pairs.  The value for a\n    simple token (not part of a parameter) is None.  Syntactically incorrect\n    headers will not necessarily be parsed as you would want.\n\n    This is easier to describe with some examples:\n\n    >>> split_header_words(['foo=\"bar\"; port=\"80,81\"; discard, bar=baz'])\n    [[('foo', 'bar'), ('port', '80,81'), ('discard', None)], [('bar', 'baz')]]\n    >>> split_header_words(['text/html; charset=\"iso-8859-1\"'])\n    [[('text/html', None), ('charset', 'iso-8859-1')]]\n    >>> split_header_words([r'Basic realm=\"\\\"foo\\bar\\\"\"'])\n    [[('Basic', None), ('realm', '\"foobar\"')]]\n\n    \"\"\"\n    assert not isinstance(header_values, basestring)\n    result = []\n    for text in header_values:\n        orig_text = text\n        pairs = []\n        while text:\n            m = HEADER_TOKEN_RE.search(text)\n            if m:\n                text = unmatched(m)\n                name = m.group(1)\n                m = HEADER_QUOTED_VALUE_RE.search(text)\n                if m:  # quoted value\n                    text = unmatched(m)\n                    value = m.group(1)\n                    value = HEADER_ESCAPE_RE.sub(r\"\\1\", value)\n                else:\n                    m = HEADER_VALUE_RE.search(text)\n                    if m:  # unquoted value\n                        text = unmatched(m)\n                        value = m.group(1)\n                        value = value.rstrip()\n                    else:\n                        # no value, a lone token\n                        value = None\n                pairs.append((name, value))\n            elif text.lstrip().startswith(\",\"):\n                # concatenated headers, as per RFC 2616 section 4.2\n                text = text.lstrip()[1:]\n                if pairs: result.append(pairs)\n                pairs = []\n            else:\n                # skip junk\n                non_junk, nr_junk_chars = re.subn(\"^[=\\s;]*\", \"\", text)\n                assert nr_junk_chars > 0, (\n                    \"split_header_words bug: '%s', '%s', %s\" %\n                    (orig_text, text, pairs))\n                text = non_junk\n        if pairs: result.append(pairs)\n    return result\n\nHEADER_JOIN_ESCAPE_RE = re.compile(r\"([\\\"\\\\])\")\ndef join_header_words(lists):\n    \"\"\"Do the inverse (almost) of the conversion done by split_header_words.\n\n    Takes a list of lists of (key, value) pairs and produces a single header\n    value.  Attribute values are quoted if needed.\n\n    >>> join_header_words([[(\"text/plain\", None), (\"charset\", \"iso-8859/1\")]])\n    'text/plain; charset=\"iso-8859/1\"'\n    >>> join_header_words([[(\"text/plain\", None)], [(\"charset\", \"iso-8859/1\")]])\n    'text/plain, charset=\"iso-8859/1\"'\n\n    \"\"\"\n    headers = []\n    for pairs in lists:\n        attr = []\n        for k, v in pairs:\n            if v is not None:\n                if not re.search(r\"^\\w+$\", v):\n                    v = HEADER_JOIN_ESCAPE_RE.sub(r\"\\\\\\1\", v)  # escape \" and \\\n                    v = '\"%s\"' % v\n                k = \"%s=%s\" % (k, v)\n            attr.append(k)\n        if attr: headers.append(\"; \".join(attr))\n    return \", \".join(headers)\n\ndef _strip_quotes(text):\n    if text.startswith('\"'):\n        text = text[1:]\n    if text.endswith('\"'):\n        text = text[:-1]\n    return text\n\ndef parse_ns_headers(ns_headers):\n    \"\"\"Ad-hoc parser for Netscape protocol cookie-attributes.\n\n    The old Netscape cookie format for Set-Cookie can for instance contain\n    an unquoted \",\" in the expires field, so we have to use this ad-hoc\n    parser instead of split_header_words.\n\n    XXX This may not make the best possible effort to parse all the crap\n    that Netscape Cookie headers contain.  Ronald Tschalar's HTTPClient\n    parser is probably better, so could do worse than following that if\n    this ever gives any trouble.\n\n    Currently, this is also used for parsing RFC 2109 cookies.\n\n    \"\"\"\n    known_attrs = (\"expires\", \"domain\", \"path\", \"secure\",\n                   # RFC 2109 attrs (may turn up in Netscape cookies, too)\n                   \"version\", \"port\", \"max-age\")\n\n    result = []\n    for ns_header in ns_headers:\n        pairs = []\n        version_set = False\n        for ii, param in enumerate(re.split(r\";\\s*\", ns_header)):\n            param = param.rstrip()\n            if param == \"\": continue\n            if \"=\" not in param:\n                k, v = param, None\n            else:\n                k, v = re.split(r\"\\s*=\\s*\", param, 1)\n                k = k.lstrip()\n            if ii != 0:\n                lc = k.lower()\n                if lc in known_attrs:\n                    k = lc\n                if k == \"version\":\n                    # This is an RFC 2109 cookie.\n                    v = _strip_quotes(v)\n                    version_set = True\n                if k == \"expires\":\n                    # convert expires date to seconds since epoch\n                    v = http2time(_strip_quotes(v))  # None if invalid\n            pairs.append((k, v))\n\n        if pairs:\n            if not version_set:\n                pairs.append((\"version\", \"0\"))\n            result.append(pairs)\n\n    return result\n\n\nIPV4_RE = re.compile(r\"\\.\\d+$\")\ndef is_HDN(text):\n    \"\"\"Return True if text is a host domain name.\"\"\"\n    # XXX\n    # This may well be wrong.  Which RFC is HDN defined in, if any (for\n    #  the purposes of RFC 2965)?\n    # For the current implementation, what about IPv6?  Remember to look\n    #  at other uses of IPV4_RE also, if change this.\n    if IPV4_RE.search(text):\n        return False\n    if text == \"\":\n        return False\n    if text[0] == \".\" or text[-1] == \".\":\n        return False\n    return True\n\ndef domain_match(A, B):\n    \"\"\"Return True if domain A domain-matches domain B, according to RFC 2965.\n\n    A and B may be host domain names or IP addresses.\n\n    RFC 2965, section 1:\n\n    Host names can be specified either as an IP address or a HDN string.\n    Sometimes we compare one host name with another.  (Such comparisons SHALL\n    be case-insensitive.)  Host A's name domain-matches host B's if\n\n         *  their host name strings string-compare equal; or\n\n         * A is a HDN string and has the form NB, where N is a non-empty\n            name string, B has the form .B', and B' is a HDN string.  (So,\n            x.y.com domain-matches .Y.com but not Y.com.)\n\n    Note that domain-match is not a commutative operation: a.b.c.com\n    domain-matches .c.com, but not the reverse.\n\n    \"\"\"\n    # Note that, if A or B are IP addresses, the only relevant part of the\n    # definition of the domain-match algorithm is the direct string-compare.\n    A = A.lower()\n    B = B.lower()\n    if A == B:\n        return True\n    if not is_HDN(A):\n        return False\n    i = A.rfind(B)\n    if i == -1 or i == 0:\n        # A does not have form NB, or N is the empty string\n        return False\n    if not B.startswith(\".\"):\n        return False\n    if not is_HDN(B[1:]):\n        return False\n    return True\n\ndef liberal_is_HDN(text):\n    \"\"\"Return True if text is a sort-of-like a host domain name.\n\n    For accepting/blocking domains.\n\n    \"\"\"\n    if IPV4_RE.search(text):\n        return False\n    return True\n\ndef user_domain_match(A, B):\n    \"\"\"For blocking/accepting domains.\n\n    A and B may be host domain names or IP addresses.\n\n    \"\"\"\n    A = A.lower()\n    B = B.lower()\n    if not (liberal_is_HDN(A) and liberal_is_HDN(B)):\n        if A == B:\n            # equal IP addresses\n            return True\n        return False\n    initial_dot = B.startswith(\".\")\n    if initial_dot and A.endswith(B):\n        return True\n    if not initial_dot and A == B:\n        return True\n    return False\n\ncut_port_re = re.compile(r\":\\d+$\")\ndef request_host(request):\n    \"\"\"Return request-host, as defined by RFC 2965.\n\n    Variation from RFC: returned value is lowercased, for convenient\n    comparison.\n\n    \"\"\"\n    url = request.get_full_url()\n    host = urlparse.urlparse(url)[1]\n    if host == \"\":\n        host = request.get_header(\"Host\", \"\")\n\n    # remove port, if present\n    host = cut_port_re.sub(\"\", host, 1)\n    return host.lower()\n\ndef eff_request_host(request):\n    \"\"\"Return a tuple (request-host, effective request-host name).\n\n    As defined by RFC 2965, except both are lowercased.\n\n    \"\"\"\n    erhn = req_host = request_host(request)\n    if req_host.find(\".\") == -1 and not IPV4_RE.search(req_host):\n        erhn = req_host + \".local\"\n    return req_host, erhn\n\ndef request_path(request):\n    \"\"\"Path component of request-URI, as defined by RFC 2965.\"\"\"\n    url = request.get_full_url()\n    parts = urlparse.urlsplit(url)\n    path = escape_path(parts.path)\n    if not path.startswith(\"/\"):\n        # fix bad RFC 2396 absoluteURI\n        path = \"/\" + path\n    return path\n\ndef request_port(request):\n    host = request.get_host()\n    i = host.find(':')\n    if i >= 0:\n        port = host[i+1:]\n        try:\n            int(port)\n        except ValueError:\n            _debug(\"nonnumeric port: '%s'\", port)\n            return None\n    else:\n        port = DEFAULT_HTTP_PORT\n    return port\n\n# Characters in addition to A-Z, a-z, 0-9, '_', '.', and '-' that don't\n# need to be escaped to form a valid HTTP URL (RFCs 2396 and 1738).\nHTTP_PATH_SAFE = \"%/;:@&=+$,!~*'()\"\nESCAPED_CHAR_RE = re.compile(r\"%([0-9a-fA-F][0-9a-fA-F])\")\ndef uppercase_escaped_char(match):\n    return \"%%%s\" % match.group(1).upper()\ndef escape_path(path):\n    \"\"\"Escape any invalid characters in HTTP URL, and uppercase all escapes.\"\"\"\n    # There's no knowing what character encoding was used to create URLs\n    # containing %-escapes, but since we have to pick one to escape invalid\n    # path characters, we pick UTF-8, as recommended in the HTML 4.0\n    # specification:\n    # http://www.w3.org/TR/REC-html40/appendix/notes.html#h-B.2.1\n    # And here, kind of: draft-fielding-uri-rfc2396bis-03\n    # (And in draft IRI specification: draft-duerst-iri-05)\n    # (And here, for new URI schemes: RFC 2718)\n    if isinstance(path, unicode):\n        path = path.encode(\"utf-8\")\n    path = urllib.quote(path, HTTP_PATH_SAFE)\n    path = ESCAPED_CHAR_RE.sub(uppercase_escaped_char, path)\n    return path\n\ndef reach(h):\n    \"\"\"Return reach of host h, as defined by RFC 2965, section 1.\n\n    The reach R of a host name H is defined as follows:\n\n       *  If\n\n          -  H is the host domain name of a host; and,\n\n          -  H has the form A.B; and\n\n          -  A has no embedded (that is, interior) dots; and\n\n          -  B has at least one embedded dot, or B is the string \"local\".\n             then the reach of H is .B.\n\n       *  Otherwise, the reach of H is H.\n\n    >>> reach(\"www.acme.com\")\n    '.acme.com'\n    >>> reach(\"acme.com\")\n    'acme.com'\n    >>> reach(\"acme.local\")\n    '.local'\n\n    \"\"\"\n    i = h.find(\".\")\n    if i >= 0:\n        #a = h[:i]  # this line is only here to show what a is\n        b = h[i+1:]\n        i = b.find(\".\")\n        if is_HDN(h) and (i >= 0 or b == \"local\"):\n            return \".\"+b\n    return h\n\ndef is_third_party(request):\n    \"\"\"\n\n    RFC 2965, section 3.3.6:\n\n        An unverifiable transaction is to a third-party host if its request-\n        host U does not domain-match the reach R of the request-host O in the\n        origin transaction.\n\n    \"\"\"\n    req_host = request_host(request)\n    if not domain_match(req_host, reach(request.get_origin_req_host())):\n        return True\n    else:\n        return False\n\n\nclass Cookie:\n    \"\"\"HTTP Cookie.\n\n    This class represents both Netscape and RFC 2965 cookies.\n\n    This is deliberately a very simple class.  It just holds attributes.  It's\n    possible to construct Cookie instances that don't comply with the cookie\n    standards.  CookieJar.make_cookies is the factory function for Cookie\n    objects -- it deals with cookie parsing, supplying defaults, and\n    normalising to the representation used in this class.  CookiePolicy is\n    responsible for checking them to see whether they should be accepted from\n    and returned to the server.\n\n    Note that the port may be present in the headers, but unspecified (\"Port\"\n    rather than\"Port=80\", for example); if this is the case, port is None.\n\n    \"\"\"\n\n    def __init__(self, version, name, value,\n                 port, port_specified,\n                 domain, domain_specified, domain_initial_dot,\n                 path, path_specified,\n                 secure,\n                 expires,\n                 discard,\n                 comment,\n                 comment_url,\n                 rest,\n                 rfc2109=False,\n                 ):\n\n        if version is not None: version = int(version)\n        if expires is not None: expires = int(expires)\n        if port is None and port_specified is True:\n            raise ValueError(\"if port is None, port_specified must be false\")\n\n        self.version = version\n        self.name = name\n        self.value = value\n        self.port = port\n        self.port_specified = port_specified\n        # normalise case, as per RFC 2965 section 3.3.3\n        self.domain = domain.lower()\n        self.domain_specified = domain_specified\n        # Sigh.  We need to know whether the domain given in the\n        # cookie-attribute had an initial dot, in order to follow RFC 2965\n        # (as clarified in draft errata).  Needed for the returned $Domain\n        # value.\n        self.domain_initial_dot = domain_initial_dot\n        self.path = path\n        self.path_specified = path_specified\n        self.secure = secure\n        self.expires = expires\n        self.discard = discard\n        self.comment = comment\n        self.comment_url = comment_url\n        self.rfc2109 = rfc2109\n\n        self._rest = copy.copy(rest)\n\n    def has_nonstandard_attr(self, name):\n        return name in self._rest\n    def get_nonstandard_attr(self, name, default=None):\n        return self._rest.get(name, default)\n    def set_nonstandard_attr(self, name, value):\n        self._rest[name] = value\n\n    def is_expired(self, now=None):\n        if now is None: now = time.time()\n        if (self.expires is not None) and (self.expires <= now):\n            return True\n        return False\n\n    def __str__(self):\n        if self.port is None: p = \"\"\n        else: p = \":\"+self.port\n        limit = self.domain + p + self.path\n        if self.value is not None:\n            namevalue = \"%s=%s\" % (self.name, self.value)\n        else:\n            namevalue = self.name\n        return \"<Cookie %s for %s>\" % (namevalue, limit)\n\n    def __repr__(self):\n        args = []\n        for name in (\"version\", \"name\", \"value\",\n                     \"port\", \"port_specified\",\n                     \"domain\", \"domain_specified\", \"domain_initial_dot\",\n                     \"path\", \"path_specified\",\n                     \"secure\", \"expires\", \"discard\", \"comment\", \"comment_url\",\n                     ):\n            attr = getattr(self, name)\n            args.append(\"%s=%s\" % (name, repr(attr)))\n        args.append(\"rest=%s\" % repr(self._rest))\n        args.append(\"rfc2109=%s\" % repr(self.rfc2109))\n        return \"Cookie(%s)\" % \", \".join(args)\n\n\nclass CookiePolicy:\n    \"\"\"Defines which cookies get accepted from and returned to server.\n\n    May also modify cookies, though this is probably a bad idea.\n\n    The subclass DefaultCookiePolicy defines the standard rules for Netscape\n    and RFC 2965 cookies -- override that if you want a customised policy.\n\n    \"\"\"\n    def set_ok(self, cookie, request):\n        \"\"\"Return true if (and only if) cookie should be accepted from server.\n\n        Currently, pre-expired cookies never get this far -- the CookieJar\n        class deletes such cookies itself.\n\n        \"\"\"\n        raise NotImplementedError()\n\n    def return_ok(self, cookie, request):\n        \"\"\"Return true if (and only if) cookie should be returned to server.\"\"\"\n        raise NotImplementedError()\n\n    def domain_return_ok(self, domain, request):\n        \"\"\"Return false if cookies should not be returned, given cookie domain.\n        \"\"\"\n        return True\n\n    def path_return_ok(self, path, request):\n        \"\"\"Return false if cookies should not be returned, given cookie path.\n        \"\"\"\n        return True\n\n\nclass DefaultCookiePolicy(CookiePolicy):\n    \"\"\"Implements the standard rules for accepting and returning cookies.\"\"\"\n\n    DomainStrictNoDots = 1\n    DomainStrictNonDomain = 2\n    DomainRFC2965Match = 4\n\n    DomainLiberal = 0\n    DomainStrict = DomainStrictNoDots|DomainStrictNonDomain\n\n    def __init__(self,\n                 blocked_domains=None, allowed_domains=None,\n                 netscape=True, rfc2965=False,\n                 rfc2109_as_netscape=None,\n                 hide_cookie2=False,\n                 strict_domain=False,\n                 strict_rfc2965_unverifiable=True,\n                 strict_ns_unverifiable=False,\n                 strict_ns_domain=DomainLiberal,\n                 strict_ns_set_initial_dollar=False,\n                 strict_ns_set_path=False,\n                 ):\n        \"\"\"Constructor arguments should be passed as keyword arguments only.\"\"\"\n        self.netscape = netscape\n        self.rfc2965 = rfc2965\n        self.rfc2109_as_netscape = rfc2109_as_netscape\n        self.hide_cookie2 = hide_cookie2\n        self.strict_domain = strict_domain\n        self.strict_rfc2965_unverifiable = strict_rfc2965_unverifiable\n        self.strict_ns_unverifiable = strict_ns_unverifiable\n        self.strict_ns_domain = strict_ns_domain\n        self.strict_ns_set_initial_dollar = strict_ns_set_initial_dollar\n        self.strict_ns_set_path = strict_ns_set_path\n\n        if blocked_domains is not None:\n            self._blocked_domains = tuple(blocked_domains)\n        else:\n            self._blocked_domains = ()\n\n        if allowed_domains is not None:\n            allowed_domains = tuple(allowed_domains)\n        self._allowed_domains = allowed_domains\n\n    def blocked_domains(self):\n        \"\"\"Return the sequence of blocked domains (as a tuple).\"\"\"\n        return self._blocked_domains\n    def set_blocked_domains(self, blocked_domains):\n        \"\"\"Set the sequence of blocked domains.\"\"\"\n        self._blocked_domains = tuple(blocked_domains)\n\n    def is_blocked(self, domain):\n        for blocked_domain in self._blocked_domains:\n            if user_domain_match(domain, blocked_domain):\n                return True\n        return False\n\n    def allowed_domains(self):\n        \"\"\"Return None, or the sequence of allowed domains (as a tuple).\"\"\"\n        return self._allowed_domains\n    def set_allowed_domains(self, allowed_domains):\n        \"\"\"Set the sequence of allowed domains, or None.\"\"\"\n        if allowed_domains is not None:\n            allowed_domains = tuple(allowed_domains)\n        self._allowed_domains = allowed_domains\n\n    def is_not_allowed(self, domain):\n        if self._allowed_domains is None:\n            return False\n        for allowed_domain in self._allowed_domains:\n            if user_domain_match(domain, allowed_domain):\n                return False\n        return True\n\n    def set_ok(self, cookie, request):\n        \"\"\"\n        If you override .set_ok(), be sure to call this method.  If it returns\n        false, so should your subclass (assuming your subclass wants to be more\n        strict about which cookies to accept).\n\n        \"\"\"\n        _debug(\" - checking cookie %s=%s\", cookie.name, cookie.value)\n\n        assert cookie.name is not None\n\n        for n in \"version\", \"verifiability\", \"name\", \"path\", \"domain\", \"port\":\n            fn_name = \"set_ok_\"+n\n            fn = getattr(self, fn_name)\n            if not fn(cookie, request):\n                return False\n\n        return True\n\n    def set_ok_version(self, cookie, request):\n        if cookie.version is None:\n            # Version is always set to 0 by parse_ns_headers if it's a Netscape\n            # cookie, so this must be an invalid RFC 2965 cookie.\n            _debug(\"   Set-Cookie2 without version attribute (%s=%s)\",\n                   cookie.name, cookie.value)\n            return False\n        if cookie.version > 0 and not self.rfc2965:\n            _debug(\"   RFC 2965 cookies are switched off\")\n            return False\n        elif cookie.version == 0 and not self.netscape:\n            _debug(\"   Netscape cookies are switched off\")\n            return False\n        return True\n\n    def set_ok_verifiability(self, cookie, request):\n        if request.is_unverifiable() and is_third_party(request):\n            if cookie.version > 0 and self.strict_rfc2965_unverifiable:\n                _debug(\"   third-party RFC 2965 cookie during \"\n                             \"unverifiable transaction\")\n                return False\n            elif cookie.version == 0 and self.strict_ns_unverifiable:\n                _debug(\"   third-party Netscape cookie during \"\n                             \"unverifiable transaction\")\n                return False\n        return True\n\n    def set_ok_name(self, cookie, request):\n        # Try and stop servers setting V0 cookies designed to hack other\n        # servers that know both V0 and V1 protocols.\n        if (cookie.version == 0 and self.strict_ns_set_initial_dollar and\n            cookie.name.startswith(\"$\")):\n            _debug(\"   illegal name (starts with '$'): '%s'\", cookie.name)\n            return False\n        return True\n\n    def set_ok_path(self, cookie, request):\n        if cookie.path_specified:\n            req_path = request_path(request)\n            if ((cookie.version > 0 or\n                 (cookie.version == 0 and self.strict_ns_set_path)) and\n                not req_path.startswith(cookie.path)):\n                _debug(\"   path attribute %s is not a prefix of request \"\n                       \"path %s\", cookie.path, req_path)\n                return False\n        return True\n\n    def set_ok_domain(self, cookie, request):\n        if self.is_blocked(cookie.domain):\n            _debug(\"   domain %s is in user block-list\", cookie.domain)\n            return False\n        if self.is_not_allowed(cookie.domain):\n            _debug(\"   domain %s is not in user allow-list\", cookie.domain)\n            return False\n        if cookie.domain_specified:\n            req_host, erhn = eff_request_host(request)\n            domain = cookie.domain\n            if self.strict_domain and (domain.count(\".\") >= 2):\n                # XXX This should probably be compared with the Konqueror\n                # (kcookiejar.cpp) and Mozilla implementations, but it's a\n                # losing battle.\n                i = domain.rfind(\".\")\n                j = domain.rfind(\".\", 0, i)\n                if j == 0:  # domain like .foo.bar\n                    tld = domain[i+1:]\n                    sld = domain[j+1:i]\n                    if sld.lower() in (\"co\", \"ac\", \"com\", \"edu\", \"org\", \"net\",\n                       \"gov\", \"mil\", \"int\", \"aero\", \"biz\", \"cat\", \"coop\",\n                       \"info\", \"jobs\", \"mobi\", \"museum\", \"name\", \"pro\",\n                       \"travel\", \"eu\") and len(tld) == 2:\n                        # domain like .co.uk\n                        _debug(\"   country-code second level domain %s\", domain)\n                        return False\n            if domain.startswith(\".\"):\n                undotted_domain = domain[1:]\n            else:\n                undotted_domain = domain\n            embedded_dots = (undotted_domain.find(\".\") >= 0)\n            if not embedded_dots and domain != \".local\":\n                _debug(\"   non-local domain %s contains no embedded dot\",\n                       domain)\n                return False\n            if cookie.version == 0:\n                if (not erhn.endswith(domain) and\n                    (not erhn.startswith(\".\") and\n                     not (\".\"+erhn).endswith(domain))):\n                    _debug(\"   effective request-host %s (even with added \"\n                           \"initial dot) does not end with %s\",\n                           erhn, domain)\n                    return False\n            if (cookie.version > 0 or\n                (self.strict_ns_domain & self.DomainRFC2965Match)):\n                if not domain_match(erhn, domain):\n                    _debug(\"   effective request-host %s does not domain-match \"\n                           \"%s\", erhn, domain)\n                    return False\n            if (cookie.version > 0 or\n                (self.strict_ns_domain & self.DomainStrictNoDots)):\n                host_prefix = req_host[:-len(domain)]\n                if (host_prefix.find(\".\") >= 0 and\n                    not IPV4_RE.search(req_host)):\n                    _debug(\"   host prefix %s for domain %s contains a dot\",\n                           host_prefix, domain)\n                    return False\n        return True\n\n    def set_ok_port(self, cookie, request):\n        if cookie.port_specified:\n            req_port = request_port(request)\n            if req_port is None:\n                req_port = \"80\"\n            else:\n                req_port = str(req_port)\n            for p in cookie.port.split(\",\"):\n                try:\n                    int(p)\n                except ValueError:\n                    _debug(\"   bad port %s (not numeric)\", p)\n                    return False\n                if p == req_port:\n                    break\n            else:\n                _debug(\"   request port (%s) not found in %s\",\n                       req_port, cookie.port)\n                return False\n        return True\n\n    def return_ok(self, cookie, request):\n        \"\"\"\n        If you override .return_ok(), be sure to call this method.  If it\n        returns false, so should your subclass (assuming your subclass wants to\n        be more strict about which cookies to return).\n\n        \"\"\"\n        # Path has already been checked by .path_return_ok(), and domain\n        # blocking done by .domain_return_ok().\n        _debug(\" - checking cookie %s=%s\", cookie.name, cookie.value)\n\n        for n in \"version\", \"verifiability\", \"secure\", \"expires\", \"port\", \"domain\":\n            fn_name = \"return_ok_\"+n\n            fn = getattr(self, fn_name)\n            if not fn(cookie, request):\n                return False\n        return True\n\n    def return_ok_version(self, cookie, request):\n        if cookie.version > 0 and not self.rfc2965:\n            _debug(\"   RFC 2965 cookies are switched off\")\n            return False\n        elif cookie.version == 0 and not self.netscape:\n            _debug(\"   Netscape cookies are switched off\")\n            return False\n        return True\n\n    def return_ok_verifiability(self, cookie, request):\n        if request.is_unverifiable() and is_third_party(request):\n            if cookie.version > 0 and self.strict_rfc2965_unverifiable:\n                _debug(\"   third-party RFC 2965 cookie during unverifiable \"\n                       \"transaction\")\n                return False\n            elif cookie.version == 0 and self.strict_ns_unverifiable:\n                _debug(\"   third-party Netscape cookie during unverifiable \"\n                       \"transaction\")\n                return False\n        return True\n\n    def return_ok_secure(self, cookie, request):\n        if cookie.secure and request.get_type() != \"https\":\n            _debug(\"   secure cookie with non-secure request\")\n            return False\n        return True\n\n    def return_ok_expires(self, cookie, request):\n        if cookie.is_expired(self._now):\n            _debug(\"   cookie expired\")\n            return False\n        return True\n\n    def return_ok_port(self, cookie, request):\n        if cookie.port:\n            req_port = request_port(request)\n            if req_port is None:\n                req_port = \"80\"\n            for p in cookie.port.split(\",\"):\n                if p == req_port:\n                    break\n            else:\n                _debug(\"   request port %s does not match cookie port %s\",\n                       req_port, cookie.port)\n                return False\n        return True\n\n    def return_ok_domain(self, cookie, request):\n        req_host, erhn = eff_request_host(request)\n        domain = cookie.domain\n\n        # strict check of non-domain cookies: Mozilla does this, MSIE5 doesn't\n        if (cookie.version == 0 and\n            (self.strict_ns_domain & self.DomainStrictNonDomain) and\n            not cookie.domain_specified and domain != erhn):\n            _debug(\"   cookie with unspecified domain does not string-compare \"\n                   \"equal to request domain\")\n            return False\n\n        if cookie.version > 0 and not domain_match(erhn, domain):\n            _debug(\"   effective request-host name %s does not domain-match \"\n                   \"RFC 2965 cookie domain %s\", erhn, domain)\n            return False\n        if cookie.version == 0 and not (\".\"+erhn).endswith(domain):\n            _debug(\"   request-host %s does not match Netscape cookie domain \"\n                   \"%s\", req_host, domain)\n            return False\n        return True\n\n    def domain_return_ok(self, domain, request):\n        # Liberal check of.  This is here as an optimization to avoid\n        # having to load lots of MSIE cookie files unless necessary.\n        req_host, erhn = eff_request_host(request)\n        if not req_host.startswith(\".\"):\n            req_host = \".\"+req_host\n        if not erhn.startswith(\".\"):\n            erhn = \".\"+erhn\n        if not (req_host.endswith(domain) or erhn.endswith(domain)):\n            #_debug(\"   request domain %s does not match cookie domain %s\",\n            #       req_host, domain)\n            return False\n\n        if self.is_blocked(domain):\n            _debug(\"   domain %s is in user block-list\", domain)\n            return False\n        if self.is_not_allowed(domain):\n            _debug(\"   domain %s is not in user allow-list\", domain)\n            return False\n\n        return True\n\n    def path_return_ok(self, path, request):\n        _debug(\"- checking cookie path=%s\", path)\n        req_path = request_path(request)\n        if not req_path.startswith(path):\n            _debug(\"  %s does not path-match %s\", req_path, path)\n            return False\n        return True\n\n\ndef vals_sorted_by_key(adict):\n    keys = adict.keys()\n    keys.sort()\n    return map(adict.get, keys)\n\ndef deepvalues(mapping):\n    \"\"\"Iterates over nested mapping, depth-first, in sorted order by key.\"\"\"\n    values = vals_sorted_by_key(mapping)\n    for obj in values:\n        mapping = False\n        try:\n            obj.items\n        except AttributeError:\n            pass\n        else:\n            mapping = True\n            for subobj in deepvalues(obj):\n                yield subobj\n        if not mapping:\n            yield obj\n\n\n# Used as second parameter to dict.get() method, to distinguish absent\n# dict key from one with a None value.\nclass Absent: pass\n\nclass CookieJar:\n    \"\"\"Collection of HTTP cookies.\n\n    You may not need to know about this class: try\n    urllib2.build_opener(HTTPCookieProcessor).open(url).\n\n    \"\"\"\n\n    non_word_re = re.compile(r\"\\W\")\n    quote_re = re.compile(r\"([\\\"\\\\])\")\n    strict_domain_re = re.compile(r\"\\.?[^.]*\")\n    domain_re = re.compile(r\"[^.]*\")\n    dots_re = re.compile(r\"^\\.+\")\n\n    magic_re = r\"^\\#LWP-Cookies-(\\d+\\.\\d+)\"\n\n    def __init__(self, policy=None):\n        if policy is None:\n            policy = DefaultCookiePolicy()\n        self._policy = policy\n\n        self._cookies_lock = _threading.RLock()\n        self._cookies = {}\n\n    def set_policy(self, policy):\n        self._policy = policy\n\n    def _cookies_for_domain(self, domain, request):\n        cookies = []\n        if not self._policy.domain_return_ok(domain, request):\n            return []\n        _debug(\"Checking %s for cookies to return\", domain)\n        cookies_by_path = self._cookies[domain]\n        for path in cookies_by_path.keys():\n            if not self._policy.path_return_ok(path, request):\n                continue\n            cookies_by_name = cookies_by_path[path]\n            for cookie in cookies_by_name.values():\n                if not self._policy.return_ok(cookie, request):\n                    _debug(\"   not returning cookie\")\n                    continue\n                _debug(\"   it's a match\")\n                cookies.append(cookie)\n        return cookies\n\n    def _cookies_for_request(self, request):\n        \"\"\"Return a list of cookies to be returned to server.\"\"\"\n        cookies = []\n        for domain in self._cookies.keys():\n            cookies.extend(self._cookies_for_domain(domain, request))\n        return cookies\n\n    def _cookie_attrs(self, cookies):\n        \"\"\"Return a list of cookie-attributes to be returned to server.\n\n        like ['foo=\"bar\"; $Path=\"/\"', ...]\n\n        The $Version attribute is also added when appropriate (currently only\n        once per request).\n\n        \"\"\"\n        # add cookies in order of most specific (ie. longest) path first\n        cookies.sort(key=lambda arg: len(arg.path), reverse=True)\n\n        version_set = False\n\n        attrs = []\n        for cookie in cookies:\n            # set version of Cookie header\n            # XXX\n            # What should it be if multiple matching Set-Cookie headers have\n            #  different versions themselves?\n            # Answer: there is no answer; was supposed to be settled by\n            #  RFC 2965 errata, but that may never appear...\n            version = cookie.version\n            if not version_set:\n                version_set = True\n                if version > 0:\n                    attrs.append(\"$Version=%s\" % version)\n\n            # quote cookie value if necessary\n            # (not for Netscape protocol, which already has any quotes\n            #  intact, due to the poorly-specified Netscape Cookie: syntax)\n            if ((cookie.value is not None) and\n                self.non_word_re.search(cookie.value) and version > 0):\n                value = self.quote_re.sub(r\"\\\\\\1\", cookie.value)\n            else:\n                value = cookie.value\n\n            # add cookie-attributes to be returned in Cookie header\n            if cookie.value is None:\n                attrs.append(cookie.name)\n            else:\n                attrs.append(\"%s=%s\" % (cookie.name, value))\n            if version > 0:\n                if cookie.path_specified:\n                    attrs.append('$Path=\"%s\"' % cookie.path)\n                if cookie.domain.startswith(\".\"):\n                    domain = cookie.domain\n                    if (not cookie.domain_initial_dot and\n                        domain.startswith(\".\")):\n                        domain = domain[1:]\n                    attrs.append('$Domain=\"%s\"' % domain)\n                if cookie.port is not None:\n                    p = \"$Port\"\n                    if cookie.port_specified:\n                        p = p + ('=\"%s\"' % cookie.port)\n                    attrs.append(p)\n\n        return attrs\n\n    def add_cookie_header(self, request):\n        \"\"\"Add correct Cookie: header to request (urllib2.Request object).\n\n        The Cookie2 header is also added unless policy.hide_cookie2 is true.\n\n        \"\"\"\n        _debug(\"add_cookie_header\")\n        self._cookies_lock.acquire()\n        try:\n\n            self._policy._now = self._now = int(time.time())\n\n            cookies = self._cookies_for_request(request)\n\n            attrs = self._cookie_attrs(cookies)\n            if attrs:\n                if not request.has_header(\"Cookie\"):\n                    request.add_unredirected_header(\n                        \"Cookie\", \"; \".join(attrs))\n\n            # if necessary, advertise that we know RFC 2965\n            if (self._policy.rfc2965 and not self._policy.hide_cookie2 and\n                not request.has_header(\"Cookie2\")):\n                for cookie in cookies:\n                    if cookie.version != 1:\n                        request.add_unredirected_header(\"Cookie2\", '$Version=\"1\"')\n                        break\n\n        finally:\n            self._cookies_lock.release()\n\n        self.clear_expired_cookies()\n\n    def _normalized_cookie_tuples(self, attrs_set):\n        \"\"\"Return list of tuples containing normalised cookie information.\n\n        attrs_set is the list of lists of key,value pairs extracted from\n        the Set-Cookie or Set-Cookie2 headers.\n\n        Tuples are name, value, standard, rest, where name and value are the\n        cookie name and value, standard is a dictionary containing the standard\n        cookie-attributes (discard, secure, version, expires or max-age,\n        domain, path and port) and rest is a dictionary containing the rest of\n        the cookie-attributes.\n\n        \"\"\"\n        cookie_tuples = []\n\n        boolean_attrs = \"discard\", \"secure\"\n        value_attrs = (\"version\",\n                       \"expires\", \"max-age\",\n                       \"domain\", \"path\", \"port\",\n                       \"comment\", \"commenturl\")\n\n        for cookie_attrs in attrs_set:\n            name, value = cookie_attrs[0]\n\n            # Build dictionary of standard cookie-attributes (standard) and\n            # dictionary of other cookie-attributes (rest).\n\n            # Note: expiry time is normalised to seconds since epoch.  V0\n            # cookies should have the Expires cookie-attribute, and V1 cookies\n            # should have Max-Age, but since V1 includes RFC 2109 cookies (and\n            # since V0 cookies may be a mish-mash of Netscape and RFC 2109), we\n            # accept either (but prefer Max-Age).\n            max_age_set = False\n\n            bad_cookie = False\n\n            standard = {}\n            rest = {}\n            for k, v in cookie_attrs[1:]:\n                lc = k.lower()\n                # don't lose case distinction for unknown fields\n                if lc in value_attrs or lc in boolean_attrs:\n                    k = lc\n                if k in boolean_attrs and v is None:\n                    # boolean cookie-attribute is present, but has no value\n                    # (like \"discard\", rather than \"port=80\")\n                    v = True\n                if k in standard:\n                    # only first value is significant\n                    continue\n                if k == \"domain\":\n                    if v is None:\n                        _debug(\"   missing value for domain attribute\")\n                        bad_cookie = True\n                        break\n                    # RFC 2965 section 3.3.3\n                    v = v.lower()\n                if k == \"expires\":\n                    if max_age_set:\n                        # Prefer max-age to expires (like Mozilla)\n                        continue\n                    if v is None:\n                        _debug(\"   missing or invalid value for expires \"\n                              \"attribute: treating as session cookie\")\n                        continue\n                if k == \"max-age\":\n                    max_age_set = True\n                    try:\n                        v = int(v)\n                    except ValueError:\n                        _debug(\"   missing or invalid (non-numeric) value for \"\n                              \"max-age attribute\")\n                        bad_cookie = True\n                        break\n                    # convert RFC 2965 Max-Age to seconds since epoch\n                    # XXX Strictly you're supposed to follow RFC 2616\n                    #   age-calculation rules.  Remember that zero Max-Age is a\n                    #   is a request to discard (old and new) cookie, though.\n                    k = \"expires\"\n                    v = self._now + v\n                if (k in value_attrs) or (k in boolean_attrs):\n                    if (v is None and\n                        k not in (\"port\", \"comment\", \"commenturl\")):\n                        _debug(\"   missing value for %s attribute\" % k)\n                        bad_cookie = True\n                        break\n                    standard[k] = v\n                else:\n                    rest[k] = v\n\n            if bad_cookie:\n                continue\n\n            cookie_tuples.append((name, value, standard, rest))\n\n        return cookie_tuples\n\n    def _cookie_from_cookie_tuple(self, tup, request):\n        # standard is dict of standard cookie-attributes, rest is dict of the\n        # rest of them\n        name, value, standard, rest = tup\n\n        domain = standard.get(\"domain\", Absent)\n        path = standard.get(\"path\", Absent)\n        port = standard.get(\"port\", Absent)\n        expires = standard.get(\"expires\", Absent)\n\n        # set the easy defaults\n        version = standard.get(\"version\", None)\n        if version is not None:\n            try:\n                version = int(version)\n            except ValueError:\n                return None  # invalid version, ignore cookie\n        secure = standard.get(\"secure\", False)\n        # (discard is also set if expires is Absent)\n        discard = standard.get(\"discard\", False)\n        comment = standard.get(\"comment\", None)\n        comment_url = standard.get(\"commenturl\", None)\n\n        # set default path\n        if path is not Absent and path != \"\":\n            path_specified = True\n            path = escape_path(path)\n        else:\n            path_specified = False\n            path = request_path(request)\n            i = path.rfind(\"/\")\n            if i != -1:\n                if version == 0:\n                    # Netscape spec parts company from reality here\n                    path = path[:i]\n                else:\n                    path = path[:i+1]\n            if len(path) == 0: path = \"/\"\n\n        # set default domain\n        domain_specified = domain is not Absent\n        # but first we have to remember whether it starts with a dot\n        domain_initial_dot = False\n        if domain_specified:\n            domain_initial_dot = bool(domain.startswith(\".\"))\n        if domain is Absent:\n            req_host, erhn = eff_request_host(request)\n            domain = erhn\n        elif not domain.startswith(\".\"):\n            domain = \".\"+domain\n\n        # set default port\n        port_specified = False\n        if port is not Absent:\n            if port is None:\n                # Port attr present, but has no value: default to request port.\n                # Cookie should then only be sent back on that port.\n                port = request_port(request)\n            else:\n                port_specified = True\n                port = re.sub(r\"\\s+\", \"\", port)\n        else:\n            # No port attr present.  Cookie can be sent back on any port.\n            port = None\n\n        # set default expires and discard\n        if expires is Absent:\n            expires = None\n            discard = True\n        elif expires <= self._now:\n            # Expiry date in past is request to delete cookie.  This can't be\n            # in DefaultCookiePolicy, because can't delete cookies there.\n            try:\n                self.clear(domain, path, name)\n            except KeyError:\n                pass\n            _debug(\"Expiring cookie, domain='%s', path='%s', name='%s'\",\n                   domain, path, name)\n            return None\n\n        return Cookie(version,\n                      name, value,\n                      port, port_specified,\n                      domain, domain_specified, domain_initial_dot,\n                      path, path_specified,\n                      secure,\n                      expires,\n                      discard,\n                      comment,\n                      comment_url,\n                      rest)\n\n    def _cookies_from_attrs_set(self, attrs_set, request):\n        cookie_tuples = self._normalized_cookie_tuples(attrs_set)\n\n        cookies = []\n        for tup in cookie_tuples:\n            cookie = self._cookie_from_cookie_tuple(tup, request)\n            if cookie: cookies.append(cookie)\n        return cookies\n\n    def _process_rfc2109_cookies(self, cookies):\n        rfc2109_as_ns = getattr(self._policy, 'rfc2109_as_netscape', None)\n        if rfc2109_as_ns is None:\n            rfc2109_as_ns = not self._policy.rfc2965\n        for cookie in cookies:\n            if cookie.version == 1:\n                cookie.rfc2109 = True\n                if rfc2109_as_ns:\n                    # treat 2109 cookies as Netscape cookies rather than\n                    # as RFC2965 cookies\n                    cookie.version = 0\n\n    def make_cookies(self, response, request):\n        \"\"\"Return sequence of Cookie objects extracted from response object.\"\"\"\n        # get cookie-attributes for RFC 2965 and Netscape protocols\n        headers = response.info()\n        rfc2965_hdrs = headers.getheaders(\"Set-Cookie2\")\n        ns_hdrs = headers.getheaders(\"Set-Cookie\")\n\n        rfc2965 = self._policy.rfc2965\n        netscape = self._policy.netscape\n\n        if ((not rfc2965_hdrs and not ns_hdrs) or\n            (not ns_hdrs and not rfc2965) or\n            (not rfc2965_hdrs and not netscape) or\n            (not netscape and not rfc2965)):\n            return []  # no relevant cookie headers: quick exit\n\n        try:\n            cookies = self._cookies_from_attrs_set(\n                split_header_words(rfc2965_hdrs), request)\n        except Exception:\n            _warn_unhandled_exception()\n            cookies = []\n\n        if ns_hdrs and netscape:\n            try:\n                # RFC 2109 and Netscape cookies\n                ns_cookies = self._cookies_from_attrs_set(\n                    parse_ns_headers(ns_hdrs), request)\n            except Exception:\n                _warn_unhandled_exception()\n                ns_cookies = []\n            self._process_rfc2109_cookies(ns_cookies)\n\n            # Look for Netscape cookies (from Set-Cookie headers) that match\n            # corresponding RFC 2965 cookies (from Set-Cookie2 headers).\n            # For each match, keep the RFC 2965 cookie and ignore the Netscape\n            # cookie (RFC 2965 section 9.1).  Actually, RFC 2109 cookies are\n            # bundled in with the Netscape cookies for this purpose, which is\n            # reasonable behaviour.\n            if rfc2965:\n                lookup = {}\n                for cookie in cookies:\n                    lookup[(cookie.domain, cookie.path, cookie.name)] = None\n\n                def no_matching_rfc2965(ns_cookie, lookup=lookup):\n                    key = ns_cookie.domain, ns_cookie.path, ns_cookie.name\n                    return key not in lookup\n                ns_cookies = filter(no_matching_rfc2965, ns_cookies)\n\n            if ns_cookies:\n                cookies.extend(ns_cookies)\n\n        return cookies\n\n    def set_cookie_if_ok(self, cookie, request):\n        \"\"\"Set a cookie if policy says it's OK to do so.\"\"\"\n        self._cookies_lock.acquire()\n        try:\n            self._policy._now = self._now = int(time.time())\n\n            if self._policy.set_ok(cookie, request):\n                self.set_cookie(cookie)\n\n\n        finally:\n            self._cookies_lock.release()\n\n    def set_cookie(self, cookie):\n        \"\"\"Set a cookie, without checking whether or not it should be set.\"\"\"\n        c = self._cookies\n        self._cookies_lock.acquire()\n        try:\n            if cookie.domain not in c: c[cookie.domain] = {}\n            c2 = c[cookie.domain]\n            if cookie.path not in c2: c2[cookie.path] = {}\n            c3 = c2[cookie.path]\n            c3[cookie.name] = cookie\n        finally:\n            self._cookies_lock.release()\n\n    def extract_cookies(self, response, request):\n        \"\"\"Extract cookies from response, where allowable given the request.\"\"\"\n        _debug(\"extract_cookies: %s\", response.info())\n        self._cookies_lock.acquire()\n        try:\n            self._policy._now = self._now = int(time.time())\n\n            for cookie in self.make_cookies(response, request):\n                if self._policy.set_ok(cookie, request):\n                    _debug(\" setting cookie: %s\", cookie)\n                    self.set_cookie(cookie)\n        finally:\n            self._cookies_lock.release()\n\n    def clear(self, domain=None, path=None, name=None):\n        \"\"\"Clear some cookies.\n\n        Invoking this method without arguments will clear all cookies.  If\n        given a single argument, only cookies belonging to that domain will be\n        removed.  If given two arguments, cookies belonging to the specified\n        path within that domain are removed.  If given three arguments, then\n        the cookie with the specified name, path and domain is removed.\n\n        Raises KeyError if no matching cookie exists.\n\n        \"\"\"\n        if name is not None:\n            if (domain is None) or (path is None):\n                raise ValueError(\n                    \"domain and path must be given to remove a cookie by name\")\n            del self._cookies[domain][path][name]\n        elif path is not None:\n            if domain is None:\n                raise ValueError(\n                    \"domain must be given to remove cookies by path\")\n            del self._cookies[domain][path]\n        elif domain is not None:\n            del self._cookies[domain]\n        else:\n            self._cookies = {}\n\n    def clear_session_cookies(self):\n        \"\"\"Discard all session cookies.\n\n        Note that the .save() method won't save session cookies anyway, unless\n        you ask otherwise by passing a true ignore_discard argument.\n\n        \"\"\"\n        self._cookies_lock.acquire()\n        try:\n            for cookie in self:\n                if cookie.discard:\n                    self.clear(cookie.domain, cookie.path, cookie.name)\n        finally:\n            self._cookies_lock.release()\n\n    def clear_expired_cookies(self):\n        \"\"\"Discard all expired cookies.\n\n        You probably don't need to call this method: expired cookies are never\n        sent back to the server (provided you're using DefaultCookiePolicy),\n        this method is called by CookieJar itself every so often, and the\n        .save() method won't save expired cookies anyway (unless you ask\n        otherwise by passing a true ignore_expires argument).\n\n        \"\"\"\n        self._cookies_lock.acquire()\n        try:\n            now = time.time()\n            for cookie in self:\n                if cookie.is_expired(now):\n                    self.clear(cookie.domain, cookie.path, cookie.name)\n        finally:\n            self._cookies_lock.release()\n\n    def __iter__(self):\n        return deepvalues(self._cookies)\n\n    def __len__(self):\n        \"\"\"Return number of contained cookies.\"\"\"\n        i = 0\n        for cookie in self: i = i + 1\n        return i\n\n    def __repr__(self):\n        r = []\n        for cookie in self: r.append(repr(cookie))\n        return \"<%s[%s]>\" % (self.__class__.__name__, \", \".join(r))\n\n    def __str__(self):\n        r = []\n        for cookie in self: r.append(str(cookie))\n        return \"<%s[%s]>\" % (self.__class__.__name__, \", \".join(r))\n\n\n# derives from IOError for backwards-compatibility with Python 2.4.0\nclass LoadError(IOError): pass\n\nclass FileCookieJar(CookieJar):\n    \"\"\"CookieJar that can be loaded from and saved to a file.\"\"\"\n\n    def __init__(self, filename=None, delayload=False, policy=None):\n        \"\"\"\n        Cookies are NOT loaded from the named file until either the .load() or\n        .revert() method is called.\n\n        \"\"\"\n        CookieJar.__init__(self, policy)\n        if filename is not None:\n            try:\n                filename+\"\"\n            except:\n                raise ValueError(\"filename must be string-like\")\n        self.filename = filename\n        self.delayload = bool(delayload)\n\n    def save(self, filename=None, ignore_discard=False, ignore_expires=False):\n        \"\"\"Save cookies to a file.\"\"\"\n        raise NotImplementedError()\n\n    def load(self, filename=None, ignore_discard=False, ignore_expires=False):\n        \"\"\"Load cookies from a file.\"\"\"\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        f = open(filename)\n        try:\n            self._really_load(f, filename, ignore_discard, ignore_expires)\n        finally:\n            f.close()\n\n    def revert(self, filename=None,\n               ignore_discard=False, ignore_expires=False):\n        \"\"\"Clear all cookies and reload cookies from a saved file.\n\n        Raises LoadError (or IOError) if reversion is not successful; the\n        object's state will not be altered if this happens.\n\n        \"\"\"\n        if filename is None:\n            if self.filename is not None: filename = self.filename\n            else: raise ValueError(MISSING_FILENAME_TEXT)\n\n        self._cookies_lock.acquire()\n        try:\n\n            old_state = copy.deepcopy(self._cookies)\n            self._cookies = {}\n            try:\n                self.load(filename, ignore_discard, ignore_expires)\n            except (LoadError, IOError):\n                self._cookies = old_state\n                raise\n\n        finally:\n            self._cookies_lock.release()\n\nfrom _LWPCookieJar import LWPCookieJar, lwp_cookie_str\nfrom _MozillaCookieJar import MozillaCookieJar\n", 
    "copy": "\"\"\"Generic (shallow and deep) copying operations.\n\nInterface summary:\n\n        import copy\n\n        x = copy.copy(y)        # make a shallow copy of y\n        x = copy.deepcopy(y)    # make a deep copy of y\n\nFor module specific errors, copy.Error is raised.\n\nThe difference between shallow and deep copying is only relevant for\ncompound objects (objects that contain other objects, like lists or\nclass instances).\n\n- A shallow copy constructs a new compound object and then (to the\n  extent possible) inserts *the same objects* into it that the\n  original contains.\n\n- A deep copy constructs a new compound object and then, recursively,\n  inserts *copies* into it of the objects found in the original.\n\nTwo problems often exist with deep copy operations that don't exist\nwith shallow copy operations:\n\n a) recursive objects (compound objects that, directly or indirectly,\n    contain a reference to themselves) may cause a recursive loop\n\n b) because deep copy copies *everything* it may copy too much, e.g.\n    administrative data structures that should be shared even between\n    copies\n\nPython's deep copy operation avoids these problems by:\n\n a) keeping a table of objects already copied during the current\n    copying pass\n\n b) letting user-defined classes override the copying operation or the\n    set of components copied\n\nThis version does not copy types like module, class, function, method,\nnor stack trace, stack frame, nor file, socket, window, nor array, nor\nany similar types.\n\nClasses can use the same interfaces to control copying that they use\nto control pickling: they can define methods called __getinitargs__(),\n__getstate__() and __setstate__().  See the documentation for module\n\"pickle\" for information on these methods.\n\"\"\"\n\nimport types\nimport weakref\nfrom copy_reg import dispatch_table\n\nclass Error(Exception):\n    pass\nerror = Error   # backward compatibility\n\ntry:\n    from org.python.core import PyStringMap\nexcept ImportError:\n    PyStringMap = None\n\n__all__ = [\"Error\", \"copy\", \"deepcopy\"]\n\ndef copy(x):\n    \"\"\"Shallow copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    cls = type(x)\n\n    copier = _copy_dispatch.get(cls)\n    if copier:\n        return copier(x)\n\n    copier = getattr(cls, \"__copy__\", None)\n    if copier:\n        return copier(x)\n\n    reductor = dispatch_table.get(cls)\n    if reductor:\n        rv = reductor(x)\n    else:\n        reductor = getattr(x, \"__reduce_ex__\", None)\n        if reductor:\n            rv = reductor(2)\n        else:\n            reductor = getattr(x, \"__reduce__\", None)\n            if reductor:\n                rv = reductor()\n            else:\n                raise Error(\"un(shallow)copyable object of type %s\" % cls)\n\n    return _reconstruct(x, rv, 0)\n\n\n_copy_dispatch = d = {}\n\ndef _copy_immutable(x):\n    return x\nfor t in (type(None), int, long, float, bool, str, tuple,\n          frozenset, type, xrange, types.ClassType,\n          types.BuiltinFunctionType, type(Ellipsis),\n          types.FunctionType, weakref.ref):\n    d[t] = _copy_immutable\nfor name in (\"ComplexType\", \"UnicodeType\", \"CodeType\"):\n    t = getattr(types, name, None)\n    if t is not None:\n        d[t] = _copy_immutable\n\ndef _copy_with_constructor(x):\n    return type(x)(x)\nfor t in (list, dict, set):\n    d[t] = _copy_with_constructor\n\ndef _copy_with_copy_method(x):\n    return x.copy()\nif PyStringMap is not None:\n    d[PyStringMap] = _copy_with_copy_method\n\ndef _copy_inst(x):\n    if hasattr(x, '__copy__'):\n        return x.__copy__()\n    if hasattr(x, '__getinitargs__'):\n        args = x.__getinitargs__()\n        y = x.__class__(*args)\n    else:\n        y = _EmptyClass()\n        y.__class__ = x.__class__\n    if hasattr(x, '__getstate__'):\n        state = x.__getstate__()\n    else:\n        state = x.__dict__\n    if hasattr(y, '__setstate__'):\n        y.__setstate__(state)\n    else:\n        y.__dict__.update(state)\n    return y\nd[types.InstanceType] = _copy_inst\n\ndel d\n\ndef deepcopy(x, memo=None, _nil=[]):\n    \"\"\"Deep copy operation on arbitrary Python objects.\n\n    See the module's __doc__ string for more info.\n    \"\"\"\n\n    if memo is None:\n        memo = {}\n\n    d = id(x)\n    y = memo.get(d, _nil)\n    if y is not _nil:\n        return y\n\n    cls = type(x)\n\n    copier = _deepcopy_dispatch.get(cls)\n    if copier:\n        y = copier(x, memo)\n    else:\n        try:\n            issc = issubclass(cls, type)\n        except TypeError: # cls is not a class (old Boost; see SF #502085)\n            issc = 0\n        if issc:\n            y = _deepcopy_atomic(x, memo)\n        else:\n            copier = getattr(x, \"__deepcopy__\", None)\n            if copier:\n                y = copier(memo)\n            else:\n                reductor = dispatch_table.get(cls)\n                if reductor:\n                    rv = reductor(x)\n                else:\n                    reductor = getattr(x, \"__reduce_ex__\", None)\n                    if reductor:\n                        rv = reductor(2)\n                    else:\n                        reductor = getattr(x, \"__reduce__\", None)\n                        if reductor:\n                            rv = reductor()\n                        else:\n                            raise Error(\n                                \"un(deep)copyable object of type %s\" % cls)\n                y = _reconstruct(x, rv, 1, memo)\n\n    memo[d] = y\n    _keep_alive(x, memo) # Make sure x lives at least as long as d\n    return y\n\n_deepcopy_dispatch = d = {}\n\ndef _deepcopy_atomic(x, memo):\n    return x\nd[type(None)] = _deepcopy_atomic\nd[type(Ellipsis)] = _deepcopy_atomic\nd[int] = _deepcopy_atomic\nd[long] = _deepcopy_atomic\nd[float] = _deepcopy_atomic\nd[bool] = _deepcopy_atomic\ntry:\n    d[complex] = _deepcopy_atomic\nexcept NameError:\n    pass\nd[str] = _deepcopy_atomic\ntry:\n    d[unicode] = _deepcopy_atomic\nexcept NameError:\n    pass\ntry:\n    d[types.CodeType] = _deepcopy_atomic\nexcept AttributeError:\n    pass\nd[type] = _deepcopy_atomic\nd[xrange] = _deepcopy_atomic\nd[types.ClassType] = _deepcopy_atomic\nd[types.BuiltinFunctionType] = _deepcopy_atomic\nd[types.FunctionType] = _deepcopy_atomic\nd[weakref.ref] = _deepcopy_atomic\n\ndef _deepcopy_list(x, memo):\n    y = []\n    memo[id(x)] = y\n    for a in x:\n        y.append(deepcopy(a, memo))\n    return y\nd[list] = _deepcopy_list\n\ndef _deepcopy_tuple(x, memo):\n    y = []\n    for a in x:\n        y.append(deepcopy(a, memo))\n    d = id(x)\n    try:\n        return memo[d]\n    except KeyError:\n        pass\n    for i in range(len(x)):\n        if x[i] is not y[i]:\n            y = tuple(y)\n            break\n    else:\n        y = x\n    memo[d] = y\n    return y\nd[tuple] = _deepcopy_tuple\n\ndef _deepcopy_dict(x, memo):\n    y = {}\n    memo[id(x)] = y\n    for key, value in x.iteritems():\n        y[deepcopy(key, memo)] = deepcopy(value, memo)\n    return y\nd[dict] = _deepcopy_dict\nif PyStringMap is not None:\n    d[PyStringMap] = _deepcopy_dict\n\ndef _deepcopy_method(x, memo): # Copy instance methods\n    return type(x)(x.im_func, deepcopy(x.im_self, memo), x.im_class)\n_deepcopy_dispatch[types.MethodType] = _deepcopy_method\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\ndef _deepcopy_inst(x, memo):\n    if hasattr(x, '__deepcopy__'):\n        return x.__deepcopy__(memo)\n    if hasattr(x, '__getinitargs__'):\n        args = x.__getinitargs__()\n        args = deepcopy(args, memo)\n        y = x.__class__(*args)\n    else:\n        y = _EmptyClass()\n        y.__class__ = x.__class__\n    memo[id(x)] = y\n    if hasattr(x, '__getstate__'):\n        state = x.__getstate__()\n    else:\n        state = x.__dict__\n    state = deepcopy(state, memo)\n    if hasattr(y, '__setstate__'):\n        y.__setstate__(state)\n    else:\n        y.__dict__.update(state)\n    return y\nd[types.InstanceType] = _deepcopy_inst\n\ndef _reconstruct(x, info, deep, memo=None):\n    if isinstance(info, str):\n        return x\n    assert isinstance(info, tuple)\n    if memo is None:\n        memo = {}\n    n = len(info)\n    assert n in (2, 3, 4, 5)\n    callable, args = info[:2]\n    if n > 2:\n        state = info[2]\n    else:\n        state = {}\n    if n > 3:\n        listiter = info[3]\n    else:\n        listiter = None\n    if n > 4:\n        dictiter = info[4]\n    else:\n        dictiter = None\n    if deep:\n        args = deepcopy(args, memo)\n    y = callable(*args)\n    memo[id(x)] = y\n\n    if state:\n        if deep:\n            state = deepcopy(state, memo)\n        if hasattr(y, '__setstate__'):\n            y.__setstate__(state)\n        else:\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            else:\n                slotstate = None\n            if state is not None:\n                y.__dict__.update(state)\n            if slotstate is not None:\n                for key, value in slotstate.iteritems():\n                    setattr(y, key, value)\n\n    if listiter is not None:\n        for item in listiter:\n            if deep:\n                item = deepcopy(item, memo)\n            y.append(item)\n    if dictiter is not None:\n        for key, value in dictiter:\n            if deep:\n                key = deepcopy(key, memo)\n                value = deepcopy(value, memo)\n            y[key] = value\n    return y\n\ndel d\n\ndel types\n\n# Helper for instance creation without calling __init__\nclass _EmptyClass:\n    pass\n\ndef _test():\n    l = [None, 1, 2L, 3.14, 'xyzzy', (1, 2L), [3.14, 'abc'],\n         {'abc': 'ABC'}, (), [], {}]\n    l1 = copy(l)\n    print l1==l\n    l1 = map(copy, l)\n    print l1==l\n    l1 = deepcopy(l)\n    print l1==l\n    class C:\n        def __init__(self, arg=None):\n            self.a = 1\n            self.arg = arg\n            if __name__ == '__main__':\n                import sys\n                file = sys.argv[0]\n            else:\n                file = __file__\n            self.fp = open(file)\n            self.fp.close()\n        def __getstate__(self):\n            return {'a': self.a, 'arg': self.arg}\n        def __setstate__(self, state):\n            for key, value in state.iteritems():\n                setattr(self, key, value)\n        def __deepcopy__(self, memo=None):\n            new = self.__class__(deepcopy(self.arg, memo))\n            new.a = self.a\n            return new\n    c = C('argument sketch')\n    l.append(c)\n    l2 = copy(l)\n    print l == l2\n    print l\n    print l2\n    l2 = deepcopy(l)\n    print l == l2\n    print l\n    print l2\n    l.append({l[1]: l, 'xyz': l[2]})\n    l3 = copy(l)\n    import repr\n    print map(repr.repr, l)\n    print map(repr.repr, l1)\n    print map(repr.repr, l2)\n    print map(repr.repr, l3)\n    l3 = deepcopy(l)\n    import repr\n    print map(repr.repr, l)\n    print map(repr.repr, l1)\n    print map(repr.repr, l2)\n    print map(repr.repr, l3)\n    class odict(dict):\n        def __init__(self, d = {}):\n            self.a = 99\n            dict.__init__(self, d)\n        def __setitem__(self, k, i):\n            dict.__setitem__(self, k, i)\n            self.a\n    o = odict({\"A\" : \"B\"})\n    x = deepcopy(o)\n    print(o, x)\n\nif __name__ == '__main__':\n    _test()\n", 
    "copy_reg": "\"\"\"Helper to provide extensibility for pickle/cPickle.\n\nThis is only useful to add pickle support for extension types defined in\nC, not for instances of user-defined classes.\n\"\"\"\n\nfrom types import ClassType as _ClassType\n\n__all__ = [\"pickle\", \"constructor\",\n           \"add_extension\", \"remove_extension\", \"clear_extension_cache\"]\n\ndispatch_table = {}\n\ndef pickle(ob_type, pickle_function, constructor_ob=None):\n    if type(ob_type) is _ClassType:\n        raise TypeError(\"copy_reg is not intended for use with classes\")\n\n    if not hasattr(pickle_function, '__call__'):\n        raise TypeError(\"reduction functions must be callable\")\n    dispatch_table[ob_type] = pickle_function\n\n    # The constructor_ob function is a vestige of safe for unpickling.\n    # There is no reason for the caller to pass it anymore.\n    if constructor_ob is not None:\n        constructor(constructor_ob)\n\ndef constructor(object):\n    if not hasattr(object, '__call__'):\n        raise TypeError(\"constructors must be callable\")\n\n# Example: provide pickling support for complex numbers.\n\ntry:\n    complex\nexcept NameError:\n    pass\nelse:\n\n    def pickle_complex(c):\n        return complex, (c.real, c.imag)\n\n    pickle(complex, pickle_complex, complex)\n\n# Support for pickling new-style objects\n\ndef _reconstructor(cls, base, state):\n    if base is object:\n        obj = object.__new__(cls)\n    else:\n        obj = base.__new__(cls, state)\n        if base.__init__ != object.__init__:\n            base.__init__(obj, state)\n    return obj\n\n_HEAPTYPE = 1<<9\n\n# Python code for object.__reduce_ex__ for protocols 0 and 1\n\ndef _reduce_ex(self, proto):\n    assert proto < 2\n    for base in self.__class__.__mro__:\n        if hasattr(base, '__flags__') and not base.__flags__ & _HEAPTYPE:\n            break\n    else:\n        base = object # not really reachable\n    if base is object:\n        state = None\n    else:\n        if base is self.__class__:\n            raise TypeError, \"can't pickle %s objects\" % base.__name__\n        state = base(self)\n    args = (self.__class__, base, state)\n    try:\n        getstate = self.__getstate__\n    except AttributeError:\n        if getattr(self, \"__slots__\", None):\n            raise TypeError(\"a class that defines __slots__ without \"\n                            \"defining __getstate__ cannot be pickled\")\n        try:\n            dict = self.__dict__\n        except AttributeError:\n            dict = None\n    else:\n        dict = getstate()\n    if dict:\n        return _reconstructor, args, dict\n    else:\n        return _reconstructor, args\n\n# Helper for __reduce_ex__ protocol 2\n\ndef __newobj__(cls, *args):\n    return cls.__new__(cls, *args)\n\ndef _slotnames(cls):\n    \"\"\"Return a list of slot names for a given class.\n\n    This needs to find slots defined by the class and its bases, so we\n    can't simply return the __slots__ attribute.  We must walk down\n    the Method Resolution Order and concatenate the __slots__ of each\n    class found there.  (This assumes classes don't modify their\n    __slots__ attribute to misrepresent their slots after the class is\n    defined.)\n    \"\"\"\n\n    # Get the value from a cache in the class if possible\n    names = cls.__dict__.get(\"__slotnames__\")\n    if names is not None:\n        return names\n\n    # Not cached -- calculate the value\n    names = []\n    if not hasattr(cls, \"__slots__\"):\n        # This class has no slots\n        pass\n    else:\n        # Slots found -- gather slot names from all base classes\n        for c in cls.__mro__:\n            if \"__slots__\" in c.__dict__:\n                slots = c.__dict__['__slots__']\n                # if class has a single slot, it can be given as a string\n                if isinstance(slots, basestring):\n                    slots = (slots,)\n                for name in slots:\n                    # special descriptors\n                    if name in (\"__dict__\", \"__weakref__\"):\n                        continue\n                    # mangled names\n                    elif name.startswith('__') and not name.endswith('__'):\n                        names.append('_%s%s' % (c.__name__, name))\n                    else:\n                        names.append(name)\n\n    # Cache the outcome in the class if at all possible\n    try:\n        cls.__slotnames__ = names\n    except:\n        pass # But don't die if we can't\n\n    return names\n\n# A registry of extension codes.  This is an ad-hoc compression\n# mechanism.  Whenever a global reference to <module>, <name> is about\n# to be pickled, the (<module>, <name>) tuple is looked up here to see\n# if it is a registered extension code for it.  Extension codes are\n# universal, so that the meaning of a pickle does not depend on\n# context.  (There are also some codes reserved for local use that\n# don't have this restriction.)  Codes are positive ints; 0 is\n# reserved.\n\n_extension_registry = {}                # key -> code\n_inverted_registry = {}                 # code -> key\n_extension_cache = {}                   # code -> object\n# Don't ever rebind those names:  cPickle grabs a reference to them when\n# it's initialized, and won't see a rebinding.\n\ndef add_extension(module, name, code):\n    \"\"\"Register an extension code.\"\"\"\n    code = int(code)\n    if not 1 <= code <= 0x7fffffff:\n        raise ValueError, \"code out of range\"\n    key = (module, name)\n    if (_extension_registry.get(key) == code and\n        _inverted_registry.get(code) == key):\n        return # Redundant registrations are benign\n    if key in _extension_registry:\n        raise ValueError(\"key %s is already registered with code %s\" %\n                         (key, _extension_registry[key]))\n    if code in _inverted_registry:\n        raise ValueError(\"code %s is already in use for key %s\" %\n                         (code, _inverted_registry[code]))\n    _extension_registry[key] = code\n    _inverted_registry[code] = key\n\ndef remove_extension(module, name, code):\n    \"\"\"Unregister an extension code.  For testing only.\"\"\"\n    key = (module, name)\n    if (_extension_registry.get(key) != code or\n        _inverted_registry.get(code) != key):\n        raise ValueError(\"key %s is not registered with code %s\" %\n                         (key, code))\n    del _extension_registry[key]\n    del _inverted_registry[code]\n    if code in _extension_cache:\n        del _extension_cache[code]\n\ndef clear_extension_cache():\n    _extension_cache.clear()\n\n# Standard extension code assignments\n\n# Reserved ranges\n\n# First  Last Count  Purpose\n#     1   127   127  Reserved for Python standard library\n#   128   191    64  Reserved for Zope\n#   192   239    48  Reserved for 3rd parties\n#   240   255    16  Reserved for private use (will never be assigned)\n#   256   Inf   Inf  Reserved for future assignment\n\n# Extension codes are assigned by the Python Software Foundation.\n", 
    "csv": "\n\"\"\"\ncsv.py - read/write/investigate CSV files\n\"\"\"\n\nimport re\nfrom functools import reduce\nfrom _csv import Error, __version__, writer, reader, register_dialect, \\\n                 unregister_dialect, get_dialect, list_dialects, \\\n                 field_size_limit, \\\n                 QUOTE_MINIMAL, QUOTE_ALL, QUOTE_NONNUMERIC, QUOTE_NONE, \\\n                 __doc__\nfrom _csv import Dialect as _Dialect\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\n__all__ = [ \"QUOTE_MINIMAL\", \"QUOTE_ALL\", \"QUOTE_NONNUMERIC\", \"QUOTE_NONE\",\n            \"Error\", \"Dialect\", \"__doc__\", \"excel\", \"excel_tab\",\n            \"field_size_limit\", \"reader\", \"writer\",\n            \"register_dialect\", \"get_dialect\", \"list_dialects\", \"Sniffer\",\n            \"unregister_dialect\", \"__version__\", \"DictReader\", \"DictWriter\" ]\n\nclass Dialect:\n    \"\"\"Describe an Excel dialect.\n\n    This must be subclassed (see csv.excel).  Valid attributes are:\n    delimiter, quotechar, escapechar, doublequote, skipinitialspace,\n    lineterminator, quoting.\n\n    \"\"\"\n    _name = \"\"\n    _valid = False\n    # placeholders\n    delimiter = None\n    quotechar = None\n    escapechar = None\n    doublequote = None\n    skipinitialspace = None\n    lineterminator = None\n    quoting = None\n\n    def __init__(self):\n        if self.__class__ != Dialect:\n            self._valid = True\n        self._validate()\n\n    def _validate(self):\n        try:\n            _Dialect(self)\n        except TypeError, e:\n            # We do this for compatibility with py2.3\n            raise Error(str(e))\n\nclass excel(Dialect):\n    \"\"\"Describe the usual properties of Excel-generated CSV files.\"\"\"\n    delimiter = ','\n    quotechar = '\"'\n    doublequote = True\n    skipinitialspace = False\n    lineterminator = '\\r\\n'\n    quoting = QUOTE_MINIMAL\nregister_dialect(\"excel\", excel)\n\nclass excel_tab(excel):\n    \"\"\"Describe the usual properties of Excel-generated TAB-delimited files.\"\"\"\n    delimiter = '\\t'\nregister_dialect(\"excel-tab\", excel_tab)\n\n\nclass DictReader:\n    def __init__(self, f, fieldnames=None, restkey=None, restval=None,\n                 dialect=\"excel\", *args, **kwds):\n        self._fieldnames = fieldnames   # list of keys for the dict\n        self.restkey = restkey          # key to catch long rows\n        self.restval = restval          # default value for short rows\n        self.reader = reader(f, dialect, *args, **kwds)\n        self.dialect = dialect\n        self.line_num = 0\n\n    def __iter__(self):\n        return self\n\n    @property\n    def fieldnames(self):\n        if self._fieldnames is None:\n            try:\n                self._fieldnames = self.reader.next()\n            except StopIteration:\n                pass\n        self.line_num = self.reader.line_num\n        return self._fieldnames\n\n    # Issue 20004: Because DictReader is a classic class, this setter is\n    # ignored.  At this point in 2.7's lifecycle, it is too late to change the\n    # base class for fear of breaking working code.  If you want to change\n    # fieldnames without overwriting the getter, set _fieldnames directly.\n    @fieldnames.setter\n    def fieldnames(self, value):\n        self._fieldnames = value\n\n    def next(self):\n        if self.line_num == 0:\n            # Used only for its side effect.\n            self.fieldnames\n        row = self.reader.next()\n        self.line_num = self.reader.line_num\n\n        # unlike the basic reader, we prefer not to return blanks,\n        # because we will typically wind up with a dict full of None\n        # values\n        while row == []:\n            row = self.reader.next()\n        d = dict(zip(self.fieldnames, row))\n        lf = len(self.fieldnames)\n        lr = len(row)\n        if lf < lr:\n            d[self.restkey] = row[lf:]\n        elif lf > lr:\n            for key in self.fieldnames[lr:]:\n                d[key] = self.restval\n        return d\n\n\nclass DictWriter:\n    def __init__(self, f, fieldnames, restval=\"\", extrasaction=\"raise\",\n                 dialect=\"excel\", *args, **kwds):\n        self.fieldnames = fieldnames    # list of keys for the dict\n        self.restval = restval          # for writing short dicts\n        if extrasaction.lower() not in (\"raise\", \"ignore\"):\n            raise ValueError, \\\n                  (\"extrasaction (%s) must be 'raise' or 'ignore'\" %\n                   extrasaction)\n        self.extrasaction = extrasaction\n        self.writer = writer(f, dialect, *args, **kwds)\n\n    def writeheader(self):\n        header = dict(zip(self.fieldnames, self.fieldnames))\n        self.writerow(header)\n\n    def _dict_to_list(self, rowdict):\n        if self.extrasaction == \"raise\":\n            wrong_fields = [k for k in rowdict if k not in self.fieldnames]\n            if wrong_fields:\n                raise ValueError(\"dict contains fields not in fieldnames: \"\n                                 + \", \".join([repr(x) for x in wrong_fields]))\n        return [rowdict.get(key, self.restval) for key in self.fieldnames]\n\n    def writerow(self, rowdict):\n        return self.writer.writerow(self._dict_to_list(rowdict))\n\n    def writerows(self, rowdicts):\n        rows = []\n        for rowdict in rowdicts:\n            rows.append(self._dict_to_list(rowdict))\n        return self.writer.writerows(rows)\n\n# Guard Sniffer's type checking against builds that exclude complex()\ntry:\n    complex\nexcept NameError:\n    complex = float\n\nclass Sniffer:\n    '''\n    \"Sniffs\" the format of a CSV file (i.e. delimiter, quotechar)\n    Returns a Dialect object.\n    '''\n    def __init__(self):\n        # in case there is more than one possible delimiter\n        self.preferred = [',', '\\t', ';', ' ', ':']\n\n\n    def sniff(self, sample, delimiters=None):\n        \"\"\"\n        Returns a dialect (or None) corresponding to the sample\n        \"\"\"\n\n        quotechar, doublequote, delimiter, skipinitialspace = \\\n                   self._guess_quote_and_delimiter(sample, delimiters)\n        if not delimiter:\n            delimiter, skipinitialspace = self._guess_delimiter(sample,\n                                                                delimiters)\n\n        if not delimiter:\n            raise Error, \"Could not determine delimiter\"\n\n        class dialect(Dialect):\n            _name = \"sniffed\"\n            lineterminator = '\\r\\n'\n            quoting = QUOTE_MINIMAL\n            # escapechar = ''\n\n        dialect.doublequote = doublequote\n        dialect.delimiter = delimiter\n        # _csv.reader won't accept a quotechar of ''\n        dialect.quotechar = quotechar or '\"'\n        dialect.skipinitialspace = skipinitialspace\n\n        return dialect\n\n\n    def _guess_quote_and_delimiter(self, data, delimiters):\n        \"\"\"\n        Looks for text enclosed between two identical quotes\n        (the probable quotechar) which are preceded and followed\n        by the same character (the probable delimiter).\n        For example:\n                         ,'some text',\n        The quote with the most wins, same with the delimiter.\n        If there is no quotechar the delimiter can't be determined\n        this way.\n        \"\"\"\n\n        matches = []\n        for restr in ('(?P<delim>[^\\w\\n\"\\'])(?P<space> ?)(?P<quote>[\"\\']).*?(?P=quote)(?P=delim)', # ,\".*?\",\n                      '(?:^|\\n)(?P<quote>[\"\\']).*?(?P=quote)(?P<delim>[^\\w\\n\"\\'])(?P<space> ?)',   #  \".*?\",\n                      '(?P<delim>>[^\\w\\n\"\\'])(?P<space> ?)(?P<quote>[\"\\']).*?(?P=quote)(?:$|\\n)',  # ,\".*?\"\n                      '(?:^|\\n)(?P<quote>[\"\\']).*?(?P=quote)(?:$|\\n)'):                            #  \".*?\" (no delim, no space)\n            regexp = re.compile(restr, re.DOTALL | re.MULTILINE)\n            matches = regexp.findall(data)\n            if matches:\n                break\n\n        if not matches:\n            # (quotechar, doublequote, delimiter, skipinitialspace)\n            return ('', False, None, 0)\n        quotes = {}\n        delims = {}\n        spaces = 0\n        for m in matches:\n            n = regexp.groupindex['quote'] - 1\n            key = m[n]\n            if key:\n                quotes[key] = quotes.get(key, 0) + 1\n            try:\n                n = regexp.groupindex['delim'] - 1\n                key = m[n]\n            except KeyError:\n                continue\n            if key and (delimiters is None or key in delimiters):\n                delims[key] = delims.get(key, 0) + 1\n            try:\n                n = regexp.groupindex['space'] - 1\n            except KeyError:\n                continue\n            if m[n]:\n                spaces += 1\n\n        quotechar = reduce(lambda a, b, quotes = quotes:\n                           (quotes[a] > quotes[b]) and a or b, quotes.keys())\n\n        if delims:\n            delim = reduce(lambda a, b, delims = delims:\n                           (delims[a] > delims[b]) and a or b, delims.keys())\n            skipinitialspace = delims[delim] == spaces\n            if delim == '\\n': # most likely a file with a single column\n                delim = ''\n        else:\n            # there is *no* delimiter, it's a single column of quoted data\n            delim = ''\n            skipinitialspace = 0\n\n        # if we see an extra quote between delimiters, we've got a\n        # double quoted format\n        dq_regexp = re.compile(\n                               r\"((%(delim)s)|^)\\W*%(quote)s[^%(delim)s\\n]*%(quote)s[^%(delim)s\\n]*%(quote)s\\W*((%(delim)s)|$)\" % \\\n                               {'delim':re.escape(delim), 'quote':quotechar}, re.MULTILINE)\n\n\n\n        if dq_regexp.search(data):\n            doublequote = True\n        else:\n            doublequote = False\n\n        return (quotechar, doublequote, delim, skipinitialspace)\n\n\n    def _guess_delimiter(self, data, delimiters):\n        \"\"\"\n        The delimiter /should/ occur the same number of times on\n        each row. However, due to malformed data, it may not. We don't want\n        an all or nothing approach, so we allow for small variations in this\n        number.\n          1) build a table of the frequency of each character on every line.\n          2) build a table of frequencies of this frequency (meta-frequency?),\n             e.g.  'x occurred 5 times in 10 rows, 6 times in 1000 rows,\n             7 times in 2 rows'\n          3) use the mode of the meta-frequency to determine the /expected/\n             frequency for that character\n          4) find out how often the character actually meets that goal\n          5) the character that best meets its goal is the delimiter\n        For performance reasons, the data is evaluated in chunks, so it can\n        try and evaluate the smallest portion of the data possible, evaluating\n        additional chunks as necessary.\n        \"\"\"\n\n        data = filter(None, data.split('\\n'))\n\n        ascii = [chr(c) for c in range(127)] # 7-bit ASCII\n\n        # build frequency tables\n        chunkLength = min(10, len(data))\n        iteration = 0\n        charFrequency = {}\n        modes = {}\n        delims = {}\n        start, end = 0, min(chunkLength, len(data))\n        while start < len(data):\n            iteration += 1\n            for line in data[start:end]:\n                for char in ascii:\n                    metaFrequency = charFrequency.get(char, {})\n                    # must count even if frequency is 0\n                    freq = line.count(char)\n                    # value is the mode\n                    metaFrequency[freq] = metaFrequency.get(freq, 0) + 1\n                    charFrequency[char] = metaFrequency\n\n            for char in charFrequency.keys():\n                items = charFrequency[char].items()\n                if len(items) == 1 and items[0][0] == 0:\n                    continue\n                # get the mode of the frequencies\n                if len(items) > 1:\n                    modes[char] = reduce(lambda a, b: a[1] > b[1] and a or b,\n                                         items)\n                    # adjust the mode - subtract the sum of all\n                    # other frequencies\n                    items.remove(modes[char])\n                    modes[char] = (modes[char][0], modes[char][1]\n                                   - reduce(lambda a, b: (0, a[1] + b[1]),\n                                            items)[1])\n                else:\n                    modes[char] = items[0]\n\n            # build a list of possible delimiters\n            modeList = modes.items()\n            total = float(chunkLength * iteration)\n            # (rows of consistent data) / (number of rows) = 100%\n            consistency = 1.0\n            # minimum consistency threshold\n            threshold = 0.9\n            while len(delims) == 0 and consistency >= threshold:\n                for k, v in modeList:\n                    if v[0] > 0 and v[1] > 0:\n                        if ((v[1]/total) >= consistency and\n                            (delimiters is None or k in delimiters)):\n                            delims[k] = v\n                consistency -= 0.01\n\n            if len(delims) == 1:\n                delim = delims.keys()[0]\n                skipinitialspace = (data[0].count(delim) ==\n                                    data[0].count(\"%c \" % delim))\n                return (delim, skipinitialspace)\n\n            # analyze another chunkLength lines\n            start = end\n            end += chunkLength\n\n        if not delims:\n            return ('', 0)\n\n        # if there's more than one, fall back to a 'preferred' list\n        if len(delims) > 1:\n            for d in self.preferred:\n                if d in delims.keys():\n                    skipinitialspace = (data[0].count(d) ==\n                                        data[0].count(\"%c \" % d))\n                    return (d, skipinitialspace)\n\n        # nothing else indicates a preference, pick the character that\n        # dominates(?)\n        items = [(v,k) for (k,v) in delims.items()]\n        items.sort()\n        delim = items[-1][1]\n\n        skipinitialspace = (data[0].count(delim) ==\n                            data[0].count(\"%c \" % delim))\n        return (delim, skipinitialspace)\n\n\n    def has_header(self, sample):\n        # Creates a dictionary of types of data in each column. If any\n        # column is of a single type (say, integers), *except* for the first\n        # row, then the first row is presumed to be labels. If the type\n        # can't be determined, it is assumed to be a string in which case\n        # the length of the string is the determining factor: if all of the\n        # rows except for the first are the same length, it's a header.\n        # Finally, a 'vote' is taken at the end for each column, adding or\n        # subtracting from the likelihood of the first row being a header.\n\n        rdr = reader(StringIO(sample), self.sniff(sample))\n\n        header = rdr.next() # assume first row is header\n\n        columns = len(header)\n        columnTypes = {}\n        for i in range(columns): columnTypes[i] = None\n\n        checked = 0\n        for row in rdr:\n            # arbitrary number of rows to check, to keep it sane\n            if checked > 20:\n                break\n            checked += 1\n\n            if len(row) != columns:\n                continue # skip rows that have irregular number of columns\n\n            for col in columnTypes.keys():\n\n                for thisType in [int, long, float, complex]:\n                    try:\n                        thisType(row[col])\n                        break\n                    except (ValueError, OverflowError):\n                        pass\n                else:\n                    # fallback to length of string\n                    thisType = len(row[col])\n\n                # treat longs as ints\n                if thisType == long:\n                    thisType = int\n\n                if thisType != columnTypes[col]:\n                    if columnTypes[col] is None: # add new column type\n                        columnTypes[col] = thisType\n                    else:\n                        # type is inconsistent, remove column from\n                        # consideration\n                        del columnTypes[col]\n\n        # finally, compare results against first row and \"vote\"\n        # on whether it's a header\n        hasHeader = 0\n        for col, colType in columnTypes.items():\n            if type(colType) == type(0): # it's a length\n                if len(header[col]) != colType:\n                    hasHeader += 1\n                else:\n                    hasHeader -= 1\n            else: # attempt typecast\n                try:\n                    colType(header[col])\n                except (ValueError, TypeError):\n                    hasHeader += 1\n                else:\n                    hasHeader -= 1\n\n        return hasHeader > 0\n", 
    "datetime": "\"\"\"Concrete date/time and related types -- prototype implemented in Python.\n\nSee http://www.zope.org/Members/fdrake/DateTimeWiki/FrontPage\n\nSee also http://dir.yahoo.com/Reference/calendars/\n\nFor a primer on DST, including many current DST rules, see\nhttp://webexhibits.org/daylightsaving/\n\nFor more about DST than you ever wanted to know, see\nftp://elsie.nci.nih.gov/pub/\n\nSources for time zone and DST data: http://www.twinsun.com/tz/tz-link.htm\n\nThis was originally copied from the sandbox of the CPython CVS repository.\nThanks to Tim Peters for suggesting using it.\n\"\"\"\n\nfrom __future__ import division\nimport time as _time\nimport math as _math\nimport struct as _struct\n\ndef _cmp(x, y):\n    return 0 if x == y else 1 if x > y else -1\n\ndef _round(x):\n    return int(_math.floor(x + 0.5) if x >= 0.0 else _math.ceil(x - 0.5))\n\nMINYEAR = 1\nMAXYEAR = 9999\n_MINYEARFMT = 1900\n\n# Utility functions, adapted from Python's Demo/classes/Dates.py, which\n# also assumes the current Gregorian calendar indefinitely extended in\n# both directions.  Difference:  Dates.py calls January 1 of year 0 day\n# number 1.  The code here calls January 1 of year 1 day number 1.  This is\n# to match the definition of the \"proleptic Gregorian\" calendar in Dershowitz\n# and Reingold's \"Calendrical Calculations\", where it's the base calendar\n# for all computations.  See the book for algorithms for converting between\n# proleptic Gregorian ordinals and many other calendar systems.\n\n_DAYS_IN_MONTH = [-1, 31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31]\n\n_DAYS_BEFORE_MONTH = [-1]\ndbm = 0\nfor dim in _DAYS_IN_MONTH[1:]:\n    _DAYS_BEFORE_MONTH.append(dbm)\n    dbm += dim\ndel dbm, dim\n\ndef _is_leap(year):\n    \"year -> 1 if leap year, else 0.\"\n    return year % 4 == 0 and (year % 100 != 0 or year % 400 == 0)\n\ndef _days_before_year(year):\n    \"year -> number of days before January 1st of year.\"\n    y = year - 1\n    return y*365 + y//4 - y//100 + y//400\n\ndef _days_in_month(year, month):\n    \"year, month -> number of days in that month in that year.\"\n    assert 1 <= month <= 12, month\n    if month == 2 and _is_leap(year):\n        return 29\n    return _DAYS_IN_MONTH[month]\n\ndef _days_before_month(year, month):\n    \"year, month -> number of days in year preceding first day of month.\"\n    assert 1 <= month <= 12, 'month must be in 1..12'\n    return _DAYS_BEFORE_MONTH[month] + (month > 2 and _is_leap(year))\n\ndef _ymd2ord(year, month, day):\n    \"year, month, day -> ordinal, considering 01-Jan-0001 as day 1.\"\n    assert 1 <= month <= 12, 'month must be in 1..12'\n    dim = _days_in_month(year, month)\n    assert 1 <= day <= dim, ('day must be in 1..%d' % dim)\n    return (_days_before_year(year) +\n            _days_before_month(year, month) +\n            day)\n\n_DI400Y = _days_before_year(401)    # number of days in 400 years\n_DI100Y = _days_before_year(101)    #    \"    \"   \"   \" 100   \"\n_DI4Y   = _days_before_year(5)      #    \"    \"   \"   \"   4   \"\n\n# A 4-year cycle has an extra leap day over what we'd get from pasting\n# together 4 single years.\nassert _DI4Y == 4 * 365 + 1\n\n# Similarly, a 400-year cycle has an extra leap day over what we'd get from\n# pasting together 4 100-year cycles.\nassert _DI400Y == 4 * _DI100Y + 1\n\n# OTOH, a 100-year cycle has one fewer leap day than we'd get from\n# pasting together 25 4-year cycles.\nassert _DI100Y == 25 * _DI4Y - 1\n\ndef _ord2ymd(n):\n    \"ordinal -> (year, month, day), considering 01-Jan-0001 as day 1.\"\n\n    # n is a 1-based index, starting at 1-Jan-1.  The pattern of leap years\n    # repeats exactly every 400 years.  The basic strategy is to find the\n    # closest 400-year boundary at or before n, then work with the offset\n    # from that boundary to n.  Life is much clearer if we subtract 1 from\n    # n first -- then the values of n at 400-year boundaries are exactly\n    # those divisible by _DI400Y:\n    #\n    #     D  M   Y            n              n-1\n    #     -- --- ----        ----------     ----------------\n    #     31 Dec -400        -_DI400Y       -_DI400Y -1\n    #      1 Jan -399         -_DI400Y +1   -_DI400Y      400-year boundary\n    #     ...\n    #     30 Dec  000        -1             -2\n    #     31 Dec  000         0             -1\n    #      1 Jan  001         1              0            400-year boundary\n    #      2 Jan  001         2              1\n    #      3 Jan  001         3              2\n    #     ...\n    #     31 Dec  400         _DI400Y        _DI400Y -1\n    #      1 Jan  401         _DI400Y +1     _DI400Y      400-year boundary\n    n -= 1\n    n400, n = divmod(n, _DI400Y)\n    year = n400 * 400 + 1   # ..., -399, 1, 401, ...\n\n    # Now n is the (non-negative) offset, in days, from January 1 of year, to\n    # the desired date.  Now compute how many 100-year cycles precede n.\n    # Note that it's possible for n100 to equal 4!  In that case 4 full\n    # 100-year cycles precede the desired day, which implies the desired\n    # day is December 31 at the end of a 400-year cycle.\n    n100, n = divmod(n, _DI100Y)\n\n    # Now compute how many 4-year cycles precede it.\n    n4, n = divmod(n, _DI4Y)\n\n    # And now how many single years.  Again n1 can be 4, and again meaning\n    # that the desired day is December 31 at the end of the 4-year cycle.\n    n1, n = divmod(n, 365)\n\n    year += n100 * 100 + n4 * 4 + n1\n    if n1 == 4 or n100 == 4:\n        assert n == 0\n        return year-1, 12, 31\n\n    # Now the year is correct, and n is the offset from January 1.  We find\n    # the month via an estimate that's either exact or one too large.\n    leapyear = n1 == 3 and (n4 != 24 or n100 == 3)\n    assert leapyear == _is_leap(year)\n    month = (n + 50) >> 5\n    preceding = _DAYS_BEFORE_MONTH[month] + (month > 2 and leapyear)\n    if preceding > n:  # estimate is too large\n        month -= 1\n        preceding -= _DAYS_IN_MONTH[month] + (month == 2 and leapyear)\n    n -= preceding\n    assert 0 <= n < _days_in_month(year, month)\n\n    # Now the year and month are correct, and n is the offset from the\n    # start of that month:  we're done!\n    return year, month, n+1\n\n# Month and day names.  For localized versions, see the calendar module.\n_MONTHNAMES = [None, \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                     \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n_DAYNAMES = [None, \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\n\n\ndef _build_struct_time(y, m, d, hh, mm, ss, dstflag):\n    wday = (_ymd2ord(y, m, d) + 6) % 7\n    dnum = _days_before_month(y, m) + d\n    return _time.struct_time((y, m, d, hh, mm, ss, wday, dnum, dstflag))\n\ndef _format_time(hh, mm, ss, us):\n    # Skip trailing microseconds when us==0.\n    result = \"%02d:%02d:%02d\" % (hh, mm, ss)\n    if us:\n        result += \".%06d\" % us\n    return result\n\n# Correctly substitute for %z and %Z escapes in strftime formats.\ndef _wrap_strftime(object, format, timetuple):\n    year = timetuple[0]\n    if year < _MINYEARFMT:\n        raise ValueError(\"year=%d is before %d; the datetime strftime() \"\n                         \"methods require year >= %d\" %\n                         (year, _MINYEARFMT, _MINYEARFMT))\n    # Don't call utcoffset() or tzname() unless actually needed.\n    freplace = None  # the string to use for %f\n    zreplace = None  # the string to use for %z\n    Zreplace = None  # the string to use for %Z\n\n    # Scan format for %z and %Z escapes, replacing as needed.\n    newformat = []\n    push = newformat.append\n    i, n = 0, len(format)\n    while i < n:\n        ch = format[i]\n        i += 1\n        if ch == '%':\n            if i < n:\n                ch = format[i]\n                i += 1\n                if ch == 'f':\n                    if freplace is None:\n                        freplace = '%06d' % getattr(object,\n                                                    'microsecond', 0)\n                    newformat.append(freplace)\n                elif ch == 'z':\n                    if zreplace is None:\n                        zreplace = \"\"\n                        if hasattr(object, \"_utcoffset\"):\n                            offset = object._utcoffset()\n                            if offset is not None:\n                                sign = '+'\n                                if offset < 0:\n                                    offset = -offset\n                                    sign = '-'\n                                h, m = divmod(offset, 60)\n                                zreplace = '%c%02d%02d' % (sign, h, m)\n                    assert '%' not in zreplace\n                    newformat.append(zreplace)\n                elif ch == 'Z':\n                    if Zreplace is None:\n                        Zreplace = \"\"\n                        if hasattr(object, \"tzname\"):\n                            s = object.tzname()\n                            if s is not None:\n                                # strftime is going to have at this: escape %\n                                Zreplace = s.replace('%', '%%')\n                    newformat.append(Zreplace)\n                else:\n                    push('%')\n                    push(ch)\n            else:\n                push('%')\n        else:\n            push(ch)\n    newformat = \"\".join(newformat)\n    return _time.strftime(newformat, timetuple)\n\n# Just raise TypeError if the arg isn't None or a string.\ndef _check_tzname(name):\n    if name is not None and not isinstance(name, str):\n        raise TypeError(\"tzinfo.tzname() must return None or string, \"\n                        \"not '%s'\" % type(name))\n\n# name is the offset-producing method, \"utcoffset\" or \"dst\".\n# offset is what it returned.\n# If offset isn't None or timedelta, raises TypeError.\n# If offset is None, returns None.\n# Else offset is checked for being in range, and a whole # of minutes.\n# If it is, its integer value is returned.  Else ValueError is raised.\ndef _check_utc_offset(name, offset):\n    assert name in (\"utcoffset\", \"dst\")\n    if offset is None:\n        return\n    if not isinstance(offset, timedelta):\n        raise TypeError(\"tzinfo.%s() must return None \"\n                        \"or timedelta, not '%s'\" % (name, type(offset)))\n    days = offset.days\n    if days < -1 or days > 0:\n        offset = 1440  # trigger out-of-range\n    else:\n        seconds = days * 86400 + offset.seconds\n        minutes, seconds = divmod(seconds, 60)\n        if seconds or offset.microseconds:\n            raise ValueError(\"tzinfo.%s() must return a whole number \"\n                             \"of minutes\" % name)\n        offset = minutes\n    if not -1440 < offset < 1440:\n        raise ValueError(\"%s()=%d, must be in -1439..1439\" % (name, offset))\n    return offset\n\ndef _check_int_field(value):\n    if isinstance(value, int):\n        return value\n    if not isinstance(value, float):\n        try:\n            value = value.__int__()\n        except AttributeError:\n            pass\n        else:\n            if isinstance(value, (int, long)):\n                return value\n            raise TypeError('__int__ method should return an integer')\n        raise TypeError('an integer is required')\n    raise TypeError('integer argument expected, got float')\n\ndef _check_date_fields(year, month, day):\n    year = _check_int_field(year)\n    month = _check_int_field(month)\n    day = _check_int_field(day)\n    if not MINYEAR <= year <= MAXYEAR:\n        raise ValueError('year must be in %d..%d' % (MINYEAR, MAXYEAR), year)\n    if not 1 <= month <= 12:\n        raise ValueError('month must be in 1..12', month)\n    dim = _days_in_month(year, month)\n    if not 1 <= day <= dim:\n        raise ValueError('day must be in 1..%d' % dim, day)\n    return year, month, day\n\ndef _check_time_fields(hour, minute, second, microsecond):\n    hour = _check_int_field(hour)\n    minute = _check_int_field(minute)\n    second = _check_int_field(second)\n    microsecond = _check_int_field(microsecond)\n    if not 0 <= hour <= 23:\n        raise ValueError('hour must be in 0..23', hour)\n    if not 0 <= minute <= 59:\n        raise ValueError('minute must be in 0..59', minute)\n    if not 0 <= second <= 59:\n        raise ValueError('second must be in 0..59', second)\n    if not 0 <= microsecond <= 999999:\n        raise ValueError('microsecond must be in 0..999999', microsecond)\n    return hour, minute, second, microsecond\n\ndef _check_tzinfo_arg(tz):\n    if tz is not None and not isinstance(tz, tzinfo):\n        raise TypeError(\"tzinfo argument must be None or of a tzinfo subclass\")\n\n\n# Notes on comparison:  In general, datetime module comparison operators raise\n# TypeError when they don't know how to do a comparison themself.  If they\n# returned NotImplemented instead, comparison could (silently) fall back to\n# the default compare-objects-by-comparing-their-memory-addresses strategy,\n# and that's not helpful.  There are two exceptions:\n#\n# 1. For date and datetime, if the other object has a \"timetuple\" attr,\n#    NotImplemented is returned.  This is a hook to allow other kinds of\n#    datetime-like objects a chance to intercept the comparison.\n#\n# 2. Else __eq__ and __ne__ return False and True, respectively.  This is\n#    so opertaions like\n#\n#        x == y\n#        x != y\n#        x in sequence\n#        x not in sequence\n#        dict[x] = y\n#\n#    don't raise annoying TypeErrors just because a datetime object\n#    is part of a heterogeneous collection.  If there's no known way to\n#    compare X to a datetime, saying they're not equal is reasonable.\n\ndef _cmperror(x, y):\n    raise TypeError(\"can't compare '%s' to '%s'\" % (\n                    type(x).__name__, type(y).__name__))\n\n# This is a start at a struct tm workalike.  Goals:\n#\n# + Works the same way across platforms.\n# + Handles all the fields datetime needs handled, without 1970-2038 glitches.\n#\n# Note:  I suspect it's best if this flavor of tm does *not* try to\n# second-guess timezones or DST.  Instead fold whatever adjustments you want\n# into the minutes argument (and the constructor will normalize).\n\nclass _tmxxx:\n\n    ordinal = None\n\n    def __init__(self, year, month, day, hour=0, minute=0, second=0,\n                 microsecond=0):\n        # Normalize all the inputs, and store the normalized values.\n        if not 0 <= microsecond <= 999999:\n            carry, microsecond = divmod(microsecond, 1000000)\n            second += carry\n        if not 0 <= second <= 59:\n            carry, second = divmod(second, 60)\n            minute += carry\n        if not 0 <= minute <= 59:\n            carry, minute = divmod(minute, 60)\n            hour += carry\n        if not 0 <= hour <= 23:\n            carry, hour = divmod(hour, 24)\n            day += carry\n\n        # That was easy.  Now it gets muddy:  the proper range for day\n        # can't be determined without knowing the correct month and year,\n        # but if day is, e.g., plus or minus a million, the current month\n        # and year values make no sense (and may also be out of bounds\n        # themselves).\n        # Saying 12 months == 1 year should be non-controversial.\n        if not 1 <= month <= 12:\n            carry, month = divmod(month-1, 12)\n            year += carry\n            month += 1\n            assert 1 <= month <= 12\n\n        # Now only day can be out of bounds (year may also be out of bounds\n        # for a datetime object, but we don't care about that here).\n        # If day is out of bounds, what to do is arguable, but at least the\n        # method here is principled and explainable.\n        dim = _days_in_month(year, month)\n        if not 1 <= day <= dim:\n            # Move day-1 days from the first of the month.  First try to\n            # get off cheap if we're only one day out of range (adjustments\n            # for timezone alone can't be worse than that).\n            if day == 0:    # move back a day\n                month -= 1\n                if month > 0:\n                    day = _days_in_month(year, month)\n                else:\n                    year, month, day = year-1, 12, 31\n            elif day == dim + 1:    # move forward a day\n                month += 1\n                day = 1\n                if month > 12:\n                    month = 1\n                    year += 1\n            else:\n                self.ordinal = _ymd2ord(year, month, 1) + (day - 1)\n                year, month, day = _ord2ymd(self.ordinal)\n\n        self.year, self.month, self.day = year, month, day\n        self.hour, self.minute, self.second = hour, minute, second\n        self.microsecond = microsecond\n\nclass timedelta(object):\n    \"\"\"Represent the difference between two datetime objects.\n\n    Supported operators:\n\n    - add, subtract timedelta\n    - unary plus, minus, abs\n    - compare to timedelta\n    - multiply, divide by int/long\n\n    In addition, datetime supports subtraction of two datetime objects\n    returning a timedelta, and addition or subtraction of a datetime\n    and a timedelta giving a datetime.\n\n    Representation: (days, seconds, microseconds).  Why?  Because I\n    felt like it.\n    \"\"\"\n    __slots__ = '_days', '_seconds', '_microseconds', '_hashcode'\n\n    def __new__(cls, days=0, seconds=0, microseconds=0,\n                milliseconds=0, minutes=0, hours=0, weeks=0):\n        # Doing this efficiently and accurately in C is going to be difficult\n        # and error-prone, due to ubiquitous overflow possibilities, and that\n        # C double doesn't have enough bits of precision to represent\n        # microseconds over 10K years faithfully.  The code here tries to make\n        # explicit where go-fast assumptions can be relied on, in order to\n        # guide the C implementation; it's way more convoluted than speed-\n        # ignoring auto-overflow-to-long idiomatic Python could be.\n\n        # XXX Check that all inputs are ints, longs or floats.\n\n        # Final values, all integer.\n        # s and us fit in 32-bit signed ints; d isn't bounded.\n        d = s = us = 0\n\n        # Normalize everything to days, seconds, microseconds.\n        days += weeks*7\n        seconds += minutes*60 + hours*3600\n        microseconds += milliseconds*1000\n\n        # Get rid of all fractions, and normalize s and us.\n        # Take a deep breath <wink>.\n        if isinstance(days, float):\n            dayfrac, days = _math.modf(days)\n            daysecondsfrac, daysecondswhole = _math.modf(dayfrac * (24.*3600.))\n            assert daysecondswhole == int(daysecondswhole)  # can't overflow\n            s = int(daysecondswhole)\n            assert days == int(days)\n            d = int(days)\n        else:\n            daysecondsfrac = 0.0\n            d = days\n        assert isinstance(daysecondsfrac, float)\n        assert abs(daysecondsfrac) <= 1.0\n        assert isinstance(d, (int, long))\n        assert abs(s) <= 24 * 3600\n        # days isn't referenced again before redefinition\n\n        if isinstance(seconds, float):\n            secondsfrac, seconds = _math.modf(seconds)\n            assert seconds == int(seconds)\n            seconds = int(seconds)\n            secondsfrac += daysecondsfrac\n            assert abs(secondsfrac) <= 2.0\n        else:\n            secondsfrac = daysecondsfrac\n        # daysecondsfrac isn't referenced again\n        assert isinstance(secondsfrac, float)\n        assert abs(secondsfrac) <= 2.0\n\n        assert isinstance(seconds, (int, long))\n        days, seconds = divmod(seconds, 24*3600)\n        d += days\n        s += int(seconds)    # can't overflow\n        assert isinstance(s, int)\n        assert abs(s) <= 2 * 24 * 3600\n        # seconds isn't referenced again before redefinition\n\n        usdouble = secondsfrac * 1e6\n        assert abs(usdouble) < 2.1e6    # exact value not critical\n        # secondsfrac isn't referenced again\n\n        if isinstance(microseconds, float):\n            microseconds = _round(microseconds + usdouble)\n            seconds, microseconds = divmod(microseconds, 1000000)\n            days, seconds = divmod(seconds, 24*3600)\n            d += days\n            s += int(seconds)\n            microseconds = int(microseconds)\n        else:\n            microseconds = int(microseconds)\n            seconds, microseconds = divmod(microseconds, 1000000)\n            days, seconds = divmod(seconds, 24*3600)\n            d += days\n            s += int(seconds)\n            microseconds = _round(microseconds + usdouble)\n        assert isinstance(s, int)\n        assert isinstance(microseconds, int)\n        assert abs(s) <= 3 * 24 * 3600\n        assert abs(microseconds) < 3.1e6\n\n        # Just a little bit of carrying possible for microseconds and seconds.\n        seconds, us = divmod(microseconds, 1000000)\n        s += seconds\n        days, s = divmod(s, 24*3600)\n        d += days\n\n        assert isinstance(d, (int, long))\n        assert isinstance(s, int) and 0 <= s < 24*3600\n        assert isinstance(us, int) and 0 <= us < 1000000\n\n        if abs(d) > 999999999:\n            raise OverflowError(\"timedelta # of days is too large: %d\" % d)\n\n        self = object.__new__(cls)\n        self._days = d\n        self._seconds = s\n        self._microseconds = us\n        self._hashcode = -1\n        return self\n\n    def __repr__(self):\n        if self._microseconds:\n            return \"%s(%d, %d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                       self._days,\n                                       self._seconds,\n                                       self._microseconds)\n        if self._seconds:\n            return \"%s(%d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                   self._days,\n                                   self._seconds)\n        return \"%s(%d)\" % ('datetime.' + self.__class__.__name__, self._days)\n\n    def __str__(self):\n        mm, ss = divmod(self._seconds, 60)\n        hh, mm = divmod(mm, 60)\n        s = \"%d:%02d:%02d\" % (hh, mm, ss)\n        if self._days:\n            def plural(n):\n                return n, abs(n) != 1 and \"s\" or \"\"\n            s = (\"%d day%s, \" % plural(self._days)) + s\n        if self._microseconds:\n            s = s + \".%06d\" % self._microseconds\n        return s\n\n    def total_seconds(self):\n        \"\"\"Total seconds in the duration.\"\"\"\n        return ((self.days * 86400 + self.seconds) * 10**6 +\n                self.microseconds) / 10**6\n\n    # Read-only field accessors\n    @property\n    def days(self):\n        \"\"\"days\"\"\"\n        return self._days\n\n    @property\n    def seconds(self):\n        \"\"\"seconds\"\"\"\n        return self._seconds\n\n    @property\n    def microseconds(self):\n        \"\"\"microseconds\"\"\"\n        return self._microseconds\n\n    def __add__(self, other):\n        if isinstance(other, timedelta):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days + other._days,\n                             self._seconds + other._seconds,\n                             self._microseconds + other._microseconds)\n        return NotImplemented\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        if isinstance(other, timedelta):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days - other._days,\n                             self._seconds - other._seconds,\n                             self._microseconds - other._microseconds)\n        return NotImplemented\n\n    def __rsub__(self, other):\n        if isinstance(other, timedelta):\n            return -self + other\n        return NotImplemented\n\n    def __neg__(self):\n        # for CPython compatibility, we cannot use\n        # our __class__ here, but need a real timedelta\n        return timedelta(-self._days,\n                         -self._seconds,\n                         -self._microseconds)\n\n    def __pos__(self):\n        return self\n\n    def __abs__(self):\n        if self._days < 0:\n            return -self\n        else:\n            return self\n\n    def __mul__(self, other):\n        if isinstance(other, (int, long)):\n            # for CPython compatibility, we cannot use\n            # our __class__ here, but need a real timedelta\n            return timedelta(self._days * other,\n                             self._seconds * other,\n                             self._microseconds * other)\n        return NotImplemented\n\n    __rmul__ = __mul__\n\n    def _to_microseconds(self):\n        return ((self._days * (24*3600) + self._seconds) * 1000000 +\n                self._microseconds)\n\n    def __div__(self, other):\n        if not isinstance(other, (int, long)):\n            return NotImplemented\n        usec = self._to_microseconds()\n        return timedelta(0, 0, usec // other)\n\n    __floordiv__ = __div__\n\n    # Comparisons of timedelta objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) == 0\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) != 0\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) <= 0\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) < 0\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) >= 0\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, timedelta):\n            return self._cmp(other) > 0\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, timedelta)\n        return _cmp(self._getstate(), other._getstate())\n\n    def __hash__(self):\n        if self._hashcode == -1:\n            self._hashcode = hash(self._getstate())\n        return self._hashcode\n\n    def __nonzero__(self):\n        return (self._days != 0 or\n                self._seconds != 0 or\n                self._microseconds != 0)\n\n    # Pickle support.\n\n    def _getstate(self):\n        return (self._days, self._seconds, self._microseconds)\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\ntimedelta.min = timedelta(-999999999)\ntimedelta.max = timedelta(days=999999999, hours=23, minutes=59, seconds=59,\n                          microseconds=999999)\ntimedelta.resolution = timedelta(microseconds=1)\n\nclass date(object):\n    \"\"\"Concrete date type.\n\n    Constructors:\n\n    __new__()\n    fromtimestamp()\n    today()\n    fromordinal()\n\n    Operators:\n\n    __repr__, __str__\n    __cmp__, __hash__\n    __add__, __radd__, __sub__ (add/radd only with timedelta arg)\n\n    Methods:\n\n    timetuple()\n    toordinal()\n    weekday()\n    isoweekday(), isocalendar(), isoformat()\n    ctime()\n    strftime()\n\n    Properties (readonly):\n    year, month, day\n    \"\"\"\n    __slots__ = '_year', '_month', '_day', '_hashcode'\n\n    def __new__(cls, year, month=None, day=None):\n        \"\"\"Constructor.\n\n        Arguments:\n\n        year, month, day (required, base 1)\n        \"\"\"\n        if month is None and isinstance(year, bytes) and len(year) == 4 and \\\n                1 <= ord(year[2]) <= 12:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(year)\n            self._hashcode = -1\n            return self\n        year, month, day = _check_date_fields(year, month, day)\n        self = object.__new__(cls)\n        self._year = year\n        self._month = month\n        self._day = day\n        self._hashcode = -1\n        return self\n\n    # Additional constructors\n\n    @classmethod\n    def fromtimestamp(cls, t):\n        \"Construct a date from a POSIX timestamp (like time.time()).\"\n        y, m, d, hh, mm, ss, weekday, jday, dst = _time.localtime(t)\n        return cls(y, m, d)\n\n    @classmethod\n    def today(cls):\n        \"Construct a date from time.time().\"\n        t = _time.time()\n        return cls.fromtimestamp(t)\n\n    @classmethod\n    def fromordinal(cls, n):\n        \"\"\"Contruct a date from a proleptic Gregorian ordinal.\n\n        January 1 of year 1 is day 1.  Only the year, month and day are\n        non-zero in the result.\n        \"\"\"\n        y, m, d = _ord2ymd(n)\n        return cls(y, m, d)\n\n    # Conversions to string\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\n\n        >>> dt = datetime(2010, 1, 1)\n        >>> repr(dt)\n        'datetime.datetime(2010, 1, 1, 0, 0)'\n\n        >>> dt = datetime(2010, 1, 1, tzinfo=timezone.utc)\n        >>> repr(dt)\n        'datetime.datetime(2010, 1, 1, 0, 0, tzinfo=datetime.timezone.utc)'\n        \"\"\"\n        return \"%s(%d, %d, %d)\" % ('datetime.' + self.__class__.__name__,\n                                   self._year,\n                                   self._month,\n                                   self._day)\n\n    # XXX These shouldn't depend on time.localtime(), because that\n    # clips the usable dates to [1970 .. 2038).  At least ctime() is\n    # easily done without using strftime() -- that's better too because\n    # strftime(\"%c\", ...) is locale specific.\n\n    def ctime(self):\n        \"Return ctime() style string.\"\n        weekday = self.toordinal() % 7 or 7\n        return \"%s %s %2d 00:00:00 %04d\" % (\n            _DAYNAMES[weekday],\n            _MONTHNAMES[self._month],\n            self._day, self._year)\n\n    def strftime(self, format):\n        \"Format using strftime().\"\n        return _wrap_strftime(self, format, self.timetuple())\n\n    def __format__(self, fmt):\n        if not isinstance(fmt, (str, unicode)):\n            raise ValueError(\"__format__ expects str or unicode, not %s\" %\n                             fmt.__class__.__name__)\n        if len(fmt) != 0:\n            return self.strftime(fmt)\n        return str(self)\n\n    def isoformat(self):\n        \"\"\"Return the date formatted according to ISO.\n\n        This is 'YYYY-MM-DD'.\n\n        References:\n        - http://www.w3.org/TR/NOTE-datetime\n        - http://www.cl.cam.ac.uk/~mgk25/iso-time.html\n        \"\"\"\n        return \"%04d-%02d-%02d\" % (self._year, self._month, self._day)\n\n    __str__ = isoformat\n\n    # Read-only field accessors\n    @property\n    def year(self):\n        \"\"\"year (1-9999)\"\"\"\n        return self._year\n\n    @property\n    def month(self):\n        \"\"\"month (1-12)\"\"\"\n        return self._month\n\n    @property\n    def day(self):\n        \"\"\"day (1-31)\"\"\"\n        return self._day\n\n    # Standard conversions, __cmp__, __hash__ (and helpers)\n\n    def timetuple(self):\n        \"Return local time tuple compatible with time.localtime().\"\n        return _build_struct_time(self._year, self._month, self._day,\n                                  0, 0, 0, -1)\n\n    def toordinal(self):\n        \"\"\"Return proleptic Gregorian ordinal for the year, month and day.\n\n        January 1 of year 1 is day 1.  Only the year, month and day values\n        contribute to the result.\n        \"\"\"\n        return _ymd2ord(self._year, self._month, self._day)\n\n    def replace(self, year=None, month=None, day=None):\n        \"\"\"Return a new date with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self._year\n        if month is None:\n            month = self._month\n        if day is None:\n            day = self._day\n        return date(year, month, day)\n\n    # Comparisons of date objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) == 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) != 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) <= 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) < 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) >= 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, date):\n            return self._cmp(other) > 0\n        elif hasattr(other, \"timetuple\"):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, date)\n        y, m, d = self._year, self._month, self._day\n        y2, m2, d2 = other._year, other._month, other._day\n        return _cmp((y, m, d), (y2, m2, d2))\n\n    def __hash__(self):\n        \"Hash.\"\n        if self._hashcode == -1:\n            self._hashcode = hash(self._getstate())\n        return self._hashcode\n\n    # Computations\n\n    def _checkOverflow(self, year):\n        if not MINYEAR <= year <= MAXYEAR:\n            raise OverflowError(\"date +/-: result year %d not in %d..%d\" %\n                                (year, MINYEAR, MAXYEAR))\n\n    def __add__(self, other):\n        \"Add a date to a timedelta.\"\n        if isinstance(other, timedelta):\n            t = _tmxxx(self._year,\n                      self._month,\n                      self._day + other.days)\n            self._checkOverflow(t.year)\n            result = date(t.year, t.month, t.day)\n            return result\n        return NotImplemented\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        \"\"\"Subtract two dates, or a date and a timedelta.\"\"\"\n        if isinstance(other, timedelta):\n            return self + timedelta(-other.days)\n        if isinstance(other, date):\n            days1 = self.toordinal()\n            days2 = other.toordinal()\n            return timedelta(days1 - days2)\n        return NotImplemented\n\n    def weekday(self):\n        \"Return day of the week, where Monday == 0 ... Sunday == 6.\"\n        return (self.toordinal() + 6) % 7\n\n    # Day-of-the-week and week-of-the-year, according to ISO\n\n    def isoweekday(self):\n        \"Return day of the week, where Monday == 1 ... Sunday == 7.\"\n        # 1-Jan-0001 is a Monday\n        return self.toordinal() % 7 or 7\n\n    def isocalendar(self):\n        \"\"\"Return a 3-tuple containing ISO year, week number, and weekday.\n\n        The first ISO week of the year is the (Mon-Sun) week\n        containing the year's first Thursday; everything else derives\n        from that.\n\n        The first week is 1; Monday is 1 ... Sunday is 7.\n\n        ISO calendar algorithm taken from\n        http://www.phys.uu.nl/~vgent/calendar/isocalendar.htm\n        \"\"\"\n        year = self._year\n        week1monday = _isoweek1monday(year)\n        today = _ymd2ord(self._year, self._month, self._day)\n        # Internally, week and day have origin 0\n        week, day = divmod(today - week1monday, 7)\n        if week < 0:\n            year -= 1\n            week1monday = _isoweek1monday(year)\n            week, day = divmod(today - week1monday, 7)\n        elif week >= 52:\n            if today >= _isoweek1monday(year+1):\n                year += 1\n                week = 0\n        return year, week+1, day+1\n\n    # Pickle support.\n\n    def _getstate(self):\n        yhi, ylo = divmod(self._year, 256)\n        return (_struct.pack('4B', yhi, ylo, self._month, self._day),)\n\n    def __setstate(self, string):\n        yhi, ylo, self._month, self._day = (ord(string[0]), ord(string[1]),\n                                            ord(string[2]), ord(string[3]))\n        self._year = yhi * 256 + ylo\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\n_date_class = date  # so functions w/ args named \"date\" can get at the class\n\ndate.min = date(1, 1, 1)\ndate.max = date(9999, 12, 31)\ndate.resolution = timedelta(days=1)\n\nclass tzinfo(object):\n    \"\"\"Abstract base class for time zone info classes.\n\n    Subclasses must override the name(), utcoffset() and dst() methods.\n    \"\"\"\n    __slots__ = ()\n\n    def tzname(self, dt):\n        \"datetime -> string name of time zone.\"\n        raise NotImplementedError(\"tzinfo subclass must override tzname()\")\n\n    def utcoffset(self, dt):\n        \"datetime -> minutes east of UTC (negative for west of UTC)\"\n        raise NotImplementedError(\"tzinfo subclass must override utcoffset()\")\n\n    def dst(self, dt):\n        \"\"\"datetime -> DST offset in minutes east of UTC.\n\n        Return 0 if DST not in effect.  utcoffset() must include the DST\n        offset.\n        \"\"\"\n        raise NotImplementedError(\"tzinfo subclass must override dst()\")\n\n    def fromutc(self, dt):\n        \"datetime in UTC -> datetime in local time.\"\n\n        if not isinstance(dt, datetime):\n            raise TypeError(\"fromutc() requires a datetime argument\")\n        if dt.tzinfo is not self:\n            raise ValueError(\"dt.tzinfo is not self\")\n\n        dtoff = dt.utcoffset()\n        if dtoff is None:\n            raise ValueError(\"fromutc() requires a non-None utcoffset() \"\n                             \"result\")\n\n        # See the long comment block at the end of this file for an\n        # explanation of this algorithm.\n        dtdst = dt.dst()\n        if dtdst is None:\n            raise ValueError(\"fromutc() requires a non-None dst() result\")\n        delta = dtoff - dtdst\n        if delta:\n            dt += delta\n            dtdst = dt.dst()\n            if dtdst is None:\n                raise ValueError(\"fromutc(): dt.dst gave inconsistent \"\n                                 \"results; cannot convert\")\n        if dtdst:\n            return dt + dtdst\n        else:\n            return dt\n\n    # Pickle support.\n\n    def __reduce__(self):\n        getinitargs = getattr(self, \"__getinitargs__\", None)\n        if getinitargs:\n            args = getinitargs()\n        else:\n            args = ()\n        getstate = getattr(self, \"__getstate__\", None)\n        if getstate:\n            state = getstate()\n        else:\n            state = getattr(self, \"__dict__\", None) or None\n        if state is None:\n            return (self.__class__, args)\n        else:\n            return (self.__class__, args, state)\n\n_tzinfo_class = tzinfo\n\nclass time(object):\n    \"\"\"Time with time zone.\n\n    Constructors:\n\n    __new__()\n\n    Operators:\n\n    __repr__, __str__\n    __cmp__, __hash__\n\n    Methods:\n\n    strftime()\n    isoformat()\n    utcoffset()\n    tzname()\n    dst()\n\n    Properties (readonly):\n    hour, minute, second, microsecond, tzinfo\n    \"\"\"\n    __slots__ = '_hour', '_minute', '_second', '_microsecond', '_tzinfo', '_hashcode'\n\n    def __new__(cls, hour=0, minute=0, second=0, microsecond=0, tzinfo=None):\n        \"\"\"Constructor.\n\n        Arguments:\n\n        hour, minute (required)\n        second, microsecond (default to zero)\n        tzinfo (default to None)\n        \"\"\"\n        if isinstance(hour, bytes) and len(hour) == 6 and ord(hour[0]) < 24:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(hour, minute or None)\n            self._hashcode = -1\n            return self\n        hour, minute, second, microsecond = _check_time_fields(\n            hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        self = object.__new__(cls)\n        self._hour = hour\n        self._minute = minute\n        self._second = second\n        self._microsecond = microsecond\n        self._tzinfo = tzinfo\n        self._hashcode = -1\n        return self\n\n    # Read-only field accessors\n    @property\n    def hour(self):\n        \"\"\"hour (0-23)\"\"\"\n        return self._hour\n\n    @property\n    def minute(self):\n        \"\"\"minute (0-59)\"\"\"\n        return self._minute\n\n    @property\n    def second(self):\n        \"\"\"second (0-59)\"\"\"\n        return self._second\n\n    @property\n    def microsecond(self):\n        \"\"\"microsecond (0-999999)\"\"\"\n        return self._microsecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"timezone info object\"\"\"\n        return self._tzinfo\n\n    # Standard conversions, __hash__ (and helpers)\n\n    # Comparisons of time objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) == 0\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) != 0\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) <= 0\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) < 0\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) >= 0\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, time):\n            return self._cmp(other) > 0\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, time)\n        mytz = self._tzinfo\n        ottz = other._tzinfo\n        myoff = otoff = None\n\n        if mytz is ottz:\n            base_compare = True\n        else:\n            myoff = self._utcoffset()\n            otoff = other._utcoffset()\n            base_compare = myoff == otoff\n\n        if base_compare:\n            return _cmp((self._hour, self._minute, self._second,\n                         self._microsecond),\n                        (other._hour, other._minute, other._second,\n                         other._microsecond))\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't compare offset-naive and offset-aware times\")\n        myhhmm = self._hour * 60 + self._minute - myoff\n        othhmm = other._hour * 60 + other._minute - otoff\n        return _cmp((myhhmm, self._second, self._microsecond),\n                    (othhmm, other._second, other._microsecond))\n\n    def __hash__(self):\n        \"\"\"Hash.\"\"\"\n        if self._hashcode == -1:\n            tzoff = self._utcoffset()\n            if not tzoff:  # zero or None\n                self._hashcode = hash(self._getstate()[0])\n            else:\n                h, m = divmod(self.hour * 60 + self.minute - tzoff, 60)\n                if 0 <= h < 24:\n                    self._hashcode = hash(time(h, m, self.second, self.microsecond))\n                else:\n                    self._hashcode = hash((h, m, self.second, self.microsecond))\n        return self._hashcode\n\n    # Conversion to string\n\n    def _tzstr(self, sep=\":\"):\n        \"\"\"Return formatted timezone offset (+xx:xx) or None.\"\"\"\n        off = self._utcoffset()\n        if off is not None:\n            if off < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, 60)\n            assert 0 <= hh < 24\n            off = \"%s%02d%s%02d\" % (sign, hh, sep, mm)\n        return off\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\"\"\"\n        if self._microsecond != 0:\n            s = \", %d, %d\" % (self._second, self._microsecond)\n        elif self._second != 0:\n            s = \", %d\" % self._second\n        else:\n            s = \"\"\n        s= \"%s(%d, %d%s)\" % ('datetime.' + self.__class__.__name__,\n                             self._hour, self._minute, s)\n        if self._tzinfo is not None:\n            assert s[-1:] == \")\"\n            s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n        return s\n\n    def isoformat(self):\n        \"\"\"Return the time formatted according to ISO.\n\n        This is 'HH:MM:SS.mmmmmm+zz:zz', or 'HH:MM:SS+zz:zz' if\n        self.microsecond == 0.\n        \"\"\"\n        s = _format_time(self._hour, self._minute, self._second,\n                         self._microsecond)\n        tz = self._tzstr()\n        if tz:\n            s += tz\n        return s\n\n    __str__ = isoformat\n\n    def strftime(self, format):\n        \"\"\"Format using strftime().  The date part of the timestamp passed\n        to underlying strftime should not be used.\n        \"\"\"\n        # The year must be >= _MINYEARFMT else Python's strftime implementation\n        # can raise a bogus exception.\n        timetuple = (1900, 1, 1,\n                     self._hour, self._minute, self._second,\n                     0, 1, -1)\n        return _wrap_strftime(self, format, timetuple)\n\n    def __format__(self, fmt):\n        if not isinstance(fmt, (str, unicode)):\n            raise ValueError(\"__format__ expects str or unicode, not %s\" %\n                             fmt.__class__.__name__)\n        if len(fmt) != 0:\n            return self.strftime(fmt)\n        return str(self)\n\n    # Timezone functions\n\n    def utcoffset(self):\n        \"\"\"Return the timezone offset in minutes east of UTC (negative west of\n        UTC).\"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(None)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _utcoffset(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(None)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        return offset\n\n    def tzname(self):\n        \"\"\"Return the timezone name.\n\n        Note that the name is 100% informational -- there's no requirement that\n        it mean anything in particular. For example, \"GMT\", \"UTC\", \"-500\",\n        \"-5:00\", \"EDT\", \"US/Eastern\", \"America/New York\" are all valid replies.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        name = self._tzinfo.tzname(None)\n        _check_tzname(name)\n        return name\n\n    def dst(self):\n        \"\"\"Return 0 if DST is not in effect, or the DST offset (in minutes\n        eastward) if DST is in effect.\n\n        This is purely informational; the DST offset has already been added to\n        the UTC offset returned by utcoffset() if applicable, so there's no\n        need to consult dst() unless you're interested in displaying the DST\n        info.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(None)\n        offset = _check_utc_offset(\"dst\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _dst(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(None)\n        offset = _check_utc_offset(\"dst\", offset)\n        return offset\n\n    def replace(self, hour=None, minute=None, second=None, microsecond=None,\n                tzinfo=True):\n        \"\"\"Return a new time with new values for the specified fields.\"\"\"\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        return time(hour, minute, second, microsecond, tzinfo)\n\n    def __nonzero__(self):\n        if self.second or self.microsecond:\n            return True\n        offset = self._utcoffset() or 0\n        return self.hour * 60 + self.minute != offset\n\n    # Pickle support.\n\n    def _getstate(self):\n        us2, us3 = divmod(self._microsecond, 256)\n        us1, us2 = divmod(us2, 256)\n        basestate = _struct.pack('6B', self._hour, self._minute, self._second,\n                                       us1, us2, us3)\n        if self._tzinfo is None:\n            return (basestate,)\n        else:\n            return (basestate, self._tzinfo)\n\n    def __setstate(self, string, tzinfo):\n        if tzinfo is not None and not isinstance(tzinfo, _tzinfo_class):\n            raise TypeError(\"bad tzinfo state arg\")\n        self._hour, self._minute, self._second, us1, us2, us3 = (\n            ord(string[0]), ord(string[1]), ord(string[2]),\n            ord(string[3]), ord(string[4]), ord(string[5]))\n        self._microsecond = (((us1 << 8) | us2) << 8) | us3\n        self._tzinfo = tzinfo\n\n    def __reduce__(self):\n        return (time, self._getstate())\n\n_time_class = time  # so functions w/ args named \"time\" can get at the class\n\ntime.min = time(0, 0, 0)\ntime.max = time(23, 59, 59, 999999)\ntime.resolution = timedelta(microseconds=1)\n\nclass datetime(date):\n    \"\"\"datetime(year, month, day[, hour[, minute[, second[, microsecond[,tzinfo]]]]])\n\n    The year, month and day arguments are required. tzinfo may be None, or an\n    instance of a tzinfo subclass. The remaining arguments may be ints or longs.\n    \"\"\"\n    __slots__ = date.__slots__ + time.__slots__\n\n    def __new__(cls, year, month=None, day=None, hour=0, minute=0, second=0,\n                microsecond=0, tzinfo=None):\n        if isinstance(year, bytes) and len(year) == 10 and \\\n                1 <= ord(year[2]) <= 12:\n            # Pickle support\n            self = object.__new__(cls)\n            self.__setstate(year, month)\n            self._hashcode = -1\n            return self\n        year, month, day = _check_date_fields(year, month, day)\n        hour, minute, second, microsecond = _check_time_fields(\n            hour, minute, second, microsecond)\n        _check_tzinfo_arg(tzinfo)\n        self = object.__new__(cls)\n        self._year = year\n        self._month = month\n        self._day = day\n        self._hour = hour\n        self._minute = minute\n        self._second = second\n        self._microsecond = microsecond\n        self._tzinfo = tzinfo\n        self._hashcode = -1\n        return self\n\n    # Read-only field accessors\n    @property\n    def hour(self):\n        \"\"\"hour (0-23)\"\"\"\n        return self._hour\n\n    @property\n    def minute(self):\n        \"\"\"minute (0-59)\"\"\"\n        return self._minute\n\n    @property\n    def second(self):\n        \"\"\"second (0-59)\"\"\"\n        return self._second\n\n    @property\n    def microsecond(self):\n        \"\"\"microsecond (0-999999)\"\"\"\n        return self._microsecond\n\n    @property\n    def tzinfo(self):\n        \"\"\"timezone info object\"\"\"\n        return self._tzinfo\n\n    @classmethod\n    def fromtimestamp(cls, timestamp, tz=None):\n        \"\"\"Construct a datetime from a POSIX timestamp (like time.time()).\n\n        A timezone info object may be passed in as well.\n        \"\"\"\n\n        _check_tzinfo_arg(tz)\n\n        converter = _time.localtime if tz is None else _time.gmtime\n\n        if isinstance(timestamp, int):\n            us = 0\n        else:\n            t_full = timestamp\n            timestamp = int(_math.floor(timestamp))\n            frac = t_full - timestamp\n            us = _round(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            timestamp += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = converter(timestamp)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        result = cls(y, m, d, hh, mm, ss, us, tz)\n        if tz is not None:\n            result = tz.fromutc(result)\n        return result\n\n    @classmethod\n    def utcfromtimestamp(cls, t):\n        \"Construct a UTC datetime from a POSIX timestamp (like time.time()).\"\n        if isinstance(t, int):\n            us = 0\n        else:\n            t_full = t\n            t = int(_math.floor(t))\n            frac = t_full - t\n            us = _round(frac * 1e6)\n\n        # If timestamp is less than one microsecond smaller than a\n        # full second, us can be rounded up to 1000000.  In this case,\n        # roll over to seconds, otherwise, ValueError is raised\n        # by the constructor.\n        if us == 1000000:\n            t += 1\n            us = 0\n        y, m, d, hh, mm, ss, weekday, jday, dst = _time.gmtime(t)\n        ss = min(ss, 59)    # clamp out leap seconds if the platform has them\n        return cls(y, m, d, hh, mm, ss, us)\n\n    @classmethod\n    def now(cls, tz=None):\n        \"Construct a datetime from time.time() and optional time zone info.\"\n        t = _time.time()\n        return cls.fromtimestamp(t, tz)\n\n    @classmethod\n    def utcnow(cls):\n        \"Construct a UTC datetime from time.time().\"\n        t = _time.time()\n        return cls.utcfromtimestamp(t)\n\n    @classmethod\n    def combine(cls, date, time):\n        \"Construct a datetime from a given date and a given time.\"\n        if not isinstance(date, _date_class):\n            raise TypeError(\"date argument must be a date instance\")\n        if not isinstance(time, _time_class):\n            raise TypeError(\"time argument must be a time instance\")\n        return cls(date.year, date.month, date.day,\n                   time.hour, time.minute, time.second, time.microsecond,\n                   time.tzinfo)\n\n    def timetuple(self):\n        \"Return local time tuple compatible with time.localtime().\"\n        dst = self._dst()\n        if dst is None:\n            dst = -1\n        elif dst:\n            dst = 1\n        return _build_struct_time(self.year, self.month, self.day,\n                                  self.hour, self.minute, self.second,\n                                  dst)\n\n    def utctimetuple(self):\n        \"Return UTC time tuple compatible with time.gmtime().\"\n        y, m, d = self.year, self.month, self.day\n        hh, mm, ss = self.hour, self.minute, self.second\n        offset = self._utcoffset()\n        if offset:  # neither None nor 0\n            tm = _tmxxx(y, m, d, hh, mm - offset)\n            y, m, d = tm.year, tm.month, tm.day\n            hh, mm = tm.hour, tm.minute\n        return _build_struct_time(y, m, d, hh, mm, ss, 0)\n\n    def date(self):\n        \"Return the date part.\"\n        return date(self._year, self._month, self._day)\n\n    def time(self):\n        \"Return the time part, with tzinfo None.\"\n        return time(self.hour, self.minute, self.second, self.microsecond)\n\n    def timetz(self):\n        \"Return the time part, with same tzinfo.\"\n        return time(self.hour, self.minute, self.second, self.microsecond,\n                    self._tzinfo)\n\n    def replace(self, year=None, month=None, day=None, hour=None,\n                minute=None, second=None, microsecond=None, tzinfo=True):\n        \"\"\"Return a new datetime with new values for the specified fields.\"\"\"\n        if year is None:\n            year = self.year\n        if month is None:\n            month = self.month\n        if day is None:\n            day = self.day\n        if hour is None:\n            hour = self.hour\n        if minute is None:\n            minute = self.minute\n        if second is None:\n            second = self.second\n        if microsecond is None:\n            microsecond = self.microsecond\n        if tzinfo is True:\n            tzinfo = self.tzinfo\n        return datetime(year, month, day, hour, minute, second, microsecond,\n                        tzinfo)\n\n    def astimezone(self, tz):\n        if not isinstance(tz, tzinfo):\n            raise TypeError(\"tz argument must be an instance of tzinfo\")\n\n        mytz = self.tzinfo\n        if mytz is None:\n            raise ValueError(\"astimezone() requires an aware datetime\")\n\n        if tz is mytz:\n            return self\n\n        # Convert self to UTC, and attach the new time zone object.\n        myoffset = self.utcoffset()\n        if myoffset is None:\n            raise ValueError(\"astimezone() requires an aware datetime\")\n        utc = (self - myoffset).replace(tzinfo=tz)\n\n        # Convert from UTC to tz's local time.\n        return tz.fromutc(utc)\n\n    # Ways to produce a string.\n\n    def ctime(self):\n        \"Return ctime() style string.\"\n        weekday = self.toordinal() % 7 or 7\n        return \"%s %s %2d %02d:%02d:%02d %04d\" % (\n            _DAYNAMES[weekday],\n            _MONTHNAMES[self._month],\n            self._day,\n            self._hour, self._minute, self._second,\n            self._year)\n\n    def isoformat(self, sep='T'):\n        \"\"\"Return the time formatted according to ISO.\n\n        This is 'YYYY-MM-DD HH:MM:SS.mmmmmm', or 'YYYY-MM-DD HH:MM:SS' if\n        self.microsecond == 0.\n\n        If self.tzinfo is not None, the UTC offset is also attached, giving\n        'YYYY-MM-DD HH:MM:SS.mmmmmm+HH:MM' or 'YYYY-MM-DD HH:MM:SS+HH:MM'.\n\n        Optional argument sep specifies the separator between date and\n        time, default 'T'.\n        \"\"\"\n        s = (\"%04d-%02d-%02d%c\" % (self._year, self._month, self._day, sep) +\n             _format_time(self._hour, self._minute, self._second,\n                          self._microsecond))\n        off = self._utcoffset()\n        if off is not None:\n            if off < 0:\n                sign = \"-\"\n                off = -off\n            else:\n                sign = \"+\"\n            hh, mm = divmod(off, 60)\n            s += \"%s%02d:%02d\" % (sign, hh, mm)\n        return s\n\n    def __repr__(self):\n        \"\"\"Convert to formal string, for repr().\"\"\"\n        L = [self._year, self._month, self._day,  # These are never zero\n             self._hour, self._minute, self._second, self._microsecond]\n        if L[-1] == 0:\n            del L[-1]\n        if L[-1] == 0:\n            del L[-1]\n        s = \", \".join(map(str, L))\n        s = \"%s(%s)\" % ('datetime.' + self.__class__.__name__, s)\n        if self._tzinfo is not None:\n            assert s[-1:] == \")\"\n            s = s[:-1] + \", tzinfo=%r\" % self._tzinfo + \")\"\n        return s\n\n    def __str__(self):\n        \"Convert to string, for str().\"\n        return self.isoformat(sep=' ')\n\n    @classmethod\n    def strptime(cls, date_string, format):\n        'string, format -> new datetime parsed from a string (like time.strptime()).'\n        from _strptime import _strptime\n        # _strptime._strptime returns a two-element tuple.  The first\n        # element is a time.struct_time object.  The second is the\n        # microseconds (which are not defined for time.struct_time).\n        struct, micros = _strptime(date_string, format)\n        return cls(*(struct[0:6] + (micros,)))\n\n    def utcoffset(self):\n        \"\"\"Return the timezone offset in minutes east of UTC (negative west of\n        UTC).\"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(self)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _utcoffset(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.utcoffset(self)\n        offset = _check_utc_offset(\"utcoffset\", offset)\n        return offset\n\n    def tzname(self):\n        \"\"\"Return the timezone name.\n\n        Note that the name is 100% informational -- there's no requirement that\n        it mean anything in particular. For example, \"GMT\", \"UTC\", \"-500\",\n        \"-5:00\", \"EDT\", \"US/Eastern\", \"America/New York\" are all valid replies.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        name = self._tzinfo.tzname(self)\n        _check_tzname(name)\n        return name\n\n    def dst(self):\n        \"\"\"Return 0 if DST is not in effect, or the DST offset (in minutes\n        eastward) if DST is in effect.\n\n        This is purely informational; the DST offset has already been added to\n        the UTC offset returned by utcoffset() if applicable, so there's no\n        need to consult dst() unless you're interested in displaying the DST\n        info.\n        \"\"\"\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(self)\n        offset = _check_utc_offset(\"dst\", offset)\n        if offset is not None:\n            offset = timedelta(minutes=offset)\n        return offset\n\n    # Return an integer (or None) instead of a timedelta (or None).\n    def _dst(self):\n        if self._tzinfo is None:\n            return None\n        offset = self._tzinfo.dst(self)\n        offset = _check_utc_offset(\"dst\", offset)\n        return offset\n\n    # Comparisons of datetime objects with other.\n\n    def __eq__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) == 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            return False\n\n    def __ne__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) != 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            return True\n\n    def __le__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) <= 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __lt__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) < 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __ge__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) >= 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def __gt__(self, other):\n        if isinstance(other, datetime):\n            return self._cmp(other) > 0\n        elif hasattr(other, \"timetuple\") and not isinstance(other, date):\n            return NotImplemented\n        else:\n            _cmperror(self, other)\n\n    def _cmp(self, other):\n        assert isinstance(other, datetime)\n        mytz = self._tzinfo\n        ottz = other._tzinfo\n        myoff = otoff = None\n\n        if mytz is ottz:\n            base_compare = True\n        else:\n            if mytz is not None:\n                myoff = self._utcoffset()\n            if ottz is not None:\n                otoff = other._utcoffset()\n            base_compare = myoff == otoff\n\n        if base_compare:\n            return _cmp((self._year, self._month, self._day,\n                         self._hour, self._minute, self._second,\n                         self._microsecond),\n                        (other._year, other._month, other._day,\n                         other._hour, other._minute, other._second,\n                         other._microsecond))\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't compare offset-naive and offset-aware datetimes\")\n        # XXX What follows could be done more efficiently...\n        diff = self - other     # this will take offsets into account\n        if diff.days < 0:\n            return -1\n        return diff and 1 or 0\n\n    def __add__(self, other):\n        \"Add a datetime and a timedelta.\"\n        if not isinstance(other, timedelta):\n            return NotImplemented\n        t = _tmxxx(self._year,\n                  self._month,\n                  self._day + other.days,\n                  self._hour,\n                  self._minute,\n                  self._second + other.seconds,\n                  self._microsecond + other.microseconds)\n        self._checkOverflow(t.year)\n        result = datetime(t.year, t.month, t.day,\n                                t.hour, t.minute, t.second,\n                                t.microsecond, tzinfo=self._tzinfo)\n        return result\n\n    __radd__ = __add__\n\n    def __sub__(self, other):\n        \"Subtract two datetimes, or a datetime and a timedelta.\"\n        if not isinstance(other, datetime):\n            if isinstance(other, timedelta):\n                return self + -other\n            return NotImplemented\n\n        days1 = self.toordinal()\n        days2 = other.toordinal()\n        secs1 = self._second + self._minute * 60 + self._hour * 3600\n        secs2 = other._second + other._minute * 60 + other._hour * 3600\n        base = timedelta(days1 - days2,\n                         secs1 - secs2,\n                         self._microsecond - other._microsecond)\n        if self._tzinfo is other._tzinfo:\n            return base\n        myoff = self._utcoffset()\n        otoff = other._utcoffset()\n        if myoff == otoff:\n            return base\n        if myoff is None or otoff is None:\n            raise TypeError(\"can't subtract offset-naive and offset-aware datetimes\")\n        return base + timedelta(minutes = otoff-myoff)\n\n    def __hash__(self):\n        if self._hashcode == -1:\n            tzoff = self._utcoffset()\n            if tzoff is None:\n                self._hashcode = hash(self._getstate()[0])\n            else:\n                days = _ymd2ord(self.year, self.month, self.day)\n                seconds = self.hour * 3600 + (self.minute - tzoff) * 60 + self.second\n                self._hashcode = hash(timedelta(days, seconds, self.microsecond))\n        return self._hashcode\n\n    # Pickle support.\n\n    def _getstate(self):\n        yhi, ylo = divmod(self._year, 256)\n        us2, us3 = divmod(self._microsecond, 256)\n        us1, us2 = divmod(us2, 256)\n        basestate = _struct.pack('10B', yhi, ylo, self._month, self._day,\n                                        self._hour, self._minute, self._second,\n                                        us1, us2, us3)\n        if self._tzinfo is None:\n            return (basestate,)\n        else:\n            return (basestate, self._tzinfo)\n\n    def __setstate(self, string, tzinfo):\n        if tzinfo is not None and not isinstance(tzinfo, _tzinfo_class):\n            raise TypeError(\"bad tzinfo state arg\")\n        (yhi, ylo, self._month, self._day, self._hour,\n            self._minute, self._second, us1, us2, us3) = (ord(string[0]),\n                ord(string[1]), ord(string[2]), ord(string[3]),\n                ord(string[4]), ord(string[5]), ord(string[6]),\n                ord(string[7]), ord(string[8]), ord(string[9]))\n        self._year = yhi * 256 + ylo\n        self._microsecond = (((us1 << 8) | us2) << 8) | us3\n        self._tzinfo = tzinfo\n\n    def __reduce__(self):\n        return (self.__class__, self._getstate())\n\n\ndatetime.min = datetime(1, 1, 1)\ndatetime.max = datetime(9999, 12, 31, 23, 59, 59, 999999)\ndatetime.resolution = timedelta(microseconds=1)\n\n\ndef _isoweek1monday(year):\n    # Helper to calculate the day number of the Monday starting week 1\n    # XXX This could be done more efficiently\n    THURSDAY = 3\n    firstday = _ymd2ord(year, 1, 1)\n    firstweekday = (firstday + 6) % 7  # See weekday() above\n    week1monday = firstday - firstweekday\n    if firstweekday > THURSDAY:\n        week1monday += 7\n    return week1monday\n\n\"\"\"\nSome time zone algebra.  For a datetime x, let\n    x.n = x stripped of its timezone -- its naive time.\n    x.o = x.utcoffset(), and assuming that doesn't raise an exception or\n          return None\n    x.d = x.dst(), and assuming that doesn't raise an exception or\n          return None\n    x.s = x's standard offset, x.o - x.d\n\nNow some derived rules, where k is a duration (timedelta).\n\n1. x.o = x.s + x.d\n   This follows from the definition of x.s.\n\n2. If x and y have the same tzinfo member, x.s = y.s.\n   This is actually a requirement, an assumption we need to make about\n   sane tzinfo classes.\n\n3. The naive UTC time corresponding to x is x.n - x.o.\n   This is again a requirement for a sane tzinfo class.\n\n4. (x+k).s = x.s\n   This follows from #2, and that datimetimetz+timedelta preserves tzinfo.\n\n5. (x+k).n = x.n + k\n   Again follows from how arithmetic is defined.\n\nNow we can explain tz.fromutc(x).  Let's assume it's an interesting case\n(meaning that the various tzinfo methods exist, and don't blow up or return\nNone when called).\n\nThe function wants to return a datetime y with timezone tz, equivalent to x.\nx is already in UTC.\n\nBy #3, we want\n\n    y.n - y.o = x.n                             [1]\n\nThe algorithm starts by attaching tz to x.n, and calling that y.  So\nx.n = y.n at the start.  Then it wants to add a duration k to y, so that [1]\nbecomes true; in effect, we want to solve [2] for k:\n\n   (y+k).n - (y+k).o = x.n                      [2]\n\nBy #1, this is the same as\n\n   (y+k).n - ((y+k).s + (y+k).d) = x.n          [3]\n\nBy #5, (y+k).n = y.n + k, which equals x.n + k because x.n=y.n at the start.\nSubstituting that into [3],\n\n   x.n + k - (y+k).s - (y+k).d = x.n; the x.n terms cancel, leaving\n   k - (y+k).s - (y+k).d = 0; rearranging,\n   k = (y+k).s - (y+k).d; by #4, (y+k).s == y.s, so\n   k = y.s - (y+k).d\n\nOn the RHS, (y+k).d can't be computed directly, but y.s can be, and we\napproximate k by ignoring the (y+k).d term at first.  Note that k can't be\nvery large, since all offset-returning methods return a duration of magnitude\nless than 24 hours.  For that reason, if y is firmly in std time, (y+k).d must\nbe 0, so ignoring it has no consequence then.\n\nIn any case, the new value is\n\n    z = y + y.s                                 [4]\n\nIt's helpful to step back at look at [4] from a higher level:  it's simply\nmapping from UTC to tz's standard time.\n\nAt this point, if\n\n    z.n - z.o = x.n                             [5]\n\nwe have an equivalent time, and are almost done.  The insecurity here is\nat the start of daylight time.  Picture US Eastern for concreteness.  The wall\ntime jumps from 1:59 to 3:00, and wall hours of the form 2:MM don't make good\nsense then.  The docs ask that an Eastern tzinfo class consider such a time to\nbe EDT (because it's \"after 2\"), which is a redundant spelling of 1:MM EST\non the day DST starts.  We want to return the 1:MM EST spelling because that's\nthe only spelling that makes sense on the local wall clock.\n\nIn fact, if [5] holds at this point, we do have the standard-time spelling,\nbut that takes a bit of proof.  We first prove a stronger result.  What's the\ndifference between the LHS and RHS of [5]?  Let\n\n    diff = x.n - (z.n - z.o)                    [6]\n\nNow\n    z.n =                       by [4]\n    (y + y.s).n =               by #5\n    y.n + y.s =                 since y.n = x.n\n    x.n + y.s =                 since z and y are have the same tzinfo member,\n                                    y.s = z.s by #2\n    x.n + z.s\n\nPlugging that back into [6] gives\n\n    diff =\n    x.n - ((x.n + z.s) - z.o) =     expanding\n    x.n - x.n - z.s + z.o =         cancelling\n    - z.s + z.o =                   by #2\n    z.d\n\nSo diff = z.d.\n\nIf [5] is true now, diff = 0, so z.d = 0 too, and we have the standard-time\nspelling we wanted in the endcase described above.  We're done.  Contrarily,\nif z.d = 0, then we have a UTC equivalent, and are also done.\n\nIf [5] is not true now, diff = z.d != 0, and z.d is the offset we need to\nadd to z (in effect, z is in tz's standard time, and we need to shift the\nlocal clock into tz's daylight time).\n\nLet\n\n    z' = z + z.d = z + diff                     [7]\n\nand we can again ask whether\n\n    z'.n - z'.o = x.n                           [8]\n\nIf so, we're done.  If not, the tzinfo class is insane, according to the\nassumptions we've made.  This also requires a bit of proof.  As before, let's\ncompute the difference between the LHS and RHS of [8] (and skipping some of\nthe justifications for the kinds of substitutions we've done several times\nalready):\n\n    diff' = x.n - (z'.n - z'.o) =           replacing z'.n via [7]\n            x.n  - (z.n + diff - z'.o) =    replacing diff via [6]\n            x.n - (z.n + x.n - (z.n - z.o) - z'.o) =\n            x.n - z.n - x.n + z.n - z.o + z'.o =    cancel x.n\n            - z.n + z.n - z.o + z'.o =              cancel z.n\n            - z.o + z'.o =                      #1 twice\n            -z.s - z.d + z'.s + z'.d =          z and z' have same tzinfo\n            z'.d - z.d\n\nSo z' is UTC-equivalent to x iff z'.d = z.d at this point.  If they are equal,\nwe've found the UTC-equivalent so are done.  In fact, we stop with [7] and\nreturn z', not bothering to compute z'.d.\n\nHow could z.d and z'd differ?  z' = z + z.d [7], so merely moving z' by\na dst() offset, and starting *from* a time already in DST (we know z.d != 0),\nwould have to change the result dst() returns:  we start in DST, and moving\na little further into it takes us out of DST.\n\nThere isn't a sane case where this can happen.  The closest it gets is at\nthe end of DST, where there's an hour in UTC with no spelling in a hybrid\ntzinfo class.  In US Eastern, that's 5:MM UTC = 0:MM EST = 1:MM EDT.  During\nthat hour, on an Eastern clock 1:MM is taken as being in standard time (6:MM\nUTC) because the docs insist on that, but 0:MM is taken as being in daylight\ntime (4:MM UTC).  There is no local time mapping to 5:MM UTC.  The local\nclock jumps from 1:59 back to 1:00 again, and repeats the 1:MM hour in\nstandard time.  Since that's what the local clock *does*, we want to map both\nUTC hours 5:MM and 6:MM to 1:MM Eastern.  The result is ambiguous\nin local time, but so it goes -- it's the way the local clock works.\n\nWhen x = 5:MM UTC is the input to this algorithm, x.o=0, y.o=-5 and y.d=0,\nso z=0:MM.  z.d=60 (minutes) then, so [5] doesn't hold and we keep going.\nz' = z + z.d = 1:MM then, and z'.d=0, and z'.d - z.d = -60 != 0 so [8]\n(correctly) concludes that z' is not UTC-equivalent to x.\n\nBecause we know z.d said z was in daylight time (else [5] would have held and\nwe would have stopped then), and we know z.d != z'.d (else [8] would have held\nand we have stopped then), and there are only 2 possible values dst() can\nreturn in Eastern, it follows that z'.d must be 0 (which it is in the example,\nbut the reasoning doesn't depend on the example -- it depends on there being\ntwo possible dst() outcomes, one zero and the other non-zero).  Therefore\nz' must be in standard time, and is the spelling we want in this case.\n\nNote again that z' is not UTC-equivalent as far as the hybrid tzinfo class is\nconcerned (because it takes z' as being in standard time rather than the\ndaylight time we intend here), but returning it gives the real-life \"local\nclock repeats an hour\" behavior when mapping the \"unspellable\" UTC hour into\ntz.\n\nWhen the input is 6:MM, z=1:MM and z.d=0, and we stop at once, again with\nthe 1:MM standard time spelling we want.\n\nSo how can this break?  One of the assumptions must be violated.  Two\npossibilities:\n\n1) [2] effectively says that y.s is invariant across all y belong to a given\n   time zone.  This isn't true if, for political reasons or continental drift,\n   a region decides to change its base offset from UTC.\n\n2) There may be versions of \"double daylight\" time where the tail end of\n   the analysis gives up a step too early.  I haven't thought about that\n   enough to say.\n\nIn any case, it's clear that the default fromutc() is strong enough to handle\n\"almost all\" time zones:  so long as the standard offset is invariant, it\ndoesn't matter if daylight time transition points change from year to year, or\nif daylight time is skipped in some years; it doesn't matter how large or\nsmall dst() may get within its bounds; and it doesn't even matter if some\nperverse time zone returns a negative dst()).  So a breaking case must be\npretty bizarre, and a tzinfo subclass can override fromutc() if it is.\n\"\"\"\n", 
    "difflib": "\"\"\"\nModule difflib -- helpers for computing deltas between objects.\n\nFunction get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    Use SequenceMatcher to return list of the best \"good enough\" matches.\n\nFunction context_diff(a, b):\n    For two lists of strings, return a delta in context diff format.\n\nFunction ndiff(a, b):\n    Return a delta: the difference between `a` and `b` (lists of strings).\n\nFunction restore(delta, which):\n    Return one of the two sequences that generated an ndiff delta.\n\nFunction unified_diff(a, b):\n    For two lists of strings, return a delta in unified diff format.\n\nClass SequenceMatcher:\n    A flexible class for comparing pairs of sequences of any type.\n\nClass Differ:\n    For producing human-readable deltas from sequences of lines of text.\n\nClass HtmlDiff:\n    For producing HTML side by side comparison with change highlights.\n\"\"\"\n\n__all__ = ['get_close_matches', 'ndiff', 'restore', 'SequenceMatcher',\n           'Differ','IS_CHARACTER_JUNK', 'IS_LINE_JUNK', 'context_diff',\n           'unified_diff', 'HtmlDiff', 'Match']\n\nimport heapq\nfrom collections import namedtuple as _namedtuple\nfrom functools import reduce\n\nMatch = _namedtuple('Match', 'a b size')\n\ndef _calculate_ratio(matches, length):\n    if length:\n        return 2.0 * matches / length\n    return 1.0\n\nclass SequenceMatcher:\n\n    \"\"\"\n    SequenceMatcher is a flexible class for comparing pairs of sequences of\n    any type, so long as the sequence elements are hashable.  The basic\n    algorithm predates, and is a little fancier than, an algorithm\n    published in the late 1980's by Ratcliff and Obershelp under the\n    hyperbolic name \"gestalt pattern matching\".  The basic idea is to find\n    the longest contiguous matching subsequence that contains no \"junk\"\n    elements (R-O doesn't address junk).  The same idea is then applied\n    recursively to the pieces of the sequences to the left and to the right\n    of the matching subsequence.  This does not yield minimal edit\n    sequences, but does tend to yield matches that \"look right\" to people.\n\n    SequenceMatcher tries to compute a \"human-friendly diff\" between two\n    sequences.  Unlike e.g. UNIX(tm) diff, the fundamental notion is the\n    longest *contiguous* & junk-free matching subsequence.  That's what\n    catches peoples' eyes.  The Windows(tm) windiff has another interesting\n    notion, pairing up elements that appear uniquely in each sequence.\n    That, and the method here, appear to yield more intuitive difference\n    reports than does diff.  This method appears to be the least vulnerable\n    to synching up on blocks of \"junk lines\", though (like blank lines in\n    ordinary text files, or maybe \"<P>\" lines in HTML files).  That may be\n    because this is the only method of the 3 that has a *concept* of\n    \"junk\" <wink>.\n\n    Example, comparing two strings, and considering blanks to be \"junk\":\n\n    >>> s = SequenceMatcher(lambda x: x == \" \",\n    ...                     \"private Thread currentThread;\",\n    ...                     \"private volatile Thread currentThread;\")\n    >>>\n\n    .ratio() returns a float in [0, 1], measuring the \"similarity\" of the\n    sequences.  As a rule of thumb, a .ratio() value over 0.6 means the\n    sequences are close matches:\n\n    >>> print round(s.ratio(), 3)\n    0.866\n    >>>\n\n    If you're only interested in where the sequences match,\n    .get_matching_blocks() is handy:\n\n    >>> for block in s.get_matching_blocks():\n    ...     print \"a[%d] and b[%d] match for %d elements\" % block\n    a[0] and b[0] match for 8 elements\n    a[8] and b[17] match for 21 elements\n    a[29] and b[38] match for 0 elements\n\n    Note that the last tuple returned by .get_matching_blocks() is always a\n    dummy, (len(a), len(b), 0), and this is the only case in which the last\n    tuple element (number of elements matched) is 0.\n\n    If you want to know how to change the first sequence into the second,\n    use .get_opcodes():\n\n    >>> for opcode in s.get_opcodes():\n    ...     print \"%6s a[%d:%d] b[%d:%d]\" % opcode\n     equal a[0:8] b[0:8]\n    insert a[8:8] b[8:17]\n     equal a[8:29] b[17:38]\n\n    See the Differ class for a fancy human-friendly file differencer, which\n    uses SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    See also function get_close_matches() in this module, which shows how\n    simple code building on SequenceMatcher can be used to do useful work.\n\n    Timing:  Basic R-O is cubic time worst case and quadratic time expected\n    case.  SequenceMatcher is quadratic time for the worst case and has\n    expected-case behavior dependent in a complicated way on how many\n    elements the sequences have in common; best case time is linear.\n\n    Methods:\n\n    __init__(isjunk=None, a='', b='')\n        Construct a SequenceMatcher.\n\n    set_seqs(a, b)\n        Set the two sequences to be compared.\n\n    set_seq1(a)\n        Set the first sequence to be compared.\n\n    set_seq2(b)\n        Set the second sequence to be compared.\n\n    find_longest_match(alo, ahi, blo, bhi)\n        Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n    get_matching_blocks()\n        Return list of triples describing matching subsequences.\n\n    get_opcodes()\n        Return list of 5-tuples describing how to turn a into b.\n\n    ratio()\n        Return a measure of the sequences' similarity (float in [0,1]).\n\n    quick_ratio()\n        Return an upper bound on .ratio() relatively quickly.\n\n    real_quick_ratio()\n        Return an upper bound on ratio() very quickly.\n    \"\"\"\n\n    def __init__(self, isjunk=None, a='', b='', autojunk=True):\n        \"\"\"Construct a SequenceMatcher.\n\n        Optional arg isjunk is None (the default), or a one-argument\n        function that takes a sequence element and returns true iff the\n        element is junk.  None is equivalent to passing \"lambda x: 0\", i.e.\n        no elements are considered to be junk.  For example, pass\n            lambda x: x in \" \\\\t\"\n        if you're comparing lines as sequences of characters, and don't\n        want to synch up on blanks or hard tabs.\n\n        Optional arg a is the first of two sequences to be compared.  By\n        default, an empty string.  The elements of a must be hashable.  See\n        also .set_seqs() and .set_seq1().\n\n        Optional arg b is the second of two sequences to be compared.  By\n        default, an empty string.  The elements of b must be hashable. See\n        also .set_seqs() and .set_seq2().\n\n        Optional arg autojunk should be set to False to disable the\n        \"automatic junk heuristic\" that treats popular elements as junk\n        (see module documentation for more information).\n        \"\"\"\n\n        # Members:\n        # a\n        #      first sequence\n        # b\n        #      second sequence; differences are computed as \"what do\n        #      we need to do to 'a' to change it into 'b'?\"\n        # b2j\n        #      for x in b, b2j[x] is a list of the indices (into b)\n        #      at which x appears; junk elements do not appear\n        # fullbcount\n        #      for x in b, fullbcount[x] == the number of times x\n        #      appears in b; only materialized if really needed (used\n        #      only for computing quick_ratio())\n        # matching_blocks\n        #      a list of (i, j, k) triples, where a[i:i+k] == b[j:j+k];\n        #      ascending & non-overlapping in i and in j; terminated by\n        #      a dummy (len(a), len(b), 0) sentinel\n        # opcodes\n        #      a list of (tag, i1, i2, j1, j2) tuples, where tag is\n        #      one of\n        #          'replace'   a[i1:i2] should be replaced by b[j1:j2]\n        #          'delete'    a[i1:i2] should be deleted\n        #          'insert'    b[j1:j2] should be inserted\n        #          'equal'     a[i1:i2] == b[j1:j2]\n        # isjunk\n        #      a user-supplied function taking a sequence element and\n        #      returning true iff the element is \"junk\" -- this has\n        #      subtle but helpful effects on the algorithm, which I'll\n        #      get around to writing up someday <0.9 wink>.\n        #      DON'T USE!  Only __chain_b uses this.  Use isbjunk.\n        # isbjunk\n        #      for x in b, isbjunk(x) == isjunk(x) but much faster;\n        #      it's really the __contains__ method of a hidden dict.\n        #      DOES NOT WORK for x in a!\n        # isbpopular\n        #      for x in b, isbpopular(x) is true iff b is reasonably long\n        #      (at least 200 elements) and x accounts for more than 1 + 1% of\n        #      its elements (when autojunk is enabled).\n        #      DOES NOT WORK for x in a!\n\n        self.isjunk = isjunk\n        self.a = self.b = None\n        self.autojunk = autojunk\n        self.set_seqs(a, b)\n\n    def set_seqs(self, a, b):\n        \"\"\"Set the two sequences to be compared.\n\n        >>> s = SequenceMatcher()\n        >>> s.set_seqs(\"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        \"\"\"\n\n        self.set_seq1(a)\n        self.set_seq2(b)\n\n    def set_seq1(self, a):\n        \"\"\"Set the first sequence to be compared.\n\n        The second sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq1(\"bcde\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq2().\n        \"\"\"\n\n        if a is self.a:\n            return\n        self.a = a\n        self.matching_blocks = self.opcodes = None\n\n    def set_seq2(self, b):\n        \"\"\"Set the second sequence to be compared.\n\n        The first sequence to be compared is not changed.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.set_seq2(\"abcd\")\n        >>> s.ratio()\n        1.0\n        >>>\n\n        SequenceMatcher computes and caches detailed information about the\n        second sequence, so if you want to compare one sequence S against\n        many sequences, use .set_seq2(S) once and call .set_seq1(x)\n        repeatedly for each of the other sequences.\n\n        See also set_seqs() and set_seq1().\n        \"\"\"\n\n        if b is self.b:\n            return\n        self.b = b\n        self.matching_blocks = self.opcodes = None\n        self.fullbcount = None\n        self.__chain_b()\n\n    # For each element x in b, set b2j[x] to a list of the indices in\n    # b where x appears; the indices are in increasing order; note that\n    # the number of times x appears in b is len(b2j[x]) ...\n    # when self.isjunk is defined, junk elements don't show up in this\n    # map at all, which stops the central find_longest_match method\n    # from starting any matching block at a junk element ...\n    # also creates the fast isbjunk function ...\n    # b2j also does not contain entries for \"popular\" elements, meaning\n    # elements that account for more than 1 + 1% of the total elements, and\n    # when the sequence is reasonably large (>= 200 elements); this can\n    # be viewed as an adaptive notion of semi-junk, and yields an enormous\n    # speedup when, e.g., comparing program files with hundreds of\n    # instances of \"return NULL;\" ...\n    # note that this is only called when b changes; so for cross-product\n    # kinds of matches, it's best to call set_seq2 once, then set_seq1\n    # repeatedly\n\n    def __chain_b(self):\n        # Because isjunk is a user-defined (not C) function, and we test\n        # for junk a LOT, it's important to minimize the number of calls.\n        # Before the tricks described here, __chain_b was by far the most\n        # time-consuming routine in the whole module!  If anyone sees\n        # Jim Roskind, thank him again for profile.py -- I never would\n        # have guessed that.\n        # The first trick is to build b2j ignoring the possibility\n        # of junk.  I.e., we don't call isjunk at all yet.  Throwing\n        # out the junk later is much cheaper than building b2j \"right\"\n        # from the start.\n        b = self.b\n        self.b2j = b2j = {}\n\n        for i, elt in enumerate(b):\n            indices = b2j.setdefault(elt, [])\n            indices.append(i)\n\n        # Purge junk elements\n        junk = set()\n        isjunk = self.isjunk\n        if isjunk:\n            for elt in list(b2j.keys()):  # using list() since b2j is modified\n                if isjunk(elt):\n                    junk.add(elt)\n                    del b2j[elt]\n\n        # Purge popular elements that are not junk\n        popular = set()\n        n = len(b)\n        if self.autojunk and n >= 200:\n            ntest = n // 100 + 1\n            for elt, idxs in list(b2j.items()):\n                if len(idxs) > ntest:\n                    popular.add(elt)\n                    del b2j[elt]\n\n        # Now for x in b, isjunk(x) == x in junk, but the latter is much faster.\n        # Sicne the number of *unique* junk elements is probably small, the\n        # memory burden of keeping this set alive is likely trivial compared to\n        # the size of b2j.\n        self.isbjunk = junk.__contains__\n        self.isbpopular = popular.__contains__\n\n    def find_longest_match(self, alo, ahi, blo, bhi):\n        \"\"\"Find longest matching block in a[alo:ahi] and b[blo:bhi].\n\n        If isjunk is not defined:\n\n        Return (i,j,k) such that a[i:i+k] is equal to b[j:j+k], where\n            alo <= i <= i+k <= ahi\n            blo <= j <= j+k <= bhi\n        and for all (i',j',k') meeting those conditions,\n            k >= k'\n            i <= i'\n            and if i == i', j <= j'\n\n        In other words, of all maximal matching blocks, return one that\n        starts earliest in a, and of all those maximal matching blocks that\n        start earliest in a, return the one that starts earliest in b.\n\n        >>> s = SequenceMatcher(None, \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=0, b=4, size=5)\n\n        If isjunk is defined, first the longest matching block is\n        determined as above, but with the additional restriction that no\n        junk element appears in the block.  Then that block is extended as\n        far as possible by matching (only) junk elements on both sides.  So\n        the resulting block never matches on junk except as identical junk\n        happens to be adjacent to an \"interesting\" match.\n\n        Here's the same example as before, but considering blanks to be\n        junk.  That prevents \" abcd\" from matching the \" abcd\" at the tail\n        end of the second sequence directly.  Instead only the \"abcd\" can\n        match, and matches the leftmost \"abcd\" in the second sequence:\n\n        >>> s = SequenceMatcher(lambda x: x==\" \", \" abcd\", \"abcd abcd\")\n        >>> s.find_longest_match(0, 5, 0, 9)\n        Match(a=1, b=0, size=4)\n\n        If no blocks match, return (alo, blo, 0).\n\n        >>> s = SequenceMatcher(None, \"ab\", \"c\")\n        >>> s.find_longest_match(0, 2, 0, 1)\n        Match(a=0, b=0, size=0)\n        \"\"\"\n\n        # CAUTION:  stripping common prefix or suffix would be incorrect.\n        # E.g.,\n        #    ab\n        #    acab\n        # Longest matching block is \"ab\", but if common prefix is\n        # stripped, it's \"a\" (tied with \"b\").  UNIX(tm) diff does so\n        # strip, so ends up claiming that ab is changed to acab by\n        # inserting \"ca\" in the middle.  That's minimal but unintuitive:\n        # \"it's obvious\" that someone inserted \"ac\" at the front.\n        # Windiff ends up at the same place as diff, but by pairing up\n        # the unique 'b's and then matching the first two 'a's.\n\n        a, b, b2j, isbjunk = self.a, self.b, self.b2j, self.isbjunk\n        besti, bestj, bestsize = alo, blo, 0\n        # find longest junk-free match\n        # during an iteration of the loop, j2len[j] = length of longest\n        # junk-free match ending with a[i-1] and b[j]\n        j2len = {}\n        nothing = []\n        for i in xrange(alo, ahi):\n            # look at all instances of a[i] in b; note that because\n            # b2j has no junk keys, the loop is skipped if a[i] is junk\n            j2lenget = j2len.get\n            newj2len = {}\n            for j in b2j.get(a[i], nothing):\n                # a[i] matches b[j]\n                if j < blo:\n                    continue\n                if j >= bhi:\n                    break\n                k = newj2len[j] = j2lenget(j-1, 0) + 1\n                if k > bestsize:\n                    besti, bestj, bestsize = i-k+1, j-k+1, k\n            j2len = newj2len\n\n        # Extend the best by non-junk elements on each end.  In particular,\n        # \"popular\" non-junk elements aren't in b2j, which greatly speeds\n        # the inner loop above, but also means \"the best\" match so far\n        # doesn't contain any junk *or* popular non-junk elements.\n        while besti > alo and bestj > blo and \\\n              not isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              not isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize += 1\n\n        # Now that we have a wholly interesting match (albeit possibly\n        # empty!), we may as well suck up the matching junk on each\n        # side of it too.  Can't think of a good reason not to, and it\n        # saves post-processing the (possibly considerable) expense of\n        # figuring out what to do with it.  In the case of an empty\n        # interesting match, this is clearly the right thing to do,\n        # because no other kind of match is possible in the regions.\n        while besti > alo and bestj > blo and \\\n              isbjunk(b[bestj-1]) and \\\n              a[besti-1] == b[bestj-1]:\n            besti, bestj, bestsize = besti-1, bestj-1, bestsize+1\n        while besti+bestsize < ahi and bestj+bestsize < bhi and \\\n              isbjunk(b[bestj+bestsize]) and \\\n              a[besti+bestsize] == b[bestj+bestsize]:\n            bestsize = bestsize + 1\n\n        return Match(besti, bestj, bestsize)\n\n    def get_matching_blocks(self):\n        \"\"\"Return list of triples describing matching subsequences.\n\n        Each triple is of the form (i, j, n), and means that\n        a[i:i+n] == b[j:j+n].  The triples are monotonically increasing in\n        i and in j.  New in Python 2.5, it's also guaranteed that if\n        (i, j, n) and (i', j', n') are adjacent triples in the list, and\n        the second is not the last triple in the list, then i+n != i' or\n        j+n != j'.  IOW, adjacent triples never describe adjacent equal\n        blocks.\n\n        The last triple is a dummy, (len(a), len(b), 0), and is the only\n        triple with n==0.\n\n        >>> s = SequenceMatcher(None, \"abxcd\", \"abcd\")\n        >>> s.get_matching_blocks()\n        [Match(a=0, b=0, size=2), Match(a=3, b=2, size=2), Match(a=5, b=4, size=0)]\n        \"\"\"\n\n        if self.matching_blocks is not None:\n            return self.matching_blocks\n        la, lb = len(self.a), len(self.b)\n\n        # This is most naturally expressed as a recursive algorithm, but\n        # at least one user bumped into extreme use cases that exceeded\n        # the recursion limit on their box.  So, now we maintain a list\n        # ('queue`) of blocks we still need to look at, and append partial\n        # results to `matching_blocks` in a loop; the matches are sorted\n        # at the end.\n        queue = [(0, la, 0, lb)]\n        matching_blocks = []\n        while queue:\n            alo, ahi, blo, bhi = queue.pop()\n            i, j, k = x = self.find_longest_match(alo, ahi, blo, bhi)\n            # a[alo:i] vs b[blo:j] unknown\n            # a[i:i+k] same as b[j:j+k]\n            # a[i+k:ahi] vs b[j+k:bhi] unknown\n            if k:   # if k is 0, there was no matching block\n                matching_blocks.append(x)\n                if alo < i and blo < j:\n                    queue.append((alo, i, blo, j))\n                if i+k < ahi and j+k < bhi:\n                    queue.append((i+k, ahi, j+k, bhi))\n        matching_blocks.sort()\n\n        # It's possible that we have adjacent equal blocks in the\n        # matching_blocks list now.  Starting with 2.5, this code was added\n        # to collapse them.\n        i1 = j1 = k1 = 0\n        non_adjacent = []\n        for i2, j2, k2 in matching_blocks:\n            # Is this block adjacent to i1, j1, k1?\n            if i1 + k1 == i2 and j1 + k1 == j2:\n                # Yes, so collapse them -- this just increases the length of\n                # the first block by the length of the second, and the first\n                # block so lengthened remains the block to compare against.\n                k1 += k2\n            else:\n                # Not adjacent.  Remember the first block (k1==0 means it's\n                # the dummy we started with), and make the second block the\n                # new block to compare against.\n                if k1:\n                    non_adjacent.append((i1, j1, k1))\n                i1, j1, k1 = i2, j2, k2\n        if k1:\n            non_adjacent.append((i1, j1, k1))\n\n        non_adjacent.append( (la, lb, 0) )\n        self.matching_blocks = map(Match._make, non_adjacent)\n        return self.matching_blocks\n\n    def get_opcodes(self):\n        \"\"\"Return list of 5-tuples describing how to turn a into b.\n\n        Each tuple is of the form (tag, i1, i2, j1, j2).  The first tuple\n        has i1 == j1 == 0, and remaining tuples have i1 == the i2 from the\n        tuple preceding it, and likewise for j1 == the previous j2.\n\n        The tags are strings, with these meanings:\n\n        'replace':  a[i1:i2] should be replaced by b[j1:j2]\n        'delete':   a[i1:i2] should be deleted.\n                    Note that j1==j2 in this case.\n        'insert':   b[j1:j2] should be inserted at a[i1:i1].\n                    Note that i1==i2 in this case.\n        'equal':    a[i1:i2] == b[j1:j2]\n\n        >>> a = \"qabxcd\"\n        >>> b = \"abycdf\"\n        >>> s = SequenceMatcher(None, a, b)\n        >>> for tag, i1, i2, j1, j2 in s.get_opcodes():\n        ...    print (\"%7s a[%d:%d] (%s) b[%d:%d] (%s)\" %\n        ...           (tag, i1, i2, a[i1:i2], j1, j2, b[j1:j2]))\n         delete a[0:1] (q) b[0:0] ()\n          equal a[1:3] (ab) b[0:2] (ab)\n        replace a[3:4] (x) b[2:3] (y)\n          equal a[4:6] (cd) b[3:5] (cd)\n         insert a[6:6] () b[5:6] (f)\n        \"\"\"\n\n        if self.opcodes is not None:\n            return self.opcodes\n        i = j = 0\n        self.opcodes = answer = []\n        for ai, bj, size in self.get_matching_blocks():\n            # invariant:  we've pumped out correct diffs to change\n            # a[:i] into b[:j], and the next matching block is\n            # a[ai:ai+size] == b[bj:bj+size].  So we need to pump\n            # out a diff to change a[i:ai] into b[j:bj], pump out\n            # the matching block, and move (i,j) beyond the match\n            tag = ''\n            if i < ai and j < bj:\n                tag = 'replace'\n            elif i < ai:\n                tag = 'delete'\n            elif j < bj:\n                tag = 'insert'\n            if tag:\n                answer.append( (tag, i, ai, j, bj) )\n            i, j = ai+size, bj+size\n            # the list of matching blocks is terminated by a\n            # sentinel with size 0\n            if size:\n                answer.append( ('equal', ai, i, bj, j) )\n        return answer\n\n    def get_grouped_opcodes(self, n=3):\n        \"\"\" Isolate change clusters by eliminating ranges with no changes.\n\n        Return a generator of groups with up to n lines of context.\n        Each group is in the same format as returned by get_opcodes().\n\n        >>> from pprint import pprint\n        >>> a = map(str, range(1,40))\n        >>> b = a[:]\n        >>> b[8:8] = ['i']     # Make an insertion\n        >>> b[20] += 'x'       # Make a replacement\n        >>> b[23:28] = []      # Make a deletion\n        >>> b[30] += 'y'       # Make another replacement\n        >>> pprint(list(SequenceMatcher(None,a,b).get_grouped_opcodes()))\n        [[('equal', 5, 8, 5, 8), ('insert', 8, 8, 8, 9), ('equal', 8, 11, 9, 12)],\n         [('equal', 16, 19, 17, 20),\n          ('replace', 19, 20, 20, 21),\n          ('equal', 20, 22, 21, 23),\n          ('delete', 22, 27, 23, 23),\n          ('equal', 27, 30, 23, 26)],\n         [('equal', 31, 34, 27, 30),\n          ('replace', 34, 35, 30, 31),\n          ('equal', 35, 38, 31, 34)]]\n        \"\"\"\n\n        codes = self.get_opcodes()\n        if not codes:\n            codes = [(\"equal\", 0, 1, 0, 1)]\n        # Fixup leading and trailing groups if they show no changes.\n        if codes[0][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[0]\n            codes[0] = tag, max(i1, i2-n), i2, max(j1, j2-n), j2\n        if codes[-1][0] == 'equal':\n            tag, i1, i2, j1, j2 = codes[-1]\n            codes[-1] = tag, i1, min(i2, i1+n), j1, min(j2, j1+n)\n\n        nn = n + n\n        group = []\n        for tag, i1, i2, j1, j2 in codes:\n            # End the current group and start a new one whenever\n            # there is a large range with no changes.\n            if tag == 'equal' and i2-i1 > nn:\n                group.append((tag, i1, min(i2, i1+n), j1, min(j2, j1+n)))\n                yield group\n                group = []\n                i1, j1 = max(i1, i2-n), max(j1, j2-n)\n            group.append((tag, i1, i2, j1 ,j2))\n        if group and not (len(group)==1 and group[0][0] == 'equal'):\n            yield group\n\n    def ratio(self):\n        \"\"\"Return a measure of the sequences' similarity (float in [0,1]).\n\n        Where T is the total number of elements in both sequences, and\n        M is the number of matches, this is 2.0*M / T.\n        Note that this is 1 if the sequences are identical, and 0 if\n        they have nothing in common.\n\n        .ratio() is expensive to compute if you haven't already computed\n        .get_matching_blocks() or .get_opcodes(), in which case you may\n        want to try .quick_ratio() or .real_quick_ratio() first to get an\n        upper bound.\n\n        >>> s = SequenceMatcher(None, \"abcd\", \"bcde\")\n        >>> s.ratio()\n        0.75\n        >>> s.quick_ratio()\n        0.75\n        >>> s.real_quick_ratio()\n        1.0\n        \"\"\"\n\n        matches = reduce(lambda sum, triple: sum + triple[-1],\n                         self.get_matching_blocks(), 0)\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() relatively quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute.\n        \"\"\"\n\n        # viewing a and b as multisets, set matches to the cardinality\n        # of their intersection; this counts the number of matches\n        # without regard to order, so is clearly an upper bound\n        if self.fullbcount is None:\n            self.fullbcount = fullbcount = {}\n            for elt in self.b:\n                fullbcount[elt] = fullbcount.get(elt, 0) + 1\n        fullbcount = self.fullbcount\n        # avail[x] is the number of times x appears in 'b' less the\n        # number of times we've seen it in 'a' so far ... kinda\n        avail = {}\n        availhas, matches = avail.__contains__, 0\n        for elt in self.a:\n            if availhas(elt):\n                numb = avail[elt]\n            else:\n                numb = fullbcount.get(elt, 0)\n            avail[elt] = numb - 1\n            if numb > 0:\n                matches = matches + 1\n        return _calculate_ratio(matches, len(self.a) + len(self.b))\n\n    def real_quick_ratio(self):\n        \"\"\"Return an upper bound on ratio() very quickly.\n\n        This isn't defined beyond that it is an upper bound on .ratio(), and\n        is faster to compute than either .ratio() or .quick_ratio().\n        \"\"\"\n\n        la, lb = len(self.a), len(self.b)\n        # can't have more matches than the number of elements in the\n        # shorter sequence\n        return _calculate_ratio(min(la, lb), la + lb)\n\ndef get_close_matches(word, possibilities, n=3, cutoff=0.6):\n    \"\"\"Use SequenceMatcher to return list of the best \"good enough\" matches.\n\n    word is a sequence for which close matches are desired (typically a\n    string).\n\n    possibilities is a list of sequences against which to match word\n    (typically a list of strings).\n\n    Optional arg n (default 3) is the maximum number of close matches to\n    return.  n must be > 0.\n\n    Optional arg cutoff (default 0.6) is a float in [0, 1].  Possibilities\n    that don't score at least that similar to word are ignored.\n\n    The best (no more than n) matches among the possibilities are returned\n    in a list, sorted by similarity score, most similar first.\n\n    >>> get_close_matches(\"appel\", [\"ape\", \"apple\", \"peach\", \"puppy\"])\n    ['apple', 'ape']\n    >>> import keyword as _keyword\n    >>> get_close_matches(\"wheel\", _keyword.kwlist)\n    ['while']\n    >>> get_close_matches(\"apple\", _keyword.kwlist)\n    []\n    >>> get_close_matches(\"accept\", _keyword.kwlist)\n    ['except']\n    \"\"\"\n\n    if not n >  0:\n        raise ValueError(\"n must be > 0: %r\" % (n,))\n    if not 0.0 <= cutoff <= 1.0:\n        raise ValueError(\"cutoff must be in [0.0, 1.0]: %r\" % (cutoff,))\n    result = []\n    s = SequenceMatcher()\n    s.set_seq2(word)\n    for x in possibilities:\n        s.set_seq1(x)\n        if s.real_quick_ratio() >= cutoff and \\\n           s.quick_ratio() >= cutoff and \\\n           s.ratio() >= cutoff:\n            result.append((s.ratio(), x))\n\n    # Move the best scorers to head of list\n    result = heapq.nlargest(n, result)\n    # Strip scores for the best n matches\n    return [x for score, x in result]\n\ndef _count_leading(line, ch):\n    \"\"\"\n    Return number of `ch` characters at the start of `line`.\n\n    Example:\n\n    >>> _count_leading('   abc', ' ')\n    3\n    \"\"\"\n\n    i, n = 0, len(line)\n    while i < n and line[i] == ch:\n        i += 1\n    return i\n\nclass Differ:\n    r\"\"\"\n    Differ is a class for comparing sequences of lines of text, and\n    producing human-readable differences or deltas.  Differ uses\n    SequenceMatcher both to compare sequences of lines, and to compare\n    sequences of characters within similar (near-matching) lines.\n\n    Each line of a Differ delta begins with a two-letter code:\n\n        '- '    line unique to sequence 1\n        '+ '    line unique to sequence 2\n        '  '    line common to both sequences\n        '? '    line not present in either input sequence\n\n    Lines beginning with '? ' attempt to guide the eye to intraline\n    differences, and were not present in either input sequence.  These lines\n    can be confusing if the sequences contain tab characters.\n\n    Note that Differ makes no claim to produce a *minimal* diff.  To the\n    contrary, minimal diffs are often counter-intuitive, because they synch\n    up anywhere possible, sometimes accidental matches 100 pages apart.\n    Restricting synch points to contiguous matches preserves some notion of\n    locality, at the occasional cost of producing a longer diff.\n\n    Example: Comparing two texts.\n\n    First we set up the texts, sequences of individual single-line strings\n    ending with newlines (such sequences can also be obtained from the\n    `readlines()` method of file-like objects):\n\n    >>> text1 = '''  1. Beautiful is better than ugly.\n    ...   2. Explicit is better than implicit.\n    ...   3. Simple is better than complex.\n    ...   4. Complex is better than complicated.\n    ... '''.splitlines(1)\n    >>> len(text1)\n    4\n    >>> text1[0][-1]\n    '\\n'\n    >>> text2 = '''  1. Beautiful is better than ugly.\n    ...   3.   Simple is better than complex.\n    ...   4. Complicated is better than complex.\n    ...   5. Flat is better than nested.\n    ... '''.splitlines(1)\n\n    Next we instantiate a Differ object:\n\n    >>> d = Differ()\n\n    Note that when instantiating a Differ object we may pass functions to\n    filter out line and character 'junk'.  See Differ.__init__ for details.\n\n    Finally, we compare the two:\n\n    >>> result = list(d.compare(text1, text2))\n\n    'result' is a list of strings, so let's pretty-print it:\n\n    >>> from pprint import pprint as _pprint\n    >>> _pprint(result)\n    ['    1. Beautiful is better than ugly.\\n',\n     '-   2. Explicit is better than implicit.\\n',\n     '-   3. Simple is better than complex.\\n',\n     '+   3.   Simple is better than complex.\\n',\n     '?     ++\\n',\n     '-   4. Complex is better than complicated.\\n',\n     '?            ^                     ---- ^\\n',\n     '+   4. Complicated is better than complex.\\n',\n     '?           ++++ ^                      ^\\n',\n     '+   5. Flat is better than nested.\\n']\n\n    As a single multi-line string it looks like this:\n\n    >>> print ''.join(result),\n        1. Beautiful is better than ugly.\n    -   2. Explicit is better than implicit.\n    -   3. Simple is better than complex.\n    +   3.   Simple is better than complex.\n    ?     ++\n    -   4. Complex is better than complicated.\n    ?            ^                     ---- ^\n    +   4. Complicated is better than complex.\n    ?           ++++ ^                      ^\n    +   5. Flat is better than nested.\n\n    Methods:\n\n    __init__(linejunk=None, charjunk=None)\n        Construct a text differencer, with optional filters.\n\n    compare(a, b)\n        Compare two sequences of lines; generate the resulting delta.\n    \"\"\"\n\n    def __init__(self, linejunk=None, charjunk=None):\n        \"\"\"\n        Construct a text differencer, with optional filters.\n\n        The two optional keyword parameters are for filter functions:\n\n        - `linejunk`: A function that should accept a single string argument,\n          and return true iff the string is junk. The module-level function\n          `IS_LINE_JUNK` may be used to filter out lines without visible\n          characters, except for at most one splat ('#').  It is recommended\n          to leave linejunk None; as of Python 2.3, the underlying\n          SequenceMatcher class has grown an adaptive notion of \"noise\" lines\n          that's better than any static definition the author has ever been\n          able to craft.\n\n        - `charjunk`: A function that should accept a string of length 1. The\n          module-level function `IS_CHARACTER_JUNK` may be used to filter out\n          whitespace characters (a blank or tab; **note**: bad idea to include\n          newline in this!).  Use of IS_CHARACTER_JUNK is recommended.\n        \"\"\"\n\n        self.linejunk = linejunk\n        self.charjunk = charjunk\n\n    def compare(self, a, b):\n        r\"\"\"\n        Compare two sequences of lines; generate the resulting delta.\n\n        Each sequence must contain individual single-line strings ending with\n        newlines. Such sequences can be obtained from the `readlines()` method\n        of file-like objects.  The delta generated also consists of newline-\n        terminated strings, ready to be printed as-is via the writeline()\n        method of a file-like object.\n\n        Example:\n\n        >>> print ''.join(Differ().compare('one\\ntwo\\nthree\\n'.splitlines(1),\n        ...                                'ore\\ntree\\nemu\\n'.splitlines(1))),\n        - one\n        ?  ^\n        + ore\n        ?  ^\n        - two\n        - three\n        ?  -\n        + tree\n        + emu\n        \"\"\"\n\n        cruncher = SequenceMatcher(self.linejunk, a, b)\n        for tag, alo, ahi, blo, bhi in cruncher.get_opcodes():\n            if tag == 'replace':\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            elif tag == 'delete':\n                g = self._dump('-', a, alo, ahi)\n            elif tag == 'insert':\n                g = self._dump('+', b, blo, bhi)\n            elif tag == 'equal':\n                g = self._dump(' ', a, alo, ahi)\n            else:\n                raise ValueError, 'unknown tag %r' % (tag,)\n\n            for line in g:\n                yield line\n\n    def _dump(self, tag, x, lo, hi):\n        \"\"\"Generate comparison results for a same-tagged range.\"\"\"\n        for i in xrange(lo, hi):\n            yield '%s %s' % (tag, x[i])\n\n    def _plain_replace(self, a, alo, ahi, b, blo, bhi):\n        assert alo < ahi and blo < bhi\n        # dump the shorter block first -- reduces the burden on short-term\n        # memory if the blocks are of very different sizes\n        if bhi - blo < ahi - alo:\n            first  = self._dump('+', b, blo, bhi)\n            second = self._dump('-', a, alo, ahi)\n        else:\n            first  = self._dump('-', a, alo, ahi)\n            second = self._dump('+', b, blo, bhi)\n\n        for g in first, second:\n            for line in g:\n                yield line\n\n    def _fancy_replace(self, a, alo, ahi, b, blo, bhi):\n        r\"\"\"\n        When replacing one block of lines with another, search the blocks\n        for *similar* lines; the best-matching pair (if any) is used as a\n        synch point, and intraline difference marking is done on the\n        similar pair. Lots of work, but often worth it.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._fancy_replace(['abcDefghiJkl\\n'], 0, 1,\n        ...                            ['abcdefGhijkl\\n'], 0, 1)\n        >>> print ''.join(results),\n        - abcDefghiJkl\n        ?    ^  ^  ^\n        + abcdefGhijkl\n        ?    ^  ^  ^\n        \"\"\"\n\n        # don't synch up unless the lines have a similarity score of at\n        # least cutoff; best_ratio tracks the best score seen so far\n        best_ratio, cutoff = 0.74, 0.75\n        cruncher = SequenceMatcher(self.charjunk)\n        eqi, eqj = None, None   # 1st indices of equal lines (if any)\n\n        # search for the pair that matches best without being identical\n        # (identical lines must be junk lines, & we don't want to synch up\n        # on junk -- unless we have to)\n        for j in xrange(blo, bhi):\n            bj = b[j]\n            cruncher.set_seq2(bj)\n            for i in xrange(alo, ahi):\n                ai = a[i]\n                if ai == bj:\n                    if eqi is None:\n                        eqi, eqj = i, j\n                    continue\n                cruncher.set_seq1(ai)\n                # computing similarity is expensive, so use the quick\n                # upper bounds first -- have seen this speed up messy\n                # compares by a factor of 3.\n                # note that ratio() is only expensive to compute the first\n                # time it's called on a sequence pair; the expensive part\n                # of the computation is cached by cruncher\n                if cruncher.real_quick_ratio() > best_ratio and \\\n                      cruncher.quick_ratio() > best_ratio and \\\n                      cruncher.ratio() > best_ratio:\n                    best_ratio, best_i, best_j = cruncher.ratio(), i, j\n        if best_ratio < cutoff:\n            # no non-identical \"pretty close\" pair\n            if eqi is None:\n                # no identical pair either -- treat it as a straight replace\n                for line in self._plain_replace(a, alo, ahi, b, blo, bhi):\n                    yield line\n                return\n            # no close pair, but an identical pair -- synch up on that\n            best_i, best_j, best_ratio = eqi, eqj, 1.0\n        else:\n            # there's a close pair, so forget the identical pair (if any)\n            eqi = None\n\n        # a[best_i] very similar to b[best_j]; eqi is None iff they're not\n        # identical\n\n        # pump out diffs from before the synch point\n        for line in self._fancy_helper(a, alo, best_i, b, blo, best_j):\n            yield line\n\n        # do intraline marking on the synch pair\n        aelt, belt = a[best_i], b[best_j]\n        if eqi is None:\n            # pump out a '-', '?', '+', '?' quad for the synched lines\n            atags = btags = \"\"\n            cruncher.set_seqs(aelt, belt)\n            for tag, ai1, ai2, bj1, bj2 in cruncher.get_opcodes():\n                la, lb = ai2 - ai1, bj2 - bj1\n                if tag == 'replace':\n                    atags += '^' * la\n                    btags += '^' * lb\n                elif tag == 'delete':\n                    atags += '-' * la\n                elif tag == 'insert':\n                    btags += '+' * lb\n                elif tag == 'equal':\n                    atags += ' ' * la\n                    btags += ' ' * lb\n                else:\n                    raise ValueError, 'unknown tag %r' % (tag,)\n            for line in self._qformat(aelt, belt, atags, btags):\n                yield line\n        else:\n            # the synch pair is identical\n            yield '  ' + aelt\n\n        # pump out diffs from after the synch point\n        for line in self._fancy_helper(a, best_i+1, ahi, b, best_j+1, bhi):\n            yield line\n\n    def _fancy_helper(self, a, alo, ahi, b, blo, bhi):\n        g = []\n        if alo < ahi:\n            if blo < bhi:\n                g = self._fancy_replace(a, alo, ahi, b, blo, bhi)\n            else:\n                g = self._dump('-', a, alo, ahi)\n        elif blo < bhi:\n            g = self._dump('+', b, blo, bhi)\n\n        for line in g:\n            yield line\n\n    def _qformat(self, aline, bline, atags, btags):\n        r\"\"\"\n        Format \"?\" output and deal with leading tabs.\n\n        Example:\n\n        >>> d = Differ()\n        >>> results = d._qformat('\\tabcDefghiJkl\\n', '\\tabcdefGhijkl\\n',\n        ...                      '  ^ ^  ^      ', '  ^ ^  ^      ')\n        >>> for line in results: print repr(line)\n        ...\n        '- \\tabcDefghiJkl\\n'\n        '? \\t ^ ^  ^\\n'\n        '+ \\tabcdefGhijkl\\n'\n        '? \\t ^ ^  ^\\n'\n        \"\"\"\n\n        # Can hurt, but will probably help most of the time.\n        common = min(_count_leading(aline, \"\\t\"),\n                     _count_leading(bline, \"\\t\"))\n        common = min(common, _count_leading(atags[:common], \" \"))\n        common = min(common, _count_leading(btags[:common], \" \"))\n        atags = atags[common:].rstrip()\n        btags = btags[common:].rstrip()\n\n        yield \"- \" + aline\n        if atags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, atags)\n\n        yield \"+ \" + bline\n        if btags:\n            yield \"? %s%s\\n\" % (\"\\t\" * common, btags)\n\n# With respect to junk, an earlier version of ndiff simply refused to\n# *start* a match with a junk element.  The result was cases like this:\n#     before: private Thread currentThread;\n#     after:  private volatile Thread currentThread;\n# If you consider whitespace to be junk, the longest contiguous match\n# not starting with junk is \"e Thread currentThread\".  So ndiff reported\n# that \"e volatil\" was inserted between the 't' and the 'e' in \"private\".\n# While an accurate view, to people that's absurd.  The current version\n# looks for matching blocks that are entirely junk-free, then extends the\n# longest one of those as far as possible but only with matching junk.\n# So now \"currentThread\" is matched, then extended to suck up the\n# preceding blank; then \"private\" is matched, and extended to suck up the\n# following blank; then \"Thread\" is matched; and finally ndiff reports\n# that \"volatile \" was inserted before \"Thread\".  The only quibble\n# remaining is that perhaps it was really the case that \" volatile\"\n# was inserted after \"private\".  I can live with that <wink>.\n\nimport re\n\ndef IS_LINE_JUNK(line, pat=re.compile(r\"\\s*#?\\s*$\").match):\n    r\"\"\"\n    Return 1 for ignorable line: iff `line` is blank or contains a single '#'.\n\n    Examples:\n\n    >>> IS_LINE_JUNK('\\n')\n    True\n    >>> IS_LINE_JUNK('  #   \\n')\n    True\n    >>> IS_LINE_JUNK('hello\\n')\n    False\n    \"\"\"\n\n    return pat(line) is not None\n\ndef IS_CHARACTER_JUNK(ch, ws=\" \\t\"):\n    r\"\"\"\n    Return 1 for ignorable character: iff `ch` is a space or tab.\n\n    Examples:\n\n    >>> IS_CHARACTER_JUNK(' ')\n    True\n    >>> IS_CHARACTER_JUNK('\\t')\n    True\n    >>> IS_CHARACTER_JUNK('\\n')\n    False\n    >>> IS_CHARACTER_JUNK('x')\n    False\n    \"\"\"\n\n    return ch in ws\n\n\n########################################################################\n###  Unified Diff\n########################################################################\n\ndef _format_range_unified(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if length == 1:\n        return '{}'.format(beginning)\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    return '{},{}'.format(beginning, length)\n\ndef unified_diff(a, b, fromfile='', tofile='', fromfiledate='',\n                 tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a unified diff.\n\n    Unified diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with ---, +++, or @@) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The unidiff format normally has a header for filenames and modification\n    times.  Any or all of these may be specified using strings for\n    'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n\n    Example:\n\n    >>> for line in unified_diff('one two three four'.split(),\n    ...             'zero one tree four'.split(), 'Original', 'Current',\n    ...             '2005-01-26 23:30:50', '2010-04-02 10:20:52',\n    ...             lineterm=''):\n    ...     print line                  # doctest: +NORMALIZE_WHITESPACE\n    --- Original        2005-01-26 23:30:50\n    +++ Current         2010-04-02 10:20:52\n    @@ -1,4 +1,4 @@\n    +zero\n     one\n    -two\n    -three\n    +tree\n     four\n    \"\"\"\n\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '--- {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '+++ {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        file1_range = _format_range_unified(first[1], last[2])\n        file2_range = _format_range_unified(first[3], last[4])\n        yield '@@ -{} +{} @@{}'.format(file1_range, file2_range, lineterm)\n\n        for tag, i1, i2, j1, j2 in group:\n            if tag == 'equal':\n                for line in a[i1:i2]:\n                    yield ' ' + line\n                continue\n            if tag in ('replace', 'delete'):\n                for line in a[i1:i2]:\n                    yield '-' + line\n            if tag in ('replace', 'insert'):\n                for line in b[j1:j2]:\n                    yield '+' + line\n\n\n########################################################################\n###  Context Diff\n########################################################################\n\ndef _format_range_context(start, stop):\n    'Convert range to the \"ed\" format'\n    # Per the diff spec at http://www.unix.org/single_unix_specification/\n    beginning = start + 1     # lines start numbering with one\n    length = stop - start\n    if not length:\n        beginning -= 1        # empty ranges begin at line just before the range\n    if length <= 1:\n        return '{}'.format(beginning)\n    return '{},{}'.format(beginning, beginning + length - 1)\n\n# See http://www.unix.org/single_unix_specification/\ndef context_diff(a, b, fromfile='', tofile='',\n                 fromfiledate='', tofiledate='', n=3, lineterm='\\n'):\n    r\"\"\"\n    Compare two sequences of lines; generate the delta as a context diff.\n\n    Context diffs are a compact way of showing line changes and a few\n    lines of context.  The number of context lines is set by 'n' which\n    defaults to three.\n\n    By default, the diff control lines (those with *** or ---) are\n    created with a trailing newline.  This is helpful so that inputs\n    created from file.readlines() result in diffs that are suitable for\n    file.writelines() since both the inputs and outputs have trailing\n    newlines.\n\n    For inputs that do not have trailing newlines, set the lineterm\n    argument to \"\" so that the output will be uniformly newline free.\n\n    The context diff format normally has a header for filenames and\n    modification times.  Any or all of these may be specified using\n    strings for 'fromfile', 'tofile', 'fromfiledate', and 'tofiledate'.\n    The modification times are normally expressed in the ISO 8601 format.\n    If not specified, the strings default to blanks.\n\n    Example:\n\n    >>> print ''.join(context_diff('one\\ntwo\\nthree\\nfour\\n'.splitlines(1),\n    ...       'zero\\none\\ntree\\nfour\\n'.splitlines(1), 'Original', 'Current')),\n    *** Original\n    --- Current\n    ***************\n    *** 1,4 ****\n      one\n    ! two\n    ! three\n      four\n    --- 1,4 ----\n    + zero\n      one\n    ! tree\n      four\n    \"\"\"\n\n    prefix = dict(insert='+ ', delete='- ', replace='! ', equal='  ')\n    started = False\n    for group in SequenceMatcher(None,a,b).get_grouped_opcodes(n):\n        if not started:\n            started = True\n            fromdate = '\\t{}'.format(fromfiledate) if fromfiledate else ''\n            todate = '\\t{}'.format(tofiledate) if tofiledate else ''\n            yield '*** {}{}{}'.format(fromfile, fromdate, lineterm)\n            yield '--- {}{}{}'.format(tofile, todate, lineterm)\n\n        first, last = group[0], group[-1]\n        yield '***************' + lineterm\n\n        file1_range = _format_range_context(first[1], last[2])\n        yield '*** {} ****{}'.format(file1_range, lineterm)\n\n        if any(tag in ('replace', 'delete') for tag, _, _, _, _ in group):\n            for tag, i1, i2, _, _ in group:\n                if tag != 'insert':\n                    for line in a[i1:i2]:\n                        yield prefix[tag] + line\n\n        file2_range = _format_range_context(first[3], last[4])\n        yield '--- {} ----{}'.format(file2_range, lineterm)\n\n        if any(tag in ('replace', 'insert') for tag, _, _, _, _ in group):\n            for tag, _, _, j1, j2 in group:\n                if tag != 'delete':\n                    for line in b[j1:j2]:\n                        yield prefix[tag] + line\n\ndef ndiff(a, b, linejunk=None, charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"\n    Compare `a` and `b` (lists of strings); return a `Differ`-style delta.\n\n    Optional keyword parameters `linejunk` and `charjunk` are for filter\n    functions (or None):\n\n    - linejunk: A function that should accept a single string argument, and\n      return true iff the string is junk.  The default is None, and is\n      recommended; as of Python 2.3, an adaptive notion of \"noise\" lines is\n      used that does a good job on its own.\n\n    - charjunk: A function that should accept a string of length 1. The\n      default is module-level function IS_CHARACTER_JUNK, which filters out\n      whitespace characters (a blank or tab; note: bad idea to include newline\n      in this!).\n\n    Tools/scripts/ndiff.py is a command-line front-end to this function.\n\n    Example:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> print ''.join(diff),\n    - one\n    ?  ^\n    + ore\n    ?  ^\n    - two\n    - three\n    ?  -\n    + tree\n    + emu\n    \"\"\"\n    return Differ(linejunk, charjunk).compare(a, b)\n\ndef _mdiff(fromlines, tolines, context=None, linejunk=None,\n           charjunk=IS_CHARACTER_JUNK):\n    r\"\"\"Returns generator yielding marked up from/to side by side differences.\n\n    Arguments:\n    fromlines -- list of text lines to compared to tolines\n    tolines -- list of text lines to be compared to fromlines\n    context -- number of context lines to display on each side of difference,\n               if None, all from/to text lines will be generated.\n    linejunk -- passed on to ndiff (see ndiff documentation)\n    charjunk -- passed on to ndiff (see ndiff documentation)\n\n    This function returns an iterator which returns a tuple:\n    (from line tuple, to line tuple, boolean flag)\n\n    from/to line tuple -- (line num, line text)\n        line num -- integer or None (to indicate a context separation)\n        line text -- original line text with following markers inserted:\n            '\\0+' -- marks start of added text\n            '\\0-' -- marks start of deleted text\n            '\\0^' -- marks start of changed text\n            '\\1' -- marks end of added/deleted/changed text\n\n    boolean flag -- None indicates context separation, True indicates\n        either \"from\" or \"to\" line contains a change, otherwise False.\n\n    This function/iterator was originally developed to generate side by side\n    file difference for making HTML pages (see HtmlDiff class for example\n    usage).\n\n    Note, this function utilizes the ndiff function to generate the side by\n    side difference markup.  Optional ndiff arguments may be passed to this\n    function and they in turn will be passed to ndiff.\n    \"\"\"\n    import re\n\n    # regular expression for finding intraline change indices\n    change_re = re.compile('(\\++|\\-+|\\^+)')\n\n    # create the difference iterator to generate the differences\n    diff_lines_iterator = ndiff(fromlines,tolines,linejunk,charjunk)\n\n    def _make_line(lines, format_key, side, num_lines=[0,0]):\n        \"\"\"Returns line of text with user's change markup and line formatting.\n\n        lines -- list of lines from the ndiff generator to produce a line of\n                 text from.  When producing the line of text to return, the\n                 lines used are removed from this list.\n        format_key -- '+' return first line in list with \"add\" markup around\n                          the entire line.\n                      '-' return first line in list with \"delete\" markup around\n                          the entire line.\n                      '?' return first line in list with add/delete/change\n                          intraline markup (indices obtained from second line)\n                      None return first line in list with no markup\n        side -- indice into the num_lines list (0=from,1=to)\n        num_lines -- from/to current line number.  This is NOT intended to be a\n                     passed parameter.  It is present as a keyword argument to\n                     maintain memory of the current line numbers between calls\n                     of this function.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        num_lines[side] += 1\n        # Handle case where no user markup is to be added, just return line of\n        # text with user's line format to allow for usage of the line number.\n        if format_key is None:\n            return (num_lines[side],lines.pop(0)[2:])\n        # Handle case of intraline changes\n        if format_key == '?':\n            text, markers = lines.pop(0), lines.pop(0)\n            # find intraline changes (store change type and indices in tuples)\n            sub_info = []\n            def record_sub_info(match_object,sub_info=sub_info):\n                sub_info.append([match_object.group(1)[0],match_object.span()])\n                return match_object.group(1)\n            change_re.sub(record_sub_info,markers)\n            # process each tuple inserting our special marks that won't be\n            # noticed by an xml/html escaper.\n            for key,(begin,end) in sub_info[::-1]:\n                text = text[0:begin]+'\\0'+key+text[begin:end]+'\\1'+text[end:]\n            text = text[2:]\n        # Handle case of add/delete entire line\n        else:\n            text = lines.pop(0)[2:]\n            # if line of text is just a newline, insert a space so there is\n            # something for the user to highlight and see.\n            if not text:\n                text = ' '\n            # insert marks that won't be noticed by an xml/html escaper.\n            text = '\\0' + format_key + text + '\\1'\n        # Return line of text, first allow user's line formatter to do its\n        # thing (such as adding the line number) then replace the special\n        # marks with what the user's change markup.\n        return (num_lines[side],text)\n\n    def _line_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from a\n        differencing iterator, processes them and yields them.  When it can\n        it yields both a \"from\" and a \"to\" line, otherwise it will yield one\n        or the other.  In addition to yielding the lines of from/to text, a\n        boolean flag is yielded to indicate if the text line(s) have\n        differences in them.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        lines = []\n        num_blanks_pending, num_blanks_to_yield = 0, 0\n        while True:\n            # Load up next 4 lines so we can look ahead, create strings which\n            # are a concatenation of the first character of each of the 4 lines\n            # so we can do some very readable comparisons.\n            while len(lines) < 4:\n                try:\n                    lines.append(diff_lines_iterator.next())\n                except StopIteration:\n                    lines.append('X')\n            s = ''.join([line[0] for line in lines])\n            if s.startswith('X'):\n                # When no more lines, pump out any remaining blank lines so the\n                # corresponding add/delete lines get a matching blank line so\n                # all line pairs get yielded at the next level.\n                num_blanks_to_yield = num_blanks_pending\n            elif s.startswith('-?+?'):\n                # simple intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('--++'):\n                # in delete block, add block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the delete line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith(('--?+', '--+', '- ')):\n                # in delete block and see a intraline change or unchanged line\n                # coming: yield the delete line and then blanks\n                from_line,to_line = _make_line(lines,'-',0), None\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending-1,0\n            elif s.startswith('-+?'):\n                # intraline change\n                yield _make_line(lines,None,0), _make_line(lines,'?',1), True\n                continue\n            elif s.startswith('-?+'):\n                # intraline change\n                yield _make_line(lines,'?',0), _make_line(lines,None,1), True\n                continue\n            elif s.startswith('-'):\n                # delete FROM line\n                num_blanks_pending -= 1\n                yield _make_line(lines,'-',0), None, True\n                continue\n            elif s.startswith('+--'):\n                # in add block, delete block coming: we do NOT want to get\n                # caught up on blank lines yet, just process the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(('+ ', '+-')):\n                # will be leaving an add block: yield blanks then add line\n                from_line, to_line = None, _make_line(lines,'+',1)\n                num_blanks_to_yield,num_blanks_pending = num_blanks_pending+1,0\n            elif s.startswith('+'):\n                # inside an add block, yield the add line\n                num_blanks_pending += 1\n                yield None, _make_line(lines,'+',1), True\n                continue\n            elif s.startswith(' '):\n                # unchanged text, yield it to both sides\n                yield _make_line(lines[:],None,0),_make_line(lines,None,1),False\n                continue\n            # Catch up on the blank lines so when we yield the next from/to\n            # pair, they are lined up.\n            while(num_blanks_to_yield < 0):\n                num_blanks_to_yield += 1\n                yield None,('','\\n'),True\n            while(num_blanks_to_yield > 0):\n                num_blanks_to_yield -= 1\n                yield ('','\\n'),None,True\n            if s.startswith('X'):\n                raise StopIteration\n            else:\n                yield from_line,to_line,True\n\n    def _line_pair_iterator():\n        \"\"\"Yields from/to lines of text with a change indication.\n\n        This function is an iterator.  It itself pulls lines from the line\n        iterator.  Its difference from that iterator is that this function\n        always yields a pair of from/to text lines (with the change\n        indication).  If necessary it will collect single from/to lines\n        until it has a matching pair from/to pair to yield.\n\n        Note, this function is purposefully not defined at the module scope so\n        that data it needs from its parent function (within whose context it\n        is defined) does not need to be of module scope.\n        \"\"\"\n        line_iterator = _line_iterator()\n        fromlines,tolines=[],[]\n        while True:\n            # Collecting lines of text until we have a from/to pair\n            while (len(fromlines)==0 or len(tolines)==0):\n                from_line, to_line, found_diff =line_iterator.next()\n                if from_line is not None:\n                    fromlines.append((from_line,found_diff))\n                if to_line is not None:\n                    tolines.append((to_line,found_diff))\n            # Once we have a pair, remove them from the collection and yield it\n            from_line, fromDiff = fromlines.pop(0)\n            to_line, to_diff = tolines.pop(0)\n            yield (from_line,to_line,fromDiff or to_diff)\n\n    # Handle case where user does not want context differencing, just yield\n    # them up without doing anything else with them.\n    line_pair_iterator = _line_pair_iterator()\n    if context is None:\n        while True:\n            yield line_pair_iterator.next()\n    # Handle case where user wants context differencing.  We must do some\n    # storage of lines until we know for sure that they are to be yielded.\n    else:\n        context += 1\n        lines_to_write = 0\n        while True:\n            # Store lines up until we find a difference, note use of a\n            # circular queue because we only need to keep around what\n            # we need for context.\n            index, contextLines = 0, [None]*(context)\n            found_diff = False\n            while(found_diff is False):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                i = index % context\n                contextLines[i] = (from_line, to_line, found_diff)\n                index += 1\n            # Yield lines that we have collected so far, but first yield\n            # the user's separator.\n            if index > context:\n                yield None, None, None\n                lines_to_write = context\n            else:\n                lines_to_write = index\n                index = 0\n            while(lines_to_write):\n                i = index % context\n                index += 1\n                yield contextLines[i]\n                lines_to_write -= 1\n            # Now yield the context lines after the change\n            lines_to_write = context-1\n            while(lines_to_write):\n                from_line, to_line, found_diff = line_pair_iterator.next()\n                # If another change within the context, extend the context\n                if found_diff:\n                    lines_to_write = context-1\n                else:\n                    lines_to_write -= 1\n                yield from_line, to_line, found_diff\n\n\n_file_template = \"\"\"\n<!DOCTYPE html PUBLIC \"-//W3C//DTD XHTML 1.0 Transitional//EN\"\n          \"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd\">\n\n<html>\n\n<head>\n    <meta http-equiv=\"Content-Type\"\n          content=\"text/html; charset=ISO-8859-1\" />\n    <title></title>\n    <style type=\"text/css\">%(styles)s\n    </style>\n</head>\n\n<body>\n    %(table)s%(legend)s\n</body>\n\n</html>\"\"\"\n\n_styles = \"\"\"\n        table.diff {font-family:Courier; border:medium;}\n        .diff_header {background-color:#e0e0e0}\n        td.diff_header {text-align:right}\n        .diff_next {background-color:#c0c0c0}\n        .diff_add {background-color:#aaffaa}\n        .diff_chg {background-color:#ffff77}\n        .diff_sub {background-color:#ffaaaa}\"\"\"\n\n_table_template = \"\"\"\n    <table class=\"diff\" id=\"difflib_chg_%(prefix)s_top\"\n           cellspacing=\"0\" cellpadding=\"0\" rules=\"groups\" >\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        <colgroup></colgroup> <colgroup></colgroup> <colgroup></colgroup>\n        %(header_row)s\n        <tbody>\n%(data_rows)s        </tbody>\n    </table>\"\"\"\n\n_legend = \"\"\"\n    <table class=\"diff\" summary=\"Legends\">\n        <tr> <th colspan=\"2\"> Legends </th> </tr>\n        <tr> <td> <table border=\"\" summary=\"Colors\">\n                      <tr><th> Colors </th> </tr>\n                      <tr><td class=\"diff_add\">&nbsp;Added&nbsp;</td></tr>\n                      <tr><td class=\"diff_chg\">Changed</td> </tr>\n                      <tr><td class=\"diff_sub\">Deleted</td> </tr>\n                  </table></td>\n             <td> <table border=\"\" summary=\"Links\">\n                      <tr><th colspan=\"2\"> Links </th> </tr>\n                      <tr><td>(f)irst change</td> </tr>\n                      <tr><td>(n)ext change</td> </tr>\n                      <tr><td>(t)op</td> </tr>\n                  </table></td> </tr>\n    </table>\"\"\"\n\nclass HtmlDiff(object):\n    \"\"\"For producing HTML side by side comparison with change highlights.\n\n    This class can be used to create an HTML table (or a complete HTML file\n    containing the table) showing a side by side, line by line comparison\n    of text with inter-line and intra-line change highlights.  The table can\n    be generated in either full or contextual difference mode.\n\n    The following methods are provided for HTML generation:\n\n    make_table -- generates HTML for a single side by side table\n    make_file -- generates complete HTML file with a single side by side table\n\n    See tools/scripts/diff.py for an example usage of this class.\n    \"\"\"\n\n    _file_template = _file_template\n    _styles = _styles\n    _table_template = _table_template\n    _legend = _legend\n    _default_prefix = 0\n\n    def __init__(self,tabsize=8,wrapcolumn=None,linejunk=None,\n                 charjunk=IS_CHARACTER_JUNK):\n        \"\"\"HtmlDiff instance initializer\n\n        Arguments:\n        tabsize -- tab stop spacing, defaults to 8.\n        wrapcolumn -- column number where lines are broken and wrapped,\n            defaults to None where lines are not wrapped.\n        linejunk,charjunk -- keyword arguments passed into ndiff() (used to by\n            HtmlDiff() to generate the side by side HTML differences).  See\n            ndiff() documentation for argument default values and descriptions.\n        \"\"\"\n        self._tabsize = tabsize\n        self._wrapcolumn = wrapcolumn\n        self._linejunk = linejunk\n        self._charjunk = charjunk\n\n    def make_file(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                  numlines=5):\n        \"\"\"Returns HTML file of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        return self._file_template % dict(\n            styles = self._styles,\n            legend = self._legend,\n            table = self.make_table(fromlines,tolines,fromdesc,todesc,\n                                    context=context,numlines=numlines))\n\n    def _tab_newline_replace(self,fromlines,tolines):\n        \"\"\"Returns from/to line lists with tabs expanded and newlines removed.\n\n        Instead of tab characters being replaced by the number of spaces\n        needed to fill in to the next tab stop, this function will fill\n        the space with tab characters.  This is done so that the difference\n        algorithms can identify changes in a file when tabs are replaced by\n        spaces and vice versa.  At the end of the HTML generation, the tab\n        characters will be replaced with a nonbreakable space.\n        \"\"\"\n        def expand_tabs(line):\n            # hide real spaces\n            line = line.replace(' ','\\0')\n            # expand tabs into spaces\n            line = line.expandtabs(self._tabsize)\n            # replace spaces from expanded tabs back into tab characters\n            # (we'll replace them with markup after we do differencing)\n            line = line.replace(' ','\\t')\n            return line.replace('\\0',' ').rstrip('\\n')\n        fromlines = [expand_tabs(line) for line in fromlines]\n        tolines = [expand_tabs(line) for line in tolines]\n        return fromlines,tolines\n\n    def _split_line(self,data_list,line_num,text):\n        \"\"\"Builds list of text lines by splitting text lines at wrap point\n\n        This function will determine if the input text line needs to be\n        wrapped (split) into separate lines.  If so, the first wrap point\n        will be determined and the first line appended to the output\n        text line list.  This function is used recursively to handle\n        the second part of the split line to further split it.\n        \"\"\"\n        # if blank line or context separator, just add it to the output list\n        if not line_num:\n            data_list.append((line_num,text))\n            return\n\n        # if line text doesn't need wrapping, just add it to the output list\n        size = len(text)\n        max = self._wrapcolumn\n        if (size <= max) or ((size -(text.count('\\0')*3)) <= max):\n            data_list.append((line_num,text))\n            return\n\n        # scan text looking for the wrap point, keeping track if the wrap\n        # point is inside markers\n        i = 0\n        n = 0\n        mark = ''\n        while n < max and i < size:\n            if text[i] == '\\0':\n                i += 1\n                mark = text[i]\n                i += 1\n            elif text[i] == '\\1':\n                i += 1\n                mark = ''\n            else:\n                i += 1\n                n += 1\n\n        # wrap point is inside text, break it up into separate lines\n        line1 = text[:i]\n        line2 = text[i:]\n\n        # if wrap point is inside markers, place end marker at end of first\n        # line and start marker at beginning of second line because each\n        # line will have its own table tag markup around it.\n        if mark:\n            line1 = line1 + '\\1'\n            line2 = '\\0' + mark + line2\n\n        # tack on first line onto the output list\n        data_list.append((line_num,line1))\n\n        # use this routine again to wrap the remaining text\n        self._split_line(data_list,'>',line2)\n\n    def _line_wrapper(self,diffs):\n        \"\"\"Returns iterator that splits (wraps) mdiff text lines\"\"\"\n\n        # pull from/to data and flags from mdiff iterator\n        for fromdata,todata,flag in diffs:\n            # check for context separators and pass them through\n            if flag is None:\n                yield fromdata,todata,flag\n                continue\n            (fromline,fromtext),(toline,totext) = fromdata,todata\n            # for each from/to line split it at the wrap column to form\n            # list of text lines.\n            fromlist,tolist = [],[]\n            self._split_line(fromlist,fromline,fromtext)\n            self._split_line(tolist,toline,totext)\n            # yield from/to line in pairs inserting blank lines as\n            # necessary when one side has more wrapped lines\n            while fromlist or tolist:\n                if fromlist:\n                    fromdata = fromlist.pop(0)\n                else:\n                    fromdata = ('',' ')\n                if tolist:\n                    todata = tolist.pop(0)\n                else:\n                    todata = ('',' ')\n                yield fromdata,todata,flag\n\n    def _collect_lines(self,diffs):\n        \"\"\"Collects mdiff output into separate lists\n\n        Before storing the mdiff from/to data into a list, it is converted\n        into a single line of text with HTML markup.\n        \"\"\"\n\n        fromlist,tolist,flaglist = [],[],[]\n        # pull from/to data and flags from mdiff style iterator\n        for fromdata,todata,flag in diffs:\n            try:\n                # store HTML markup of the lines into the lists\n                fromlist.append(self._format_line(0,flag,*fromdata))\n                tolist.append(self._format_line(1,flag,*todata))\n            except TypeError:\n                # exceptions occur for lines where context separators go\n                fromlist.append(None)\n                tolist.append(None)\n            flaglist.append(flag)\n        return fromlist,tolist,flaglist\n\n    def _format_line(self,side,flag,linenum,text):\n        \"\"\"Returns HTML markup of \"from\" / \"to\" text lines\n\n        side -- 0 or 1 indicating \"from\" or \"to\" text\n        flag -- indicates if difference on line\n        linenum -- line number (used for line number column)\n        text -- line text to be marked up\n        \"\"\"\n        try:\n            linenum = '%d' % linenum\n            id = ' id=\"%s%s\"' % (self._prefix[side],linenum)\n        except TypeError:\n            # handle blank lines where linenum is '>' or ''\n            id = ''\n        # replace those things that would get confused with HTML symbols\n        text=text.replace(\"&\",\"&amp;\").replace(\">\",\"&gt;\").replace(\"<\",\"&lt;\")\n\n        # make space non-breakable so they don't get compressed or line wrapped\n        text = text.replace(' ','&nbsp;').rstrip()\n\n        return '<td class=\"diff_header\"%s>%s</td><td nowrap=\"nowrap\">%s</td>' \\\n               % (id,linenum,text)\n\n    def _make_prefix(self):\n        \"\"\"Create unique anchor prefixes\"\"\"\n\n        # Generate a unique anchor prefix so multiple tables\n        # can exist on the same HTML page without conflicts.\n        fromprefix = \"from%d_\" % HtmlDiff._default_prefix\n        toprefix = \"to%d_\" % HtmlDiff._default_prefix\n        HtmlDiff._default_prefix += 1\n        # store prefixes so line format method has access\n        self._prefix = [fromprefix,toprefix]\n\n    def _convert_flags(self,fromlist,tolist,flaglist,context,numlines):\n        \"\"\"Makes list of \"next\" links\"\"\"\n\n        # all anchor names will be generated using the unique \"to\" prefix\n        toprefix = self._prefix[1]\n\n        # process change flags, generating middle column of next anchors/links\n        next_id = ['']*len(flaglist)\n        next_href = ['']*len(flaglist)\n        num_chg, in_change = 0, False\n        last = 0\n        for i,flag in enumerate(flaglist):\n            if flag:\n                if not in_change:\n                    in_change = True\n                    last = i\n                    # at the beginning of a change, drop an anchor a few lines\n                    # (the context lines) before the change for the previous\n                    # link\n                    i = max([0,i-numlines])\n                    next_id[i] = ' id=\"difflib_chg_%s_%d\"' % (toprefix,num_chg)\n                    # at the beginning of a change, drop a link to the next\n                    # change\n                    num_chg += 1\n                    next_href[last] = '<a href=\"#difflib_chg_%s_%d\">n</a>' % (\n                         toprefix,num_chg)\n            else:\n                in_change = False\n        # check for cases where there is no content to avoid exceptions\n        if not flaglist:\n            flaglist = [False]\n            next_id = ['']\n            next_href = ['']\n            last = 0\n            if context:\n                fromlist = ['<td></td><td>&nbsp;No Differences Found&nbsp;</td>']\n                tolist = fromlist\n            else:\n                fromlist = tolist = ['<td></td><td>&nbsp;Empty File&nbsp;</td>']\n        # if not a change on first line, drop a link\n        if not flaglist[0]:\n            next_href[0] = '<a href=\"#difflib_chg_%s_0\">f</a>' % toprefix\n        # redo the last link to link to the top\n        next_href[last] = '<a href=\"#difflib_chg_%s_top\">t</a>' % (toprefix)\n\n        return fromlist,tolist,flaglist,next_href,next_id\n\n    def make_table(self,fromlines,tolines,fromdesc='',todesc='',context=False,\n                   numlines=5):\n        \"\"\"Returns HTML table of side by side comparison with change highlights\n\n        Arguments:\n        fromlines -- list of \"from\" lines\n        tolines -- list of \"to\" lines\n        fromdesc -- \"from\" file column header string\n        todesc -- \"to\" file column header string\n        context -- set to True for contextual differences (defaults to False\n            which shows full differences).\n        numlines -- number of context lines.  When context is set True,\n            controls number of lines displayed before and after the change.\n            When context is False, controls the number of lines to place\n            the \"next\" link anchors before the next change (so click of\n            \"next\" link jumps to just before the change).\n        \"\"\"\n\n        # make unique anchor prefixes so that multiple tables may exist\n        # on the same page without conflict.\n        self._make_prefix()\n\n        # change tabs to spaces before it gets more difficult after we insert\n        # markup\n        fromlines,tolines = self._tab_newline_replace(fromlines,tolines)\n\n        # create diffs iterator which generates side by side from/to data\n        if context:\n            context_lines = numlines\n        else:\n            context_lines = None\n        diffs = _mdiff(fromlines,tolines,context_lines,linejunk=self._linejunk,\n                      charjunk=self._charjunk)\n\n        # set up iterator to wrap lines that exceed desired width\n        if self._wrapcolumn:\n            diffs = self._line_wrapper(diffs)\n\n        # collect up from/to lines and flags into lists (also format the lines)\n        fromlist,tolist,flaglist = self._collect_lines(diffs)\n\n        # process change flags, generating middle column of next anchors/links\n        fromlist,tolist,flaglist,next_href,next_id = self._convert_flags(\n            fromlist,tolist,flaglist,context,numlines)\n\n        s = []\n        fmt = '            <tr><td class=\"diff_next\"%s>%s</td>%s' + \\\n              '<td class=\"diff_next\">%s</td>%s</tr>\\n'\n        for i in range(len(flaglist)):\n            if flaglist[i] is None:\n                # mdiff yields None on separator lines skip the bogus ones\n                # generated for the first line\n                if i > 0:\n                    s.append('        </tbody>        \\n        <tbody>\\n')\n            else:\n                s.append( fmt % (next_id[i],next_href[i],fromlist[i],\n                                           next_href[i],tolist[i]))\n        if fromdesc or todesc:\n            header_row = '<thead><tr>%s%s%s%s</tr></thead>' % (\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % fromdesc,\n                '<th class=\"diff_next\"><br /></th>',\n                '<th colspan=\"2\" class=\"diff_header\">%s</th>' % todesc)\n        else:\n            header_row = ''\n\n        table = self._table_template % dict(\n            data_rows=''.join(s),\n            header_row=header_row,\n            prefix=self._prefix[1])\n\n        return table.replace('\\0+','<span class=\"diff_add\">'). \\\n                     replace('\\0-','<span class=\"diff_sub\">'). \\\n                     replace('\\0^','<span class=\"diff_chg\">'). \\\n                     replace('\\1','</span>'). \\\n                     replace('\\t','&nbsp;')\n\ndel re\n\ndef restore(delta, which):\n    r\"\"\"\n    Generate one of the two sequences that generated a delta.\n\n    Given a `delta` produced by `Differ.compare()` or `ndiff()`, extract\n    lines originating from file 1 or 2 (parameter `which`), stripping off line\n    prefixes.\n\n    Examples:\n\n    >>> diff = ndiff('one\\ntwo\\nthree\\n'.splitlines(1),\n    ...              'ore\\ntree\\nemu\\n'.splitlines(1))\n    >>> diff = list(diff)\n    >>> print ''.join(restore(diff, 1)),\n    one\n    two\n    three\n    >>> print ''.join(restore(diff, 2)),\n    ore\n    tree\n    emu\n    \"\"\"\n    try:\n        tag = {1: \"- \", 2: \"+ \"}[int(which)]\n    except KeyError:\n        raise ValueError, ('unknown delta choice (must be 1 or 2): %r'\n                           % which)\n    prefixes = (\"  \", tag)\n    for line in delta:\n        if line[:2] in prefixes:\n            yield line[2:]\n\ndef _test():\n    import doctest, difflib\n    return doctest.testmod(difflib)\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "dis": "\"\"\"Disassembler of Python byte code into mnemonics.\"\"\"\n\nimport sys\nimport types\n\nfrom opcode import *\nfrom opcode import __all__ as _opcodes_all\n\n__all__ = [\"dis\", \"disassemble\", \"distb\", \"disco\",\n           \"findlinestarts\", \"findlabels\"] + _opcodes_all\ndel _opcodes_all\n\n_have_code = (types.MethodType, types.FunctionType, types.CodeType,\n              types.ClassType, type)\n\ndef dis(x=None):\n    \"\"\"Disassemble classes, methods, functions, or code.\n\n    With no argument, disassemble the last traceback.\n\n    \"\"\"\n    if x is None:\n        distb()\n        return\n    if isinstance(x, types.InstanceType):\n        x = x.__class__\n    if hasattr(x, 'im_func'):\n        x = x.im_func\n    if hasattr(x, 'func_code'):\n        x = x.func_code\n    if hasattr(x, '__dict__'):\n        items = x.__dict__.items()\n        items.sort()\n        for name, x1 in items:\n            if isinstance(x1, _have_code):\n                print \"Disassembly of %s:\" % name\n                try:\n                    dis(x1)\n                except TypeError, msg:\n                    print \"Sorry:\", msg\n                print\n    elif hasattr(x, 'co_code'):\n        disassemble(x)\n    elif isinstance(x, str):\n        disassemble_string(x)\n    else:\n        raise TypeError, \\\n              \"don't know how to disassemble %s objects\" % \\\n              type(x).__name__\n\ndef distb(tb=None):\n    \"\"\"Disassemble a traceback (default: last traceback).\"\"\"\n    if tb is None:\n        try:\n            tb = sys.last_traceback\n        except AttributeError:\n            raise RuntimeError, \"no last traceback to disassemble\"\n        while tb.tb_next: tb = tb.tb_next\n    disassemble(tb.tb_frame.f_code, tb.tb_lasti)\n\ndef disassemble(co, lasti=-1):\n    \"\"\"Disassemble a code object.\"\"\"\n    code = co.co_code\n    labels = findlabels(code)\n    linestarts = dict(findlinestarts(co))\n    n = len(code)\n    i = 0\n    extended_arg = 0\n    free = None\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i in linestarts:\n            if i > 0:\n                print\n            print \"%3d\" % linestarts[i],\n        else:\n            print '   ',\n\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(20),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256 + extended_arg\n            extended_arg = 0\n            i = i+2\n            if op == EXTENDED_ARG:\n                extended_arg = oparg*65536L\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                print '(' + repr(co.co_consts[oparg]) + ')',\n            elif op in hasname:\n                print '(' + co.co_names[oparg] + ')',\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                print '(' + co.co_varnames[oparg] + ')',\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n            elif op in hasfree:\n                if free is None:\n                    free = co.co_cellvars + co.co_freevars\n                print '(' + free[oparg] + ')',\n        print\n\ndef disassemble_string(code, lasti=-1, varnames=None, names=None,\n                       constants=None):\n    labels = findlabels(code)\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        if i == lasti: print '-->',\n        else: print '   ',\n        if i in labels: print '>>',\n        else: print '  ',\n        print repr(i).rjust(4),\n        print opname[op].ljust(15),\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            print repr(oparg).rjust(5),\n            if op in hasconst:\n                if constants:\n                    print '(' + repr(constants[oparg]) + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasname:\n                if names is not None:\n                    print '(' + names[oparg] + ')',\n                else:\n                    print '(%d)'%oparg,\n            elif op in hasjrel:\n                print '(to ' + repr(i + oparg) + ')',\n            elif op in haslocal:\n                if varnames:\n                    print '(' + varnames[oparg] + ')',\n                else:\n                    print '(%d)' % oparg,\n            elif op in hascompare:\n                print '(' + cmp_op[oparg] + ')',\n        print\n\ndisco = disassemble                     # XXX For backwards compatibility\n\ndef findlabels(code):\n    \"\"\"Detect all offsets in a byte code which are jump targets.\n\n    Return the list of offsets.\n\n    \"\"\"\n    labels = []\n    n = len(code)\n    i = 0\n    while i < n:\n        c = code[i]\n        op = ord(c)\n        i = i+1\n        if op >= HAVE_ARGUMENT:\n            oparg = ord(code[i]) + ord(code[i+1])*256\n            i = i+2\n            label = -1\n            if op in hasjrel:\n                label = i+oparg\n            elif op in hasjabs:\n                label = oparg\n            if label >= 0:\n                if label not in labels:\n                    labels.append(label)\n    return labels\n\ndef findlinestarts(code):\n    \"\"\"Find the offsets in a byte code which are start of lines in the source.\n\n    Generate pairs (offset, lineno) as described in Python/compile.c.\n\n    \"\"\"\n    byte_increments = [ord(c) for c in code.co_lnotab[0::2]]\n    line_increments = [ord(c) for c in code.co_lnotab[1::2]]\n\n    lastlineno = None\n    lineno = code.co_firstlineno\n    addr = 0\n    for byte_incr, line_incr in zip(byte_increments, line_increments):\n        if byte_incr:\n            if lineno != lastlineno:\n                yield (addr, lineno)\n                lastlineno = lineno\n            addr += byte_incr\n        lineno += line_incr\n    if lineno != lastlineno:\n        yield (addr, lineno)\n\ndef _test():\n    \"\"\"Simple test program to disassemble a file.\"\"\"\n    if sys.argv[1:]:\n        if sys.argv[2:]:\n            sys.stderr.write(\"usage: python dis.py [-|file]\\n\")\n            sys.exit(2)\n        fn = sys.argv[1]\n        if not fn or fn == \"-\":\n            fn = None\n    else:\n        fn = None\n    if fn is None:\n        f = sys.stdin\n    else:\n        f = open(fn)\n    source = f.read()\n    if fn is not None:\n        f.close()\n    else:\n        fn = \"<stdin>\"\n    code = compile(source, fn, \"exec\")\n    dis(code)\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "distutils.__init__": "\"\"\"distutils\n\nThe main package for the Python Module Distribution Utilities.  Normally\nused from a setup script as\n\n   from distutils.core import setup\n\n   setup (...)\n\"\"\"\n\n__revision__ = \"$Id$\"\n\n# Distutils version\n#\n# Updated automatically by the Python release process.\n#\n#--start constants--\n__version__ = \"2.7.9\"\n#--end constants--\n", 
    "distutils.debug": "import os\n\n__revision__ = \"$Id$\"\n\n# If DISTUTILS_DEBUG is anything other than the empty string, we run in\n# debug mode.\nDEBUG = os.environ.get('DISTUTILS_DEBUG')\n", 
    "distutils.errors": "\"\"\"distutils.errors\n\nProvides exceptions used by the Distutils modules.  Note that Distutils\nmodules may raise standard exceptions; in particular, SystemExit is\nusually raised for errors that are obviously the end-user's fault\n(eg. bad command-line arguments).\n\nThis module is safe to use in \"from ... import *\" mode; it only exports\nsymbols whose names start with \"Distutils\" and end with \"Error\".\"\"\"\n\n__revision__ = \"$Id$\"\n\nclass DistutilsError(Exception):\n    \"\"\"The root of all Distutils evil.\"\"\"\n\nclass DistutilsModuleError(DistutilsError):\n    \"\"\"Unable to load an expected module, or to find an expected class\n    within some module (in particular, command modules and classes).\"\"\"\n\nclass DistutilsClassError(DistutilsError):\n    \"\"\"Some command class (or possibly distribution class, if anyone\n    feels a need to subclass Distribution) is found not to be holding\n    up its end of the bargain, ie. implementing some part of the\n    \"command \"interface.\"\"\"\n\nclass DistutilsGetoptError(DistutilsError):\n    \"\"\"The option table provided to 'fancy_getopt()' is bogus.\"\"\"\n\nclass DistutilsArgError(DistutilsError):\n    \"\"\"Raised by fancy_getopt in response to getopt.error -- ie. an\n    error in the command line usage.\"\"\"\n\nclass DistutilsFileError(DistutilsError):\n    \"\"\"Any problems in the filesystem: expected file not found, etc.\n    Typically this is for problems that we detect before IOError or\n    OSError could be raised.\"\"\"\n\nclass DistutilsOptionError(DistutilsError):\n    \"\"\"Syntactic/semantic errors in command options, such as use of\n    mutually conflicting options, or inconsistent options,\n    badly-spelled values, etc.  No distinction is made between option\n    values originating in the setup script, the command line, config\n    files, or what-have-you -- but if we *know* something originated in\n    the setup script, we'll raise DistutilsSetupError instead.\"\"\"\n\nclass DistutilsSetupError(DistutilsError):\n    \"\"\"For errors that can be definitely blamed on the setup script,\n    such as invalid keyword arguments to 'setup()'.\"\"\"\n\nclass DistutilsPlatformError(DistutilsError):\n    \"\"\"We don't know how to do something on the current platform (but\n    we do know how to do it on some platform) -- eg. trying to compile\n    C files on a platform not supported by a CCompiler subclass.\"\"\"\n\nclass DistutilsExecError(DistutilsError):\n    \"\"\"Any problems executing an external program (such as the C\n    compiler, when compiling C files).\"\"\"\n\nclass DistutilsInternalError(DistutilsError):\n    \"\"\"Internal inconsistencies or impossibilities (obviously, this\n    should never be seen if the code is working!).\"\"\"\n\nclass DistutilsTemplateError(DistutilsError):\n    \"\"\"Syntax error in a file list template.\"\"\"\n\nclass DistutilsByteCompileError(DistutilsError):\n    \"\"\"Byte compile error.\"\"\"\n\n# Exception classes used by the CCompiler implementation classes\nclass CCompilerError(Exception):\n    \"\"\"Some compile/link operation failed.\"\"\"\n\nclass PreprocessError(CCompilerError):\n    \"\"\"Failure to preprocess one or more C/C++ files.\"\"\"\n\nclass CompileError(CCompilerError):\n    \"\"\"Failure to compile one or more C/C++ source files.\"\"\"\n\nclass LibError(CCompilerError):\n    \"\"\"Failure to create a static library from one or more C/C++ object\n    files.\"\"\"\n\nclass LinkError(CCompilerError):\n    \"\"\"Failure to link one or more C/C++ object files into an executable\n    or shared library file.\"\"\"\n\nclass UnknownFileError(CCompilerError):\n    \"\"\"Attempt to process an unknown file type.\"\"\"\n", 
    "distutils.log": "\"\"\"A simple log mechanism styled after PEP 282.\"\"\"\n\n# The class here is styled after PEP 282 so that it could later be\n# replaced with a standard Python logging implementation.\n\nDEBUG = 1\nINFO = 2\nWARN = 3\nERROR = 4\nFATAL = 5\n\nimport sys\n\nclass Log:\n\n    def __init__(self, threshold=WARN):\n        self.threshold = threshold\n\n    def _log(self, level, msg, args):\n        if level not in (DEBUG, INFO, WARN, ERROR, FATAL):\n            raise ValueError('%s wrong log level' % str(level))\n\n        if level >= self.threshold:\n            if args:\n                msg = msg % args\n            if level in (WARN, ERROR, FATAL):\n                stream = sys.stderr\n            else:\n                stream = sys.stdout\n            stream.write('%s\\n' % msg)\n            stream.flush()\n\n    def log(self, level, msg, *args):\n        self._log(level, msg, args)\n\n    def debug(self, msg, *args):\n        self._log(DEBUG, msg, args)\n\n    def info(self, msg, *args):\n        self._log(INFO, msg, args)\n\n    def warn(self, msg, *args):\n        self._log(WARN, msg, args)\n\n    def error(self, msg, *args):\n        self._log(ERROR, msg, args)\n\n    def fatal(self, msg, *args):\n        self._log(FATAL, msg, args)\n\n_global_log = Log()\nlog = _global_log.log\ndebug = _global_log.debug\ninfo = _global_log.info\nwarn = _global_log.warn\nerror = _global_log.error\nfatal = _global_log.fatal\n\ndef set_threshold(level):\n    # return the old threshold for use from tests\n    old = _global_log.threshold\n    _global_log.threshold = level\n    return old\n\ndef set_verbosity(v):\n    if v <= 0:\n        set_threshold(WARN)\n    elif v == 1:\n        set_threshold(INFO)\n    elif v >= 2:\n        set_threshold(DEBUG)\n", 
    "distutils.spawn": "\"\"\"distutils.spawn\n\nProvides the 'spawn()' function, a front-end to various platform-\nspecific functions for launching another program in a sub-process.\nAlso provides the 'find_executable()' to search the path for a given\nexecutable name.\n\"\"\"\n\n__revision__ = \"$Id$\"\n\nimport sys\nimport os\n\nfrom distutils.errors import DistutilsPlatformError, DistutilsExecError\nfrom distutils.debug import DEBUG\nfrom distutils import log\n\ndef spawn(cmd, search_path=1, verbose=0, dry_run=0):\n    \"\"\"Run another program, specified as a command list 'cmd', in a new process.\n\n    'cmd' is just the argument list for the new process, ie.\n    cmd[0] is the program to run and cmd[1:] are the rest of its arguments.\n    There is no way to run a program with a name different from that of its\n    executable.\n\n    If 'search_path' is true (the default), the system's executable\n    search path will be used to find the program; otherwise, cmd[0]\n    must be the exact path to the executable.  If 'dry_run' is true,\n    the command will not actually be run.\n\n    Raise DistutilsExecError if running the program fails in any way; just\n    return on success.\n    \"\"\"\n    # cmd is documented as a list, but just in case some code passes a tuple\n    # in, protect our %-formatting code against horrible death\n    cmd = list(cmd)\n    if os.name == 'posix':\n        _spawn_posix(cmd, search_path, dry_run=dry_run)\n    elif os.name == 'nt':\n        _spawn_nt(cmd, search_path, dry_run=dry_run)\n    elif os.name == 'os2':\n        _spawn_os2(cmd, search_path, dry_run=dry_run)\n    else:\n        raise DistutilsPlatformError, \\\n              \"don't know how to spawn programs on platform '%s'\" % os.name\n\ndef _nt_quote_args(args):\n    \"\"\"Quote command-line arguments for DOS/Windows conventions.\n\n    Just wraps every argument which contains blanks in double quotes, and\n    returns a new argument list.\n    \"\"\"\n    # XXX this doesn't seem very robust to me -- but if the Windows guys\n    # say it'll work, I guess I'll have to accept it.  (What if an arg\n    # contains quotes?  What other magic characters, other than spaces,\n    # have to be escaped?  Is there an escaping mechanism other than\n    # quoting?)\n    for i, arg in enumerate(args):\n        if ' ' in arg:\n            args[i] = '\"%s\"' % arg\n    return args\n\ndef _spawn_nt(cmd, search_path=1, verbose=0, dry_run=0):\n    executable = cmd[0]\n    if search_path:\n        # either we find one or it stays the same\n        executable = find_executable(executable) or executable\n    log.info(' '.join([executable] + cmd[1:]))\n    if not dry_run:\n        # spawn for NT requires a full path to the .exe\n        try:\n            import subprocess\n            rc = subprocess.call(cmd)\n        except OSError, exc:\n            # this seems to happen when the command isn't found\n            if not DEBUG:\n                cmd = executable\n            raise DistutilsExecError, \\\n                  \"command %r failed: %s\" % (cmd, exc[-1])\n        if rc != 0:\n            # and this reflects the command running but failing\n            if not DEBUG:\n                cmd = executable\n            raise DistutilsExecError, \\\n                  \"command %r failed with exit status %d\" % (cmd, rc)\n\ndef _spawn_os2(cmd, search_path=1, verbose=0, dry_run=0):\n    executable = cmd[0]\n    if search_path:\n        # either we find one or it stays the same\n        executable = find_executable(executable) or executable\n    log.info(' '.join([executable] + cmd[1:]))\n    if not dry_run:\n        # spawnv for OS/2 EMX requires a full path to the .exe\n        try:\n            rc = os.spawnv(os.P_WAIT, executable, cmd)\n        except OSError, exc:\n            # this seems to happen when the command isn't found\n            if not DEBUG:\n                cmd = executable\n            raise DistutilsExecError, \\\n                  \"command %r failed: %s\" % (cmd, exc[-1])\n        if rc != 0:\n            # and this reflects the command running but failing\n            if not DEBUG:\n                cmd = executable\n            log.debug(\"command %r failed with exit status %d\" % (cmd, rc))\n            raise DistutilsExecError, \\\n                  \"command %r failed with exit status %d\" % (cmd, rc)\n\nif sys.platform == 'darwin':\n    from distutils import sysconfig\n    _cfg_target = None\n    _cfg_target_split = None\n\ndef _spawn_posix(cmd, search_path=1, verbose=0, dry_run=0):\n    log.info(' '.join(cmd))\n    if dry_run:\n        return\n    executable = cmd[0]\n    exec_fn = search_path and os.execvp or os.execv\n    env = None\n    if sys.platform == 'darwin':\n        global _cfg_target, _cfg_target_split\n        if _cfg_target is None:\n            _cfg_target = sysconfig.get_config_var(\n                                  'MACOSX_DEPLOYMENT_TARGET') or ''\n            if _cfg_target:\n                _cfg_target_split = [int(x) for x in _cfg_target.split('.')]\n        if _cfg_target:\n            # ensure that the deployment target of build process is not less\n            # than that used when the interpreter was built. This ensures\n            # extension modules are built with correct compatibility values\n            cur_target = os.environ.get('MACOSX_DEPLOYMENT_TARGET', _cfg_target)\n            if _cfg_target_split > [int(x) for x in cur_target.split('.')]:\n                my_msg = ('$MACOSX_DEPLOYMENT_TARGET mismatch: '\n                          'now \"%s\" but \"%s\" during configure'\n                                % (cur_target, _cfg_target))\n                raise DistutilsPlatformError(my_msg)\n            env = dict(os.environ,\n                       MACOSX_DEPLOYMENT_TARGET=cur_target)\n            exec_fn = search_path and os.execvpe or os.execve\n    pid = os.fork()\n\n    if pid == 0:  # in the child\n        try:\n            if env is None:\n                exec_fn(executable, cmd)\n            else:\n                exec_fn(executable, cmd, env)\n        except OSError, e:\n            if not DEBUG:\n                cmd = executable\n            sys.stderr.write(\"unable to execute %r: %s\\n\" %\n                             (cmd, e.strerror))\n            os._exit(1)\n\n        if not DEBUG:\n            cmd = executable\n        sys.stderr.write(\"unable to execute %r for unknown reasons\" % cmd)\n        os._exit(1)\n    else:   # in the parent\n        # Loop until the child either exits or is terminated by a signal\n        # (ie. keep waiting if it's merely stopped)\n        while 1:\n            try:\n                pid, status = os.waitpid(pid, 0)\n            except OSError, exc:\n                import errno\n                if exc.errno == errno.EINTR:\n                    continue\n                if not DEBUG:\n                    cmd = executable\n                raise DistutilsExecError, \\\n                      \"command %r failed: %s\" % (cmd, exc[-1])\n            if os.WIFSIGNALED(status):\n                if not DEBUG:\n                    cmd = executable\n                raise DistutilsExecError, \\\n                      \"command %r terminated by signal %d\" % \\\n                      (cmd, os.WTERMSIG(status))\n\n            elif os.WIFEXITED(status):\n                exit_status = os.WEXITSTATUS(status)\n                if exit_status == 0:\n                    return   # hey, it succeeded!\n                else:\n                    if not DEBUG:\n                        cmd = executable\n                    raise DistutilsExecError, \\\n                          \"command %r failed with exit status %d\" % \\\n                          (cmd, exit_status)\n\n            elif os.WIFSTOPPED(status):\n                continue\n\n            else:\n                if not DEBUG:\n                    cmd = executable\n                raise DistutilsExecError, \\\n                      \"unknown error executing %r: termination status %d\" % \\\n                      (cmd, status)\n\ndef find_executable(executable, path=None):\n    \"\"\"Tries to find 'executable' in the directories listed in 'path'.\n\n    A string listing directories separated by 'os.pathsep'; defaults to\n    os.environ['PATH'].  Returns the complete filename or None if not found.\n    \"\"\"\n    if path is None:\n        path = os.environ['PATH']\n    paths = path.split(os.pathsep)\n    base, ext = os.path.splitext(executable)\n\n    if (sys.platform == 'win32' or os.name == 'os2') and (ext != '.exe'):\n        executable = executable + '.exe'\n\n    if not os.path.isfile(executable):\n        for p in paths:\n            f = os.path.join(p, executable)\n            if os.path.isfile(f):\n                # the file exists, we have a shot at spawn working\n                return f\n        return None\n    else:\n        return executable\n", 
    "distutils.sysconfig": "# The content of this file is redirected from\n# sysconfig_cpython or sysconfig_pypy.\n# All underscore names are imported too, because\n# people like to use undocumented sysconfig._xxx\n# directly.\n\nimport sys\nif '__pypy__' in sys.builtin_module_names:\n    from distutils import sysconfig_pypy as _sysconfig_module\nelse:\n    from distutils import sysconfig_cpython as _sysconfig_module\nglobals().update(_sysconfig_module.__dict__)\n", 
    "distutils.sysconfig_cpython": "\"\"\"Provide access to Python's configuration information.  The specific\nconfiguration variables available depend heavily on the platform and\nconfiguration.  The values may be retrieved using\nget_config_var(name), and the list of variables is available via\nget_config_vars().keys().  Additional convenience functions are also\navailable.\n\nWritten by:   Fred L. Drake, Jr.\nEmail:        <fdrake@acm.org>\n\"\"\"\n\n__revision__ = \"$Id: sysconfig.py 85358 2010-10-10 09:54:59Z antoine.pitrou $\"\n\nimport os\nimport re\nimport string\nimport sys\n\nfrom distutils.errors import DistutilsPlatformError\n\n# These are needed in a couple of spots, so just compute them once.\nPREFIX = os.path.normpath(sys.prefix)\nEXEC_PREFIX = os.path.normpath(sys.exec_prefix)\n\n# Path to the base directory of the project. On Windows the binary may\n# live in project/PCBuild9.  If we're dealing with an x64 Windows build,\n# it'll live in project/PCbuild/amd64.\nproject_base = os.path.dirname(os.path.abspath(sys.executable))\nif os.name == \"nt\" and \"pcbuild\" in project_base[-8:].lower():\n    project_base = os.path.abspath(os.path.join(project_base, os.path.pardir))\n# PC/VS7.1\nif os.name == \"nt\" and \"\\\\pc\\\\v\" in project_base[-10:].lower():\n    project_base = os.path.abspath(os.path.join(project_base, os.path.pardir,\n                                                os.path.pardir))\n# PC/AMD64\nif os.name == \"nt\" and \"\\\\pcbuild\\\\amd64\" in project_base[-14:].lower():\n    project_base = os.path.abspath(os.path.join(project_base, os.path.pardir,\n                                                os.path.pardir))\n\n# set for cross builds\nif \"_PYTHON_PROJECT_BASE\" in os.environ:\n    # this is the build directory, at least for posix\n    project_base = os.path.normpath(os.environ[\"_PYTHON_PROJECT_BASE\"])\n\n# python_build: (Boolean) if true, we're either building Python or\n# building an extension with an un-installed Python, so we use\n# different (hard-wired) directories.\n# Setup.local is available for Makefile builds including VPATH builds,\n# Setup.dist is available on Windows\ndef _python_build():\n    for fn in (\"Setup.dist\", \"Setup.local\"):\n        if os.path.isfile(os.path.join(project_base, \"Modules\", fn)):\n            return True\n    return False\npython_build = _python_build()\n\n\ndef get_python_version():\n    \"\"\"Return a string containing the major and minor Python version,\n    leaving off the patchlevel.  Sample return values could be '1.5'\n    or '2.2'.\n    \"\"\"\n    return sys.version[:3]\n\n\ndef get_python_inc(plat_specific=0, prefix=None):\n    \"\"\"Return the directory containing installed Python header files.\n\n    If 'plat_specific' is false (the default), this is the path to the\n    non-platform-specific header files, i.e. Python.h and so on;\n    otherwise, this is the path to platform-specific header files\n    (namely pyconfig.h).\n\n    If 'prefix' is supplied, use it instead of sys.prefix or\n    sys.exec_prefix -- i.e., ignore 'plat_specific'.\n    \"\"\"\n    if prefix is None:\n        prefix = plat_specific and EXEC_PREFIX or PREFIX\n\n    if os.name == \"posix\":\n        if python_build:\n            buildir = os.path.dirname(sys.executable)\n            if plat_specific:\n                # python.h is located in the buildir\n                inc_dir = buildir\n            else:\n                # the source dir is relative to the buildir\n                srcdir = os.path.abspath(os.path.join(buildir,\n                                         get_config_var('srcdir')))\n                # Include is located in the srcdir\n                inc_dir = os.path.join(srcdir, \"Include\")\n            return inc_dir\n        return os.path.join(prefix, \"include\", \"python\" + get_python_version())\n    elif os.name == \"nt\":\n        return os.path.join(prefix, \"include\")\n    elif os.name == \"os2\":\n        return os.path.join(prefix, \"Include\")\n    else:\n        raise DistutilsPlatformError(\n            \"I don't know where Python installs its C header files \"\n            \"on platform '%s'\" % os.name)\n\n\ndef get_python_lib(plat_specific=0, standard_lib=0, prefix=None):\n    \"\"\"Return the directory containing the Python library (standard or\n    site additions).\n\n    If 'plat_specific' is true, return the directory containing\n    platform-specific modules, i.e. any module from a non-pure-Python\n    module distribution; otherwise, return the platform-shared library\n    directory.  If 'standard_lib' is true, return the directory\n    containing standard Python library modules; otherwise, return the\n    directory for site-specific modules.\n\n    If 'prefix' is supplied, use it instead of sys.prefix or\n    sys.exec_prefix -- i.e., ignore 'plat_specific'.\n    \"\"\"\n    if prefix is None:\n        prefix = plat_specific and EXEC_PREFIX or PREFIX\n\n    if os.name == \"posix\":\n        libpython = os.path.join(prefix,\n                                 \"lib\", \"python\" + get_python_version())\n        if standard_lib:\n            return libpython\n        else:\n            return os.path.join(libpython, \"site-packages\")\n\n    elif os.name == \"nt\":\n        if standard_lib:\n            return os.path.join(prefix, \"Lib\")\n        else:\n            if get_python_version() < \"2.2\":\n                return prefix\n            else:\n                return os.path.join(prefix, \"Lib\", \"site-packages\")\n\n    elif os.name == \"os2\":\n        if standard_lib:\n            return os.path.join(prefix, \"Lib\")\n        else:\n            return os.path.join(prefix, \"Lib\", \"site-packages\")\n\n    else:\n        raise DistutilsPlatformError(\n            \"I don't know where Python installs its library \"\n            \"on platform '%s'\" % os.name)\n\n\n\ndef customize_compiler(compiler):\n    \"\"\"Do any platform-specific customization of a CCompiler instance.\n\n    Mainly needed on Unix, so we can plug in the information that\n    varies across Unices and is stored in Python's Makefile.\n    \"\"\"\n    if compiler.compiler_type == \"unix\":\n        if sys.platform == \"darwin\":\n            # Perform first-time customization of compiler-related\n            # config vars on OS X now that we know we need a compiler.\n            # This is primarily to support Pythons from binary\n            # installers.  The kind and paths to build tools on\n            # the user system may vary significantly from the system\n            # that Python itself was built on.  Also the user OS\n            # version and build tools may not support the same set\n            # of CPU architectures for universal builds.\n            global _config_vars\n            # Use get_config_var() to ensure _config_vars is initialized.\n            if not get_config_var('CUSTOMIZED_OSX_COMPILER'):\n                import _osx_support\n                _osx_support.customize_compiler(_config_vars)\n                _config_vars['CUSTOMIZED_OSX_COMPILER'] = 'True'\n\n        (cc, cxx, opt, cflags, ccshared, ldshared, so_ext, ar, ar_flags) = \\\n            get_config_vars('CC', 'CXX', 'OPT', 'CFLAGS',\n                            'CCSHARED', 'LDSHARED', 'SO', 'AR',\n                            'ARFLAGS')\n\n        if 'CC' in os.environ:\n            newcc = os.environ['CC']\n            if (sys.platform == 'darwin'\n                    and 'LDSHARED' not in os.environ\n                    and ldshared.startswith(cc)):\n                # On OS X, if CC is overridden, use that as the default\n                #       command for LDSHARED as well\n                ldshared = newcc + ldshared[len(cc):]\n            cc = newcc\n        if 'CXX' in os.environ:\n            cxx = os.environ['CXX']\n        if 'LDSHARED' in os.environ:\n            ldshared = os.environ['LDSHARED']\n        if 'CPP' in os.environ:\n            cpp = os.environ['CPP']\n        else:\n            cpp = cc + \" -E\"           # not always\n        if 'LDFLAGS' in os.environ:\n            ldshared = ldshared + ' ' + os.environ['LDFLAGS']\n        if 'CFLAGS' in os.environ:\n            cflags = opt + ' ' + os.environ['CFLAGS']\n            ldshared = ldshared + ' ' + os.environ['CFLAGS']\n        if 'CPPFLAGS' in os.environ:\n            cpp = cpp + ' ' + os.environ['CPPFLAGS']\n            cflags = cflags + ' ' + os.environ['CPPFLAGS']\n            ldshared = ldshared + ' ' + os.environ['CPPFLAGS']\n        if 'AR' in os.environ:\n            ar = os.environ['AR']\n        if 'ARFLAGS' in os.environ:\n            archiver = ar + ' ' + os.environ['ARFLAGS']\n        else:\n            archiver = ar + ' ' + ar_flags\n\n        cc_cmd = cc + ' ' + cflags\n        compiler.set_executables(\n            preprocessor=cpp,\n            compiler=cc_cmd,\n            compiler_so=cc_cmd + ' ' + ccshared,\n            compiler_cxx=cxx,\n            linker_so=ldshared,\n            linker_exe=cc,\n            archiver=archiver)\n\n        compiler.shared_lib_extension = so_ext\n\n\ndef get_config_h_filename():\n    \"\"\"Return full pathname of installed pyconfig.h file.\"\"\"\n    if python_build:\n        if os.name == \"nt\":\n            inc_dir = os.path.join(project_base, \"PC\")\n        else:\n            inc_dir = project_base\n    else:\n        inc_dir = get_python_inc(plat_specific=1)\n    if get_python_version() < '2.2':\n        config_h = 'config.h'\n    else:\n        # The name of the config.h file changed in 2.2\n        config_h = 'pyconfig.h'\n    return os.path.join(inc_dir, config_h)\n\n\ndef get_makefile_filename():\n    \"\"\"Return full pathname of installed Makefile from the Python build.\"\"\"\n    if python_build:\n        return os.path.join(project_base, \"Makefile\")\n    lib_dir = get_python_lib(plat_specific=1, standard_lib=1)\n    return os.path.join(lib_dir, \"config\", \"Makefile\")\n\n\ndef parse_config_h(fp, g=None):\n    \"\"\"Parse a config.h-style file.\n\n    A dictionary containing name/value pairs is returned.  If an\n    optional dictionary is passed in as the second argument, it is\n    used instead of a new dictionary.\n    \"\"\"\n    if g is None:\n        g = {}\n    define_rx = re.compile(\"#define ([A-Z][A-Za-z0-9_]+) (.*)\\n\")\n    undef_rx = re.compile(\"/[*] #undef ([A-Z][A-Za-z0-9_]+) [*]/\\n\")\n    #\n    while 1:\n        line = fp.readline()\n        if not line:\n            break\n        m = define_rx.match(line)\n        if m:\n            n, v = m.group(1, 2)\n            try: v = int(v)\n            except ValueError: pass\n            g[n] = v\n        else:\n            m = undef_rx.match(line)\n            if m:\n                g[m.group(1)] = 0\n    return g\n\n\n# Regexes needed for parsing Makefile (and similar syntaxes,\n# like old-style Setup files).\n_variable_rx = re.compile(\"([a-zA-Z][a-zA-Z0-9_]+)\\s*=\\s*(.*)\")\n_findvar1_rx = re.compile(r\"\\$\\(([A-Za-z][A-Za-z0-9_]*)\\)\")\n_findvar2_rx = re.compile(r\"\\${([A-Za-z][A-Za-z0-9_]*)}\")\n\ndef parse_makefile(fn, g=None):\n    \"\"\"Parse a Makefile-style file.\n\n    A dictionary containing name/value pairs is returned.  If an\n    optional dictionary is passed in as the second argument, it is\n    used instead of a new dictionary.\n    \"\"\"\n    from distutils.text_file import TextFile\n    fp = TextFile(fn, strip_comments=1, skip_blanks=1, join_lines=1)\n\n    if g is None:\n        g = {}\n    done = {}\n    notdone = {}\n\n    while 1:\n        line = fp.readline()\n        if line is None:  # eof\n            break\n        m = _variable_rx.match(line)\n        if m:\n            n, v = m.group(1, 2)\n            v = v.strip()\n            # `$$' is a literal `$' in make\n            tmpv = v.replace('$$', '')\n\n            if \"$\" in tmpv:\n                notdone[n] = v\n            else:\n                try:\n                    v = int(v)\n                except ValueError:\n                    # insert literal `$'\n                    done[n] = v.replace('$$', '$')\n                else:\n                    done[n] = v\n\n    # do variable interpolation here\n    while notdone:\n        for name in notdone.keys():\n            value = notdone[name]\n            m = _findvar1_rx.search(value) or _findvar2_rx.search(value)\n            if m:\n                n = m.group(1)\n                found = True\n                if n in done:\n                    item = str(done[n])\n                elif n in notdone:\n                    # get it on a subsequent round\n                    found = False\n                elif n in os.environ:\n                    # do it like make: fall back to environment\n                    item = os.environ[n]\n                else:\n                    done[n] = item = \"\"\n                if found:\n                    after = value[m.end():]\n                    value = value[:m.start()] + item + after\n                    if \"$\" in after:\n                        notdone[name] = value\n                    else:\n                        try: value = int(value)\n                        except ValueError:\n                            done[name] = value.strip()\n                        else:\n                            done[name] = value\n                        del notdone[name]\n            else:\n                # bogus variable reference; just drop it since we can't deal\n                del notdone[name]\n\n    fp.close()\n\n    # strip spurious spaces\n    for k, v in done.items():\n        if isinstance(v, str):\n            done[k] = v.strip()\n\n    # save the results in the global dictionary\n    g.update(done)\n    return g\n\n\ndef expand_makefile_vars(s, vars):\n    \"\"\"Expand Makefile-style variables -- \"${foo}\" or \"$(foo)\" -- in\n    'string' according to 'vars' (a dictionary mapping variable names to\n    values).  Variables not present in 'vars' are silently expanded to the\n    empty string.  The variable values in 'vars' should not contain further\n    variable expansions; if 'vars' is the output of 'parse_makefile()',\n    you're fine.  Returns a variable-expanded version of 's'.\n    \"\"\"\n\n    # This algorithm does multiple expansion, so if vars['foo'] contains\n    # \"${bar}\", it will expand ${foo} to ${bar}, and then expand\n    # ${bar}... and so forth.  This is fine as long as 'vars' comes from\n    # 'parse_makefile()', which takes care of such expansions eagerly,\n    # according to make's variable expansion semantics.\n\n    while 1:\n        m = _findvar1_rx.search(s) or _findvar2_rx.search(s)\n        if m:\n            (beg, end) = m.span()\n            s = s[0:beg] + vars.get(m.group(1)) + s[end:]\n        else:\n            break\n    return s\n\n\n_config_vars = None\n\ndef _init_posix():\n    \"\"\"Initialize the module as appropriate for POSIX systems.\"\"\"\n    # _sysconfigdata is generated at build time, see the sysconfig module\n    from _sysconfigdata import build_time_vars\n    global _config_vars\n    _config_vars = {}\n    _config_vars.update(build_time_vars)\n\n\ndef _init_nt():\n    \"\"\"Initialize the module as appropriate for NT\"\"\"\n    g = {}\n    # set basic install directories\n    g['LIBDEST'] = get_python_lib(plat_specific=0, standard_lib=1)\n    g['BINLIBDEST'] = get_python_lib(plat_specific=1, standard_lib=1)\n\n    # XXX hmmm.. a normal install puts include files here\n    g['INCLUDEPY'] = get_python_inc(plat_specific=0)\n\n    g['SO'] = '.pyd'\n    g['EXE'] = \".exe\"\n    g['VERSION'] = get_python_version().replace(\".\", \"\")\n    g['BINDIR'] = os.path.dirname(os.path.abspath(sys.executable))\n\n    global _config_vars\n    _config_vars = g\n\n\ndef _init_os2():\n    \"\"\"Initialize the module as appropriate for OS/2\"\"\"\n    g = {}\n    # set basic install directories\n    g['LIBDEST'] = get_python_lib(plat_specific=0, standard_lib=1)\n    g['BINLIBDEST'] = get_python_lib(plat_specific=1, standard_lib=1)\n\n    # XXX hmmm.. a normal install puts include files here\n    g['INCLUDEPY'] = get_python_inc(plat_specific=0)\n\n    g['SO'] = '.pyd'\n    g['EXE'] = \".exe\"\n\n    global _config_vars\n    _config_vars = g\n\n\ndef get_config_vars(*args):\n    \"\"\"With no arguments, return a dictionary of all configuration\n    variables relevant for the current platform.  Generally this includes\n    everything needed to build extensions and install both pure modules and\n    extensions.  On Unix, this means every variable defined in Python's\n    installed Makefile; on Windows and Mac OS it's a much smaller set.\n\n    With arguments, return a list of values that result from looking up\n    each argument in the configuration variable dictionary.\n    \"\"\"\n    global _config_vars\n    if _config_vars is None:\n        func = globals().get(\"_init_\" + os.name)\n        if func:\n            func()\n        else:\n            _config_vars = {}\n\n        # Normalized versions of prefix and exec_prefix are handy to have;\n        # in fact, these are the standard versions used most places in the\n        # Distutils.\n        _config_vars['prefix'] = PREFIX\n        _config_vars['exec_prefix'] = EXEC_PREFIX\n\n        # OS X platforms require special customization to handle\n        # multi-architecture, multi-os-version installers\n        if sys.platform == 'darwin':\n            import _osx_support\n            _osx_support.customize_config_vars(_config_vars)\n\n    if args:\n        vals = []\n        for name in args:\n            vals.append(_config_vars.get(name))\n        return vals\n    else:\n        return _config_vars\n\ndef get_config_var(name):\n    \"\"\"Return the value of a single variable using the dictionary\n    returned by 'get_config_vars()'.  Equivalent to\n    get_config_vars().get(name)\n    \"\"\"\n    return get_config_vars().get(name)\n", 
    "distutils.sysconfig_pypy": "\"\"\"Provide access to Python's configuration information.\nThis is actually PyPy's minimal configuration information.\n\nThe specific configuration variables available depend heavily on the\nplatform and configuration.  The values may be retrieved using\nget_config_var(name), and the list of variables is available via\nget_config_vars().keys().  Additional convenience functions are also\navailable.\n\"\"\"\n\n__revision__ = \"$Id: sysconfig.py 85358 2010-10-10 09:54:59Z antoine.pitrou $\"\n\nimport sys\nimport os\nimport shlex\n\nfrom distutils.errors import DistutilsPlatformError\n\n\nPREFIX = os.path.normpath(sys.prefix)\nEXEC_PREFIX = os.path.normpath(sys.exec_prefix)\nproject_base = os.path.dirname(os.path.abspath(sys.executable))\npython_build = False\n\n\ndef get_python_inc(plat_specific=0, prefix=None):\n    from os.path import join as j\n    return j(sys.prefix, 'include')\n\ndef get_python_version():\n    \"\"\"Return a string containing the major and minor Python version,\n    leaving off the patchlevel.  Sample return values could be '1.5'\n    or '2.2'.\n    \"\"\"\n    return sys.version[:3]\n\n\ndef get_python_lib(plat_specific=0, standard_lib=0, prefix=None):\n    \"\"\"Return the directory containing the Python library (standard or\n    site additions).\n\n    If 'plat_specific' is true, return the directory containing\n    platform-specific modules, i.e. any module from a non-pure-Python\n    module distribution; otherwise, return the platform-shared library\n    directory.  If 'standard_lib' is true, return the directory\n    containing standard Python library modules; otherwise, return the\n    directory for site-specific modules.\n\n    If 'prefix' is supplied, use it instead of sys.prefix or\n    sys.exec_prefix -- i.e., ignore 'plat_specific'.\n    \"\"\"\n    if prefix is None:\n        prefix = PREFIX\n    if standard_lib:\n        return os.path.join(prefix, \"lib-python\", get_python_version())\n    return os.path.join(prefix, 'site-packages')\n\n\n_config_vars = None\n\ndef _init_posix():\n    \"\"\"Initialize the module as appropriate for POSIX systems.\"\"\"\n    g = {}\n    g['EXE'] = \"\"\n    g['SO'] = \".so\"\n    g['SOABI'] = g['SO'].rsplit('.')[0]\n    g['LIBDIR'] = os.path.join(sys.prefix, 'lib')\n    g['CC'] = \"gcc -pthread\" # -pthread might not be valid on OS/X, check\n\n    global _config_vars\n    _config_vars = g\n\n\ndef _init_nt():\n    \"\"\"Initialize the module as appropriate for NT\"\"\"\n    g = {}\n    g['EXE'] = \".exe\"\n    g['SO'] = \".pyd\"\n    g['SOABI'] = g['SO'].rsplit('.')[0]\n\n    global _config_vars\n    _config_vars = g\n\n\ndef get_config_vars(*args):\n    \"\"\"With no arguments, return a dictionary of all configuration\n    variables relevant for the current platform.  Generally this includes\n    everything needed to build extensions and install both pure modules and\n    extensions.  On Unix, this means every variable defined in Python's\n    installed Makefile; on Windows and Mac OS it's a much smaller set.\n\n    With arguments, return a list of values that result from looking up\n    each argument in the configuration variable dictionary.\n    \"\"\"\n    global _config_vars\n    if _config_vars is None:\n        func = globals().get(\"_init_\" + os.name)\n        if func:\n            func()\n        else:\n            _config_vars = {}\n\n        _config_vars['prefix'] = PREFIX\n        _config_vars['exec_prefix'] = EXEC_PREFIX\n\n    if args:\n        vals = []\n        for name in args:\n            vals.append(_config_vars.get(name))\n        return vals\n    else:\n        return _config_vars\n\ndef get_config_var(name):\n    \"\"\"Return the value of a single variable using the dictionary\n    returned by 'get_config_vars()'.  Equivalent to\n    get_config_vars().get(name)\n    \"\"\"\n    return get_config_vars().get(name)\n\ndef customize_compiler(compiler):\n    \"\"\"Dummy method to let some easy_install packages that have\n    optional C speedup components.\n    \"\"\"\n    if compiler.compiler_type == \"unix\":\n        compiler.compiler_so.extend(['-O2', '-fPIC', '-Wimplicit'])\n        compiler.shared_lib_extension = get_config_var('SO')\n        if \"CPPFLAGS\" in os.environ:\n            cppflags = shlex.split(os.environ[\"CPPFLAGS\"])\n            compiler.compiler.extend(cppflags)\n            compiler.compiler_so.extend(cppflags)\n            compiler.linker_so.extend(cppflags)\n        if \"CFLAGS\" in os.environ:\n            cflags = shlex.split(os.environ[\"CFLAGS\"])\n            compiler.compiler.extend(cflags)\n            compiler.compiler_so.extend(cflags)\n            compiler.linker_so.extend(cflags)\n        if \"LDFLAGS\" in os.environ:\n            ldflags = shlex.split(os.environ[\"LDFLAGS\"])\n            compiler.linker_so.extend(ldflags)\n\n\nfrom sysconfig_cpython import (\n    parse_makefile, _variable_rx, expand_makefile_vars)\n\n", 
    "distutils.text_file": "\"\"\"text_file\n\nprovides the TextFile class, which gives an interface to text files\nthat (optionally) takes care of stripping comments, ignoring blank\nlines, and joining lines with backslashes.\"\"\"\n\n__revision__ = \"$Id$\"\n\nimport sys\n\n\nclass TextFile:\n\n    \"\"\"Provides a file-like object that takes care of all the things you\n       commonly want to do when processing a text file that has some\n       line-by-line syntax: strip comments (as long as \"#\" is your\n       comment character), skip blank lines, join adjacent lines by\n       escaping the newline (ie. backslash at end of line), strip\n       leading and/or trailing whitespace.  All of these are optional\n       and independently controllable.\n\n       Provides a 'warn()' method so you can generate warning messages that\n       report physical line number, even if the logical line in question\n       spans multiple physical lines.  Also provides 'unreadline()' for\n       implementing line-at-a-time lookahead.\n\n       Constructor is called as:\n\n           TextFile (filename=None, file=None, **options)\n\n       It bombs (RuntimeError) if both 'filename' and 'file' are None;\n       'filename' should be a string, and 'file' a file object (or\n       something that provides 'readline()' and 'close()' methods).  It is\n       recommended that you supply at least 'filename', so that TextFile\n       can include it in warning messages.  If 'file' is not supplied,\n       TextFile creates its own using the 'open()' builtin.\n\n       The options are all boolean, and affect the value returned by\n       'readline()':\n         strip_comments [default: true]\n           strip from \"#\" to end-of-line, as well as any whitespace\n           leading up to the \"#\" -- unless it is escaped by a backslash\n         lstrip_ws [default: false]\n           strip leading whitespace from each line before returning it\n         rstrip_ws [default: true]\n           strip trailing whitespace (including line terminator!) from\n           each line before returning it\n         skip_blanks [default: true}\n           skip lines that are empty *after* stripping comments and\n           whitespace.  (If both lstrip_ws and rstrip_ws are false,\n           then some lines may consist of solely whitespace: these will\n           *not* be skipped, even if 'skip_blanks' is true.)\n         join_lines [default: false]\n           if a backslash is the last non-newline character on a line\n           after stripping comments and whitespace, join the following line\n           to it to form one \"logical line\"; if N consecutive lines end\n           with a backslash, then N+1 physical lines will be joined to\n           form one logical line.\n         collapse_join [default: false]\n           strip leading whitespace from lines that are joined to their\n           predecessor; only matters if (join_lines and not lstrip_ws)\n\n       Note that since 'rstrip_ws' can strip the trailing newline, the\n       semantics of 'readline()' must differ from those of the builtin file\n       object's 'readline()' method!  In particular, 'readline()' returns\n       None for end-of-file: an empty string might just be a blank line (or\n       an all-whitespace line), if 'rstrip_ws' is true but 'skip_blanks' is\n       not.\"\"\"\n\n    default_options = { 'strip_comments': 1,\n                        'skip_blanks':    1,\n                        'lstrip_ws':      0,\n                        'rstrip_ws':      1,\n                        'join_lines':     0,\n                        'collapse_join':  0,\n                      }\n\n    def __init__ (self, filename=None, file=None, **options):\n        \"\"\"Construct a new TextFile object.  At least one of 'filename'\n           (a string) and 'file' (a file-like object) must be supplied.\n           They keyword argument options are described above and affect\n           the values returned by 'readline()'.\"\"\"\n\n        if filename is None and file is None:\n            raise RuntimeError, \\\n                  \"you must supply either or both of 'filename' and 'file'\"\n\n        # set values for all options -- either from client option hash\n        # or fallback to default_options\n        for opt in self.default_options.keys():\n            if opt in options:\n                setattr (self, opt, options[opt])\n\n            else:\n                setattr (self, opt, self.default_options[opt])\n\n        # sanity check client option hash\n        for opt in options.keys():\n            if opt not in self.default_options:\n                raise KeyError, \"invalid TextFile option '%s'\" % opt\n\n        if file is None:\n            self.open (filename)\n        else:\n            self.filename = filename\n            self.file = file\n            self.current_line = 0       # assuming that file is at BOF!\n\n        # 'linebuf' is a stack of lines that will be emptied before we\n        # actually read from the file; it's only populated by an\n        # 'unreadline()' operation\n        self.linebuf = []\n\n\n    def open (self, filename):\n        \"\"\"Open a new file named 'filename'.  This overrides both the\n           'filename' and 'file' arguments to the constructor.\"\"\"\n\n        self.filename = filename\n        self.file = open (self.filename, 'r')\n        self.current_line = 0\n\n\n    def close (self):\n        \"\"\"Close the current file and forget everything we know about it\n           (filename, current line number).\"\"\"\n\n        self.file.close ()\n        self.file = None\n        self.filename = None\n        self.current_line = None\n\n\n    def gen_error (self, msg, line=None):\n        outmsg = []\n        if line is None:\n            line = self.current_line\n        outmsg.append(self.filename + \", \")\n        if isinstance(line, (list, tuple)):\n            outmsg.append(\"lines %d-%d: \" % tuple (line))\n        else:\n            outmsg.append(\"line %d: \" % line)\n        outmsg.append(str(msg))\n        return ''.join(outmsg)\n\n\n    def error (self, msg, line=None):\n        raise ValueError, \"error: \" + self.gen_error(msg, line)\n\n    def warn (self, msg, line=None):\n        \"\"\"Print (to stderr) a warning message tied to the current logical\n           line in the current file.  If the current logical line in the\n           file spans multiple physical lines, the warning refers to the\n           whole range, eg. \"lines 3-5\".  If 'line' supplied, it overrides\n           the current line number; it may be a list or tuple to indicate a\n           range of physical lines, or an integer for a single physical\n           line.\"\"\"\n        sys.stderr.write(\"warning: \" + self.gen_error(msg, line) + \"\\n\")\n\n\n    def readline (self):\n        \"\"\"Read and return a single logical line from the current file (or\n           from an internal buffer if lines have previously been \"unread\"\n           with 'unreadline()').  If the 'join_lines' option is true, this\n           may involve reading multiple physical lines concatenated into a\n           single string.  Updates the current line number, so calling\n           'warn()' after 'readline()' emits a warning about the physical\n           line(s) just read.  Returns None on end-of-file, since the empty\n           string can occur if 'rstrip_ws' is true but 'strip_blanks' is\n           not.\"\"\"\n\n        # If any \"unread\" lines waiting in 'linebuf', return the top\n        # one.  (We don't actually buffer read-ahead data -- lines only\n        # get put in 'linebuf' if the client explicitly does an\n        # 'unreadline()'.\n        if self.linebuf:\n            line = self.linebuf[-1]\n            del self.linebuf[-1]\n            return line\n\n        buildup_line = ''\n\n        while 1:\n            # read the line, make it None if EOF\n            line = self.file.readline()\n            if line == '': line = None\n\n            if self.strip_comments and line:\n\n                # Look for the first \"#\" in the line.  If none, never\n                # mind.  If we find one and it's the first character, or\n                # is not preceded by \"\\\", then it starts a comment --\n                # strip the comment, strip whitespace before it, and\n                # carry on.  Otherwise, it's just an escaped \"#\", so\n                # unescape it (and any other escaped \"#\"'s that might be\n                # lurking in there) and otherwise leave the line alone.\n\n                pos = line.find(\"#\")\n                if pos == -1:           # no \"#\" -- no comments\n                    pass\n\n                # It's definitely a comment -- either \"#\" is the first\n                # character, or it's elsewhere and unescaped.\n                elif pos == 0 or line[pos-1] != \"\\\\\":\n                    # Have to preserve the trailing newline, because it's\n                    # the job of a later step (rstrip_ws) to remove it --\n                    # and if rstrip_ws is false, we'd better preserve it!\n                    # (NB. this means that if the final line is all comment\n                    # and has no trailing newline, we will think that it's\n                    # EOF; I think that's OK.)\n                    eol = (line[-1] == '\\n') and '\\n' or ''\n                    line = line[0:pos] + eol\n\n                    # If all that's left is whitespace, then skip line\n                    # *now*, before we try to join it to 'buildup_line' --\n                    # that way constructs like\n                    #   hello \\\\\n                    #   # comment that should be ignored\n                    #   there\n                    # result in \"hello there\".\n                    if line.strip() == \"\":\n                        continue\n\n                else:                   # it's an escaped \"#\"\n                    line = line.replace(\"\\\\#\", \"#\")\n\n\n            # did previous line end with a backslash? then accumulate\n            if self.join_lines and buildup_line:\n                # oops: end of file\n                if line is None:\n                    self.warn (\"continuation line immediately precedes \"\n                               \"end-of-file\")\n                    return buildup_line\n\n                if self.collapse_join:\n                    line = line.lstrip()\n                line = buildup_line + line\n\n                # careful: pay attention to line number when incrementing it\n                if isinstance(self.current_line, list):\n                    self.current_line[1] = self.current_line[1] + 1\n                else:\n                    self.current_line = [self.current_line,\n                                         self.current_line+1]\n            # just an ordinary line, read it as usual\n            else:\n                if line is None:        # eof\n                    return None\n\n                # still have to be careful about incrementing the line number!\n                if isinstance(self.current_line, list):\n                    self.current_line = self.current_line[1] + 1\n                else:\n                    self.current_line = self.current_line + 1\n\n\n            # strip whitespace however the client wants (leading and\n            # trailing, or one or the other, or neither)\n            if self.lstrip_ws and self.rstrip_ws:\n                line = line.strip()\n            elif self.lstrip_ws:\n                line = line.lstrip()\n            elif self.rstrip_ws:\n                line = line.rstrip()\n\n            # blank line (whether we rstrip'ed or not)? skip to next line\n            # if appropriate\n            if (line == '' or line == '\\n') and self.skip_blanks:\n                continue\n\n            if self.join_lines:\n                if line[-1] == '\\\\':\n                    buildup_line = line[:-1]\n                    continue\n\n                if line[-2:] == '\\\\\\n':\n                    buildup_line = line[0:-2] + '\\n'\n                    continue\n\n            # well, I guess there's some actual content there: return it\n            return line\n\n    # readline ()\n\n\n    def readlines (self):\n        \"\"\"Read and return the list of all logical lines remaining in the\n           current file.\"\"\"\n\n        lines = []\n        while 1:\n            line = self.readline()\n            if line is None:\n                return lines\n            lines.append (line)\n\n\n    def unreadline (self, line):\n        \"\"\"Push 'line' (a string) onto an internal buffer that will be\n           checked by future 'readline()' calls.  Handy for implementing\n           a parser with line-at-a-time lookahead.\"\"\"\n\n        self.linebuf.append (line)\n", 
    "doctest": "# Module doctest.\n# Released to the public domain 16-Jan-2001, by Tim Peters (tim@python.org).\n# Major enhancements and refactoring by:\n#     Jim Fulton\n#     Edward Loper\n\n# Provided as-is; use at your own risk; no warranty; no promises; enjoy!\n\nr\"\"\"Module doctest -- a framework for running examples in docstrings.\n\nIn simplest use, end each module M to be tested with:\n\ndef _test():\n    import doctest\n    doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n\nThen running the module as a script will cause the examples in the\ndocstrings to get executed and verified:\n\npython M.py\n\nThis won't display anything unless an example fails, in which case the\nfailing example(s) and the cause(s) of the failure(s) are printed to stdout\n(why not stderr? because stderr is a lame hack <0.2 wink>), and the final\nline of output is \"Test failed.\".\n\nRun it with the -v switch instead:\n\npython M.py -v\n\nand a detailed report of all examples tried is printed to stdout, along\nwith assorted summaries at the end.\n\nYou can force verbose mode by passing \"verbose=True\" to testmod, or prohibit\nit by passing \"verbose=False\".  In either of those cases, sys.argv is not\nexamined by testmod.\n\nThere are a variety of other ways to run doctests, including integration\nwith the unittest framework, and support for running non-Python text\nfiles containing doctests.  There are also many ways to override parts\nof doctest's default behaviors.  See the Library Reference Manual for\ndetails.\n\"\"\"\n\n__docformat__ = 'reStructuredText en'\n\n__all__ = [\n    # 0, Option Flags\n    'register_optionflag',\n    'DONT_ACCEPT_TRUE_FOR_1',\n    'DONT_ACCEPT_BLANKLINE',\n    'NORMALIZE_WHITESPACE',\n    'ELLIPSIS',\n    'SKIP',\n    'IGNORE_EXCEPTION_DETAIL',\n    'COMPARISON_FLAGS',\n    'REPORT_UDIFF',\n    'REPORT_CDIFF',\n    'REPORT_NDIFF',\n    'REPORT_ONLY_FIRST_FAILURE',\n    'REPORTING_FLAGS',\n    # 1. Utility Functions\n    # 2. Example & DocTest\n    'Example',\n    'DocTest',\n    # 3. Doctest Parser\n    'DocTestParser',\n    # 4. Doctest Finder\n    'DocTestFinder',\n    # 5. Doctest Runner\n    'DocTestRunner',\n    'OutputChecker',\n    'DocTestFailure',\n    'UnexpectedException',\n    'DebugRunner',\n    # 6. Test Functions\n    'testmod',\n    'testfile',\n    'run_docstring_examples',\n    # 7. Tester\n    'Tester',\n    # 8. Unittest Support\n    'DocTestSuite',\n    'DocFileSuite',\n    'set_unittest_reportflags',\n    # 9. Debugging Support\n    'script_from_examples',\n    'testsource',\n    'debug_src',\n    'debug',\n]\n\nimport __future__\n\nimport sys, traceback, inspect, linecache, os, re\nimport unittest, difflib, pdb, tempfile\nimport warnings\nfrom StringIO import StringIO\nfrom collections import namedtuple\n\nTestResults = namedtuple('TestResults', 'failed attempted')\n\n# There are 4 basic classes:\n#  - Example: a <source, want> pair, plus an intra-docstring line number.\n#  - DocTest: a collection of examples, parsed from a docstring, plus\n#    info about where the docstring came from (name, filename, lineno).\n#  - DocTestFinder: extracts DocTests from a given object's docstring and\n#    its contained objects' docstrings.\n#  - DocTestRunner: runs DocTest cases, and accumulates statistics.\n#\n# So the basic picture is:\n#\n#                             list of:\n# +------+                   +---------+                   +-------+\n# |object| --DocTestFinder-> | DocTest | --DocTestRunner-> |results|\n# +------+                   +---------+                   +-------+\n#                            | Example |\n#                            |   ...   |\n#                            | Example |\n#                            +---------+\n\n# Option constants.\n\nOPTIONFLAGS_BY_NAME = {}\ndef register_optionflag(name):\n    # Create a new flag unless `name` is already known.\n    return OPTIONFLAGS_BY_NAME.setdefault(name, 1 << len(OPTIONFLAGS_BY_NAME))\n\nDONT_ACCEPT_TRUE_FOR_1 = register_optionflag('DONT_ACCEPT_TRUE_FOR_1')\nDONT_ACCEPT_BLANKLINE = register_optionflag('DONT_ACCEPT_BLANKLINE')\nNORMALIZE_WHITESPACE = register_optionflag('NORMALIZE_WHITESPACE')\nELLIPSIS = register_optionflag('ELLIPSIS')\nSKIP = register_optionflag('SKIP')\nIGNORE_EXCEPTION_DETAIL = register_optionflag('IGNORE_EXCEPTION_DETAIL')\n\nCOMPARISON_FLAGS = (DONT_ACCEPT_TRUE_FOR_1 |\n                    DONT_ACCEPT_BLANKLINE |\n                    NORMALIZE_WHITESPACE |\n                    ELLIPSIS |\n                    SKIP |\n                    IGNORE_EXCEPTION_DETAIL)\n\nREPORT_UDIFF = register_optionflag('REPORT_UDIFF')\nREPORT_CDIFF = register_optionflag('REPORT_CDIFF')\nREPORT_NDIFF = register_optionflag('REPORT_NDIFF')\nREPORT_ONLY_FIRST_FAILURE = register_optionflag('REPORT_ONLY_FIRST_FAILURE')\n\nREPORTING_FLAGS = (REPORT_UDIFF |\n                   REPORT_CDIFF |\n                   REPORT_NDIFF |\n                   REPORT_ONLY_FIRST_FAILURE)\n\n# Special string markers for use in `want` strings:\nBLANKLINE_MARKER = '<BLANKLINE>'\nELLIPSIS_MARKER = '...'\n\n######################################################################\n## Table of Contents\n######################################################################\n#  1. Utility Functions\n#  2. Example & DocTest -- store test cases\n#  3. DocTest Parser -- extracts examples from strings\n#  4. DocTest Finder -- extracts test cases from objects\n#  5. DocTest Runner -- runs test cases\n#  6. Test Functions -- convenient wrappers for testing\n#  7. Tester Class -- for backwards compatibility\n#  8. Unittest Support\n#  9. Debugging Support\n# 10. Example Usage\n\n######################################################################\n## 1. Utility Functions\n######################################################################\n\ndef _extract_future_flags(globs):\n    \"\"\"\n    Return the compiler-flags associated with the future features that\n    have been imported into the given namespace (globs).\n    \"\"\"\n    flags = 0\n    for fname in __future__.all_feature_names:\n        feature = globs.get(fname, None)\n        if feature is getattr(__future__, fname):\n            flags |= feature.compiler_flag\n    return flags\n\ndef _normalize_module(module, depth=2):\n    \"\"\"\n    Return the module specified by `module`.  In particular:\n      - If `module` is a module, then return module.\n      - If `module` is a string, then import and return the\n        module with that name.\n      - If `module` is None, then return the calling module.\n        The calling module is assumed to be the module of\n        the stack frame at the given depth in the call stack.\n    \"\"\"\n    if inspect.ismodule(module):\n        return module\n    elif isinstance(module, (str, unicode)):\n        return __import__(module, globals(), locals(), [\"*\"])\n    elif module is None:\n        return sys.modules[sys._getframe(depth).f_globals['__name__']]\n    else:\n        raise TypeError(\"Expected a module, string, or None\")\n\ndef _load_testfile(filename, package, module_relative):\n    if module_relative:\n        package = _normalize_module(package, 3)\n        filename = _module_relative_path(package, filename)\n        if hasattr(package, '__loader__'):\n            if hasattr(package.__loader__, 'get_data'):\n                file_contents = package.__loader__.get_data(filename)\n                # get_data() opens files as 'rb', so one must do the equivalent\n                # conversion as universal newlines would do.\n                return file_contents.replace(os.linesep, '\\n'), filename\n    with open(filename, 'U') as f:\n        return f.read(), filename\n\n# Use sys.stdout encoding for ouput.\n_encoding = getattr(sys.__stdout__, 'encoding', None) or 'utf-8'\n\ndef _indent(s, indent=4):\n    \"\"\"\n    Add the given number of space characters to the beginning of\n    every non-blank line in `s`, and return the result.\n    If the string `s` is Unicode, it is encoded using the stdout\n    encoding and the `backslashreplace` error handler.\n    \"\"\"\n    if isinstance(s, unicode):\n        s = s.encode(_encoding, 'backslashreplace')\n    # This regexp matches the start of non-blank lines:\n    return re.sub('(?m)^(?!$)', indent*' ', s)\n\ndef _exception_traceback(exc_info):\n    \"\"\"\n    Return a string containing a traceback message for the given\n    exc_info tuple (as returned by sys.exc_info()).\n    \"\"\"\n    # Get a traceback message.\n    excout = StringIO()\n    exc_type, exc_val, exc_tb = exc_info\n    traceback.print_exception(exc_type, exc_val, exc_tb, file=excout)\n    return excout.getvalue()\n\n# Override some StringIO methods.\nclass _SpoofOut(StringIO):\n    def getvalue(self):\n        result = StringIO.getvalue(self)\n        # If anything at all was written, make sure there's a trailing\n        # newline.  There's no way for the expected output to indicate\n        # that a trailing newline is missing.\n        if result and not result.endswith(\"\\n\"):\n            result += \"\\n\"\n        # Prevent softspace from screwing up the next test case, in\n        # case they used print with a trailing comma in an example.\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        return result\n\n    def truncate(self,   size=None):\n        StringIO.truncate(self, size)\n        if hasattr(self, \"softspace\"):\n            del self.softspace\n        if not self.buf:\n            # Reset it to an empty string, to make sure it's not unicode.\n            self.buf = ''\n\n# Worst-case linear-time ellipsis matching.\ndef _ellipsis_match(want, got):\n    \"\"\"\n    Essentially the only subtle case:\n    >>> _ellipsis_match('aa...aa', 'aaa')\n    False\n    \"\"\"\n    if ELLIPSIS_MARKER not in want:\n        return want == got\n\n    # Find \"the real\" strings.\n    ws = want.split(ELLIPSIS_MARKER)\n    assert len(ws) >= 2\n\n    # Deal with exact matches possibly needed at one or both ends.\n    startpos, endpos = 0, len(got)\n    w = ws[0]\n    if w:   # starts with exact match\n        if got.startswith(w):\n            startpos = len(w)\n            del ws[0]\n        else:\n            return False\n    w = ws[-1]\n    if w:   # ends with exact match\n        if got.endswith(w):\n            endpos -= len(w)\n            del ws[-1]\n        else:\n            return False\n\n    if startpos > endpos:\n        # Exact end matches required more characters than we have, as in\n        # _ellipsis_match('aa...aa', 'aaa')\n        return False\n\n    # For the rest, we only need to find the leftmost non-overlapping\n    # match for each piece.  If there's no overall match that way alone,\n    # there's no overall match period.\n    for w in ws:\n        # w may be '' at times, if there are consecutive ellipses, or\n        # due to an ellipsis at the start or end of `want`.  That's OK.\n        # Search for an empty string succeeds, and doesn't change startpos.\n        startpos = got.find(w, startpos, endpos)\n        if startpos < 0:\n            return False\n        startpos += len(w)\n\n    return True\n\ndef _comment_line(line):\n    \"Return a commented form of the given line\"\n    line = line.rstrip()\n    if line:\n        return '# '+line\n    else:\n        return '#'\n\ndef _strip_exception_details(msg):\n    # Support for IGNORE_EXCEPTION_DETAIL.\n    # Get rid of everything except the exception name; in particular, drop\n    # the possibly dotted module path (if any) and the exception message (if\n    # any).  We assume that a colon is never part of a dotted name, or of an\n    # exception name.\n    # E.g., given\n    #    \"foo.bar.MyError: la di da\"\n    # return \"MyError\"\n    # Or for \"abc.def\" or \"abc.def:\\n\" return \"def\".\n\n    start, end = 0, len(msg)\n    # The exception name must appear on the first line.\n    i = msg.find(\"\\n\")\n    if i >= 0:\n        end = i\n    # retain up to the first colon (if any)\n    i = msg.find(':', 0, end)\n    if i >= 0:\n        end = i\n    # retain just the exception name\n    i = msg.rfind('.', 0, end)\n    if i >= 0:\n        start = i+1\n    return msg[start: end]\n\nclass _OutputRedirectingPdb(pdb.Pdb):\n    \"\"\"\n    A specialized version of the python debugger that redirects stdout\n    to a given stream when interacting with the user.  Stdout is *not*\n    redirected when traced code is executed.\n    \"\"\"\n    def __init__(self, out):\n        self.__out = out\n        self.__debugger_used = False\n        pdb.Pdb.__init__(self, stdout=out)\n        # still use input() to get user input\n        self.use_rawinput = 1\n\n    def set_trace(self, frame=None):\n        self.__debugger_used = True\n        if frame is None:\n            frame = sys._getframe().f_back\n        pdb.Pdb.set_trace(self, frame)\n\n    def set_continue(self):\n        # Calling set_continue unconditionally would break unit test\n        # coverage reporting, as Bdb.set_continue calls sys.settrace(None).\n        if self.__debugger_used:\n            pdb.Pdb.set_continue(self)\n\n    def trace_dispatch(self, *args):\n        # Redirect stdout to the given stream.\n        save_stdout = sys.stdout\n        sys.stdout = self.__out\n        # Call Pdb's trace dispatch method.\n        try:\n            return pdb.Pdb.trace_dispatch(self, *args)\n        finally:\n            sys.stdout = save_stdout\n\n# [XX] Normalize with respect to os.path.pardir?\ndef _module_relative_path(module, path):\n    if not inspect.ismodule(module):\n        raise TypeError, 'Expected a module: %r' % module\n    if path.startswith('/'):\n        raise ValueError, 'Module-relative files may not have absolute paths'\n\n    # Find the base directory for the path.\n    if hasattr(module, '__file__'):\n        # A normal module/package\n        basedir = os.path.split(module.__file__)[0]\n    elif module.__name__ == '__main__':\n        # An interactive session.\n        if len(sys.argv)>0 and sys.argv[0] != '':\n            basedir = os.path.split(sys.argv[0])[0]\n        else:\n            basedir = os.curdir\n    else:\n        # A module w/o __file__ (this includes builtins)\n        raise ValueError(\"Can't resolve paths relative to the module \" +\n                         module + \" (it has no __file__)\")\n\n    # Combine the base directory and the path.\n    return os.path.join(basedir, *(path.split('/')))\n\n######################################################################\n## 2. Example & DocTest\n######################################################################\n## - An \"example\" is a <source, want> pair, where \"source\" is a\n##   fragment of source code, and \"want\" is the expected output for\n##   \"source.\"  The Example class also includes information about\n##   where the example was extracted from.\n##\n## - A \"doctest\" is a collection of examples, typically extracted from\n##   a string (such as an object's docstring).  The DocTest class also\n##   includes information about where the string was extracted from.\n\nclass Example:\n    \"\"\"\n    A single doctest example, consisting of source code and expected\n    output.  `Example` defines the following attributes:\n\n      - source: A single Python statement, always ending with a newline.\n        The constructor adds a newline if needed.\n\n      - want: The expected output from running the source code (either\n        from stdout, or a traceback in case of exception).  `want` ends\n        with a newline unless it's empty, in which case it's an empty\n        string.  The constructor adds a newline if needed.\n\n      - exc_msg: The exception message generated by the example, if\n        the example is expected to generate an exception; or `None` if\n        it is not expected to generate an exception.  This exception\n        message is compared against the return value of\n        `traceback.format_exception_only()`.  `exc_msg` ends with a\n        newline unless it's `None`.  The constructor adds a newline\n        if needed.\n\n      - lineno: The line number within the DocTest string containing\n        this Example where the Example begins.  This line number is\n        zero-based, with respect to the beginning of the DocTest.\n\n      - indent: The example's indentation in the DocTest string.\n        I.e., the number of space characters that precede the\n        example's first prompt.\n\n      - options: A dictionary mapping from option flags to True or\n        False, which is used to override default options for this\n        example.  Any option flags not contained in this dictionary\n        are left at their default value (as specified by the\n        DocTestRunner's optionflags).  By default, no options are set.\n    \"\"\"\n    def __init__(self, source, want, exc_msg=None, lineno=0, indent=0,\n                 options=None):\n        # Normalize inputs.\n        if not source.endswith('\\n'):\n            source += '\\n'\n        if want and not want.endswith('\\n'):\n            want += '\\n'\n        if exc_msg is not None and not exc_msg.endswith('\\n'):\n            exc_msg += '\\n'\n        # Store properties.\n        self.source = source\n        self.want = want\n        self.lineno = lineno\n        self.indent = indent\n        if options is None: options = {}\n        self.options = options\n        self.exc_msg = exc_msg\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.source == other.source and \\\n               self.want == other.want and \\\n               self.lineno == other.lineno and \\\n               self.indent == other.indent and \\\n               self.options == other.options and \\\n               self.exc_msg == other.exc_msg\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.source, self.want, self.lineno, self.indent,\n                     self.exc_msg))\n\n\nclass DocTest:\n    \"\"\"\n    A collection of doctest examples that should be run in a single\n    namespace.  Each `DocTest` defines the following attributes:\n\n      - examples: the list of examples.\n\n      - globs: The namespace (aka globals) that the examples should\n        be run in.\n\n      - name: A name identifying the DocTest (typically, the name of\n        the object whose docstring this DocTest was extracted from).\n\n      - filename: The name of the file that this DocTest was extracted\n        from, or `None` if the filename is unknown.\n\n      - lineno: The line number within filename where this DocTest\n        begins, or `None` if the line number is unavailable.  This\n        line number is zero-based, with respect to the beginning of\n        the file.\n\n      - docstring: The string that the examples were extracted from,\n        or `None` if the string is unavailable.\n    \"\"\"\n    def __init__(self, examples, globs, name, filename, lineno, docstring):\n        \"\"\"\n        Create a new DocTest containing the given examples.  The\n        DocTest's globals are initialized with a copy of `globs`.\n        \"\"\"\n        assert not isinstance(examples, basestring), \\\n               \"DocTest no longer accepts str; use DocTestParser instead\"\n        self.examples = examples\n        self.docstring = docstring\n        self.globs = globs.copy()\n        self.name = name\n        self.filename = filename\n        self.lineno = lineno\n\n    def __repr__(self):\n        if len(self.examples) == 0:\n            examples = 'no examples'\n        elif len(self.examples) == 1:\n            examples = '1 example'\n        else:\n            examples = '%d examples' % len(self.examples)\n        return ('<DocTest %s from %s:%s (%s)>' %\n                (self.name, self.filename, self.lineno, examples))\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self.examples == other.examples and \\\n               self.docstring == other.docstring and \\\n               self.globs == other.globs and \\\n               self.name == other.name and \\\n               self.filename == other.filename and \\\n               self.lineno == other.lineno\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self.docstring, self.name, self.filename, self.lineno))\n\n    # This lets us sort tests by name:\n    def __cmp__(self, other):\n        if not isinstance(other, DocTest):\n            return -1\n        return cmp((self.name, self.filename, self.lineno, id(self)),\n                   (other.name, other.filename, other.lineno, id(other)))\n\n######################################################################\n## 3. DocTestParser\n######################################################################\n\nclass DocTestParser:\n    \"\"\"\n    A class used to parse strings containing doctest examples.\n    \"\"\"\n    # This regular expression is used to find doctest examples in a\n    # string.  It defines three groups: `source` is the source code\n    # (including leading indentation and prompts); `indent` is the\n    # indentation of the first (PS1) line of the source code; and\n    # `want` is the expected output (including leading indentation).\n    _EXAMPLE_RE = re.compile(r'''\n        # Source consists of a PS1 line followed by zero or more PS2 lines.\n        (?P<source>\n            (?:^(?P<indent> [ ]*) >>>    .*)    # PS1 line\n            (?:\\n           [ ]*  \\.\\.\\. .*)*)  # PS2 lines\n        \\n?\n        # Want consists of any non-blank lines that do not start with PS1.\n        (?P<want> (?:(?![ ]*$)    # Not a blank line\n                     (?![ ]*>>>)  # Not a line starting with PS1\n                     .+$\\n?       # But any other line\n                  )*)\n        ''', re.MULTILINE | re.VERBOSE)\n\n    # A regular expression for handling `want` strings that contain\n    # expected exceptions.  It divides `want` into three pieces:\n    #    - the traceback header line (`hdr`)\n    #    - the traceback stack (`stack`)\n    #    - the exception message (`msg`), as generated by\n    #      traceback.format_exception_only()\n    # `msg` may have multiple lines.  We assume/require that the\n    # exception message is the first non-indented line starting with a word\n    # character following the traceback header line.\n    _EXCEPTION_RE = re.compile(r\"\"\"\n        # Grab the traceback header.  Different versions of Python have\n        # said different things on the first traceback line.\n        ^(?P<hdr> Traceback\\ \\(\n            (?: most\\ recent\\ call\\ last\n            |   innermost\\ last\n            ) \\) :\n        )\n        \\s* $                # toss trailing whitespace on the header.\n        (?P<stack> .*?)      # don't blink: absorb stuff until...\n        ^ (?P<msg> \\w+ .*)   #     a line *starts* with alphanum.\n        \"\"\", re.VERBOSE | re.MULTILINE | re.DOTALL)\n\n    # A callable returning a true value iff its argument is a blank line\n    # or contains a single comment.\n    _IS_BLANK_OR_COMMENT = re.compile(r'^[ ]*(#.*)?$').match\n\n    def parse(self, string, name='<string>'):\n        \"\"\"\n        Divide the given string into examples and intervening text,\n        and return them as a list of alternating Examples and strings.\n        Line numbers for the Examples are 0-based.  The optional\n        argument `name` is a name identifying this string, and is only\n        used for error messages.\n        \"\"\"\n        string = string.expandtabs()\n        # If all lines begin with the same indentation, then strip it.\n        min_indent = self._min_indent(string)\n        if min_indent > 0:\n            string = '\\n'.join([l[min_indent:] for l in string.split('\\n')])\n\n        output = []\n        charno, lineno = 0, 0\n        # Find all doctest examples in the string:\n        for m in self._EXAMPLE_RE.finditer(string):\n            # Add the pre-example text to `output`.\n            output.append(string[charno:m.start()])\n            # Update lineno (lines before this example)\n            lineno += string.count('\\n', charno, m.start())\n            # Extract info from the regexp match.\n            (source, options, want, exc_msg) = \\\n                     self._parse_example(m, name, lineno)\n            # Create an Example, and add it to the list.\n            if not self._IS_BLANK_OR_COMMENT(source):\n                output.append( Example(source, want, exc_msg,\n                                    lineno=lineno,\n                                    indent=min_indent+len(m.group('indent')),\n                                    options=options) )\n            # Update lineno (lines inside this example)\n            lineno += string.count('\\n', m.start(), m.end())\n            # Update charno.\n            charno = m.end()\n        # Add any remaining post-example text to `output`.\n        output.append(string[charno:])\n        return output\n\n    def get_doctest(self, string, globs, name, filename, lineno):\n        \"\"\"\n        Extract all doctest examples from the given string, and\n        collect them into a `DocTest` object.\n\n        `globs`, `name`, `filename`, and `lineno` are attributes for\n        the new `DocTest` object.  See the documentation for `DocTest`\n        for more information.\n        \"\"\"\n        return DocTest(self.get_examples(string, name), globs,\n                       name, filename, lineno, string)\n\n    def get_examples(self, string, name='<string>'):\n        \"\"\"\n        Extract all doctest examples from the given string, and return\n        them as a list of `Example` objects.  Line numbers are\n        0-based, because it's most common in doctests that nothing\n        interesting appears on the same line as opening triple-quote,\n        and so the first interesting line is called \\\"line 1\\\" then.\n\n        The optional argument `name` is a name identifying this\n        string, and is only used for error messages.\n        \"\"\"\n        return [x for x in self.parse(string, name)\n                if isinstance(x, Example)]\n\n    def _parse_example(self, m, name, lineno):\n        \"\"\"\n        Given a regular expression match from `_EXAMPLE_RE` (`m`),\n        return a pair `(source, want)`, where `source` is the matched\n        example's source code (with prompts and indentation stripped);\n        and `want` is the example's expected output (with indentation\n        stripped).\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        # Get the example's indentation level.\n        indent = len(m.group('indent'))\n\n        # Divide source into lines; check that they're properly\n        # indented; and then strip their indentation & prompts.\n        source_lines = m.group('source').split('\\n')\n        self._check_prompt_blank(source_lines, indent, name, lineno)\n        self._check_prefix(source_lines[1:], ' '*indent + '.', name, lineno)\n        source = '\\n'.join([sl[indent+4:] for sl in source_lines])\n\n        # Divide want into lines; check that it's properly indented; and\n        # then strip the indentation.  Spaces before the last newline should\n        # be preserved, so plain rstrip() isn't good enough.\n        want = m.group('want')\n        want_lines = want.split('\\n')\n        if len(want_lines) > 1 and re.match(r' *$', want_lines[-1]):\n            del want_lines[-1]  # forget final newline & spaces after it\n        self._check_prefix(want_lines, ' '*indent, name,\n                           lineno + len(source_lines))\n        want = '\\n'.join([wl[indent:] for wl in want_lines])\n\n        # If `want` contains a traceback message, then extract it.\n        m = self._EXCEPTION_RE.match(want)\n        if m:\n            exc_msg = m.group('msg')\n        else:\n            exc_msg = None\n\n        # Extract options from the source.\n        options = self._find_options(source, name, lineno)\n\n        return source, options, want, exc_msg\n\n    # This regular expression looks for option directives in the\n    # source code of an example.  Option directives are comments\n    # starting with \"doctest:\".  Warning: this may give false\n    # positives for string-literals that contain the string\n    # \"#doctest:\".  Eliminating these false positives would require\n    # actually parsing the string; but we limit them by ignoring any\n    # line containing \"#doctest:\" that is *followed* by a quote mark.\n    _OPTION_DIRECTIVE_RE = re.compile(r'#\\s*doctest:\\s*([^\\n\\'\"]*)$',\n                                      re.MULTILINE)\n\n    def _find_options(self, source, name, lineno):\n        \"\"\"\n        Return a dictionary containing option overrides extracted from\n        option directives in the given source string.\n\n        `name` is the string's name, and `lineno` is the line number\n        where the example starts; both are used for error messages.\n        \"\"\"\n        options = {}\n        # (note: with the current regexp, this will match at most once:)\n        for m in self._OPTION_DIRECTIVE_RE.finditer(source):\n            option_strings = m.group(1).replace(',', ' ').split()\n            for option in option_strings:\n                if (option[0] not in '+-' or\n                    option[1:] not in OPTIONFLAGS_BY_NAME):\n                    raise ValueError('line %r of the doctest for %s '\n                                     'has an invalid option: %r' %\n                                     (lineno+1, name, option))\n                flag = OPTIONFLAGS_BY_NAME[option[1:]]\n                options[flag] = (option[0] == '+')\n        if options and self._IS_BLANK_OR_COMMENT(source):\n            raise ValueError('line %r of the doctest for %s has an option '\n                             'directive on a line with no example: %r' %\n                             (lineno, name, source))\n        return options\n\n    # This regular expression finds the indentation of every non-blank\n    # line in a string.\n    _INDENT_RE = re.compile('^([ ]*)(?=\\S)', re.MULTILINE)\n\n    def _min_indent(self, s):\n        \"Return the minimum indentation of any non-blank line in `s`\"\n        indents = [len(indent) for indent in self._INDENT_RE.findall(s)]\n        if len(indents) > 0:\n            return min(indents)\n        else:\n            return 0\n\n    def _check_prompt_blank(self, lines, indent, name, lineno):\n        \"\"\"\n        Given the lines of a source string (including prompts and\n        leading indentation), check to make sure that every prompt is\n        followed by a space character.  If any line is not followed by\n        a space character, then raise ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if len(line) >= indent+4 and line[indent+3] != ' ':\n                raise ValueError('line %r of the docstring for %s '\n                                 'lacks blank after %s: %r' %\n                                 (lineno+i+1, name,\n                                  line[indent:indent+3], line))\n\n    def _check_prefix(self, lines, prefix, name, lineno):\n        \"\"\"\n        Check that every line in the given list starts with the given\n        prefix; if any line does not, then raise a ValueError.\n        \"\"\"\n        for i, line in enumerate(lines):\n            if line and not line.startswith(prefix):\n                raise ValueError('line %r of the docstring for %s has '\n                                 'inconsistent leading whitespace: %r' %\n                                 (lineno+i+1, name, line))\n\n\n######################################################################\n## 4. DocTest Finder\n######################################################################\n\nclass DocTestFinder:\n    \"\"\"\n    A class used to extract the DocTests that are relevant to a given\n    object, from its docstring and the docstrings of its contained\n    objects.  Doctests can currently be extracted from the following\n    object types: modules, functions, classes, methods, staticmethods,\n    classmethods, and properties.\n    \"\"\"\n\n    def __init__(self, verbose=False, parser=DocTestParser(),\n                 recurse=True, exclude_empty=True):\n        \"\"\"\n        Create a new doctest finder.\n\n        The optional argument `parser` specifies a class or\n        function that should be used to create new DocTest objects (or\n        objects that implement the same interface as DocTest).  The\n        signature for this factory function should match the signature\n        of the DocTest constructor.\n\n        If the optional argument `recurse` is false, then `find` will\n        only examine the given object, and not any contained objects.\n\n        If the optional argument `exclude_empty` is false, then `find`\n        will include tests for objects with empty docstrings.\n        \"\"\"\n        self._parser = parser\n        self._verbose = verbose\n        self._recurse = recurse\n        self._exclude_empty = exclude_empty\n\n    def find(self, obj, name=None, module=None, globs=None, extraglobs=None):\n        \"\"\"\n        Return a list of the DocTests that are defined by the given\n        object's docstring, or by any of its contained objects'\n        docstrings.\n\n        The optional parameter `module` is the module that contains\n        the given object.  If the module is not specified or is None, then\n        the test finder will attempt to automatically determine the\n        correct module.  The object's module is used:\n\n            - As a default namespace, if `globs` is not specified.\n            - To prevent the DocTestFinder from extracting DocTests\n              from objects that are imported from other modules.\n            - To find the name of the file containing the object.\n            - To help find the line number of the object within its\n              file.\n\n        Contained objects whose module does not match `module` are ignored.\n\n        If `module` is False, no attempt to find the module will be made.\n        This is obscure, of use mostly in tests:  if `module` is False, or\n        is None but cannot be found automatically, then all objects are\n        considered to belong to the (non-existent) module, so all contained\n        objects will (recursively) be searched for doctests.\n\n        The globals for each DocTest is formed by combining `globs`\n        and `extraglobs` (bindings in `extraglobs` override bindings\n        in `globs`).  A new copy of the globals dictionary is created\n        for each DocTest.  If `globs` is not specified, then it\n        defaults to the module's `__dict__`, if specified, or {}\n        otherwise.  If `extraglobs` is not specified, then it defaults\n        to {}.\n\n        \"\"\"\n        # If name was not specified, then extract it from the object.\n        if name is None:\n            name = getattr(obj, '__name__', None)\n            if name is None:\n                raise ValueError(\"DocTestFinder.find: name must be given \"\n                        \"when obj.__name__ doesn't exist: %r\" %\n                                 (type(obj),))\n\n        # Find the module that contains the given object (if obj is\n        # a module, then module=obj.).  Note: this may fail, in which\n        # case module will be None.\n        if module is False:\n            module = None\n        elif module is None:\n            module = inspect.getmodule(obj)\n\n        # Read the module's source code.  This is used by\n        # DocTestFinder._find_lineno to find the line number for a\n        # given object's docstring.\n        try:\n            file = inspect.getsourcefile(obj) or inspect.getfile(obj)\n            if module is not None:\n                # Supply the module globals in case the module was\n                # originally loaded via a PEP 302 loader and\n                # file is not a valid filesystem path\n                source_lines = linecache.getlines(file, module.__dict__)\n            else:\n                # No access to a loader, so assume it's a normal\n                # filesystem path\n                source_lines = linecache.getlines(file)\n            if not source_lines:\n                source_lines = None\n        except TypeError:\n            source_lines = None\n\n        # Initialize globals, and merge in extraglobs.\n        if globs is None:\n            if module is None:\n                globs = {}\n            else:\n                globs = module.__dict__.copy()\n        else:\n            globs = globs.copy()\n        if extraglobs is not None:\n            globs.update(extraglobs)\n        if '__name__' not in globs:\n            globs['__name__'] = '__main__'  # provide a default module name\n\n        # Recursively explore `obj`, extracting DocTests.\n        tests = []\n        self._find(tests, obj, name, module, source_lines, globs, {})\n        # Sort the tests by alpha order of names, for consistency in\n        # verbose-mode output.  This was a feature of doctest in Pythons\n        # <= 2.3 that got lost by accident in 2.4.  It was repaired in\n        # 2.4.4 and 2.5.\n        tests.sort()\n        return tests\n\n    def _from_module(self, module, object):\n        \"\"\"\n        Return true if the given object is defined in the given\n        module.\n        \"\"\"\n        if module is None:\n            return True\n        elif inspect.getmodule(object) is not None:\n            return module is inspect.getmodule(object)\n        elif inspect.isfunction(object):\n            return module.__dict__ is object.func_globals\n        elif inspect.isclass(object):\n            return module.__name__ == object.__module__\n        elif hasattr(object, '__module__'):\n            return module.__name__ == object.__module__\n        elif isinstance(object, property):\n            return True # [XX] no way not be sure.\n        else:\n            raise ValueError(\"object must be a class or function\")\n\n    def _find(self, tests, obj, name, module, source_lines, globs, seen):\n        \"\"\"\n        Find tests for the given object and any contained objects, and\n        add them to `tests`.\n        \"\"\"\n        if self._verbose:\n            print 'Finding tests in %s' % name\n\n        # If we've already processed this object, then ignore it.\n        if id(obj) in seen:\n            return\n        seen[id(obj)] = 1\n\n        # Find a test for this object, and add it to the list of tests.\n        test = self._get_test(obj, name, module, globs, source_lines)\n        if test is not None:\n            tests.append(test)\n\n        # Look for tests in a module's contained objects.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                valname = '%s.%s' % (name, valname)\n                # Recurse to functions & classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val)) and\n                    self._from_module(module, val)):\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n        # Look for tests in a module's __test__ dictionary.\n        if inspect.ismodule(obj) and self._recurse:\n            for valname, val in getattr(obj, '__test__', {}).items():\n                if not isinstance(valname, basestring):\n                    raise ValueError(\"DocTestFinder.find: __test__ keys \"\n                                     \"must be strings: %r\" %\n                                     (type(valname),))\n                if not (inspect.isfunction(val) or inspect.isclass(val) or\n                        inspect.ismethod(val) or inspect.ismodule(val) or\n                        isinstance(val, basestring)):\n                    raise ValueError(\"DocTestFinder.find: __test__ values \"\n                                     \"must be strings, functions, methods, \"\n                                     \"classes, or modules: %r\" %\n                                     (type(val),))\n                valname = '%s.__test__.%s' % (name, valname)\n                self._find(tests, val, valname, module, source_lines,\n                           globs, seen)\n\n        # Look for tests in a class's contained objects.\n        if inspect.isclass(obj) and self._recurse:\n            for valname, val in obj.__dict__.items():\n                # Special handling for staticmethod/classmethod.\n                if isinstance(val, staticmethod):\n                    val = getattr(obj, valname)\n                if isinstance(val, classmethod):\n                    val = getattr(obj, valname).im_func\n\n                # Recurse to methods, properties, and nested classes.\n                if ((inspect.isfunction(val) or inspect.isclass(val) or\n                      isinstance(val, property)) and\n                      self._from_module(module, val)):\n                    valname = '%s.%s' % (name, valname)\n                    self._find(tests, val, valname, module, source_lines,\n                               globs, seen)\n\n    def _get_test(self, obj, name, module, globs, source_lines):\n        \"\"\"\n        Return a DocTest for the given object, if it defines a docstring;\n        otherwise, return None.\n        \"\"\"\n        # Extract the object's docstring.  If it doesn't have one,\n        # then return None (no test for this object).\n        if isinstance(obj, basestring):\n            docstring = obj\n        else:\n            try:\n                if obj.__doc__ is None:\n                    docstring = ''\n                else:\n                    docstring = obj.__doc__\n                    if not isinstance(docstring, basestring):\n                        docstring = str(docstring)\n            except (TypeError, AttributeError):\n                docstring = ''\n\n        # Find the docstring's location in the file.\n        lineno = self._find_lineno(obj, source_lines)\n\n        # Don't bother if the docstring is empty.\n        if self._exclude_empty and not docstring:\n            return None\n\n        # Return a DocTest for this object.\n        if module is None:\n            filename = None\n        else:\n            filename = getattr(module, '__file__', module.__name__)\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n        return self._parser.get_doctest(docstring, globs, name,\n                                        filename, lineno)\n\n    def _find_lineno(self, obj, source_lines):\n        \"\"\"\n        Return a line number of the given object's docstring.  Note:\n        this method assumes that the object has a docstring.\n        \"\"\"\n        lineno = None\n\n        # Find the line number for modules.\n        if inspect.ismodule(obj):\n            lineno = 0\n\n        # Find the line number for classes.\n        # Note: this could be fooled if a class is defined multiple\n        # times in a single file.\n        if inspect.isclass(obj):\n            if source_lines is None:\n                return None\n            pat = re.compile(r'^\\s*class\\s*%s\\b' %\n                             getattr(obj, '__name__', '-'))\n            for i, line in enumerate(source_lines):\n                if pat.match(line):\n                    lineno = i\n                    break\n\n        # Find the line number for functions & methods.\n        if inspect.ismethod(obj): obj = obj.im_func\n        if inspect.isfunction(obj): obj = obj.func_code\n        if inspect.istraceback(obj): obj = obj.tb_frame\n        if inspect.isframe(obj): obj = obj.f_code\n        if inspect.iscode(obj):\n            lineno = getattr(obj, 'co_firstlineno', None)-1\n\n        # Find the line number where the docstring starts.  Assume\n        # that it's the first line that begins with a quote mark.\n        # Note: this could be fooled by a multiline function\n        # signature, where a continuation line begins with a quote\n        # mark.\n        if lineno is not None:\n            if source_lines is None:\n                return lineno+1\n            pat = re.compile('(^|.*:)\\s*\\w*(\"|\\')')\n            for lineno in range(lineno, len(source_lines)):\n                if pat.match(source_lines[lineno]):\n                    return lineno\n\n        # We couldn't find the line number.\n        return None\n\n######################################################################\n## 5. DocTest Runner\n######################################################################\n\nclass DocTestRunner:\n    \"\"\"\n    A class used to run DocTest test cases, and accumulate statistics.\n    The `run` method is used to process a single DocTest case.  It\n    returns a tuple `(f, t)`, where `t` is the number of test cases\n    tried, and `f` is the number of test cases that failed.\n\n        >>> tests = DocTestFinder().find(_TestClass)\n        >>> runner = DocTestRunner(verbose=False)\n        >>> tests.sort(key = lambda test: test.name)\n        >>> for test in tests:\n        ...     print test.name, '->', runner.run(test)\n        _TestClass -> TestResults(failed=0, attempted=2)\n        _TestClass.__init__ -> TestResults(failed=0, attempted=2)\n        _TestClass.get -> TestResults(failed=0, attempted=2)\n        _TestClass.square -> TestResults(failed=0, attempted=1)\n\n    The `summarize` method prints a summary of all the test cases that\n    have been run by the runner, and returns an aggregated `(f, t)`\n    tuple:\n\n        >>> runner.summarize(verbose=1)\n        4 items passed all tests:\n           2 tests in _TestClass\n           2 tests in _TestClass.__init__\n           2 tests in _TestClass.get\n           1 tests in _TestClass.square\n        7 tests in 4 items.\n        7 passed and 0 failed.\n        Test passed.\n        TestResults(failed=0, attempted=7)\n\n    The aggregated number of tried examples and failed examples is\n    also available via the `tries` and `failures` attributes:\n\n        >>> runner.tries\n        7\n        >>> runner.failures\n        0\n\n    The comparison between expected outputs and actual outputs is done\n    by an `OutputChecker`.  This comparison may be customized with a\n    number of option flags; see the documentation for `testmod` for\n    more information.  If the option flags are insufficient, then the\n    comparison may also be customized by passing a subclass of\n    `OutputChecker` to the constructor.\n\n    The test runner's display output can be controlled in two ways.\n    First, an output function (`out) can be passed to\n    `TestRunner.run`; this function will be called with strings that\n    should be displayed.  It defaults to `sys.stdout.write`.  If\n    capturing the output is not sufficient, then the display output\n    can be also customized by subclassing DocTestRunner, and\n    overriding the methods `report_start`, `report_success`,\n    `report_unexpected_exception`, and `report_failure`.\n    \"\"\"\n    # This divider string is used to separate failure messages, and to\n    # separate sections of the summary.\n    DIVIDER = \"*\" * 70\n\n    def __init__(self, checker=None, verbose=None, optionflags=0):\n        \"\"\"\n        Create a new test runner.\n\n        Optional keyword arg `checker` is the `OutputChecker` that\n        should be used to compare the expected outputs and actual\n        outputs of doctest examples.\n\n        Optional keyword arg 'verbose' prints lots of stuff if true,\n        only failures if false; by default, it's true iff '-v' is in\n        sys.argv.\n\n        Optional argument `optionflags` can be used to control how the\n        test runner compares expected output to actual output, and how\n        it displays failures.  See the documentation for `testmod` for\n        more information.\n        \"\"\"\n        self._checker = checker or OutputChecker()\n        if verbose is None:\n            verbose = '-v' in sys.argv\n        self._verbose = verbose\n        self.optionflags = optionflags\n        self.original_optionflags = optionflags\n\n        # Keep track of the examples we've run.\n        self.tries = 0\n        self.failures = 0\n        self._name2ft = {}\n\n        # Create a fake output target for capturing doctest output.\n        self._fakeout = _SpoofOut()\n\n    #/////////////////////////////////////////////////////////////////\n    # Reporting methods\n    #/////////////////////////////////////////////////////////////////\n\n    def report_start(self, out, test, example):\n        \"\"\"\n        Report that the test runner is about to process the given\n        example.  (Only displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            if example.want:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting:\\n' + _indent(example.want))\n            else:\n                out('Trying:\\n' + _indent(example.source) +\n                    'Expecting nothing\\n')\n\n    def report_success(self, out, test, example, got):\n        \"\"\"\n        Report that the given example ran successfully.  (Only\n        displays a message if verbose=True)\n        \"\"\"\n        if self._verbose:\n            out(\"ok\\n\")\n\n    def report_failure(self, out, test, example, got):\n        \"\"\"\n        Report that the given example failed.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            self._checker.output_difference(example, got, self.optionflags))\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        \"\"\"\n        Report that the given example raised an unexpected exception.\n        \"\"\"\n        out(self._failure_header(test, example) +\n            'Exception raised:\\n' + _indent(_exception_traceback(exc_info)))\n\n    def _failure_header(self, test, example):\n        out = [self.DIVIDER]\n        if test.filename:\n            if test.lineno is not None and example.lineno is not None:\n                lineno = test.lineno + example.lineno + 1\n            else:\n                lineno = '?'\n            out.append('File \"%s\", line %s, in %s' %\n                       (test.filename, lineno, test.name))\n        else:\n            out.append('Line %s, in %s' % (example.lineno+1, test.name))\n        out.append('Failed example:')\n        source = example.source\n        out.append(_indent(source))\n        return '\\n'.join(out)\n\n    #/////////////////////////////////////////////////////////////////\n    # DocTest Running\n    #/////////////////////////////////////////////////////////////////\n\n    def __run(self, test, compileflags, out):\n        \"\"\"\n        Run the examples in `test`.  Write the outcome of each example\n        with one of the `DocTestRunner.report_*` methods, using the\n        writer function `out`.  `compileflags` is the set of compiler\n        flags that should be used to execute examples.  Return a tuple\n        `(f, t)`, where `t` is the number of examples tried, and `f`\n        is the number of examples that failed.  The examples are run\n        in the namespace `test.globs`.\n        \"\"\"\n        # Keep track of the number of failures and tries.\n        failures = tries = 0\n\n        # Save the option flags (since option directives can be used\n        # to modify them).\n        original_optionflags = self.optionflags\n\n        SUCCESS, FAILURE, BOOM = range(3) # `outcome` state\n\n        check = self._checker.check_output\n\n        # Process each example.\n        for examplenum, example in enumerate(test.examples):\n\n            # If REPORT_ONLY_FIRST_FAILURE is set, then suppress\n            # reporting after the first failure.\n            quiet = (self.optionflags & REPORT_ONLY_FIRST_FAILURE and\n                     failures > 0)\n\n            # Merge in the example's options.\n            self.optionflags = original_optionflags\n            if example.options:\n                for (optionflag, val) in example.options.items():\n                    if val:\n                        self.optionflags |= optionflag\n                    else:\n                        self.optionflags &= ~optionflag\n\n            # If 'SKIP' is set, then skip this example.\n            if self.optionflags & SKIP:\n                continue\n\n            # Record that we started this example.\n            tries += 1\n            if not quiet:\n                self.report_start(out, test, example)\n\n            # Use a special filename for compile(), so we can retrieve\n            # the source code during interactive debugging (see\n            # __patched_linecache_getlines).\n            filename = '<doctest %s[%d]>' % (test.name, examplenum)\n\n            # Run the example in the given context (globs), and record\n            # any exception that gets raised.  (But don't intercept\n            # keyboard interrupts.)\n            try:\n                # Don't blink!  This is where the user's code gets run.\n                exec compile(example.source, filename, \"single\",\n                             compileflags, 1) in test.globs\n                self.debugger.set_continue() # ==== Example Finished ====\n                exception = None\n            except KeyboardInterrupt:\n                raise\n            except:\n                exception = sys.exc_info()\n                self.debugger.set_continue() # ==== Example Finished ====\n\n            got = self._fakeout.getvalue()  # the actual output\n            self._fakeout.truncate(0)\n            outcome = FAILURE   # guilty until proved innocent or insane\n\n            # If the example executed without raising any exceptions,\n            # verify its output.\n            if exception is None:\n                if check(example.want, got, self.optionflags):\n                    outcome = SUCCESS\n\n            # The example raised an exception:  check if it was expected.\n            else:\n                exc_info = sys.exc_info()\n                exc_msg = traceback.format_exception_only(*exc_info[:2])[-1]\n                if not quiet:\n                    got += _exception_traceback(exc_info)\n\n                # If `example.exc_msg` is None, then we weren't expecting\n                # an exception.\n                if example.exc_msg is None:\n                    outcome = BOOM\n\n                # We expected an exception:  see whether it matches.\n                elif check(example.exc_msg, exc_msg, self.optionflags):\n                    outcome = SUCCESS\n\n                # Another chance if they didn't care about the detail.\n                elif self.optionflags & IGNORE_EXCEPTION_DETAIL:\n                    if check(_strip_exception_details(example.exc_msg),\n                             _strip_exception_details(exc_msg),\n                             self.optionflags):\n                        outcome = SUCCESS\n\n            # Report the outcome.\n            if outcome is SUCCESS:\n                if not quiet:\n                    self.report_success(out, test, example, got)\n            elif outcome is FAILURE:\n                if not quiet:\n                    self.report_failure(out, test, example, got)\n                failures += 1\n            elif outcome is BOOM:\n                if not quiet:\n                    self.report_unexpected_exception(out, test, example,\n                                                     exc_info)\n                failures += 1\n            else:\n                assert False, (\"unknown outcome\", outcome)\n\n        # Restore the option flags (in case they were modified)\n        self.optionflags = original_optionflags\n\n        # Record and return the number of failures and tries.\n        self.__record_outcome(test, failures, tries)\n        return TestResults(failures, tries)\n\n    def __record_outcome(self, test, f, t):\n        \"\"\"\n        Record the fact that the given DocTest (`test`) generated `f`\n        failures out of `t` tried examples.\n        \"\"\"\n        f2, t2 = self._name2ft.get(test.name, (0,0))\n        self._name2ft[test.name] = (f+f2, t+t2)\n        self.failures += f\n        self.tries += t\n\n    __LINECACHE_FILENAME_RE = re.compile(r'<doctest '\n                                         r'(?P<name>.+)'\n                                         r'\\[(?P<examplenum>\\d+)\\]>$')\n    def __patched_linecache_getlines(self, filename, module_globals=None):\n        m = self.__LINECACHE_FILENAME_RE.match(filename)\n        if m and m.group('name') == self.test.name:\n            example = self.test.examples[int(m.group('examplenum'))]\n            source = example.source\n            if isinstance(source, unicode):\n                source = source.encode('ascii', 'backslashreplace')\n            return source.splitlines(True)\n        else:\n            return self.save_linecache_getlines(filename, module_globals)\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        \"\"\"\n        Run the examples in `test`, and display the results using the\n        writer function `out`.\n\n        The examples are run in the namespace `test.globs`.  If\n        `clear_globs` is true (the default), then this namespace will\n        be cleared after the test runs, to help with garbage\n        collection.  If you would like to examine the namespace after\n        the test completes, then use `clear_globs=False`.\n\n        `compileflags` gives the set of flags that should be used by\n        the Python compiler when running the examples.  If not\n        specified, then it will default to the set of future-import\n        flags that apply to `globs`.\n\n        The output of each example is checked using\n        `DocTestRunner.check_output`, and the results are formatted by\n        the `DocTestRunner.report_*` methods.\n        \"\"\"\n        self.test = test\n\n        if compileflags is None:\n            compileflags = _extract_future_flags(test.globs)\n\n        save_stdout = sys.stdout\n        if out is None:\n            out = save_stdout.write\n        sys.stdout = self._fakeout\n\n        # Patch pdb.set_trace to restore sys.stdout during interactive\n        # debugging (so it's not still redirected to self._fakeout).\n        # Note that the interactive output will go to *our*\n        # save_stdout, even if that's not the real sys.stdout; this\n        # allows us to write test cases for the set_trace behavior.\n        save_set_trace = pdb.set_trace\n        self.debugger = _OutputRedirectingPdb(save_stdout)\n        self.debugger.reset()\n        pdb.set_trace = self.debugger.set_trace\n\n        # Patch linecache.getlines, so we can see the example's source\n        # when we're inside the debugger.\n        self.save_linecache_getlines = linecache.getlines\n        linecache.getlines = self.__patched_linecache_getlines\n\n        # Make sure sys.displayhook just prints the value to stdout\n        save_displayhook = sys.displayhook\n        sys.displayhook = sys.__displayhook__\n\n        try:\n            return self.__run(test, compileflags, out)\n        finally:\n            sys.stdout = save_stdout\n            pdb.set_trace = save_set_trace\n            linecache.getlines = self.save_linecache_getlines\n            sys.displayhook = save_displayhook\n            if clear_globs:\n                test.globs.clear()\n\n    #/////////////////////////////////////////////////////////////////\n    # Summarization\n    #/////////////////////////////////////////////////////////////////\n    def summarize(self, verbose=None):\n        \"\"\"\n        Print a summary of all the test cases that have been run by\n        this DocTestRunner, and return a tuple `(f, t)`, where `f` is\n        the total number of failed examples, and `t` is the total\n        number of tried examples.\n\n        The optional `verbose` argument controls how detailed the\n        summary is.  If the verbosity is not specified, then the\n        DocTestRunner's verbosity is used.\n        \"\"\"\n        if verbose is None:\n            verbose = self._verbose\n        notests = []\n        passed = []\n        failed = []\n        totalt = totalf = 0\n        for x in self._name2ft.items():\n            name, (f, t) = x\n            assert f <= t\n            totalt += t\n            totalf += f\n            if t == 0:\n                notests.append(name)\n            elif f == 0:\n                passed.append( (name, t) )\n            else:\n                failed.append(x)\n        if verbose:\n            if notests:\n                print len(notests), \"items had no tests:\"\n                notests.sort()\n                for thing in notests:\n                    print \"   \", thing\n            if passed:\n                print len(passed), \"items passed all tests:\"\n                passed.sort()\n                for thing, count in passed:\n                    print \" %3d tests in %s\" % (count, thing)\n        if failed:\n            print self.DIVIDER\n            print len(failed), \"items had failures:\"\n            failed.sort()\n            for thing, (f, t) in failed:\n                print \" %3d of %3d in %s\" % (f, t, thing)\n        if verbose:\n            print totalt, \"tests in\", len(self._name2ft), \"items.\"\n            print totalt - totalf, \"passed and\", totalf, \"failed.\"\n        if totalf:\n            print \"***Test Failed***\", totalf, \"failures.\"\n        elif verbose:\n            print \"Test passed.\"\n        return TestResults(totalf, totalt)\n\n    #/////////////////////////////////////////////////////////////////\n    # Backward compatibility cruft to maintain doctest.master.\n    #/////////////////////////////////////////////////////////////////\n    def merge(self, other):\n        d = self._name2ft\n        for name, (f, t) in other._name2ft.items():\n            if name in d:\n                # Don't print here by default, since doing\n                #     so breaks some of the buildbots\n                #print \"*** DocTestRunner.merge: '\" + name + \"' in both\" \\\n                #    \" testers; summing outcomes.\"\n                f2, t2 = d[name]\n                f = f + f2\n                t = t + t2\n            d[name] = f, t\n\nclass OutputChecker:\n    \"\"\"\n    A class used to check the whether the actual output from a doctest\n    example matches the expected output.  `OutputChecker` defines two\n    methods: `check_output`, which compares a given pair of outputs,\n    and returns true if they match; and `output_difference`, which\n    returns a string describing the differences between two outputs.\n    \"\"\"\n    def check_output(self, want, got, optionflags):\n        \"\"\"\n        Return True iff the actual output from an example (`got`)\n        matches the expected output (`want`).  These strings are\n        always considered to match if they are identical; but\n        depending on what option flags the test runner is using,\n        several non-exact match types are also possible.  See the\n        documentation for `TestRunner` for more information about\n        option flags.\n        \"\"\"\n        # Handle the common case first, for efficiency:\n        # if they're string-identical, always return true.\n        if got == want:\n            return True\n\n        # The values True and False replaced 1 and 0 as the return\n        # value for boolean comparisons in Python 2.3.\n        if not (optionflags & DONT_ACCEPT_TRUE_FOR_1):\n            if (got,want) == (\"True\\n\", \"1\\n\"):\n                return True\n            if (got,want) == (\"False\\n\", \"0\\n\"):\n                return True\n\n        # <BLANKLINE> can be used as a special sequence to signify a\n        # blank line, unless the DONT_ACCEPT_BLANKLINE flag is used.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            # Replace <BLANKLINE> in want with a blank line.\n            want = re.sub('(?m)^%s\\s*?$' % re.escape(BLANKLINE_MARKER),\n                          '', want)\n            # If a line in got contains only spaces, then remove the\n            # spaces.\n            got = re.sub('(?m)^\\s*?$', '', got)\n            if got == want:\n                return True\n\n        # This flag causes doctest to ignore any differences in the\n        # contents of whitespace strings.  Note that this can be used\n        # in conjunction with the ELLIPSIS flag.\n        if optionflags & NORMALIZE_WHITESPACE:\n            got = ' '.join(got.split())\n            want = ' '.join(want.split())\n            if got == want:\n                return True\n\n        # The ELLIPSIS flag says to let the sequence \"...\" in `want`\n        # match any substring in `got`.\n        if optionflags & ELLIPSIS:\n            if _ellipsis_match(want, got):\n                return True\n\n        # We didn't find any match; return false.\n        return False\n\n    # Should we do a fancy diff?\n    def _do_a_fancy_diff(self, want, got, optionflags):\n        # Not unless they asked for a fancy diff.\n        if not optionflags & (REPORT_UDIFF |\n                              REPORT_CDIFF |\n                              REPORT_NDIFF):\n            return False\n\n        # If expected output uses ellipsis, a meaningful fancy diff is\n        # too hard ... or maybe not.  In two real-life failures Tim saw,\n        # a diff was a major help anyway, so this is commented out.\n        # [todo] _ellipsis_match() knows which pieces do and don't match,\n        # and could be the basis for a kick-ass diff in this case.\n        ##if optionflags & ELLIPSIS and ELLIPSIS_MARKER in want:\n        ##    return False\n\n        # ndiff does intraline difference marking, so can be useful even\n        # for 1-line differences.\n        if optionflags & REPORT_NDIFF:\n            return True\n\n        # The other diff types need at least a few lines to be helpful.\n        return want.count('\\n') > 2 and got.count('\\n') > 2\n\n    def output_difference(self, example, got, optionflags):\n        \"\"\"\n        Return a string describing the differences between the\n        expected output for a given example (`example`) and the actual\n        output (`got`).  `optionflags` is the set of option flags used\n        to compare `want` and `got`.\n        \"\"\"\n        want = example.want\n        # If <BLANKLINE>s are being used, then replace blank lines\n        # with <BLANKLINE> in the actual output string.\n        if not (optionflags & DONT_ACCEPT_BLANKLINE):\n            got = re.sub('(?m)^[ ]*(?=\\n)', BLANKLINE_MARKER, got)\n\n        # Check if we should use diff.\n        if self._do_a_fancy_diff(want, got, optionflags):\n            # Split want & got into lines.\n            want_lines = want.splitlines(True)  # True == keep line ends\n            got_lines = got.splitlines(True)\n            # Use difflib to find their differences.\n            if optionflags & REPORT_UDIFF:\n                diff = difflib.unified_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'unified diff with -expected +actual'\n            elif optionflags & REPORT_CDIFF:\n                diff = difflib.context_diff(want_lines, got_lines, n=2)\n                diff = list(diff)[2:] # strip the diff header\n                kind = 'context diff with expected followed by actual'\n            elif optionflags & REPORT_NDIFF:\n                engine = difflib.Differ(charjunk=difflib.IS_CHARACTER_JUNK)\n                diff = list(engine.compare(want_lines, got_lines))\n                kind = 'ndiff with -expected +actual'\n            else:\n                assert 0, 'Bad diff option'\n            # Remove trailing whitespace on diff output.\n            diff = [line.rstrip() + '\\n' for line in diff]\n            return 'Differences (%s):\\n' % kind + _indent(''.join(diff))\n\n        # If we're not using diff, then simply list the expected\n        # output followed by the actual output.\n        if want and got:\n            return 'Expected:\\n%sGot:\\n%s' % (_indent(want), _indent(got))\n        elif want:\n            return 'Expected:\\n%sGot nothing\\n' % _indent(want)\n        elif got:\n            return 'Expected nothing\\nGot:\\n%s' % _indent(got)\n        else:\n            return 'Expected nothing\\nGot nothing\\n'\n\nclass DocTestFailure(Exception):\n    \"\"\"A DocTest example has failed in debugging mode.\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - got: the actual output\n    \"\"\"\n    def __init__(self, test, example, got):\n        self.test = test\n        self.example = example\n        self.got = got\n\n    def __str__(self):\n        return str(self.test)\n\nclass UnexpectedException(Exception):\n    \"\"\"A DocTest example has encountered an unexpected exception\n\n    The exception instance has variables:\n\n    - test: the DocTest object being run\n\n    - example: the Example object that failed\n\n    - exc_info: the exception info\n    \"\"\"\n    def __init__(self, test, example, exc_info):\n        self.test = test\n        self.example = example\n        self.exc_info = exc_info\n\n    def __str__(self):\n        return str(self.test)\n\nclass DebugRunner(DocTestRunner):\n    r\"\"\"Run doc tests but raise an exception as soon as there is a failure.\n\n       If an unexpected exception occurs, an UnexpectedException is raised.\n       It contains the test, the example, and the original exception:\n\n         >>> runner = DebugRunner(verbose=False)\n         >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n         ...                                    {}, 'foo', 'foo.py', 0)\n         >>> try:\n         ...     runner.run(test)\n         ... except UnexpectedException, failure:\n         ...     pass\n\n         >>> failure.test is test\n         True\n\n         >>> failure.example.want\n         '42\\n'\n\n         >>> exc_info = failure.exc_info\n         >>> raise exc_info[0], exc_info[1], exc_info[2]\n         Traceback (most recent call last):\n         ...\n         KeyError\n\n       We wrap the original exception to give the calling application\n       access to the test and example information.\n\n       If the output doesn't match, then a DocTestFailure is raised:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 1\n         ...      >>> x\n         ...      2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> try:\n         ...    runner.run(test)\n         ... except DocTestFailure, failure:\n         ...    pass\n\n       DocTestFailure objects provide access to the test:\n\n         >>> failure.test is test\n         True\n\n       As well as to the example:\n\n         >>> failure.example.want\n         '2\\n'\n\n       and the actual output:\n\n         >>> failure.got\n         '1\\n'\n\n       If a failure or error occurs, the globals are left intact:\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 1}\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      >>> raise KeyError\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         Traceback (most recent call last):\n         ...\n         UnexpectedException: <DocTest foo from foo.py:0 (2 examples)>\n\n         >>> del test.globs['__builtins__']\n         >>> test.globs\n         {'x': 2}\n\n       But the globals are cleared if there is no error:\n\n         >>> test = DocTestParser().get_doctest('''\n         ...      >>> x = 2\n         ...      ''', {}, 'foo', 'foo.py', 0)\n\n         >>> runner.run(test)\n         TestResults(failed=0, attempted=1)\n\n         >>> test.globs\n         {}\n\n       \"\"\"\n\n    def run(self, test, compileflags=None, out=None, clear_globs=True):\n        r = DocTestRunner.run(self, test, compileflags, out, False)\n        if clear_globs:\n            test.globs.clear()\n        return r\n\n    def report_unexpected_exception(self, out, test, example, exc_info):\n        raise UnexpectedException(test, example, exc_info)\n\n    def report_failure(self, out, test, example, got):\n        raise DocTestFailure(test, example, got)\n\n######################################################################\n## 6. Test Functions\n######################################################################\n# These should be backwards compatible.\n\n# For backward compatibility, a global instance of a DocTestRunner\n# class, updated by testmod.\nmaster = None\n\ndef testmod(m=None, name=None, globs=None, verbose=None,\n            report=True, optionflags=0, extraglobs=None,\n            raise_on_error=False, exclude_empty=False):\n    \"\"\"m=None, name=None, globs=None, verbose=None, report=True,\n       optionflags=0, extraglobs=None, raise_on_error=False,\n       exclude_empty=False\n\n    Test examples in docstrings in functions and classes reachable\n    from module m (or the current module if m is not supplied), starting\n    with m.__doc__.\n\n    Also test examples reachable from dict m.__test__ if it exists and is\n    not None.  m.__test__ maps names to functions, classes and strings;\n    function and class docstrings are tested even if the name is private;\n    strings are tested directly, as if they were docstrings.\n\n    Return (#failures, #tests).\n\n    See help(doctest) for an overview.\n\n    Optional keyword arg \"name\" gives the name of the module; by default\n    use m.__name__.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use m.__dict__.  A copy of this\n    dict is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.  This is new in 2.4.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  This is new in 2.3.  Possible values (see the\n    docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    # If no module was given, then use __main__.\n    if m is None:\n        # DWA - m will still be None if this wasn't invoked from the command\n        # line, in which case the following TypeError is about as good an error\n        # as we should expect\n        m = sys.modules.get('__main__')\n\n    # Check that we were actually given a module.\n    if not inspect.ismodule(m):\n        raise TypeError(\"testmod: module required; %r\" % (m,))\n\n    # If no name was given, then use the module's name.\n    if name is None:\n        name = m.__name__\n\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(exclude_empty=exclude_empty)\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    for test in finder.find(m, name, globs=globs, extraglobs=extraglobs):\n        runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef testfile(filename, module_relative=True, name=None, package=None,\n             globs=None, verbose=None, report=True, optionflags=0,\n             extraglobs=None, raise_on_error=False, parser=DocTestParser(),\n             encoding=None):\n    \"\"\"\n    Test examples in the given file.  Return (#failures, #tests).\n\n    Optional keyword arg \"module_relative\" specifies how filenames\n    should be interpreted:\n\n      - If \"module_relative\" is True (the default), then \"filename\"\n         specifies a module-relative path.  By default, this path is\n         relative to the calling module's directory; but if the\n         \"package\" argument is specified, then it is relative to that\n         package.  To ensure os-independence, \"filename\" should use\n         \"/\" characters to separate path segments, and should not\n         be an absolute path (i.e., it may not begin with \"/\").\n\n      - If \"module_relative\" is False, then \"filename\" specifies an\n        os-specific path.  The path may be absolute or relative (to\n        the current working directory).\n\n    Optional keyword arg \"name\" gives the name of the test; by default\n    use the file's basename.\n\n    Optional keyword argument \"package\" is a Python package or the\n    name of a Python package whose directory should be used as the\n    base directory for a module relative filename.  If no package is\n    specified, then the calling module's directory is used as the base\n    directory for module relative filenames.  It is an error to\n    specify \"package\" if \"module_relative\" is False.\n\n    Optional keyword arg \"globs\" gives a dict to be used as the globals\n    when executing examples; by default, use {}.  A copy of this dict\n    is actually used for each docstring, so that each docstring's\n    examples start with a clean slate.\n\n    Optional keyword arg \"extraglobs\" gives a dictionary that should be\n    merged into the globals that are used to execute examples.  By\n    default, no extra globals are used.\n\n    Optional keyword arg \"verbose\" prints lots of stuff if true, prints\n    only failures if false; by default, it's true iff \"-v\" is in sys.argv.\n\n    Optional keyword arg \"report\" prints a summary at the end when true,\n    else prints nothing at the end.  In verbose mode, the summary is\n    detailed, else very brief (in fact, empty if all tests passed).\n\n    Optional keyword arg \"optionflags\" or's together module constants,\n    and defaults to 0.  Possible values (see the docs for details):\n\n        DONT_ACCEPT_TRUE_FOR_1\n        DONT_ACCEPT_BLANKLINE\n        NORMALIZE_WHITESPACE\n        ELLIPSIS\n        SKIP\n        IGNORE_EXCEPTION_DETAIL\n        REPORT_UDIFF\n        REPORT_CDIFF\n        REPORT_NDIFF\n        REPORT_ONLY_FIRST_FAILURE\n\n    Optional keyword arg \"raise_on_error\" raises an exception on the\n    first unexpected exception or failure. This allows failures to be\n    post-mortem debugged.\n\n    Optional keyword arg \"parser\" specifies a DocTestParser (or\n    subclass) that should be used to extract tests from the files.\n\n    Optional keyword arg \"encoding\" specifies an encoding that should\n    be used to convert the file to unicode.\n\n    Advanced tomfoolery:  testmod runs methods of a local instance of\n    class doctest.Tester, then merges the results into (or creates)\n    global Tester instance doctest.master.  Methods of doctest.master\n    can be called directly too, if you want to do something unusual.\n    Passing report=0 to testmod is especially useful then, to delay\n    displaying a summary.  Invoke doctest.master.summarize(verbose)\n    when you're done fiddling.\n    \"\"\"\n    global master\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path\n    text, filename = _load_testfile(filename, package, module_relative)\n\n    # If no name was given, then use the file's name.\n    if name is None:\n        name = os.path.basename(filename)\n\n    # Assemble the globals.\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n    if extraglobs is not None:\n        globs.update(extraglobs)\n    if '__name__' not in globs:\n        globs['__name__'] = '__main__'\n\n    if raise_on_error:\n        runner = DebugRunner(verbose=verbose, optionflags=optionflags)\n    else:\n        runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n\n    if encoding is not None:\n        text = text.decode(encoding)\n\n    # Read the file, convert it to a test, and run it.\n    test = parser.get_doctest(text, globs, name, filename, 0)\n    runner.run(test)\n\n    if report:\n        runner.summarize()\n\n    if master is None:\n        master = runner\n    else:\n        master.merge(runner)\n\n    return TestResults(runner.failures, runner.tries)\n\ndef run_docstring_examples(f, globs, verbose=False, name=\"NoName\",\n                           compileflags=None, optionflags=0):\n    \"\"\"\n    Test examples in the given object's docstring (`f`), using `globs`\n    as globals.  Optional argument `name` is used in failure messages.\n    If the optional argument `verbose` is true, then generate output\n    even if there are no failures.\n\n    `compileflags` gives the set of flags that should be used by the\n    Python compiler when running the examples.  If not specified, then\n    it will default to the set of future-import flags that apply to\n    `globs`.\n\n    Optional keyword arg `optionflags` specifies options for the\n    testing and output.  See the documentation for `testmod` for more\n    information.\n    \"\"\"\n    # Find, parse, and run all tests in the given module.\n    finder = DocTestFinder(verbose=verbose, recurse=False)\n    runner = DocTestRunner(verbose=verbose, optionflags=optionflags)\n    for test in finder.find(f, name, globs=globs):\n        runner.run(test, compileflags=compileflags)\n\n######################################################################\n## 7. Tester\n######################################################################\n# This is provided only for backwards compatibility.  It's not\n# actually used in any way.\n\nclass Tester:\n    def __init__(self, mod=None, globs=None, verbose=None, optionflags=0):\n\n        warnings.warn(\"class Tester is deprecated; \"\n                      \"use class doctest.DocTestRunner instead\",\n                      DeprecationWarning, stacklevel=2)\n        if mod is None and globs is None:\n            raise TypeError(\"Tester.__init__: must specify mod or globs\")\n        if mod is not None and not inspect.ismodule(mod):\n            raise TypeError(\"Tester.__init__: mod must be a module; %r\" %\n                            (mod,))\n        if globs is None:\n            globs = mod.__dict__\n        self.globs = globs\n\n        self.verbose = verbose\n        self.optionflags = optionflags\n        self.testfinder = DocTestFinder()\n        self.testrunner = DocTestRunner(verbose=verbose,\n                                        optionflags=optionflags)\n\n    def runstring(self, s, name):\n        test = DocTestParser().get_doctest(s, self.globs, name, None, None)\n        if self.verbose:\n            print \"Running string\", name\n        (f,t) = self.testrunner.run(test)\n        if self.verbose:\n            print f, \"of\", t, \"examples failed in string\", name\n        return TestResults(f,t)\n\n    def rundoc(self, object, name=None, module=None):\n        f = t = 0\n        tests = self.testfinder.find(object, name, module=module,\n                                     globs=self.globs)\n        for test in tests:\n            (f2, t2) = self.testrunner.run(test)\n            (f,t) = (f+f2, t+t2)\n        return TestResults(f,t)\n\n    def rundict(self, d, name, module=None):\n        import types\n        m = types.ModuleType(name)\n        m.__dict__.update(d)\n        if module is None:\n            module = False\n        return self.rundoc(m, name, module)\n\n    def run__test__(self, d, name):\n        import types\n        m = types.ModuleType(name)\n        m.__test__ = d\n        return self.rundoc(m, name)\n\n    def summarize(self, verbose=None):\n        return self.testrunner.summarize(verbose)\n\n    def merge(self, other):\n        self.testrunner.merge(other.testrunner)\n\n######################################################################\n## 8. Unittest Support\n######################################################################\n\n_unittest_reportflags = 0\n\ndef set_unittest_reportflags(flags):\n    \"\"\"Sets the unittest option flags.\n\n    The old flag is returned so that a runner could restore the old\n    value if it wished to:\n\n      >>> import doctest\n      >>> old = doctest._unittest_reportflags\n      >>> doctest.set_unittest_reportflags(REPORT_NDIFF |\n      ...                          REPORT_ONLY_FIRST_FAILURE) == old\n      True\n\n      >>> doctest._unittest_reportflags == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n\n    Only reporting flags can be set:\n\n      >>> doctest.set_unittest_reportflags(ELLIPSIS)\n      Traceback (most recent call last):\n      ...\n      ValueError: ('Only reporting flags allowed', 8)\n\n      >>> doctest.set_unittest_reportflags(old) == (REPORT_NDIFF |\n      ...                                   REPORT_ONLY_FIRST_FAILURE)\n      True\n    \"\"\"\n    global _unittest_reportflags\n\n    if (flags & REPORTING_FLAGS) != flags:\n        raise ValueError(\"Only reporting flags allowed\", flags)\n    old = _unittest_reportflags\n    _unittest_reportflags = flags\n    return old\n\n\nclass DocTestCase(unittest.TestCase):\n\n    def __init__(self, test, optionflags=0, setUp=None, tearDown=None,\n                 checker=None):\n\n        unittest.TestCase.__init__(self)\n        self._dt_optionflags = optionflags\n        self._dt_checker = checker\n        self._dt_test = test\n        self._dt_setUp = setUp\n        self._dt_tearDown = tearDown\n\n    def setUp(self):\n        test = self._dt_test\n\n        if self._dt_setUp is not None:\n            self._dt_setUp(test)\n\n    def tearDown(self):\n        test = self._dt_test\n\n        if self._dt_tearDown is not None:\n            self._dt_tearDown(test)\n\n        test.globs.clear()\n\n    def runTest(self):\n        test = self._dt_test\n        old = sys.stdout\n        new = StringIO()\n        optionflags = self._dt_optionflags\n\n        if not (optionflags & REPORTING_FLAGS):\n            # The option flags don't include any reporting flags,\n            # so add the default reporting flags\n            optionflags |= _unittest_reportflags\n\n        runner = DocTestRunner(optionflags=optionflags,\n                               checker=self._dt_checker, verbose=False)\n\n        try:\n            runner.DIVIDER = \"-\"*70\n            failures, tries = runner.run(\n                test, out=new.write, clear_globs=False)\n        finally:\n            sys.stdout = old\n\n        if failures:\n            raise self.failureException(self.format_failure(new.getvalue()))\n\n    def format_failure(self, err):\n        test = self._dt_test\n        if test.lineno is None:\n            lineno = 'unknown line number'\n        else:\n            lineno = '%s' % test.lineno\n        lname = '.'.join(test.name.split('.')[-1:])\n        return ('Failed doctest test for %s\\n'\n                '  File \"%s\", line %s, in %s\\n\\n%s'\n                % (test.name, test.filename, lineno, lname, err)\n                )\n\n    def debug(self):\n        r\"\"\"Run the test case without results and without catching exceptions\n\n           The unit test framework includes a debug method on test cases\n           and test suites to support post-mortem debugging.  The test code\n           is run in such a way that errors are not caught.  This way a\n           caller can catch the errors and initiate post-mortem debugging.\n\n           The DocTestCase provides a debug method that raises\n           UnexpectedException errors if there is an unexpected\n           exception:\n\n             >>> test = DocTestParser().get_doctest('>>> raise KeyError\\n42',\n             ...                {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n             >>> try:\n             ...     case.debug()\n             ... except UnexpectedException, failure:\n             ...     pass\n\n           The UnexpectedException contains the test, the example, and\n           the original exception:\n\n             >>> failure.test is test\n             True\n\n             >>> failure.example.want\n             '42\\n'\n\n             >>> exc_info = failure.exc_info\n             >>> raise exc_info[0], exc_info[1], exc_info[2]\n             Traceback (most recent call last):\n             ...\n             KeyError\n\n           If the output doesn't match, then a DocTestFailure is raised:\n\n             >>> test = DocTestParser().get_doctest('''\n             ...      >>> x = 1\n             ...      >>> x\n             ...      2\n             ...      ''', {}, 'foo', 'foo.py', 0)\n             >>> case = DocTestCase(test)\n\n             >>> try:\n             ...    case.debug()\n             ... except DocTestFailure, failure:\n             ...    pass\n\n           DocTestFailure objects provide access to the test:\n\n             >>> failure.test is test\n             True\n\n           As well as to the example:\n\n             >>> failure.example.want\n             '2\\n'\n\n           and the actual output:\n\n             >>> failure.got\n             '1\\n'\n\n           \"\"\"\n\n        self.setUp()\n        runner = DebugRunner(optionflags=self._dt_optionflags,\n                             checker=self._dt_checker, verbose=False)\n        runner.run(self._dt_test, clear_globs=False)\n        self.tearDown()\n\n    def id(self):\n        return self._dt_test.name\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self._dt_test == other._dt_test and \\\n               self._dt_optionflags == other._dt_optionflags and \\\n               self._dt_setUp == other._dt_setUp and \\\n               self._dt_tearDown == other._dt_tearDown and \\\n               self._dt_checker == other._dt_checker\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((self._dt_optionflags, self._dt_setUp, self._dt_tearDown,\n                     self._dt_checker))\n\n    def __repr__(self):\n        name = self._dt_test.name.split('.')\n        return \"%s (%s)\" % (name[-1], '.'.join(name[:-1]))\n\n    __str__ = __repr__\n\n    def shortDescription(self):\n        return \"Doctest: \" + self._dt_test.name\n\nclass SkipDocTestCase(DocTestCase):\n    def __init__(self, module):\n        self.module = module\n        DocTestCase.__init__(self, None)\n\n    def setUp(self):\n        self.skipTest(\"DocTestSuite will not work with -O2 and above\")\n\n    def test_skip(self):\n        pass\n\n    def shortDescription(self):\n        return \"Skipping tests from %s\" % self.module.__name__\n\n    __str__ = shortDescription\n\n\ndef DocTestSuite(module=None, globs=None, extraglobs=None, test_finder=None,\n                 **options):\n    \"\"\"\n    Convert doctest tests for a module to a unittest test suite.\n\n    This converts each documentation string in a module that\n    contains doctest tests to a unittest test case.  If any of the\n    tests in a doc string fail, then the test case fails.  An exception\n    is raised showing the name of the file containing the test and a\n    (sometimes approximate) line number.\n\n    The `module` argument provides the module to be tested.  The argument\n    can be either a module or a module name.\n\n    If no argument is given, the calling module is used.\n\n    A number of options may be provided as keyword arguments:\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n       A set of doctest option flags expressed as an integer.\n    \"\"\"\n\n    if test_finder is None:\n        test_finder = DocTestFinder()\n\n    module = _normalize_module(module)\n    tests = test_finder.find(module, globs=globs, extraglobs=extraglobs)\n\n    if not tests and sys.flags.optimize >=2:\n        # Skip doctests when running with -O2\n        suite = unittest.TestSuite()\n        suite.addTest(SkipDocTestCase(module))\n        return suite\n    elif not tests:\n        # Why do we want to do this? Because it reveals a bug that might\n        # otherwise be hidden.\n        # It is probably a bug that this exception is not also raised if the\n        # number of doctest examples in tests is zero (i.e. if no doctest\n        # examples were found).  However, we should probably not be raising\n        # an exception at all here, though it is too late to make this change\n        # for a maintenance release.  See also issue #14649.\n        raise ValueError(module, \"has no docstrings\")\n\n    tests.sort()\n    suite = unittest.TestSuite()\n\n    for test in tests:\n        if len(test.examples) == 0:\n            continue\n        if not test.filename:\n            filename = module.__file__\n            if filename[-4:] in (\".pyc\", \".pyo\"):\n                filename = filename[:-1]\n            test.filename = filename\n        suite.addTest(DocTestCase(test, **options))\n\n    return suite\n\nclass DocFileCase(DocTestCase):\n\n    def id(self):\n        return '_'.join(self._dt_test.name.split('.'))\n\n    def __repr__(self):\n        return self._dt_test.filename\n    __str__ = __repr__\n\n    def format_failure(self, err):\n        return ('Failed doctest test for %s\\n  File \"%s\", line 0\\n\\n%s'\n                % (self._dt_test.name, self._dt_test.filename, err)\n                )\n\ndef DocFileTest(path, module_relative=True, package=None,\n                globs=None, parser=DocTestParser(),\n                encoding=None, **options):\n    if globs is None:\n        globs = {}\n    else:\n        globs = globs.copy()\n\n    if package and not module_relative:\n        raise ValueError(\"Package may only be specified for module-\"\n                         \"relative paths.\")\n\n    # Relativize the path.\n    doc, path = _load_testfile(path, package, module_relative)\n\n    if \"__file__\" not in globs:\n        globs[\"__file__\"] = path\n\n    # Find the file and read it.\n    name = os.path.basename(path)\n\n    # If an encoding is specified, use it to convert the file to unicode\n    if encoding is not None:\n        doc = doc.decode(encoding)\n\n    # Convert it to a test, and wrap it in a DocFileCase.\n    test = parser.get_doctest(doc, globs, name, path, 0)\n    return DocFileCase(test, **options)\n\ndef DocFileSuite(*paths, **kw):\n    \"\"\"A unittest suite for one or more doctest files.\n\n    The path to each doctest file is given as a string; the\n    interpretation of that string depends on the keyword argument\n    \"module_relative\".\n\n    A number of options may be provided as keyword arguments:\n\n    module_relative\n      If \"module_relative\" is True, then the given file paths are\n      interpreted as os-independent module-relative paths.  By\n      default, these paths are relative to the calling module's\n      directory; but if the \"package\" argument is specified, then\n      they are relative to that package.  To ensure os-independence,\n      \"filename\" should use \"/\" characters to separate path\n      segments, and may not be an absolute path (i.e., it may not\n      begin with \"/\").\n\n      If \"module_relative\" is False, then the given file paths are\n      interpreted as os-specific paths.  These paths may be absolute\n      or relative (to the current working directory).\n\n    package\n      A Python package or the name of a Python package whose directory\n      should be used as the base directory for module relative paths.\n      If \"package\" is not specified, then the calling module's\n      directory is used as the base directory for module relative\n      filenames.  It is an error to specify \"package\" if\n      \"module_relative\" is False.\n\n    setUp\n      A set-up function.  This is called before running the\n      tests in each file. The setUp function will be passed a DocTest\n      object.  The setUp function can access the test globals as the\n      globs attribute of the test passed.\n\n    tearDown\n      A tear-down function.  This is called after running the\n      tests in each file.  The tearDown function will be passed a DocTest\n      object.  The tearDown function can access the test globals as the\n      globs attribute of the test passed.\n\n    globs\n      A dictionary containing initial global variables for the tests.\n\n    optionflags\n      A set of doctest option flags expressed as an integer.\n\n    parser\n      A DocTestParser (or subclass) that should be used to extract\n      tests from the files.\n\n    encoding\n      An encoding that will be used to convert the files to unicode.\n    \"\"\"\n    suite = unittest.TestSuite()\n\n    # We do this here so that _normalize_module is called at the right\n    # level.  If it were called in DocFileTest, then this function\n    # would be the caller and we might guess the package incorrectly.\n    if kw.get('module_relative', True):\n        kw['package'] = _normalize_module(kw.get('package'))\n\n    for path in paths:\n        suite.addTest(DocFileTest(path, **kw))\n\n    return suite\n\n######################################################################\n## 9. Debugging Support\n######################################################################\n\ndef script_from_examples(s):\n    r\"\"\"Extract script from text with examples.\n\n       Converts text with examples to a Python script.  Example input is\n       converted to regular code.  Example output and all other words\n       are converted to comments:\n\n       >>> text = '''\n       ...       Here are examples of simple math.\n       ...\n       ...           Python has super accurate integer addition\n       ...\n       ...           >>> 2 + 2\n       ...           5\n       ...\n       ...           And very friendly error messages:\n       ...\n       ...           >>> 1/0\n       ...           To Infinity\n       ...           And\n       ...           Beyond\n       ...\n       ...           You can use logic if you want:\n       ...\n       ...           >>> if 0:\n       ...           ...    blah\n       ...           ...    blah\n       ...           ...\n       ...\n       ...           Ho hum\n       ...           '''\n\n       >>> print script_from_examples(text)\n       # Here are examples of simple math.\n       #\n       #     Python has super accurate integer addition\n       #\n       2 + 2\n       # Expected:\n       ## 5\n       #\n       #     And very friendly error messages:\n       #\n       1/0\n       # Expected:\n       ## To Infinity\n       ## And\n       ## Beyond\n       #\n       #     You can use logic if you want:\n       #\n       if 0:\n          blah\n          blah\n       #\n       #     Ho hum\n       <BLANKLINE>\n       \"\"\"\n    output = []\n    for piece in DocTestParser().parse(s):\n        if isinstance(piece, Example):\n            # Add the example's source code (strip trailing NL)\n            output.append(piece.source[:-1])\n            # Add the expected output:\n            want = piece.want\n            if want:\n                output.append('# Expected:')\n                output += ['## '+l for l in want.split('\\n')[:-1]]\n        else:\n            # Add non-example text.\n            output += [_comment_line(l)\n                       for l in piece.split('\\n')[:-1]]\n\n    # Trim junk on both ends.\n    while output and output[-1] == '#':\n        output.pop()\n    while output and output[0] == '#':\n        output.pop(0)\n    # Combine the output, and return it.\n    # Add a courtesy newline to prevent exec from choking (see bug #1172785)\n    return '\\n'.join(output) + '\\n'\n\ndef testsource(module, name):\n    \"\"\"Extract the test sources from a doctest docstring as a script.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the doc string with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    tests = DocTestFinder().find(module)\n    test = [t for t in tests if t.name == name]\n    if not test:\n        raise ValueError(name, \"not found in tests\")\n    test = test[0]\n    testsrc = script_from_examples(test.docstring)\n    return testsrc\n\ndef debug_src(src, pm=False, globs=None):\n    \"\"\"Debug a single doctest docstring, in argument `src`'\"\"\"\n    testsrc = script_from_examples(src)\n    debug_script(testsrc, pm, globs)\n\ndef debug_script(src, pm=False, globs=None):\n    \"Debug a test script.  `src` is the script, as a string.\"\n    import pdb\n\n    # Note that tempfile.NameTemporaryFile() cannot be used.  As the\n    # docs say, a file so created cannot be opened by name a second time\n    # on modern Windows boxes, and execfile() needs to open it.\n    srcfilename = tempfile.mktemp(\".py\", \"doctestdebug\")\n    f = open(srcfilename, 'w')\n    f.write(src)\n    f.close()\n\n    try:\n        if globs:\n            globs = globs.copy()\n        else:\n            globs = {}\n\n        if pm:\n            try:\n                execfile(srcfilename, globs, globs)\n            except:\n                print sys.exc_info()[1]\n                pdb.post_mortem(sys.exc_info()[2])\n        else:\n            # Note that %r is vital here.  '%s' instead can, e.g., cause\n            # backslashes to get treated as metacharacters on Windows.\n            pdb.run(\"execfile(%r)\" % srcfilename, globs, globs)\n\n    finally:\n        os.remove(srcfilename)\n\ndef debug(module, name, pm=False):\n    \"\"\"Debug a single doctest docstring.\n\n    Provide the module (or dotted name of the module) containing the\n    test to be debugged and the name (within the module) of the object\n    with the docstring with tests to be debugged.\n    \"\"\"\n    module = _normalize_module(module)\n    testsrc = testsource(module, name)\n    debug_script(testsrc, pm, module.__dict__)\n\n######################################################################\n## 10. Example Usage\n######################################################################\nclass _TestClass:\n    \"\"\"\n    A pointless class, for sanity-checking of docstring testing.\n\n    Methods:\n        square()\n        get()\n\n    >>> _TestClass(13).get() + _TestClass(-12).get()\n    1\n    >>> hex(_TestClass(13).square().get())\n    '0xa9'\n    \"\"\"\n\n    def __init__(self, val):\n        \"\"\"val -> _TestClass object with associated value val.\n\n        >>> t = _TestClass(123)\n        >>> print t.get()\n        123\n        \"\"\"\n\n        self.val = val\n\n    def square(self):\n        \"\"\"square() -> square TestClass's associated value\n\n        >>> _TestClass(13).square().get()\n        169\n        \"\"\"\n\n        self.val = self.val ** 2\n        return self\n\n    def get(self):\n        \"\"\"get() -> return TestClass's associated value.\n\n        >>> x = _TestClass(-42)\n        >>> print x.get()\n        -42\n        \"\"\"\n\n        return self.val\n\n__test__ = {\"_TestClass\": _TestClass,\n            \"string\": r\"\"\"\n                      Example of a string object, searched as-is.\n                      >>> x = 1; y = 2\n                      >>> x + y, x * y\n                      (3, 2)\n                      \"\"\",\n\n            \"bool-int equivalence\": r\"\"\"\n                                    In 2.2, boolean expressions displayed\n                                    0 or 1.  By default, we still accept\n                                    them.  This can be disabled by passing\n                                    DONT_ACCEPT_TRUE_FOR_1 to the new\n                                    optionflags argument.\n                                    >>> 4 == 4\n                                    1\n                                    >>> 4 == 4\n                                    True\n                                    >>> 4 > 4\n                                    0\n                                    >>> 4 > 4\n                                    False\n                                    \"\"\",\n\n            \"blank lines\": r\"\"\"\n                Blank lines can be marked with <BLANKLINE>:\n                    >>> print 'foo\\n\\nbar\\n'\n                    foo\n                    <BLANKLINE>\n                    bar\n                    <BLANKLINE>\n            \"\"\",\n\n            \"ellipsis\": r\"\"\"\n                If the ellipsis flag is used, then '...' can be used to\n                elide substrings in the desired output:\n                    >>> print range(1000) #doctest: +ELLIPSIS\n                    [0, 1, 2, ..., 999]\n            \"\"\",\n\n            \"whitespace normalization\": r\"\"\"\n                If the whitespace normalization flag is used, then\n                differences in whitespace are ignored.\n                    >>> print range(30) #doctest: +NORMALIZE_WHITESPACE\n                    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14,\n                     15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26,\n                     27, 28, 29]\n            \"\"\",\n           }\n\n\ndef _test():\n    testfiles = [arg for arg in sys.argv[1:] if arg and arg[0] != '-']\n    if not testfiles:\n        name = os.path.basename(sys.argv[0])\n        if '__loader__' in globals():          # python -m\n            name, _ = os.path.splitext(name)\n        print(\"usage: {0} [-v] file ...\".format(name))\n        return 2\n    for filename in testfiles:\n        if filename.endswith(\".py\"):\n            # It is a module -- insert its dir into sys.path and try to\n            # import it. If it is part of a package, that possibly\n            # won't work because of package imports.\n            dirname, filename = os.path.split(filename)\n            sys.path.insert(0, dirname)\n            m = __import__(filename[:-3])\n            del sys.path[0]\n            failures, _ = testmod(m)\n        else:\n            failures, _ = testfile(filename, module_relative=False)\n        if failures:\n            return 1\n    return 0\n\n\nif __name__ == \"__main__\":\n    sys.exit(_test())\n", 
    "dummy_thread": "\"\"\"Drop-in replacement for the thread module.\n\nMeant to be used as a brain-dead substitute so that threaded code does\nnot need to be rewritten for when the thread module is not present.\n\nSuggested usage is::\n\n    try:\n        import thread\n    except ImportError:\n        import dummy_thread as thread\n\n\"\"\"\n# Exports only things specified by thread documentation;\n# skipping obsolete synonyms allocate(), start_new(), exit_thread().\n__all__ = ['error', 'start_new_thread', 'exit', 'get_ident', 'allocate_lock',\n           'interrupt_main', 'LockType']\n\nimport traceback as _traceback\n\nclass error(Exception):\n    \"\"\"Dummy implementation of thread.error.\"\"\"\n\n    def __init__(self, *args):\n        self.args = args\n\ndef start_new_thread(function, args, kwargs={}):\n    \"\"\"Dummy implementation of thread.start_new_thread().\n\n    Compatibility is maintained by making sure that ``args`` is a\n    tuple and ``kwargs`` is a dictionary.  If an exception is raised\n    and it is SystemExit (which can be done by thread.exit()) it is\n    caught and nothing is done; all other exceptions are printed out\n    by using traceback.print_exc().\n\n    If the executed function calls interrupt_main the KeyboardInterrupt will be\n    raised when the function returns.\n\n    \"\"\"\n    if type(args) != type(tuple()):\n        raise TypeError(\"2nd arg must be a tuple\")\n    if type(kwargs) != type(dict()):\n        raise TypeError(\"3rd arg must be a dict\")\n    global _main\n    _main = False\n    try:\n        function(*args, **kwargs)\n    except SystemExit:\n        pass\n    except:\n        _traceback.print_exc()\n    _main = True\n    global _interrupt\n    if _interrupt:\n        _interrupt = False\n        raise KeyboardInterrupt\n\ndef exit():\n    \"\"\"Dummy implementation of thread.exit().\"\"\"\n    raise SystemExit\n\ndef get_ident():\n    \"\"\"Dummy implementation of thread.get_ident().\n\n    Since this module should only be used when threadmodule is not\n    available, it is safe to assume that the current process is the\n    only thread.  Thus a constant can be safely returned.\n    \"\"\"\n    return -1\n\ndef allocate_lock():\n    \"\"\"Dummy implementation of thread.allocate_lock().\"\"\"\n    return LockType()\n\ndef stack_size(size=None):\n    \"\"\"Dummy implementation of thread.stack_size().\"\"\"\n    if size is not None:\n        raise error(\"setting thread stack size not supported\")\n    return 0\n\nclass LockType(object):\n    \"\"\"Class implementing dummy implementation of thread.LockType.\n\n    Compatibility is maintained by maintaining self.locked_status\n    which is a boolean that stores the state of the lock.  Pickling of\n    the lock, though, should not be done since if the thread module is\n    then used with an unpickled ``lock()`` from here problems could\n    occur from this class not having atomic methods.\n\n    \"\"\"\n\n    def __init__(self):\n        self.locked_status = False\n\n    def acquire(self, waitflag=None):\n        \"\"\"Dummy implementation of acquire().\n\n        For blocking calls, self.locked_status is automatically set to\n        True and returned appropriately based on value of\n        ``waitflag``.  If it is non-blocking, then the value is\n        actually checked and not set if it is already acquired.  This\n        is all done so that threading.Condition's assert statements\n        aren't triggered and throw a little fit.\n\n        \"\"\"\n        if waitflag is None or waitflag:\n            self.locked_status = True\n            return True\n        else:\n            if not self.locked_status:\n                self.locked_status = True\n                return True\n            else:\n                return False\n\n    __enter__ = acquire\n\n    def __exit__(self, typ, val, tb):\n        self.release()\n\n    def release(self):\n        \"\"\"Release the dummy lock.\"\"\"\n        # XXX Perhaps shouldn't actually bother to test?  Could lead\n        #     to problems for complex, threaded code.\n        if not self.locked_status:\n            raise error\n        self.locked_status = False\n        return True\n\n    def locked(self):\n        return self.locked_status\n\n# Used to signal that interrupt_main was called in a \"thread\"\n_interrupt = False\n# True when not executing in a \"thread\"\n_main = True\n\ndef interrupt_main():\n    \"\"\"Set _interrupt flag to True to have start_new_thread raise\n    KeyboardInterrupt upon exiting.\"\"\"\n    if _main:\n        raise KeyboardInterrupt\n    else:\n        global _interrupt\n        _interrupt = True\n", 
    "dummy_threading": "\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport sys as _sys\n\ntry:\n    import thread\nexcept ImportError:\n    import dummy_thread as thread\n\n\nimport warnings\n\nfrom collections import deque as _deque\nfrom itertools import count as _count\nfrom time import time as _time, sleep as _sleep\nfrom traceback import format_exc as _format_exc\n\n# Note regarding PEP 8 compliant aliases\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. While those names are not in any imminent danger of being\n# deprecated, starting with Python 2.6, the module now provides a\n# PEP 8 compliant alias for any such method name.\n# Using the new PEP 8 compliant names also facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n\n# Rename some stuff so \"from threading import *\" is safe\n__all__ = ['activeCount', 'active_count', 'Condition', 'currentThread',\n           'current_thread', 'enumerate', 'Event',\n           'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Timer', 'setprofile', 'settrace', 'local', 'stack_size']\n\n_start_new_thread = thread.start_new_thread\n_allocate_lock = thread.allocate_lock\n_get_ident = thread.get_ident\nThreadError = thread.error\ndel thread\n\n\n# sys.exc_clear is used to work around the fact that except blocks\n# don't fully clear the exception until 3.0.\nwarnings.filterwarnings('ignore', category=DeprecationWarning,\n                        module='threading', message='sys.exc_clear')\n\n# Debug support (adapted from ihooks.py).\n# All the major classes here derive from _Verbose.  We force that to\n# be a new-style class so that all the major classes here are new-style.\n# This helps debugging (type(instance) is more revealing for instances\n# of new-style classes).\n\n_VERBOSE = False\n\nif __debug__:\n\n    class _Verbose(object):\n\n        def __init__(self, verbose=None):\n            if verbose is None:\n                verbose = _VERBOSE\n            self.__verbose = verbose\n\n        def _note(self, format, *args):\n            if self.__verbose:\n                format = format % args\n                # Issue #4188: calling current_thread() can incur an infinite\n                # recursion if it has to create a DummyThread on the fly.\n                ident = _get_ident()\n                try:\n                    name = _active[ident].name\n                except KeyError:\n                    name = \"<OS thread %d>\" % ident\n                format = \"%s: %s\\n\" % (name, format)\n                _sys.stderr.write(format)\n\nelse:\n    # Disable this when using \"python -O\"\n    class _Verbose(object):\n        def __init__(self, verbose=None):\n            pass\n        def _note(self, *args):\n            pass\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\n# Synchronization classes\n\ndef Lock():\n    import js\n    return js.eval('new Semaphore(1)');\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    return _RLock(*args, **kwargs)\n\nclass _RLock(_Verbose):\n    \"\"\"A reentrant lock must be released by the thread that acquired it. Once a\n       thread has acquired a reentrant lock, the same thread may acquire it\n       again without blocking; the thread must release it once for each time it\n       has acquired it.\n    \"\"\"\n\n    def __init__(self, verbose=None):\n        _Verbose.__init__(self, verbose)\n        self.__block = _allocate_lock()\n        self.__owner = None\n        self.__count = 0\n\n    def __repr__(self):\n        owner = self.__owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s owner=%r count=%d>\" % (\n                self.__class__.__name__, owner, self.__count)\n\n    def acquire(self, blocking=1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        \"\"\"\n        me = _get_ident()\n        if self.__owner == me:\n            self.__count = self.__count + 1\n            if __debug__:\n                self._note(\"%s.acquire(%s): recursive success\", self, blocking)\n            return 1\n        rc = self.__block.acquire(blocking)\n        if rc:\n            self.__owner = me\n            self.__count = 1\n            if __debug__:\n                self._note(\"%s.acquire(%s): initial success\", self, blocking)\n        else:\n            if __debug__:\n                self._note(\"%s.acquire(%s): failure\", self, blocking)\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self.__owner != _get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self.__count = count = self.__count - 1\n        if not count:\n            self.__owner = None\n            self.__block.release()\n            if __debug__:\n                self._note(\"%s.release(): final release\", self)\n        else:\n            if __debug__:\n                self._note(\"%s.release(): non-final release\", self)\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, count_owner):\n        count, owner = count_owner\n        self.__block.acquire()\n        self.__count = count\n        self.__owner = owner\n        if __debug__:\n            self._note(\"%s._acquire_restore()\", self)\n\n    def _release_save(self):\n        if __debug__:\n            self._note(\"%s._release_save()\", self)\n        count = self.__count\n        self.__count = 0\n        owner = self.__owner\n        self.__owner = None\n        self.__block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self.__owner == _get_ident()\n\n\ndef Condition(*args, **kwargs):\n    \"\"\"Factory function that returns a new condition variable object.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n    return _Condition(*args, **kwargs)\n\nclass _Condition(_Verbose):\n    \"\"\"Condition variables allow one or more threads to wait until they are\n       notified by another thread.\n    \"\"\"\n\n    def __init__(self, lock=None, verbose=None):\n        _Verbose.__init__(self, verbose)\n        if lock is None:\n            lock = RLock()\n        self.__lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self.__waiters = []\n\n    def __enter__(self):\n        return self.__lock.__enter__()\n\n    def __exit__(self, *args):\n        return self.__lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self.__lock, len(self.__waiters))\n\n    def _release_save(self):\n        self.__lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self.__lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if __lock doesn't have _is_owned().\n        if self.__lock.acquire(0):\n            self.__lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notifyAll() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self.__waiters.append(waiter)\n        saved_state = self._release_save()\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                if __debug__:\n                    self._note(\"%s.wait(): got it\", self)\n            else:\n                # Balancing act:  We can't afford a pure busy loop, so we\n                # have to sleep; but if we sleep the whole timeout time,\n                # we'll be unresponsive.  The scheme here sleeps very\n                # little at first, longer as time goes on, but never longer\n                # than 20 times per second (or the timeout time remaining).\n                endtime = _time() + timeout\n                delay = 0.0005 # 500 us -> initial delay of 1 ms\n                while True:\n                    gotit = waiter.acquire(0)\n                    if gotit:\n                        break\n                    remaining = endtime - _time()\n                    if remaining <= 0:\n                        break\n                    delay = min(delay * 2, remaining, .05)\n                    _sleep(delay)\n                if not gotit:\n                    if __debug__:\n                        self._note(\"%s.wait(%s): timed out\", self, timeout)\n                    try:\n                        self.__waiters.remove(waiter)\n                    except ValueError:\n                        pass\n                else:\n                    if __debug__:\n                        self._note(\"%s.wait(%s): got it\", self, timeout)\n        finally:\n            self._acquire_restore(saved_state)\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n        #if not self._is_owned():\n        #    raise RuntimeError(\"cannot notify on un-acquired lock\")\n        __waiters = self.__waiters\n        waiters = __waiters[:n]\n        if not waiters:\n            if __debug__:\n                self._note(\"%s.notify(): no waiters\", self)\n            return\n        self._note(\"%s.notify(): notifying %d waiter%s\", self, n,\n                   n!=1 and \"s\" or \"\")\n        for waiter in waiters:\n            waiter.release()\n            try:\n                __waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notifyAll(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self.__waiters))\n\n    notify_all = notifyAll\n\n\ndef Semaphore(*args, **kwargs):\n    \"\"\"A factory function that returns a new semaphore.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n    return _Semaphore(*args, **kwargs)\n\nclass _Semaphore(_Verbose):\n    \"\"\"Semaphores manage a counter representing the number of release() calls\n       minus the number of acquire() calls, plus an initial value. The acquire()\n       method blocks if necessary until it can return without making the counter\n       negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1, verbose=None):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        _Verbose.__init__(self, verbose)\n        self.__cond = Condition(Lock())\n        self.__value = value\n\n    def acquire(self, blocking=1):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        \"\"\"\n        rc = False\n        with self.__cond:\n            while self.__value == 0:\n                if not blocking:\n                    break\n                if __debug__:\n                    self._note(\"%s.acquire(%s): blocked waiting, value=%s\",\n                            self, blocking, self.__value)\n                self.__cond.wait()\n            else:\n                self.__value = self.__value - 1\n                if __debug__:\n                    self._note(\"%s.acquire: success, value=%s\",\n                            self, self.__value)\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        with self.__cond:\n            self.__value = self.__value + 1\n            if __debug__:\n                self._note(\"%s.release: success, value=%s\",\n                        self, self.__value)\n            self.__cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\ndef BoundedSemaphore(*args, **kwargs):\n    \"\"\"A factory function that returns a new bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n    return _BoundedSemaphore(*args, **kwargs)\n\nclass _BoundedSemaphore(_Semaphore):\n    \"\"\"A bounded semaphore checks to make sure its current value doesn't exceed\n       its initial value. If it does, ValueError is raised. In most situations\n       semaphores are used to guard resources with limited capacity.\n    \"\"\"\n\n    def __init__(self, value=1, verbose=None):\n        _Semaphore.__init__(self, value, verbose)\n        self._initial_value = value\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        with self._Semaphore__cond:\n            if self._Semaphore__value >= self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._Semaphore__value += 1\n            self._Semaphore__cond.notify()\n\n\ndef Event(*args, **kwargs):\n    \"\"\"A factory function that returns a new event.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.\n\n    \"\"\"\n    return _Event(*args, **kwargs)\n\nclass _Event(_Verbose):\n    \"\"\"A factory function that returns a new event object. An event manages a\n       flag that can be set to true with the set() method and reset to false\n       with the clear() method. The wait() method blocks until the flag is true.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self, verbose=None):\n        _Verbose.__init__(self, verbose)\n        self.__cond = Condition(Lock())\n        self.__flag = False\n\n    def _reset_internal_locks(self):\n        # private!  called by Thread._reset_internal_locks by _after_fork()\n        self.__cond.__init__()\n\n    def isSet(self):\n        'Return true if and only if the internal flag is true.'\n        return self.__flag\n\n    is_set = isSet\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for the flag to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        self.__cond.acquire()\n        try:\n            self.__flag = True\n            self.__cond.notify_all()\n        finally:\n            self.__cond.release()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        self.__cond.acquire()\n        try:\n            self.__flag = False\n        finally:\n            self.__cond.release()\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        self.__cond.acquire()\n        try:\n            if not self.__flag:\n                self.__cond.wait(timeout)\n            return self.__flag\n        finally:\n            self.__cond.release()\n\n# Helper to generate new thread names\n_counter = _count().next\n_counter() # Consume 0 so first non-main thread has id 1.\ndef _newname(template=\"Thread-%d\"):\n    return template % _counter()\n\n# Active thread administration\n_active_limbo_lock = _allocate_lock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n\n\n# Main class for threads\n\nclass Thread(_Verbose):\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion.\n\n    \"\"\"\n    __initialized = False\n    # Need to store a reference to sys.exc_info for printing\n    # out exceptions when a thread tries to use a global var. during interp.\n    # shutdown and thus raises an exception about trying to perform some\n    # operation on/with a NoneType\n    __exc_info = _sys.exc_info\n    # Keep sys.exc_clear too to clear the exception just before\n    # allowing .join() to return.\n    __exc_clear = _sys.exc_clear\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, verbose=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is the argument tuple for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n\"\"\"\n        assert group is None, \"group argument must be None for now\"\n        _Verbose.__init__(self, verbose)\n        if kwargs is None:\n            kwargs = {}\n        self.__target = target\n        self.__name = str(name or _newname())\n        self.__args = args\n        self.__kwargs = kwargs\n        self.__daemonic = self._set_daemon()\n        self.__ident = None\n        self.__started = Event()\n        self.__stopped = False\n        self.__block = Condition(Lock())\n        self.__initialized = True\n        # sys.stderr is not stored in the class like\n        # sys.exc_info since it can be changed between instances\n        self.__stderr = _sys.stderr\n\n    def _reset_internal_locks(self):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        if hasattr(self, '_Thread__block'):  # DummyThread deletes self.__block\n            self.__block.__init__()\n        self.__started._reset_internal_locks()\n\n    @property\n    def _block(self):\n        # used by a unittest\n        return self.__block\n\n    def _set_daemon(self):\n        # Overridden in _MainThread and _DummyThread\n        return current_thread().daemon\n\n    def __repr__(self):\n        assert self.__initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self.__started.is_set():\n            status = \"started\"\n        if self.__stopped:\n            status = \"stopped\"\n        if self.__daemonic:\n            status += \" daemon\"\n        if self.__ident is not None:\n            status += \" %s\" % self.__ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self.__name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self.__initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n        if self.__started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n        if __debug__:\n            self._note(\"%s.start(): starting thread\", self)\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self.__bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self.__started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self.__target:\n                self.__target(*self.__args, **self.__kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self.__target, self.__args, self.__kwargs\n\n    def __bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # __bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # __bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self.__bootstrap_inner()\n        except:\n            if self.__daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self.__ident = _get_ident()\n\n    def __bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self.__started.set()\n            with _active_limbo_lock:\n                _active[self.__ident] = self\n                del _limbo[self]\n            if __debug__:\n                self._note(\"%s.__bootstrap(): thread started\", self)\n\n            if _trace_hook:\n                self._note(\"%s.__bootstrap(): registering trace hook\", self)\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                self._note(\"%s.__bootstrap(): registering profile hook\", self)\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except SystemExit:\n                if __debug__:\n                    self._note(\"%s.__bootstrap(): raised SystemExit\", self)\n            except:\n                if __debug__:\n                    self._note(\"%s.__bootstrap(): unhandled exception\", self)\n                # If sys.stderr is no more (most likely from interpreter\n                # shutdown) use self.__stderr.  Otherwise still use sys (as in\n                # _sys) in case sys.stderr was redefined since the creation of\n                # self.\n                if _sys and _sys.stderr is not None:\n                    print>>_sys.stderr, (\"Exception in thread %s:\\n%s\" %\n                                         (self.name, _format_exc()))\n                elif self.__stderr is not None:\n                    # Do the best job possible w/o a huge amt. of code to\n                    # approximate a traceback (code ideas from\n                    # Lib/traceback.py)\n                    exc_type, exc_value, exc_tb = self.__exc_info()\n                    try:\n                        print>>self.__stderr, (\n                            \"Exception in thread \" + self.name +\n                            \" (most likely raised during interpreter shutdown):\")\n                        print>>self.__stderr, (\n                            \"Traceback (most recent call last):\")\n                        while exc_tb:\n                            print>>self.__stderr, (\n                                '  File \"%s\", line %s, in %s' %\n                                (exc_tb.tb_frame.f_code.co_filename,\n                                    exc_tb.tb_lineno,\n                                    exc_tb.tb_frame.f_code.co_name))\n                            exc_tb = exc_tb.tb_next\n                        print>>self.__stderr, (\"%s: %s\" % (exc_type, exc_value))\n                    # Make sure that exc_tb gets deleted since it is a memory\n                    # hog; deleting everything else is just for thoroughness\n                    finally:\n                        del exc_type, exc_value, exc_tb\n            else:\n                if __debug__:\n                    self._note(\"%s.__bootstrap(): normal return\", self)\n            finally:\n                # Prevent a race in\n                # test_threading.test_no_refcycle_through_target when\n                # the exception keeps the target alive past when we\n                # assert that it's dead.\n                self.__exc_clear()\n        finally:\n            with _active_limbo_lock:\n                self.__stop()\n                try:\n                    # We don't call self.__delete() because it also\n                    # grabs _active_limbo_lock.\n                    del _active[_get_ident()]\n                except:\n                    pass\n\n    def __stop(self):\n        # DummyThreads delete self.__block, but they have no waiters to\n        # notify anyway (join() is forbidden on them).\n        if not hasattr(self, '_Thread__block'):\n            return\n        self.__block.acquire()\n        self.__stopped = True\n        self.__block.notify_all()\n        self.__block.release()\n\n    def __delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n\n        # Notes about running with dummy_thread:\n        #\n        # Must take care to not raise an exception if dummy_thread is being\n        # used (and thus this module is being used as an instance of\n        # dummy_threading).  dummy_thread.get_ident() always returns -1 since\n        # there is only one thread if dummy_thread is being used.  Thus\n        # len(_active) is always <= 1 here, and any Thread instance created\n        # overwrites the (if any) thread currently registered in _active.\n        #\n        # An instance of _MainThread is always created by 'threading'.  This\n        # gets overwritten the instant an instance of Thread is created; both\n        # threads return -1 from dummy_thread.get_ident() and thus have the\n        # same key in the dict.  So when the _MainThread instance created by\n        # 'threading' tries to clean itself up when atexit calls this method\n        # it gets a KeyError if another Thread instance was created.\n        #\n        # This all means that KeyError from trying to delete something from\n        # _active if dummy_threading is being used is a red herring.  But\n        # since it isn't if dummy_threading is *not* being used then don't\n        # hide the exception.\n\n        try:\n            with _active_limbo_lock:\n                del _active[_get_ident()]\n                # There must not be any python code between the previous line\n                # and after the lock is released.  Otherwise a tracing function\n                # could try to acquire the lock again in the same thread, (in\n                # current_thread()), and would block.\n        except KeyError:\n            if 'dummy_threading' not in _sys.modules:\n                raise\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        isAlive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self.__initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self.__started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if __debug__:\n            if not self.__stopped:\n                self._note(\"%s.join(): waiting until thread stops\", self)\n        self.__block.acquire()\n        try:\n            if timeout is None:\n                while not self.__stopped:\n                    self.__block.wait()\n                if __debug__:\n                    self._note(\"%s.join(): thread stopped\", self)\n            else:\n                deadline = _time() + timeout\n                while not self.__stopped:\n                    delay = deadline - _time()\n                    if delay <= 0:\n                        if __debug__:\n                            self._note(\"%s.join(): timed out\", self)\n                        break\n                    self.__block.wait(delay)\n                else:\n                    if __debug__:\n                        self._note(\"%s.join(): thread stopped\", self)\n        finally:\n            self.__block.release()\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self.__initialized, \"Thread.__init__() not called\"\n        return self.__name\n\n    @name.setter\n    def name(self, name):\n        assert self.__initialized, \"Thread.__init__() not called\"\n        self.__name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the thread.get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self.__initialized, \"Thread.__init__() not called\"\n        return self.__ident\n\n    def isAlive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. The module function enumerate()\n        returns a list of all alive threads.\n\n        \"\"\"\n        assert self.__initialized, \"Thread.__init__() not called\"\n        return self.__started.is_set() and not self.__stopped\n\n    is_alive = isAlive\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread (True) or not (False).\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when no alive non-daemon threads are\n        left.\n\n        \"\"\"\n        assert self.__initialized, \"Thread.__init__() not called\"\n        return self.__daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self.__initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self.__started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\");\n        self.__daemonic = daemonic\n\n    def isDaemon(self):\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        self.daemon = daemonic\n\n    def getName(self):\n        return self.name\n\n    def setName(self, name):\n        self.name = name\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\ndef Timer(*args, **kwargs):\n    \"\"\"Factory function to create a Timer object.\n\n    Timers call a function after a specified number of seconds:\n\n        t = Timer(30.0, f, args=[], kwargs={})\n        t.start()\n        t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n    return _Timer(*args, **kwargs)\n\nclass _Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=[], kwargs={})\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=[], kwargs={}):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args\n        self.kwargs = kwargs\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n# Special thread class to represent the main thread\n# This is garbage collected through an exit handler\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\")\n        self._Thread__started.set()\n        self._set_ident()\n        with _active_limbo_lock:\n            _active[_get_ident()] = self\n\n    def _set_daemon(self):\n        return False\n\n    def _exitfunc(self):\n        self._Thread__stop()\n        t = _pickSomeNonDaemonThread()\n        if t:\n            if __debug__:\n                self._note(\"%s: waiting for other threads\", self)\n        while t:\n            t.join()\n            t = _pickSomeNonDaemonThread()\n        if __debug__:\n            self._note(\"%s: exiting\", self)\n        self._Thread__delete()\n\ndef _pickSomeNonDaemonThread():\n    for t in enumerate():\n        if not t.daemon and t.is_alive():\n            return t\n    return None\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"))\n\n        # Thread.__block consumes an OS-level locking primitive, which\n        # can never be used by a _DummyThread.  Since a _DummyThread\n        # instance is immortal, that's bad, so release this resource.\n        del self._Thread__block\n\n        self._Thread__started.set()\n        self._set_ident()\n        with _active_limbo_lock:\n            _active[_get_ident()] = self\n\n    def _set_daemon(self):\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef currentThread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[_get_ident()]\n    except KeyError:\n        ##print \"current_thread(): no current thread for\", _get_ident()\n        return _DummyThread()\n\ncurrent_thread = currentThread\n\ndef activeCount():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\nactive_count = activeCount\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return _active.values() + _limbo.values()\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return _active.values() + _limbo.values()\n\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_shutdown = _MainThread()._exitfunc\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\nfrom _threading_local import local\n\n\ndef _after_fork():\n    # This function is called by Python/ceval.c:PyEval_ReInitThreads which\n    # is called from PyOS_AfterFork.  Here we cleanup threading module state\n    # that should not exist after a fork.\n\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock\n    _active_limbo_lock = _allocate_lock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n    current = current_thread()\n    with _active_limbo_lock:\n        for thread in _enumerate():\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if hasattr(thread, '_reset_internal_locks'):\n                thread._reset_internal_locks()\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                ident = _get_ident()\n                thread._Thread__ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._Thread__stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\n# Self-test code\n", 
    "email.__init__": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"A package for parsing, handling, and generating email messages.\"\"\"\n\n__version__ = '4.0.3'\n\n__all__ = [\n    # Old names\n    'base64MIME',\n    'Charset',\n    'Encoders',\n    'Errors',\n    'Generator',\n    'Header',\n    'Iterators',\n    'Message',\n    'MIMEAudio',\n    'MIMEBase',\n    'MIMEImage',\n    'MIMEMessage',\n    'MIMEMultipart',\n    'MIMENonMultipart',\n    'MIMEText',\n    'Parser',\n    'quopriMIME',\n    'Utils',\n    'message_from_string',\n    'message_from_file',\n    # new names\n    'base64mime',\n    'charset',\n    'encoders',\n    'errors',\n    'generator',\n    'header',\n    'iterators',\n    'message',\n    'mime',\n    'parser',\n    'quoprimime',\n    'utils',\n    ]\n\n\n\f\n# Some convenience routines.  Don't import Parser and Message as side-effects\n# of importing email since those cascadingly import most of the rest of the\n# email package.\ndef message_from_string(s, *args, **kws):\n    \"\"\"Parse a string into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from email.parser import Parser\n    return Parser(*args, **kws).parsestr(s)\n\n\ndef message_from_file(fp, *args, **kws):\n    \"\"\"Read a file and parse its contents into a Message object model.\n\n    Optional _class and strict are passed to the Parser constructor.\n    \"\"\"\n    from email.parser import Parser\n    return Parser(*args, **kws).parse(fp)\n\n\n\f\n# Lazy loading to provide name mapping from new-style names (PEP 8 compatible\n# email 4.0 module names), to old-style names (email 3.0 module names).\nimport sys\n\nclass LazyImporter(object):\n    def __init__(self, module_name):\n        self.__name__ = 'email.' + module_name\n\n    def __getattr__(self, name):\n        __import__(self.__name__)\n        mod = sys.modules[self.__name__]\n        self.__dict__.update(mod.__dict__)\n        return getattr(mod, name)\n\n\n_LOWERNAMES = [\n    # email.<old name> -> email.<new name is lowercased old name>\n    'Charset',\n    'Encoders',\n    'Errors',\n    'FeedParser',\n    'Generator',\n    'Header',\n    'Iterators',\n    'Message',\n    'Parser',\n    'Utils',\n    'base64MIME',\n    'quopriMIME',\n    ]\n\n_MIMENAMES = [\n    # email.MIME<old name> -> email.mime.<new name is lowercased old name>\n    'Audio',\n    'Base',\n    'Image',\n    'Message',\n    'Multipart',\n    'NonMultipart',\n    'Text',\n    ]\n\nfor _name in _LOWERNAMES:\n    importer = LazyImporter(_name.lower())\n    sys.modules['email.' + _name] = importer\n    setattr(sys.modules['email'], _name, importer)\n\n\nimport email.mime\nfor _name in _MIMENAMES:\n    importer = LazyImporter('mime.' + _name.lower())\n    sys.modules['email.MIME' + _name] = importer\n    setattr(sys.modules['email'], 'MIME' + _name, importer)\n    setattr(sys.modules['email.mime'], _name, importer)\n", 
    "email._parseaddr": "# Copyright (C) 2002-2007 Python Software Foundation\n# Contact: email-sig@python.org\n\n\"\"\"Email address parsing code.\n\nLifted directly from rfc822.py.  This should eventually be rewritten.\n\"\"\"\n\n__all__ = [\n    'mktime_tz',\n    'parsedate',\n    'parsedate_tz',\n    'quote',\n    ]\n\nimport time, calendar\n\nSPACE = ' '\nEMPTYSTRING = ''\nCOMMASPACE = ', '\n\n# Parse a date field\n_monthnames = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul',\n               'aug', 'sep', 'oct', 'nov', 'dec',\n               'january', 'february', 'march', 'april', 'may', 'june', 'july',\n               'august', 'september', 'october', 'november', 'december']\n\n_daynames = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n\n# The timezone table does not include the military time zones defined\n# in RFC822, other than Z.  According to RFC1123, the description in\n# RFC822 gets the signs wrong, so we can't rely on any such time\n# zones.  RFC1123 recommends that numeric timezone indicators be used\n# instead of timezone names.\n\n_timezones = {'UT':0, 'UTC':0, 'GMT':0, 'Z':0,\n              'AST': -400, 'ADT': -300,  # Atlantic (used in Canada)\n              'EST': -500, 'EDT': -400,  # Eastern\n              'CST': -600, 'CDT': -500,  # Central\n              'MST': -700, 'MDT': -600,  # Mountain\n              'PST': -800, 'PDT': -700   # Pacific\n              }\n\n\ndef parsedate_tz(data):\n    \"\"\"Convert a date string to a time tuple.\n\n    Accounts for military timezones.\n    \"\"\"\n    data = data.split()\n    # The FWS after the comma after the day-of-week is optional, so search and\n    # adjust for this.\n    if data[0].endswith(',') or data[0].lower() in _daynames:\n        # There's a dayname here. Skip it\n        del data[0]\n    else:\n        i = data[0].rfind(',')\n        if i >= 0:\n            data[0] = data[0][i+1:]\n    if len(data) == 3: # RFC 850 date, deprecated\n        stuff = data[0].split('-')\n        if len(stuff) == 3:\n            data = stuff + data[1:]\n    if len(data) == 4:\n        s = data[3]\n        i = s.find('+')\n        if i > 0:\n            data[3:] = [s[:i], s[i+1:]]\n        else:\n            data.append('') # Dummy tz\n    if len(data) < 5:\n        return None\n    data = data[:5]\n    [dd, mm, yy, tm, tz] = data\n    mm = mm.lower()\n    if mm not in _monthnames:\n        dd, mm = mm, dd.lower()\n        if mm not in _monthnames:\n            return None\n    mm = _monthnames.index(mm) + 1\n    if mm > 12:\n        mm -= 12\n    if dd[-1] == ',':\n        dd = dd[:-1]\n    i = yy.find(':')\n    if i > 0:\n        yy, tm = tm, yy\n    if yy[-1] == ',':\n        yy = yy[:-1]\n    if not yy[0].isdigit():\n        yy, tz = tz, yy\n    if tm[-1] == ',':\n        tm = tm[:-1]\n    tm = tm.split(':')\n    if len(tm) == 2:\n        [thh, tmm] = tm\n        tss = '0'\n    elif len(tm) == 3:\n        [thh, tmm, tss] = tm\n    else:\n        return None\n    try:\n        yy = int(yy)\n        dd = int(dd)\n        thh = int(thh)\n        tmm = int(tmm)\n        tss = int(tss)\n    except ValueError:\n        return None\n    # Check for a yy specified in two-digit format, then convert it to the\n    # appropriate four-digit format, according to the POSIX standard. RFC 822\n    # calls for a two-digit yy, but RFC 2822 (which obsoletes RFC 822)\n    # mandates a 4-digit yy. For more information, see the documentation for\n    # the time module.\n    if yy < 100:\n        # The year is between 1969 and 1999 (inclusive).\n        if yy > 68:\n            yy += 1900\n        # The year is between 2000 and 2068 (inclusive).\n        else:\n            yy += 2000\n    tzoffset = None\n    tz = tz.upper()\n    if tz in _timezones:\n        tzoffset = _timezones[tz]\n    else:\n        try:\n            tzoffset = int(tz)\n        except ValueError:\n            pass\n    # Convert a timezone offset into seconds ; -0500 -> -18000\n    if tzoffset:\n        if tzoffset < 0:\n            tzsign = -1\n            tzoffset = -tzoffset\n        else:\n            tzsign = 1\n        tzoffset = tzsign * ( (tzoffset//100)*3600 + (tzoffset % 100)*60)\n    # Daylight Saving Time flag is set to -1, since DST is unknown.\n    return yy, mm, dd, thh, tmm, tss, 0, 1, -1, tzoffset\n\n\ndef parsedate(data):\n    \"\"\"Convert a time string to a time tuple.\"\"\"\n    t = parsedate_tz(data)\n    if isinstance(t, tuple):\n        return t[:9]\n    else:\n        return t\n\n\ndef mktime_tz(data):\n    \"\"\"Turn a 10-tuple as returned by parsedate_tz() into a POSIX timestamp.\"\"\"\n    if data[9] is None:\n        # No zone info, so localtime is better assumption than GMT\n        return time.mktime(data[:8] + (-1,))\n    else:\n        t = calendar.timegm(data)\n        return t - data[9]\n\n\ndef quote(str):\n    \"\"\"Prepare string to be used in a quoted string.\n\n    Turns backslash and double quote characters into quoted pairs.  These\n    are the only characters that need to be quoted inside a quoted string.\n    Does not add the surrounding double quotes.\n    \"\"\"\n    return str.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n\n\nclass AddrlistClass:\n    \"\"\"Address parser class by Ben Escoto.\n\n    To understand what this class does, it helps to have a copy of RFC 2822 in\n    front of you.\n\n    Note: this class interface is deprecated and may be removed in the future.\n    Use rfc822.AddressList instead.\n    \"\"\"\n\n    def __init__(self, field):\n        \"\"\"Initialize a new instance.\n\n        `field' is an unparsed address header field, containing\n        one or more addresses.\n        \"\"\"\n        self.specials = '()<>@,:;.\\\"[]'\n        self.pos = 0\n        self.LWS = ' \\t'\n        self.CR = '\\r\\n'\n        self.FWS = self.LWS + self.CR\n        self.atomends = self.specials + self.LWS + self.CR\n        # Note that RFC 2822 now specifies `.' as obs-phrase, meaning that it\n        # is obsolete syntax.  RFC 2822 requires that we recognize obsolete\n        # syntax, so allow dots in phrases.\n        self.phraseends = self.atomends.replace('.', '')\n        self.field = field\n        self.commentlist = []\n\n    def gotonext(self):\n        \"\"\"Parse up to the start of the next address.\"\"\"\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS + '\\n\\r':\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            else:\n                break\n\n    def getaddrlist(self):\n        \"\"\"Parse all addresses.\n\n        Returns a list containing all of the addresses.\n        \"\"\"\n        result = []\n        while self.pos < len(self.field):\n            ad = self.getaddress()\n            if ad:\n                result += ad\n            else:\n                result.append(('', ''))\n        return result\n\n    def getaddress(self):\n        \"\"\"Parse the next address.\"\"\"\n        self.commentlist = []\n        self.gotonext()\n\n        oldpos = self.pos\n        oldcl = self.commentlist\n        plist = self.getphraselist()\n\n        self.gotonext()\n        returnlist = []\n\n        if self.pos >= len(self.field):\n            # Bad email address technically, no domain.\n            if plist:\n                returnlist = [(SPACE.join(self.commentlist), plist[0])]\n\n        elif self.field[self.pos] in '.@':\n            # email address is just an addrspec\n            # this isn't very efficient since we start over\n            self.pos = oldpos\n            self.commentlist = oldcl\n            addrspec = self.getaddrspec()\n            returnlist = [(SPACE.join(self.commentlist), addrspec)]\n\n        elif self.field[self.pos] == ':':\n            # address is a group\n            returnlist = []\n\n            fieldlen = len(self.field)\n            self.pos += 1\n            while self.pos < len(self.field):\n                self.gotonext()\n                if self.pos < fieldlen and self.field[self.pos] == ';':\n                    self.pos += 1\n                    break\n                returnlist = returnlist + self.getaddress()\n\n        elif self.field[self.pos] == '<':\n            # Address is a phrase then a route addr\n            routeaddr = self.getrouteaddr()\n\n            if self.commentlist:\n                returnlist = [(SPACE.join(plist) + ' (' +\n                               ' '.join(self.commentlist) + ')', routeaddr)]\n            else:\n                returnlist = [(SPACE.join(plist), routeaddr)]\n\n        else:\n            if plist:\n                returnlist = [(SPACE.join(self.commentlist), plist[0])]\n            elif self.field[self.pos] in self.specials:\n                self.pos += 1\n\n        self.gotonext()\n        if self.pos < len(self.field) and self.field[self.pos] == ',':\n            self.pos += 1\n        return returnlist\n\n    def getrouteaddr(self):\n        \"\"\"Parse a route address (Return-path value).\n\n        This method just skips all the route stuff and returns the addrspec.\n        \"\"\"\n        if self.field[self.pos] != '<':\n            return\n\n        expectroute = False\n        self.pos += 1\n        self.gotonext()\n        adlist = ''\n        while self.pos < len(self.field):\n            if expectroute:\n                self.getdomain()\n                expectroute = False\n            elif self.field[self.pos] == '>':\n                self.pos += 1\n                break\n            elif self.field[self.pos] == '@':\n                self.pos += 1\n                expectroute = True\n            elif self.field[self.pos] == ':':\n                self.pos += 1\n            else:\n                adlist = self.getaddrspec()\n                self.pos += 1\n                break\n            self.gotonext()\n\n        return adlist\n\n    def getaddrspec(self):\n        \"\"\"Parse an RFC 2822 addr-spec.\"\"\"\n        aslist = []\n\n        self.gotonext()\n        while self.pos < len(self.field):\n            if self.field[self.pos] == '.':\n                aslist.append('.')\n                self.pos += 1\n            elif self.field[self.pos] == '\"':\n                aslist.append('\"%s\"' % quote(self.getquote()))\n            elif self.field[self.pos] in self.atomends:\n                break\n            else:\n                aslist.append(self.getatom())\n            self.gotonext()\n\n        if self.pos >= len(self.field) or self.field[self.pos] != '@':\n            return EMPTYSTRING.join(aslist)\n\n        aslist.append('@')\n        self.pos += 1\n        self.gotonext()\n        return EMPTYSTRING.join(aslist) + self.getdomain()\n\n    def getdomain(self):\n        \"\"\"Get the complete domain name from an address.\"\"\"\n        sdlist = []\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS:\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] == '[':\n                sdlist.append(self.getdomainliteral())\n            elif self.field[self.pos] == '.':\n                self.pos += 1\n                sdlist.append('.')\n            elif self.field[self.pos] in self.atomends:\n                break\n            else:\n                sdlist.append(self.getatom())\n        return EMPTYSTRING.join(sdlist)\n\n    def getdelimited(self, beginchar, endchars, allowcomments=True):\n        \"\"\"Parse a header fragment delimited by special characters.\n\n        `beginchar' is the start character for the fragment.\n        If self is not looking at an instance of `beginchar' then\n        getdelimited returns the empty string.\n\n        `endchars' is a sequence of allowable end-delimiting characters.\n        Parsing stops when one of these is encountered.\n\n        If `allowcomments' is non-zero, embedded RFC 2822 comments are allowed\n        within the parsed fragment.\n        \"\"\"\n        if self.field[self.pos] != beginchar:\n            return ''\n\n        slist = ['']\n        quote = False\n        self.pos += 1\n        while self.pos < len(self.field):\n            if quote:\n                slist.append(self.field[self.pos])\n                quote = False\n            elif self.field[self.pos] in endchars:\n                self.pos += 1\n                break\n            elif allowcomments and self.field[self.pos] == '(':\n                slist.append(self.getcomment())\n                continue        # have already advanced pos from getcomment\n            elif self.field[self.pos] == '\\\\':\n                quote = True\n            else:\n                slist.append(self.field[self.pos])\n            self.pos += 1\n\n        return EMPTYSTRING.join(slist)\n\n    def getquote(self):\n        \"\"\"Get a quote-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('\"', '\"\\r', False)\n\n    def getcomment(self):\n        \"\"\"Get a parenthesis-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('(', ')\\r', True)\n\n    def getdomainliteral(self):\n        \"\"\"Parse an RFC 2822 domain-literal.\"\"\"\n        return '[%s]' % self.getdelimited('[', ']\\r', False)\n\n    def getatom(self, atomends=None):\n        \"\"\"Parse an RFC 2822 atom.\n\n        Optional atomends specifies a different set of end token delimiters\n        (the default is to use self.atomends).  This is used e.g. in\n        getphraselist() since phrase endings must not include the `.' (which\n        is legal in phrases).\"\"\"\n        atomlist = ['']\n        if atomends is None:\n            atomends = self.atomends\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in atomends:\n                break\n            else:\n                atomlist.append(self.field[self.pos])\n            self.pos += 1\n\n        return EMPTYSTRING.join(atomlist)\n\n    def getphraselist(self):\n        \"\"\"Parse a sequence of RFC 2822 phrases.\n\n        A phrase is a sequence of words, which are in turn either RFC 2822\n        atoms or quoted-strings.  Phrases are canonicalized by squeezing all\n        runs of continuous whitespace into one space.\n        \"\"\"\n        plist = []\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.FWS:\n                self.pos += 1\n            elif self.field[self.pos] == '\"':\n                plist.append(self.getquote())\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] in self.phraseends:\n                break\n            else:\n                plist.append(self.getatom(self.phraseends))\n\n        return plist\n\nclass AddressList(AddrlistClass):\n    \"\"\"An AddressList encapsulates a list of parsed RFC 2822 addresses.\"\"\"\n    def __init__(self, field):\n        AddrlistClass.__init__(self, field)\n        if field:\n            self.addresslist = self.getaddrlist()\n        else:\n            self.addresslist = []\n\n    def __len__(self):\n        return len(self.addresslist)\n\n    def __add__(self, other):\n        # Set union\n        newaddr = AddressList(None)\n        newaddr.addresslist = self.addresslist[:]\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __iadd__(self, other):\n        # Set union, in-place\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                self.addresslist.append(x)\n        return self\n\n    def __sub__(self, other):\n        # Set difference\n        newaddr = AddressList(None)\n        for x in self.addresslist:\n            if not x in other.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __isub__(self, other):\n        # Set difference, in-place\n        for x in other.addresslist:\n            if x in self.addresslist:\n                self.addresslist.remove(x)\n        return self\n\n    def __getitem__(self, index):\n        # Make indexing, slices, and 'in' work\n        return self.addresslist[index]\n", 
    "email.base64mime": "# Copyright (C) 2002-2006 Python Software Foundation\n# Author: Ben Gertzfield\n# Contact: email-sig@python.org\n\n\"\"\"Base64 content transfer encoding per RFCs 2045-2047.\n\nThis module handles the content transfer encoding method defined in RFC 2045\nto encode arbitrary 8-bit data using the three 8-bit bytes in four 7-bit\ncharacters encoding known as Base64.\n\nIt is used in the MIME standards for email to attach images, audio, and text\nusing some 8-bit character sets to messages.\n\nThis module provides an interface to encode and decode both headers and bodies\nwith Base64 encoding.\n\nRFC 2045 defines a method for including character set information in an\n`encoded-word' in a header.  This method is commonly used for 8-bit real names\nin To:, From:, Cc:, etc. fields, as well as Subject: lines.\n\nThis module does not do the line wrapping or end-of-line character conversion\nnecessary for proper internationalized headers; it only does dumb encoding and\ndecoding.  To deal with the various line wrapping issues, use the email.header\nmodule.\n\"\"\"\n\n__all__ = [\n    'base64_len',\n    'body_decode',\n    'body_encode',\n    'decode',\n    'decodestring',\n    'encode',\n    'encodestring',\n    'header_encode',\n    ]\n\n\nfrom binascii import b2a_base64, a2b_base64\nfrom email.utils import fix_eols\n\nCRLF = '\\r\\n'\nNL = '\\n'\nEMPTYSTRING = ''\n\n# See also Charset.py\nMISC_LEN = 7\n\n\n\f\n# Helpers\ndef base64_len(s):\n    \"\"\"Return the length of s when it is encoded with base64.\"\"\"\n    groups_of_3, leftover = divmod(len(s), 3)\n    # 4 bytes out for each 3 bytes (or nonzero fraction thereof) in.\n    # Thanks, Tim!\n    n = groups_of_3 * 4\n    if leftover:\n        n += 4\n    return n\n\n\n\f\ndef header_encode(header, charset='iso-8859-1', keep_eols=False,\n                  maxlinelen=76, eol=NL):\n    \"\"\"Encode a single header line with Base64 encoding in a given charset.\n\n    Defined in RFC 2045, this Base64 encoding is identical to normal Base64\n    encoding, except that each line must be intelligently wrapped (respecting\n    the Base64 encoding), and subsequent lines must start with a space.\n\n    charset names the character set to use to encode the header.  It defaults\n    to iso-8859-1.\n\n    End-of-line characters (\\\\r, \\\\n, \\\\r\\\\n) will be automatically converted\n    to the canonical email line separator \\\\r\\\\n unless the keep_eols\n    parameter is True (the default is False).\n\n    Each line of the header will be terminated in the value of eol, which\n    defaults to \"\\\\n\".  Set this to \"\\\\r\\\\n\" if you are using the result of\n    this function directly in email.\n\n    The resulting string will be in the form:\n\n    \"=?charset?b?WW/5ciBtYXp66XLrIHf8eiBhIGhhbXBzdGHuciBBIFlv+XIgbWF6euly?=\\\\n\n      =?charset?b?6yB3/HogYSBoYW1wc3Rh7nIgQkMgWW/5ciBtYXp66XLrIHf8eiBhIGhh?=\"\n\n    with each line wrapped at, at most, maxlinelen characters (defaults to 76\n    characters).\n    \"\"\"\n    # Return empty headers unchanged\n    if not header:\n        return header\n\n    if not keep_eols:\n        header = fix_eols(header)\n\n    # Base64 encode each line, in encoded chunks no greater than maxlinelen in\n    # length, after the RFC chrome is added in.\n    base64ed = []\n    max_encoded = maxlinelen - len(charset) - MISC_LEN\n    max_unencoded = max_encoded * 3 // 4\n\n    for i in range(0, len(header), max_unencoded):\n        base64ed.append(b2a_base64(header[i:i+max_unencoded]))\n\n    # Now add the RFC chrome to each encoded chunk\n    lines = []\n    for line in base64ed:\n        # Ignore the last character of each line if it is a newline\n        if line.endswith(NL):\n            line = line[:-1]\n        # Add the chrome\n        lines.append('=?%s?b?%s?=' % (charset, line))\n    # Glue the lines together and return it.  BAW: should we be able to\n    # specify the leading whitespace in the joiner?\n    joiner = eol + ' '\n    return joiner.join(lines)\n\n\n\f\ndef encode(s, binary=True, maxlinelen=76, eol=NL):\n    \"\"\"Encode a string with base64.\n\n    Each line will be wrapped at, at most, maxlinelen characters (defaults to\n    76 characters).\n\n    If binary is False, end-of-line characters will be converted to the\n    canonical email end-of-line sequence \\\\r\\\\n.  Otherwise they will be left\n    verbatim (this is the default).\n\n    Each line of encoded text will end with eol, which defaults to \"\\\\n\".  Set\n    this to \"\\\\r\\\\n\" if you will be using the result of this function directly\n    in an email.\n    \"\"\"\n    if not s:\n        return s\n\n    if not binary:\n        s = fix_eols(s)\n\n    encvec = []\n    max_unencoded = maxlinelen * 3 // 4\n    for i in range(0, len(s), max_unencoded):\n        # BAW: should encode() inherit b2a_base64()'s dubious behavior in\n        # adding a newline to the encoded string?\n        enc = b2a_base64(s[i:i + max_unencoded])\n        if enc.endswith(NL) and eol != NL:\n            enc = enc[:-1] + eol\n        encvec.append(enc)\n    return EMPTYSTRING.join(encvec)\n\n\n# For convenience and backwards compatibility w/ standard base64 module\nbody_encode = encode\nencodestring = encode\n\n\n\f\ndef decode(s, convert_eols=None):\n    \"\"\"Decode a raw base64 string.\n\n    If convert_eols is set to a string value, all canonical email linefeeds,\n    e.g. \"\\\\r\\\\n\", in the decoded text will be converted to the value of\n    convert_eols.  os.linesep is a good choice for convert_eols if you are\n    decoding a text attachment.\n\n    This function does not parse a full MIME header value encoded with\n    base64 (like =?iso-8895-1?b?bmloISBuaWgh?=) -- please use the high\n    level email.header class for that functionality.\n    \"\"\"\n    if not s:\n        return s\n\n    dec = a2b_base64(s)\n    if convert_eols:\n        return dec.replace(CRLF, convert_eols)\n    return dec\n\n\n# For convenience and backwards compatibility w/ standard base64 module\nbody_decode = decode\ndecodestring = decode\n", 
    "email.charset": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Ben Gertzfield, Barry Warsaw\n# Contact: email-sig@python.org\n\n__all__ = [\n    'Charset',\n    'add_alias',\n    'add_charset',\n    'add_codec',\n    ]\n\nimport codecs\nimport email.base64mime\nimport email.quoprimime\n\nfrom email import errors\nfrom email.encoders import encode_7or8bit\n\n\n\f\n# Flags for types of header encodings\nQP          = 1 # Quoted-Printable\nBASE64      = 2 # Base64\nSHORTEST    = 3 # the shorter of QP and base64, but only for headers\n\n# In \"=?charset?q?hello_world?=\", the =?, ?q?, and ?= add up to 7\nMISC_LEN = 7\n\nDEFAULT_CHARSET = 'us-ascii'\n\n\n\f\n# Defaults\nCHARSETS = {\n    # input        header enc  body enc output conv\n    'iso-8859-1':  (QP,        QP,      None),\n    'iso-8859-2':  (QP,        QP,      None),\n    'iso-8859-3':  (QP,        QP,      None),\n    'iso-8859-4':  (QP,        QP,      None),\n    # iso-8859-5 is Cyrillic, and not especially used\n    # iso-8859-6 is Arabic, also not particularly used\n    # iso-8859-7 is Greek, QP will not make it readable\n    # iso-8859-8 is Hebrew, QP will not make it readable\n    'iso-8859-9':  (QP,        QP,      None),\n    'iso-8859-10': (QP,        QP,      None),\n    # iso-8859-11 is Thai, QP will not make it readable\n    'iso-8859-13': (QP,        QP,      None),\n    'iso-8859-14': (QP,        QP,      None),\n    'iso-8859-15': (QP,        QP,      None),\n    'iso-8859-16': (QP,        QP,      None),\n    'windows-1252':(QP,        QP,      None),\n    'viscii':      (QP,        QP,      None),\n    'us-ascii':    (None,      None,    None),\n    'big5':        (BASE64,    BASE64,  None),\n    'gb2312':      (BASE64,    BASE64,  None),\n    'euc-jp':      (BASE64,    None,    'iso-2022-jp'),\n    'shift_jis':   (BASE64,    None,    'iso-2022-jp'),\n    'iso-2022-jp': (BASE64,    None,    None),\n    'koi8-r':      (BASE64,    BASE64,  None),\n    'utf-8':       (SHORTEST,  BASE64, 'utf-8'),\n    # We're making this one up to represent raw unencoded 8-bit\n    '8bit':        (None,      BASE64, 'utf-8'),\n    }\n\n# Aliases for other commonly-used names for character sets.  Map\n# them to the real ones used in email.\nALIASES = {\n    'latin_1': 'iso-8859-1',\n    'latin-1': 'iso-8859-1',\n    'latin_2': 'iso-8859-2',\n    'latin-2': 'iso-8859-2',\n    'latin_3': 'iso-8859-3',\n    'latin-3': 'iso-8859-3',\n    'latin_4': 'iso-8859-4',\n    'latin-4': 'iso-8859-4',\n    'latin_5': 'iso-8859-9',\n    'latin-5': 'iso-8859-9',\n    'latin_6': 'iso-8859-10',\n    'latin-6': 'iso-8859-10',\n    'latin_7': 'iso-8859-13',\n    'latin-7': 'iso-8859-13',\n    'latin_8': 'iso-8859-14',\n    'latin-8': 'iso-8859-14',\n    'latin_9': 'iso-8859-15',\n    'latin-9': 'iso-8859-15',\n    'latin_10':'iso-8859-16',\n    'latin-10':'iso-8859-16',\n    'cp949':   'ks_c_5601-1987',\n    'euc_jp':  'euc-jp',\n    'euc_kr':  'euc-kr',\n    'ascii':   'us-ascii',\n    }\n\n\n# Map charsets to their Unicode codec strings.\nCODEC_MAP = {\n    'gb2312':      'eucgb2312_cn',\n    'big5':        'big5_tw',\n    # Hack: We don't want *any* conversion for stuff marked us-ascii, as all\n    # sorts of garbage might be sent to us in the guise of 7-bit us-ascii.\n    # Let that stuff pass through without conversion to/from Unicode.\n    'us-ascii':    None,\n    }\n\n\n\f\n# Convenience functions for extending the above mappings\ndef add_charset(charset, header_enc=None, body_enc=None, output_charset=None):\n    \"\"\"Add character set properties to the global registry.\n\n    charset is the input character set, and must be the canonical name of a\n    character set.\n\n    Optional header_enc and body_enc is either Charset.QP for\n    quoted-printable, Charset.BASE64 for base64 encoding, Charset.SHORTEST for\n    the shortest of qp or base64 encoding, or None for no encoding.  SHORTEST\n    is only valid for header_enc.  It describes how message headers and\n    message bodies in the input charset are to be encoded.  Default is no\n    encoding.\n\n    Optional output_charset is the character set that the output should be\n    in.  Conversions will proceed from input charset, to Unicode, to the\n    output charset when the method Charset.convert() is called.  The default\n    is to output in the same character set as the input.\n\n    Both input_charset and output_charset must have Unicode codec entries in\n    the module's charset-to-codec mapping; use add_codec(charset, codecname)\n    to add codecs the module does not know about.  See the codecs module's\n    documentation for more information.\n    \"\"\"\n    if body_enc == SHORTEST:\n        raise ValueError('SHORTEST not allowed for body_enc')\n    CHARSETS[charset] = (header_enc, body_enc, output_charset)\n\n\ndef add_alias(alias, canonical):\n    \"\"\"Add a character set alias.\n\n    alias is the alias name, e.g. latin-1\n    canonical is the character set's canonical name, e.g. iso-8859-1\n    \"\"\"\n    ALIASES[alias] = canonical\n\n\ndef add_codec(charset, codecname):\n    \"\"\"Add a codec that map characters in the given charset to/from Unicode.\n\n    charset is the canonical name of a character set.  codecname is the name\n    of a Python codec, as appropriate for the second argument to the unicode()\n    built-in, or to the encode() method of a Unicode string.\n    \"\"\"\n    CODEC_MAP[charset] = codecname\n\n\n\f\nclass Charset:\n    \"\"\"Map character sets to their email properties.\n\n    This class provides information about the requirements imposed on email\n    for a specific character set.  It also provides convenience routines for\n    converting between character sets, given the availability of the\n    applicable codecs.  Given a character set, it will do its best to provide\n    information on how to use that character set in an email in an\n    RFC-compliant way.\n\n    Certain character sets must be encoded with quoted-printable or base64\n    when used in email headers or bodies.  Certain character sets must be\n    converted outright, and are not allowed in email.  Instances of this\n    module expose the following information about a character set:\n\n    input_charset: The initial character set specified.  Common aliases\n                   are converted to their `official' email names (e.g. latin_1\n                   is converted to iso-8859-1).  Defaults to 7-bit us-ascii.\n\n    header_encoding: If the character set must be encoded before it can be\n                     used in an email header, this attribute will be set to\n                     Charset.QP (for quoted-printable), Charset.BASE64 (for\n                     base64 encoding), or Charset.SHORTEST for the shortest of\n                     QP or BASE64 encoding.  Otherwise, it will be None.\n\n    body_encoding: Same as header_encoding, but describes the encoding for the\n                   mail message's body, which indeed may be different than the\n                   header encoding.  Charset.SHORTEST is not allowed for\n                   body_encoding.\n\n    output_charset: Some character sets must be converted before they can be\n                    used in email headers or bodies.  If the input_charset is\n                    one of them, this attribute will contain the name of the\n                    charset output will be converted to.  Otherwise, it will\n                    be None.\n\n    input_codec: The name of the Python codec used to convert the\n                 input_charset to Unicode.  If no conversion codec is\n                 necessary, this attribute will be None.\n\n    output_codec: The name of the Python codec used to convert Unicode\n                  to the output_charset.  If no conversion codec is necessary,\n                  this attribute will have the same value as the input_codec.\n    \"\"\"\n    def __init__(self, input_charset=DEFAULT_CHARSET):\n        # RFC 2046, $4.1.2 says charsets are not case sensitive.  We coerce to\n        # unicode because its .lower() is locale insensitive.  If the argument\n        # is already a unicode, we leave it at that, but ensure that the\n        # charset is ASCII, as the standard (RFC XXX) requires.\n        try:\n            if isinstance(input_charset, unicode):\n                input_charset.encode('ascii')\n            else:\n                input_charset = unicode(input_charset, 'ascii')\n        except UnicodeError:\n            raise errors.CharsetError(input_charset)\n        input_charset = input_charset.lower().encode('ascii')\n        # Set the input charset after filtering through the aliases and/or codecs\n        if not (input_charset in ALIASES or input_charset in CHARSETS):\n            try:\n                input_charset = codecs.lookup(input_charset).name\n            except LookupError:\n                pass\n        self.input_charset = ALIASES.get(input_charset, input_charset)\n        # We can try to guess which encoding and conversion to use by the\n        # charset_map dictionary.  Try that first, but let the user override\n        # it.\n        henc, benc, conv = CHARSETS.get(self.input_charset,\n                                        (SHORTEST, BASE64, None))\n        if not conv:\n            conv = self.input_charset\n        # Set the attributes, allowing the arguments to override the default.\n        self.header_encoding = henc\n        self.body_encoding = benc\n        self.output_charset = ALIASES.get(conv, conv)\n        # Now set the codecs.  If one isn't defined for input_charset,\n        # guess and try a Unicode codec with the same name as input_codec.\n        self.input_codec = CODEC_MAP.get(self.input_charset,\n                                         self.input_charset)\n        self.output_codec = CODEC_MAP.get(self.output_charset,\n                                          self.output_charset)\n\n    def __str__(self):\n        return self.input_charset.lower()\n\n    __repr__ = __str__\n\n    def __eq__(self, other):\n        return str(self) == str(other).lower()\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def get_body_encoding(self):\n        \"\"\"Return the content-transfer-encoding used for body encoding.\n\n        This is either the string `quoted-printable' or `base64' depending on\n        the encoding used, or it is a function in which case you should call\n        the function with a single argument, the Message object being\n        encoded.  The function should then set the Content-Transfer-Encoding\n        header itself to whatever is appropriate.\n\n        Returns \"quoted-printable\" if self.body_encoding is QP.\n        Returns \"base64\" if self.body_encoding is BASE64.\n        Returns \"7bit\" otherwise.\n        \"\"\"\n        assert self.body_encoding != SHORTEST\n        if self.body_encoding == QP:\n            return 'quoted-printable'\n        elif self.body_encoding == BASE64:\n            return 'base64'\n        else:\n            return encode_7or8bit\n\n    def convert(self, s):\n        \"\"\"Convert a string from the input_codec to the output_codec.\"\"\"\n        if self.input_codec != self.output_codec:\n            return unicode(s, self.input_codec).encode(self.output_codec)\n        else:\n            return s\n\n    def to_splittable(self, s):\n        \"\"\"Convert a possibly multibyte string to a safely splittable format.\n\n        Uses the input_codec to try and convert the string to Unicode, so it\n        can be safely split on character boundaries (even for multibyte\n        characters).\n\n        Returns the string as-is if it isn't known how to convert it to\n        Unicode with the input_charset.\n\n        Characters that could not be converted to Unicode will be replaced\n        with the Unicode replacement character U+FFFD.\n        \"\"\"\n        if isinstance(s, unicode) or self.input_codec is None:\n            return s\n        try:\n            return unicode(s, self.input_codec, 'replace')\n        except LookupError:\n            # Input codec not installed on system, so return the original\n            # string unchanged.\n            return s\n\n    def from_splittable(self, ustr, to_output=True):\n        \"\"\"Convert a splittable string back into an encoded string.\n\n        Uses the proper codec to try and convert the string from Unicode back\n        into an encoded format.  Return the string as-is if it is not Unicode,\n        or if it could not be converted from Unicode.\n\n        Characters that could not be converted from Unicode will be replaced\n        with an appropriate character (usually '?').\n\n        If to_output is True (the default), uses output_codec to convert to an\n        encoded format.  If to_output is False, uses input_codec.\n        \"\"\"\n        if to_output:\n            codec = self.output_codec\n        else:\n            codec = self.input_codec\n        if not isinstance(ustr, unicode) or codec is None:\n            return ustr\n        try:\n            return ustr.encode(codec, 'replace')\n        except LookupError:\n            # Output codec not installed\n            return ustr\n\n    def get_output_charset(self):\n        \"\"\"Return the output character set.\n\n        This is self.output_charset if that is not None, otherwise it is\n        self.input_charset.\n        \"\"\"\n        return self.output_charset or self.input_charset\n\n    def encoded_header_len(self, s):\n        \"\"\"Return the length of the encoded header string.\"\"\"\n        cset = self.get_output_charset()\n        # The len(s) of a 7bit encoding is len(s)\n        if self.header_encoding == BASE64:\n            return email.base64mime.base64_len(s) + len(cset) + MISC_LEN\n        elif self.header_encoding == QP:\n            return email.quoprimime.header_quopri_len(s) + len(cset) + MISC_LEN\n        elif self.header_encoding == SHORTEST:\n            lenb64 = email.base64mime.base64_len(s)\n            lenqp = email.quoprimime.header_quopri_len(s)\n            return min(lenb64, lenqp) + len(cset) + MISC_LEN\n        else:\n            return len(s)\n\n    def header_encode(self, s, convert=False):\n        \"\"\"Header-encode a string, optionally converting it to output_charset.\n\n        If convert is True, the string will be converted from the input\n        charset to the output charset automatically.  This is not useful for\n        multibyte character sets, which have line length issues (multibyte\n        characters must be split on a character, not a byte boundary); use the\n        high-level Header class to deal with these issues.  convert defaults\n        to False.\n\n        The type of encoding (base64 or quoted-printable) will be based on\n        self.header_encoding.\n        \"\"\"\n        cset = self.get_output_charset()\n        if convert:\n            s = self.convert(s)\n        # 7bit/8bit encodings return the string unchanged (modulo conversions)\n        if self.header_encoding == BASE64:\n            return email.base64mime.header_encode(s, cset)\n        elif self.header_encoding == QP:\n            return email.quoprimime.header_encode(s, cset, maxlinelen=None)\n        elif self.header_encoding == SHORTEST:\n            lenb64 = email.base64mime.base64_len(s)\n            lenqp = email.quoprimime.header_quopri_len(s)\n            if lenb64 < lenqp:\n                return email.base64mime.header_encode(s, cset)\n            else:\n                return email.quoprimime.header_encode(s, cset, maxlinelen=None)\n        else:\n            return s\n\n    def body_encode(self, s, convert=True):\n        \"\"\"Body-encode a string and convert it to output_charset.\n\n        If convert is True (the default), the string will be converted from\n        the input charset to output charset automatically.  Unlike\n        header_encode(), there are no issues with byte boundaries and\n        multibyte charsets in email bodies, so this is usually pretty safe.\n\n        The type of encoding (base64 or quoted-printable) will be based on\n        self.body_encoding.\n        \"\"\"\n        if convert:\n            s = self.convert(s)\n        # 7bit/8bit encodings return the string unchanged (module conversions)\n        if self.body_encoding is BASE64:\n            return email.base64mime.body_encode(s)\n        elif self.body_encoding is QP:\n            return email.quoprimime.body_encode(s)\n        else:\n            return s\n", 
    "email.encoders": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Encodings and related functions.\"\"\"\n\n__all__ = [\n    'encode_7or8bit',\n    'encode_base64',\n    'encode_noop',\n    'encode_quopri',\n    ]\n\nimport base64\n\nfrom quopri import encodestring as _encodestring\n\n\n\f\ndef _qencode(s):\n    enc = _encodestring(s, quotetabs=True)\n    # Must encode spaces, which quopri.encodestring() doesn't do\n    return enc.replace(' ', '=20')\n\n\ndef _bencode(s):\n    # We can't quite use base64.encodestring() since it tacks on a \"courtesy\n    # newline\".  Blech!\n    if not s:\n        return s\n    hasnewline = (s[-1] == '\\n')\n    value = base64.encodestring(s)\n    if not hasnewline and value[-1] == '\\n':\n        return value[:-1]\n    return value\n\n\n\f\ndef encode_base64(msg):\n    \"\"\"Encode the message's payload in Base64.\n\n    Also, add an appropriate Content-Transfer-Encoding header.\n    \"\"\"\n    orig = msg.get_payload()\n    encdata = _bencode(orig)\n    msg.set_payload(encdata)\n    msg['Content-Transfer-Encoding'] = 'base64'\n\n\n\f\ndef encode_quopri(msg):\n    \"\"\"Encode the message's payload in quoted-printable.\n\n    Also, add an appropriate Content-Transfer-Encoding header.\n    \"\"\"\n    orig = msg.get_payload()\n    encdata = _qencode(orig)\n    msg.set_payload(encdata)\n    msg['Content-Transfer-Encoding'] = 'quoted-printable'\n\n\n\f\ndef encode_7or8bit(msg):\n    \"\"\"Set the Content-Transfer-Encoding header to 7bit or 8bit.\"\"\"\n    orig = msg.get_payload()\n    if orig is None:\n        # There's no payload.  For backwards compatibility we use 7bit\n        msg['Content-Transfer-Encoding'] = '7bit'\n        return\n    # We play a trick to make this go fast.  If encoding to ASCII succeeds, we\n    # know the data must be 7bit, otherwise treat it as 8bit.\n    try:\n        orig.encode('ascii')\n    except UnicodeError:\n        msg['Content-Transfer-Encoding'] = '8bit'\n    else:\n        msg['Content-Transfer-Encoding'] = '7bit'\n\n\n\f\ndef encode_noop(msg):\n    \"\"\"Do nothing.\"\"\"\n", 
    "email.errors": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"email package exception classes.\"\"\"\n\n\n\f\nclass MessageError(Exception):\n    \"\"\"Base class for errors in the email package.\"\"\"\n\n\nclass MessageParseError(MessageError):\n    \"\"\"Base class for message parsing errors.\"\"\"\n\n\nclass HeaderParseError(MessageParseError):\n    \"\"\"Error while parsing headers.\"\"\"\n\n\nclass BoundaryError(MessageParseError):\n    \"\"\"Couldn't find terminating boundary.\"\"\"\n\n\nclass MultipartConversionError(MessageError, TypeError):\n    \"\"\"Conversion to a multipart is prohibited.\"\"\"\n\n\nclass CharsetError(MessageError):\n    \"\"\"An illegal charset was given.\"\"\"\n\n\n\f\n# These are parsing defects which the parser was able to work around.\nclass MessageDefect:\n    \"\"\"Base class for a message defect.\"\"\"\n\n    def __init__(self, line=None):\n        self.line = line\n\nclass NoBoundaryInMultipartDefect(MessageDefect):\n    \"\"\"A message claimed to be a multipart but had no boundary parameter.\"\"\"\n\nclass StartBoundaryNotFoundDefect(MessageDefect):\n    \"\"\"The claimed start boundary was never found.\"\"\"\n\nclass FirstHeaderLineIsContinuationDefect(MessageDefect):\n    \"\"\"A message had a continuation line as its first header line.\"\"\"\n\nclass MisplacedEnvelopeHeaderDefect(MessageDefect):\n    \"\"\"A 'Unix-from' header was found in the middle of a header block.\"\"\"\n\nclass MalformedHeaderDefect(MessageDefect):\n    \"\"\"Found a header that was missing a colon, or was otherwise malformed.\"\"\"\n\nclass MultipartInvariantViolationDefect(MessageDefect):\n    \"\"\"A message claimed to be a multipart but no subparts were found.\"\"\"\n", 
    "email.feedparser": "# Copyright (C) 2004-2006 Python Software Foundation\n# Authors: Baxter, Wouters and Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"FeedParser - An email feed parser.\n\nThe feed parser implements an interface for incrementally parsing an email\nmessage, line by line.  This has advantages for certain applications, such as\nthose reading email messages off a socket.\n\nFeedParser.feed() is the primary interface for pushing new data into the\nparser.  It returns when there's nothing more it can do with the available\ndata.  When you have no more data to push into the parser, call .close().\nThis completes the parsing and returns the root message object.\n\nThe other advantage of this parser is that it will never raise a parsing\nexception.  Instead, when it finds something unexpected, it adds a 'defect' to\nthe current message.  Defects are just instances that live on the message\nobject's .defects attribute.\n\"\"\"\n\n__all__ = ['FeedParser']\n\nimport re\n\nfrom email import errors\nfrom email import message\n\nNLCRE = re.compile('\\r\\n|\\r|\\n')\nNLCRE_bol = re.compile('(\\r\\n|\\r|\\n)')\nNLCRE_eol = re.compile('(\\r\\n|\\r|\\n)\\Z')\nNLCRE_crack = re.compile('(\\r\\n|\\r|\\n)')\n# RFC 2822 $3.6.8 Optional fields.  ftext is %d33-57 / %d59-126, Any character\n# except controls, SP, and \":\".\nheaderRE = re.compile(r'^(From |[\\041-\\071\\073-\\176]{1,}:|[\\t ])')\nEMPTYSTRING = ''\nNL = '\\n'\n\nNeedMoreData = object()\n\n\n\f\nclass BufferedSubFile(object):\n    \"\"\"A file-ish object that can have new data loaded into it.\n\n    You can also push and pop line-matching predicates onto a stack.  When the\n    current predicate matches the current line, a false EOF response\n    (i.e. empty string) is returned instead.  This lets the parser adhere to a\n    simple abstraction -- it parses until EOF closes the current message.\n    \"\"\"\n    def __init__(self):\n        # Chunks of the last partial line pushed into this object.\n        self._partial = []\n        # The list of full, pushed lines, in reverse order\n        self._lines = []\n        # The stack of false-EOF checking predicates.\n        self._eofstack = []\n        # A flag indicating whether the file has been closed or not.\n        self._closed = False\n\n    def push_eof_matcher(self, pred):\n        self._eofstack.append(pred)\n\n    def pop_eof_matcher(self):\n        return self._eofstack.pop()\n\n    def close(self):\n        # Don't forget any trailing partial line.\n        self.pushlines(''.join(self._partial).splitlines(True))\n        self._partial = []\n        self._closed = True\n\n    def readline(self):\n        if not self._lines:\n            if self._closed:\n                return ''\n            return NeedMoreData\n        # Pop the line off the stack and see if it matches the current\n        # false-EOF predicate.\n        line = self._lines.pop()\n        # RFC 2046, section 5.1.2 requires us to recognize outer level\n        # boundaries at any level of inner nesting.  Do this, but be sure it's\n        # in the order of most to least nested.\n        for ateof in self._eofstack[::-1]:\n            if ateof(line):\n                # We're at the false EOF.  But push the last line back first.\n                self._lines.append(line)\n                return ''\n        return line\n\n    def unreadline(self, line):\n        # Let the consumer push a line back into the buffer.\n        assert line is not NeedMoreData\n        self._lines.append(line)\n\n    def push(self, data):\n        \"\"\"Push some new data into this object.\"\"\"\n        # Crack into lines, but preserve the linesep characters on the end of each\n        parts = data.splitlines(True)\n\n        if not parts or not parts[0].endswith(('\\n', '\\r')):\n            # No new complete lines, so just accumulate partials\n            self._partial += parts\n            return\n\n        if self._partial:\n            # If there are previous leftovers, complete them now\n            self._partial.append(parts[0])\n            parts[0:1] = ''.join(self._partial).splitlines(True)\n            del self._partial[:]\n\n        # If the last element of the list does not end in a newline, then treat\n        # it as a partial line.  We only check for '\\n' here because a line\n        # ending with '\\r' might be a line that was split in the middle of a\n        # '\\r\\n' sequence (see bugs 1555570 and 1721862).\n        if not parts[-1].endswith('\\n'):\n            self._partial = [parts.pop()]\n        self.pushlines(parts)\n\n    def pushlines(self, lines):\n        # Crack into lines, but preserve the newlines on the end of each\n        parts = NLCRE_crack.split(data)\n        # The *ahem* interesting behaviour of re.split when supplied grouping\n        # parentheses is that the last element of the resulting list is the\n        # data after the final RE.  In the case of a NL/CR terminated string,\n        # this is the empty string.\n        self._partial = parts.pop()\n        #GAN 29Mar09  bugs 1555570, 1721862  Confusion at 8K boundary ending with \\r:\n        # is there a \\n to follow later?\n        if not self._partial and parts and parts[-1].endswith('\\r'):\n            self._partial = parts.pop(-2)+parts.pop()\n        # parts is a list of strings, alternating between the line contents\n        # and the eol character(s).  Gather up a list of lines after\n        # re-attaching the newlines.\n        lines = []\n        for i in range(len(parts) // 2):\n            lines.append(parts[i*2] + parts[i*2+1])\n        self.pushlines(lines)\n\n    def pushlines(self, lines):\n        # Reverse and insert at the front of the lines.\n        self._lines[:0] = lines[::-1]\n\n    def is_closed(self):\n        return self._closed\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        line = self.readline()\n        if line == '':\n            raise StopIteration\n        return line\n\n\n\f\nclass FeedParser:\n    \"\"\"A feed-style parser of email.\"\"\"\n\n    def __init__(self, _factory=message.Message):\n        \"\"\"_factory is called with no arguments to create a new message obj\"\"\"\n        self._factory = _factory\n        self._input = BufferedSubFile()\n        self._msgstack = []\n        self._parse = self._parsegen().next\n        self._cur = None\n        self._last = None\n        self._headersonly = False\n\n    # Non-public interface for supporting Parser's headersonly flag\n    def _set_headersonly(self):\n        self._headersonly = True\n\n    def feed(self, data):\n        \"\"\"Push more data into the parser.\"\"\"\n        self._input.push(data)\n        self._call_parse()\n\n    def _call_parse(self):\n        try:\n            self._parse()\n        except StopIteration:\n            pass\n\n    def close(self):\n        \"\"\"Parse all remaining data and return the root message object.\"\"\"\n        self._input.close()\n        self._call_parse()\n        root = self._pop_message()\n        assert not self._msgstack\n        # Look for final set of defects\n        if root.get_content_maintype() == 'multipart' \\\n               and not root.is_multipart():\n            root.defects.append(errors.MultipartInvariantViolationDefect())\n        return root\n\n    def _new_message(self):\n        msg = self._factory()\n        if self._cur and self._cur.get_content_type() == 'multipart/digest':\n            msg.set_default_type('message/rfc822')\n        if self._msgstack:\n            self._msgstack[-1].attach(msg)\n        self._msgstack.append(msg)\n        self._cur = msg\n        self._last = msg\n\n    def _pop_message(self):\n        retval = self._msgstack.pop()\n        if self._msgstack:\n            self._cur = self._msgstack[-1]\n        else:\n            self._cur = None\n        return retval\n\n    def _parsegen(self):\n        # Create a new message and start by parsing headers.\n        self._new_message()\n        headers = []\n        # Collect the headers, searching for a line that doesn't match the RFC\n        # 2822 header or continuation pattern (including an empty line).\n        for line in self._input:\n            if line is NeedMoreData:\n                yield NeedMoreData\n                continue\n            if not headerRE.match(line):\n                # If we saw the RFC defined header/body separator\n                # (i.e. newline), just throw it away. Otherwise the line is\n                # part of the body so push it back.\n                if not NLCRE.match(line):\n                    self._input.unreadline(line)\n                break\n            headers.append(line)\n        # Done with the headers, so parse them and figure out what we're\n        # supposed to see in the body of the message.\n        self._parse_headers(headers)\n        # Headers-only parsing is a backwards compatibility hack, which was\n        # necessary in the older parser, which could raise errors.  All\n        # remaining lines in the input are thrown into the message body.\n        if self._headersonly:\n            lines = []\n            while True:\n                line = self._input.readline()\n                if line is NeedMoreData:\n                    yield NeedMoreData\n                    continue\n                if line == '':\n                    break\n                lines.append(line)\n            self._cur.set_payload(EMPTYSTRING.join(lines))\n            return\n        if self._cur.get_content_type() == 'message/delivery-status':\n            # message/delivery-status contains blocks of headers separated by\n            # a blank line.  We'll represent each header block as a separate\n            # nested message object, but the processing is a bit different\n            # than standard message/* types because there is no body for the\n            # nested messages.  A blank line separates the subparts.\n            while True:\n                self._input.push_eof_matcher(NLCRE.match)\n                for retval in self._parsegen():\n                    if retval is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                    break\n                msg = self._pop_message()\n                # We need to pop the EOF matcher in order to tell if we're at\n                # the end of the current file, not the end of the last block\n                # of message headers.\n                self._input.pop_eof_matcher()\n                # The input stream must be sitting at the newline or at the\n                # EOF.  We want to see if we're at the end of this subpart, so\n                # first consume the blank line, then test the next line to see\n                # if we're at this subpart's EOF.\n                while True:\n                    line = self._input.readline()\n                    if line is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                    break\n                while True:\n                    line = self._input.readline()\n                    if line is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                    break\n                if line == '':\n                    break\n                # Not at EOF so this is a line we're going to need.\n                self._input.unreadline(line)\n            return\n        if self._cur.get_content_maintype() == 'message':\n            # The message claims to be a message/* type, then what follows is\n            # another RFC 2822 message.\n            for retval in self._parsegen():\n                if retval is NeedMoreData:\n                    yield NeedMoreData\n                    continue\n                break\n            self._pop_message()\n            return\n        if self._cur.get_content_maintype() == 'multipart':\n            boundary = self._cur.get_boundary()\n            if boundary is None:\n                # The message /claims/ to be a multipart but it has not\n                # defined a boundary.  That's a problem which we'll handle by\n                # reading everything until the EOF and marking the message as\n                # defective.\n                self._cur.defects.append(errors.NoBoundaryInMultipartDefect())\n                lines = []\n                for line in self._input:\n                    if line is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                    lines.append(line)\n                self._cur.set_payload(EMPTYSTRING.join(lines))\n                return\n            # Create a line match predicate which matches the inter-part\n            # boundary as well as the end-of-multipart boundary.  Don't push\n            # this onto the input stream until we've scanned past the\n            # preamble.\n            separator = '--' + boundary\n            boundaryre = re.compile(\n                '(?P<sep>' + re.escape(separator) +\n                r')(?P<end>--)?(?P<ws>[ \\t]*)(?P<linesep>\\r\\n|\\r|\\n)?$')\n            capturing_preamble = True\n            preamble = []\n            linesep = False\n            while True:\n                line = self._input.readline()\n                if line is NeedMoreData:\n                    yield NeedMoreData\n                    continue\n                if line == '':\n                    break\n                mo = boundaryre.match(line)\n                if mo:\n                    # If we're looking at the end boundary, we're done with\n                    # this multipart.  If there was a newline at the end of\n                    # the closing boundary, then we need to initialize the\n                    # epilogue with the empty string (see below).\n                    if mo.group('end'):\n                        linesep = mo.group('linesep')\n                        break\n                    # We saw an inter-part boundary.  Were we in the preamble?\n                    if capturing_preamble:\n                        if preamble:\n                            # According to RFC 2046, the last newline belongs\n                            # to the boundary.\n                            lastline = preamble[-1]\n                            eolmo = NLCRE_eol.search(lastline)\n                            if eolmo:\n                                preamble[-1] = lastline[:-len(eolmo.group(0))]\n                            self._cur.preamble = EMPTYSTRING.join(preamble)\n                        capturing_preamble = False\n                        self._input.unreadline(line)\n                        continue\n                    # We saw a boundary separating two parts.  Consume any\n                    # multiple boundary lines that may be following.  Our\n                    # interpretation of RFC 2046 BNF grammar does not produce\n                    # body parts within such double boundaries.\n                    while True:\n                        line = self._input.readline()\n                        if line is NeedMoreData:\n                            yield NeedMoreData\n                            continue\n                        mo = boundaryre.match(line)\n                        if not mo:\n                            self._input.unreadline(line)\n                            break\n                    # Recurse to parse this subpart; the input stream points\n                    # at the subpart's first line.\n                    self._input.push_eof_matcher(boundaryre.match)\n                    for retval in self._parsegen():\n                        if retval is NeedMoreData:\n                            yield NeedMoreData\n                            continue\n                        break\n                    # Because of RFC 2046, the newline preceding the boundary\n                    # separator actually belongs to the boundary, not the\n                    # previous subpart's payload (or epilogue if the previous\n                    # part is a multipart).\n                    if self._last.get_content_maintype() == 'multipart':\n                        epilogue = self._last.epilogue\n                        if epilogue == '':\n                            self._last.epilogue = None\n                        elif epilogue is not None:\n                            mo = NLCRE_eol.search(epilogue)\n                            if mo:\n                                end = len(mo.group(0))\n                                self._last.epilogue = epilogue[:-end]\n                    else:\n                        payload = self._last.get_payload()\n                        if isinstance(payload, basestring):\n                            mo = NLCRE_eol.search(payload)\n                            if mo:\n                                payload = payload[:-len(mo.group(0))]\n                                self._last.set_payload(payload)\n                    self._input.pop_eof_matcher()\n                    self._pop_message()\n                    # Set the multipart up for newline cleansing, which will\n                    # happen if we're in a nested multipart.\n                    self._last = self._cur\n                else:\n                    # I think we must be in the preamble\n                    assert capturing_preamble\n                    preamble.append(line)\n            # We've seen either the EOF or the end boundary.  If we're still\n            # capturing the preamble, we never saw the start boundary.  Note\n            # that as a defect and store the captured text as the payload.\n            # Everything from here to the EOF is epilogue.\n            if capturing_preamble:\n                self._cur.defects.append(errors.StartBoundaryNotFoundDefect())\n                self._cur.set_payload(EMPTYSTRING.join(preamble))\n                epilogue = []\n                for line in self._input:\n                    if line is NeedMoreData:\n                        yield NeedMoreData\n                        continue\n                self._cur.epilogue = EMPTYSTRING.join(epilogue)\n                return\n            # If the end boundary ended in a newline, we'll need to make sure\n            # the epilogue isn't None\n            if linesep:\n                epilogue = ['']\n            else:\n                epilogue = []\n            for line in self._input:\n                if line is NeedMoreData:\n                    yield NeedMoreData\n                    continue\n                epilogue.append(line)\n            # Any CRLF at the front of the epilogue is not technically part of\n            # the epilogue.  Also, watch out for an empty string epilogue,\n            # which means a single newline.\n            if epilogue:\n                firstline = epilogue[0]\n                bolmo = NLCRE_bol.match(firstline)\n                if bolmo:\n                    epilogue[0] = firstline[len(bolmo.group(0)):]\n            self._cur.epilogue = EMPTYSTRING.join(epilogue)\n            return\n        # Otherwise, it's some non-multipart type, so the entire rest of the\n        # file contents becomes the payload.\n        lines = []\n        for line in self._input:\n            if line is NeedMoreData:\n                yield NeedMoreData\n                continue\n            lines.append(line)\n        self._cur.set_payload(EMPTYSTRING.join(lines))\n\n    def _parse_headers(self, lines):\n        # Passed a list of lines that make up the headers for the current msg\n        lastheader = ''\n        lastvalue = []\n        for lineno, line in enumerate(lines):\n            # Check for continuation\n            if line[0] in ' \\t':\n                if not lastheader:\n                    # The first line of the headers was a continuation.  This\n                    # is illegal, so let's note the defect, store the illegal\n                    # line, and ignore it for purposes of headers.\n                    defect = errors.FirstHeaderLineIsContinuationDefect(line)\n                    self._cur.defects.append(defect)\n                    continue\n                lastvalue.append(line)\n                continue\n            if lastheader:\n                # XXX reconsider the joining of folded lines\n                lhdr = EMPTYSTRING.join(lastvalue)[:-1].rstrip('\\r\\n')\n                self._cur[lastheader] = lhdr\n                lastheader, lastvalue = '', []\n            # Check for envelope header, i.e. unix-from\n            if line.startswith('From '):\n                if lineno == 0:\n                    # Strip off the trailing newline\n                    mo = NLCRE_eol.search(line)\n                    if mo:\n                        line = line[:-len(mo.group(0))]\n                    self._cur.set_unixfrom(line)\n                    continue\n                elif lineno == len(lines) - 1:\n                    # Something looking like a unix-from at the end - it's\n                    # probably the first line of the body, so push back the\n                    # line and stop.\n                    self._input.unreadline(line)\n                    return\n                else:\n                    # Weirdly placed unix-from line.  Note this as a defect\n                    # and ignore it.\n                    defect = errors.MisplacedEnvelopeHeaderDefect(line)\n                    self._cur.defects.append(defect)\n                    continue\n            # Split the line on the colon separating field name from value.\n            i = line.find(':')\n            if i < 0:\n                defect = errors.MalformedHeaderDefect(line)\n                self._cur.defects.append(defect)\n                continue\n            lastheader = line[:i]\n            lastvalue = [line[i+1:].lstrip()]\n        # Done with all the lines, so handle the last header.\n        if lastheader:\n            # XXX reconsider the joining of folded lines\n            self._cur[lastheader] = EMPTYSTRING.join(lastvalue).rstrip('\\r\\n')\n", 
    "email.generator": "# Copyright (C) 2001-2010 Python Software Foundation\n# Contact: email-sig@python.org\n\n\"\"\"Classes to generate plain text from a message object tree.\"\"\"\n\n__all__ = ['Generator', 'DecodedGenerator']\n\nimport re\nimport sys\nimport time\nimport random\nimport warnings\n\nfrom cStringIO import StringIO\nfrom email.header import Header\n\nUNDERSCORE = '_'\nNL = '\\n'\n\nfcre = re.compile(r'^From ', re.MULTILINE)\n\ndef _is8bitstring(s):\n    if isinstance(s, str):\n        try:\n            unicode(s, 'us-ascii')\n        except UnicodeError:\n            return True\n    return False\n\n\n\f\nclass Generator:\n    \"\"\"Generates output from a Message object tree.\n\n    This basic generator writes the message to the given file object as plain\n    text.\n    \"\"\"\n    #\n    # Public interface\n    #\n\n    def __init__(self, outfp, mangle_from_=True, maxheaderlen=78):\n        \"\"\"Create the generator for message flattening.\n\n        outfp is the output file-like object for writing the message to.  It\n        must have a write() method.\n\n        Optional mangle_from_ is a flag that, when True (the default), escapes\n        From_ lines in the body of the message by putting a `>' in front of\n        them.\n\n        Optional maxheaderlen specifies the longest length for a non-continued\n        header.  When a header line is longer (in characters, with tabs\n        expanded to 8 spaces) than maxheaderlen, the header will split as\n        defined in the Header class.  Set maxheaderlen to zero to disable\n        header wrapping.  The default is 78, as recommended (but not required)\n        by RFC 2822.\n        \"\"\"\n        self._fp = outfp\n        self._mangle_from_ = mangle_from_\n        self._maxheaderlen = maxheaderlen\n\n    def write(self, s):\n        # Just delegate to the file object\n        self._fp.write(s)\n\n    def flatten(self, msg, unixfrom=False):\n        \"\"\"Print the message object tree rooted at msg to the output file\n        specified when the Generator instance was created.\n\n        unixfrom is a flag that forces the printing of a Unix From_ delimiter\n        before the first object in the message tree.  If the original message\n        has no From_ delimiter, a `standard' one is crafted.  By default, this\n        is False to inhibit the printing of any From_ delimiter.\n\n        Note that for subobjects, no From_ line is printed.\n        \"\"\"\n        if unixfrom:\n            ufrom = msg.get_unixfrom()\n            if not ufrom:\n                ufrom = 'From nobody ' + time.ctime(time.time())\n            print >> self._fp, ufrom\n        self._write(msg)\n\n    def clone(self, fp):\n        \"\"\"Clone this generator with the exact same options.\"\"\"\n        return self.__class__(fp, self._mangle_from_, self._maxheaderlen)\n\n    #\n    # Protected interface - undocumented ;/\n    #\n\n    def _write(self, msg):\n        # We can't write the headers yet because of the following scenario:\n        # say a multipart message includes the boundary string somewhere in\n        # its body.  We'd have to calculate the new boundary /before/ we write\n        # the headers so that we can write the correct Content-Type:\n        # parameter.\n        #\n        # The way we do this, so as to make the _handle_*() methods simpler,\n        # is to cache any subpart writes into a StringIO.  The we write the\n        # headers and the StringIO contents.  That way, subpart handlers can\n        # Do The Right Thing, and can still modify the Content-Type: header if\n        # necessary.\n        oldfp = self._fp\n        try:\n            self._fp = sfp = StringIO()\n            self._dispatch(msg)\n        finally:\n            self._fp = oldfp\n        # Write the headers.  First we see if the message object wants to\n        # handle that itself.  If not, we'll do it generically.\n        meth = getattr(msg, '_write_headers', None)\n        if meth is None:\n            self._write_headers(msg)\n        else:\n            meth(self)\n        self._fp.write(sfp.getvalue())\n\n    def _dispatch(self, msg):\n        # Get the Content-Type: for the message, then try to dispatch to\n        # self._handle_<maintype>_<subtype>().  If there's no handler for the\n        # full MIME type, then dispatch to self._handle_<maintype>().  If\n        # that's missing too, then dispatch to self._writeBody().\n        main = msg.get_content_maintype()\n        sub = msg.get_content_subtype()\n        specific = UNDERSCORE.join((main, sub)).replace('-', '_')\n        meth = getattr(self, '_handle_' + specific, None)\n        if meth is None:\n            generic = main.replace('-', '_')\n            meth = getattr(self, '_handle_' + generic, None)\n            if meth is None:\n                meth = self._writeBody\n        meth(msg)\n\n    #\n    # Default handlers\n    #\n\n    def _write_headers(self, msg):\n        for h, v in msg.items():\n            print >> self._fp, '%s:' % h,\n            if self._maxheaderlen == 0:\n                # Explicit no-wrapping\n                print >> self._fp, v\n            elif isinstance(v, Header):\n                # Header instances know what to do\n                print >> self._fp, v.encode()\n            elif _is8bitstring(v):\n                # If we have raw 8bit data in a byte string, we have no idea\n                # what the encoding is.  There is no safe way to split this\n                # string.  If it's ascii-subset, then we could do a normal\n                # ascii split, but if it's multibyte then we could break the\n                # string.  There's no way to know so the least harm seems to\n                # be to not split the string and risk it being too long.\n                print >> self._fp, v\n            else:\n                # Header's got lots of smarts, so use it.  Note that this is\n                # fundamentally broken though because we lose idempotency when\n                # the header string is continued with tabs.  It will now be\n                # continued with spaces.  This was reversedly broken before we\n                # fixed bug 1974.  Either way, we lose.\n                print >> self._fp, Header(\n                    v, maxlinelen=self._maxheaderlen, header_name=h).encode()\n        # A blank line always separates headers from body\n        print >> self._fp\n\n    #\n    # Handlers for writing types and subtypes\n    #\n\n    def _handle_text(self, msg):\n        payload = msg.get_payload()\n        if payload is None:\n            return\n        if not isinstance(payload, basestring):\n            raise TypeError('string payload expected: %s' % type(payload))\n        if self._mangle_from_:\n            payload = fcre.sub('>From ', payload)\n        self._fp.write(payload)\n\n    # Default body handler\n    _writeBody = _handle_text\n\n    def _handle_multipart(self, msg):\n        # The trick here is to write out each part separately, merge them all\n        # together, and then make sure that the boundary we've chosen isn't\n        # present in the payload.\n        msgtexts = []\n        subparts = msg.get_payload()\n        if subparts is None:\n            subparts = []\n        elif isinstance(subparts, basestring):\n            # e.g. a non-strict parse of a message with no starting boundary.\n            self._fp.write(subparts)\n            return\n        elif not isinstance(subparts, list):\n            # Scalar payload\n            subparts = [subparts]\n        for part in subparts:\n            s = StringIO()\n            g = self.clone(s)\n            g.flatten(part, unixfrom=False)\n            msgtexts.append(s.getvalue())\n        # BAW: What about boundaries that are wrapped in double-quotes?\n        boundary = msg.get_boundary()\n        if not boundary:\n            # Create a boundary that doesn't appear in any of the\n            # message texts.\n            alltext = NL.join(msgtexts)\n            boundary = _make_boundary(alltext)\n            msg.set_boundary(boundary)\n        # If there's a preamble, write it out, with a trailing CRLF\n        if msg.preamble is not None:\n            if self._mangle_from_:\n                preamble = fcre.sub('>From ', msg.preamble)\n            else:\n                preamble = msg.preamble\n            print >> self._fp, preamble\n        # dash-boundary transport-padding CRLF\n        print >> self._fp, '--' + boundary\n        # body-part\n        if msgtexts:\n            self._fp.write(msgtexts.pop(0))\n        # *encapsulation\n        # --> delimiter transport-padding\n        # --> CRLF body-part\n        for body_part in msgtexts:\n            # delimiter transport-padding CRLF\n            print >> self._fp, '\\n--' + boundary\n            # body-part\n            self._fp.write(body_part)\n        # close-delimiter transport-padding\n        self._fp.write('\\n--' + boundary + '--' + NL)\n        if msg.epilogue is not None:\n            if self._mangle_from_:\n                epilogue = fcre.sub('>From ', msg.epilogue)\n            else:\n                epilogue = msg.epilogue\n            self._fp.write(epilogue)\n\n    def _handle_multipart_signed(self, msg):\n        # The contents of signed parts has to stay unmodified in order to keep\n        # the signature intact per RFC1847 2.1, so we disable header wrapping.\n        # RDM: This isn't enough to completely preserve the part, but it helps.\n        old_maxheaderlen = self._maxheaderlen\n        try:\n            self._maxheaderlen = 0\n            self._handle_multipart(msg)\n        finally:\n            self._maxheaderlen = old_maxheaderlen\n\n    def _handle_message_delivery_status(self, msg):\n        # We can't just write the headers directly to self's file object\n        # because this will leave an extra newline between the last header\n        # block and the boundary.  Sigh.\n        blocks = []\n        for part in msg.get_payload():\n            s = StringIO()\n            g = self.clone(s)\n            g.flatten(part, unixfrom=False)\n            text = s.getvalue()\n            lines = text.split('\\n')\n            # Strip off the unnecessary trailing empty line\n            if lines and lines[-1] == '':\n                blocks.append(NL.join(lines[:-1]))\n            else:\n                blocks.append(text)\n        # Now join all the blocks with an empty line.  This has the lovely\n        # effect of separating each block with an empty line, but not adding\n        # an extra one after the last one.\n        self._fp.write(NL.join(blocks))\n\n    def _handle_message(self, msg):\n        s = StringIO()\n        g = self.clone(s)\n        # The payload of a message/rfc822 part should be a multipart sequence\n        # of length 1.  The zeroth element of the list should be the Message\n        # object for the subpart.  Extract that object, stringify it, and\n        # write it out.\n        # Except, it turns out, when it's a string instead, which happens when\n        # and only when HeaderParser is used on a message of mime type\n        # message/rfc822.  Such messages are generated by, for example,\n        # Groupwise when forwarding unadorned messages.  (Issue 7970.)  So\n        # in that case we just emit the string body.\n        payload = msg.get_payload()\n        if isinstance(payload, list):\n            g.flatten(msg.get_payload(0), unixfrom=False)\n            payload = s.getvalue()\n        self._fp.write(payload)\n\n\n\f\n_FMT = '[Non-text (%(type)s) part of message omitted, filename %(filename)s]'\n\nclass DecodedGenerator(Generator):\n    \"\"\"Generates a text representation of a message.\n\n    Like the Generator base class, except that non-text parts are substituted\n    with a format string representing the part.\n    \"\"\"\n    def __init__(self, outfp, mangle_from_=True, maxheaderlen=78, fmt=None):\n        \"\"\"Like Generator.__init__() except that an additional optional\n        argument is allowed.\n\n        Walks through all subparts of a message.  If the subpart is of main\n        type `text', then it prints the decoded payload of the subpart.\n\n        Otherwise, fmt is a format string that is used instead of the message\n        payload.  fmt is expanded with the following keywords (in\n        %(keyword)s format):\n\n        type       : Full MIME type of the non-text part\n        maintype   : Main MIME type of the non-text part\n        subtype    : Sub-MIME type of the non-text part\n        filename   : Filename of the non-text part\n        description: Description associated with the non-text part\n        encoding   : Content transfer encoding of the non-text part\n\n        The default value for fmt is None, meaning\n\n        [Non-text (%(type)s) part of message omitted, filename %(filename)s]\n        \"\"\"\n        Generator.__init__(self, outfp, mangle_from_, maxheaderlen)\n        if fmt is None:\n            self._fmt = _FMT\n        else:\n            self._fmt = fmt\n\n    def _dispatch(self, msg):\n        for part in msg.walk():\n            maintype = part.get_content_maintype()\n            if maintype == 'text':\n                print >> self, part.get_payload(decode=True)\n            elif maintype == 'multipart':\n                # Just skip this\n                pass\n            else:\n                print >> self, self._fmt % {\n                    'type'       : part.get_content_type(),\n                    'maintype'   : part.get_content_maintype(),\n                    'subtype'    : part.get_content_subtype(),\n                    'filename'   : part.get_filename('[no filename]'),\n                    'description': part.get('Content-Description',\n                                            '[no description]'),\n                    'encoding'   : part.get('Content-Transfer-Encoding',\n                                            '[no encoding]'),\n                    }\n\n\n\f\n# Helper\n_width = len(repr(sys.maxint-1))\n_fmt = '%%0%dd' % _width\n\ndef _make_boundary(text=None):\n    # Craft a random boundary.  If text is given, ensure that the chosen\n    # boundary doesn't appear in the text.\n    token = random.randrange(sys.maxint)\n    boundary = ('=' * 15) + (_fmt % token) + '=='\n    if text is None:\n        return boundary\n    b = boundary\n    counter = 0\n    while True:\n        cre = re.compile('^--' + re.escape(b) + '(--)?$', re.MULTILINE)\n        if not cre.search(text):\n            break\n        b = boundary + '.' + str(counter)\n        counter += 1\n    return b\n", 
    "email.header": "# Copyright (C) 2002-2006 Python Software Foundation\n# Author: Ben Gertzfield, Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Header encoding and decoding functionality.\"\"\"\n\n__all__ = [\n    'Header',\n    'decode_header',\n    'make_header',\n    ]\n\nimport re\nimport binascii\n\nimport email.quoprimime\nimport email.base64mime\n\nfrom email.errors import HeaderParseError\nfrom email.charset import Charset\n\nNL = '\\n'\nSPACE = ' '\nUSPACE = u' '\nSPACE8 = ' ' * 8\nUEMPTYSTRING = u''\n\nMAXLINELEN = 76\n\nUSASCII = Charset('us-ascii')\nUTF8 = Charset('utf-8')\n\n# Match encoded-word strings in the form =?charset?q?Hello_World?=\necre = re.compile(r'''\n  =\\?                   # literal =?\n  (?P<charset>[^?]*?)   # non-greedy up to the next ? is the charset\n  \\?                    # literal ?\n  (?P<encoding>[qb])    # either a \"q\" or a \"b\", case insensitive\n  \\?                    # literal ?\n  (?P<encoded>.*?)      # non-greedy up to the next ?= is the encoded string\n  \\?=                   # literal ?=\n  (?=[ \\t]|$)           # whitespace or the end of the string\n  ''', re.VERBOSE | re.IGNORECASE | re.MULTILINE)\n\n# Field name regexp, including trailing colon, but not separating whitespace,\n# according to RFC 2822.  Character range is from tilde to exclamation mark.\n# For use with .match()\nfcre = re.compile(r'[\\041-\\176]+:$')\n\n# Find a header embedded in a putative header value.  Used to check for\n# header injection attack.\n_embeded_header = re.compile(r'\\n[^ \\t]+:')\n\n\n\f\n# Helpers\n_max_append = email.quoprimime._max_append\n\n\n\f\ndef decode_header(header):\n    \"\"\"Decode a message header value without converting charset.\n\n    Returns a list of (decoded_string, charset) pairs containing each of the\n    decoded parts of the header.  Charset is None for non-encoded parts of the\n    header, otherwise a lower-case string containing the name of the character\n    set specified in the encoded string.\n\n    An email.errors.HeaderParseError may be raised when certain decoding error\n    occurs (e.g. a base64 decoding exception).\n    \"\"\"\n    # If no encoding, just return the header\n    header = str(header)\n    if not ecre.search(header):\n        return [(header, None)]\n    decoded = []\n    dec = ''\n    for line in header.splitlines():\n        # This line might not have an encoding in it\n        if not ecre.search(line):\n            decoded.append((line, None))\n            continue\n        parts = ecre.split(line)\n        while parts:\n            unenc = parts.pop(0).strip()\n            if unenc:\n                # Should we continue a long line?\n                if decoded and decoded[-1][1] is None:\n                    decoded[-1] = (decoded[-1][0] + SPACE + unenc, None)\n                else:\n                    decoded.append((unenc, None))\n            if parts:\n                charset, encoding = [s.lower() for s in parts[0:2]]\n                encoded = parts[2]\n                dec = None\n                if encoding == 'q':\n                    dec = email.quoprimime.header_decode(encoded)\n                elif encoding == 'b':\n                    paderr = len(encoded) % 4   # Postel's law: add missing padding\n                    if paderr:\n                        encoded += '==='[:4 - paderr]\n                    try:\n                        dec = email.base64mime.decode(encoded)\n                    except binascii.Error:\n                        # Turn this into a higher level exception.  BAW: Right\n                        # now we throw the lower level exception away but\n                        # when/if we get exception chaining, we'll preserve it.\n                        raise HeaderParseError\n                if dec is None:\n                    dec = encoded\n\n                if decoded and decoded[-1][1] == charset:\n                    decoded[-1] = (decoded[-1][0] + dec, decoded[-1][1])\n                else:\n                    decoded.append((dec, charset))\n            del parts[0:3]\n    return decoded\n\n\n\f\ndef make_header(decoded_seq, maxlinelen=None, header_name=None,\n                continuation_ws=' '):\n    \"\"\"Create a Header from a sequence of pairs as returned by decode_header()\n\n    decode_header() takes a header value string and returns a sequence of\n    pairs of the format (decoded_string, charset) where charset is the string\n    name of the character set.\n\n    This function takes one of those sequence of pairs and returns a Header\n    instance.  Optional maxlinelen, header_name, and continuation_ws are as in\n    the Header constructor.\n    \"\"\"\n    h = Header(maxlinelen=maxlinelen, header_name=header_name,\n               continuation_ws=continuation_ws)\n    for s, charset in decoded_seq:\n        # None means us-ascii but we can simply pass it on to h.append()\n        if charset is not None and not isinstance(charset, Charset):\n            charset = Charset(charset)\n        h.append(s, charset)\n    return h\n\n\n\f\nclass Header:\n    def __init__(self, s=None, charset=None,\n                 maxlinelen=None, header_name=None,\n                 continuation_ws=' ', errors='strict'):\n        \"\"\"Create a MIME-compliant header that can contain many character sets.\n\n        Optional s is the initial header value.  If None, the initial header\n        value is not set.  You can later append to the header with .append()\n        method calls.  s may be a byte string or a Unicode string, but see the\n        .append() documentation for semantics.\n\n        Optional charset serves two purposes: it has the same meaning as the\n        charset argument to the .append() method.  It also sets the default\n        character set for all subsequent .append() calls that omit the charset\n        argument.  If charset is not provided in the constructor, the us-ascii\n        charset is used both as s's initial charset and as the default for\n        subsequent .append() calls.\n\n        The maximum line length can be specified explicit via maxlinelen.  For\n        splitting the first line to a shorter value (to account for the field\n        header which isn't included in s, e.g. `Subject') pass in the name of\n        the field in header_name.  The default maxlinelen is 76.\n\n        continuation_ws must be RFC 2822 compliant folding whitespace (usually\n        either a space or a hard tab) which will be prepended to continuation\n        lines.\n\n        errors is passed through to the .append() call.\n        \"\"\"\n        if charset is None:\n            charset = USASCII\n        if not isinstance(charset, Charset):\n            charset = Charset(charset)\n        self._charset = charset\n        self._continuation_ws = continuation_ws\n        cws_expanded_len = len(continuation_ws.replace('\\t', SPACE8))\n        # BAW: I believe `chunks' and `maxlinelen' should be non-public.\n        self._chunks = []\n        if s is not None:\n            self.append(s, charset, errors)\n        if maxlinelen is None:\n            maxlinelen = MAXLINELEN\n        if header_name is None:\n            # We don't know anything about the field header so the first line\n            # is the same length as subsequent lines.\n            self._firstlinelen = maxlinelen\n        else:\n            # The first line should be shorter to take into account the field\n            # header.  Also subtract off 2 extra for the colon and space.\n            self._firstlinelen = maxlinelen - len(header_name) - 2\n        # Second and subsequent lines should subtract off the length in\n        # columns of the continuation whitespace prefix.\n        self._maxlinelen = maxlinelen - cws_expanded_len\n\n    def __str__(self):\n        \"\"\"A synonym for self.encode().\"\"\"\n        return self.encode()\n\n    def __unicode__(self):\n        \"\"\"Helper for the built-in unicode function.\"\"\"\n        uchunks = []\n        lastcs = None\n        for s, charset in self._chunks:\n            # We must preserve spaces between encoded and non-encoded word\n            # boundaries, which means for us we need to add a space when we go\n            # from a charset to None/us-ascii, or from None/us-ascii to a\n            # charset.  Only do this for the second and subsequent chunks.\n            nextcs = charset\n            if uchunks:\n                if lastcs not in (None, 'us-ascii'):\n                    if nextcs in (None, 'us-ascii'):\n                        uchunks.append(USPACE)\n                        nextcs = None\n                elif nextcs not in (None, 'us-ascii'):\n                    uchunks.append(USPACE)\n            lastcs = nextcs\n            uchunks.append(unicode(s, str(charset)))\n        return UEMPTYSTRING.join(uchunks)\n\n    # Rich comparison operators for equality only.  BAW: does it make sense to\n    # have or explicitly disable <, <=, >, >= operators?\n    def __eq__(self, other):\n        # other may be a Header or a string.  Both are fine so coerce\n        # ourselves to a string, swap the args and do another comparison.\n        return other == self.encode()\n\n    def __ne__(self, other):\n        return not self == other\n\n    def append(self, s, charset=None, errors='strict'):\n        \"\"\"Append a string to the MIME header.\n\n        Optional charset, if given, should be a Charset instance or the name\n        of a character set (which will be converted to a Charset instance).  A\n        value of None (the default) means that the charset given in the\n        constructor is used.\n\n        s may be a byte string or a Unicode string.  If it is a byte string\n        (i.e. isinstance(s, str) is true), then charset is the encoding of\n        that byte string, and a UnicodeError will be raised if the string\n        cannot be decoded with that charset.  If s is a Unicode string, then\n        charset is a hint specifying the character set of the characters in\n        the string.  In this case, when producing an RFC 2822 compliant header\n        using RFC 2047 rules, the Unicode string will be encoded using the\n        following charsets in order: us-ascii, the charset hint, utf-8.  The\n        first character set not to provoke a UnicodeError is used.\n\n        Optional `errors' is passed as the third argument to any unicode() or\n        ustr.encode() call.\n        \"\"\"\n        if charset is None:\n            charset = self._charset\n        elif not isinstance(charset, Charset):\n            charset = Charset(charset)\n        # If the charset is our faux 8bit charset, leave the string unchanged\n        if charset != '8bit':\n            # We need to test that the string can be converted to unicode and\n            # back to a byte string, given the input and output codecs of the\n            # charset.\n            if isinstance(s, str):\n                # Possibly raise UnicodeError if the byte string can't be\n                # converted to a unicode with the input codec of the charset.\n                incodec = charset.input_codec or 'us-ascii'\n                ustr = unicode(s, incodec, errors)\n                # Now make sure that the unicode could be converted back to a\n                # byte string with the output codec, which may be different\n                # than the iput coded.  Still, use the original byte string.\n                outcodec = charset.output_codec or 'us-ascii'\n                ustr.encode(outcodec, errors)\n            elif isinstance(s, unicode):\n                # Now we have to be sure the unicode string can be converted\n                # to a byte string with a reasonable output codec.  We want to\n                # use the byte string in the chunk.\n                for charset in USASCII, charset, UTF8:\n                    try:\n                        outcodec = charset.output_codec or 'us-ascii'\n                        s = s.encode(outcodec, errors)\n                        break\n                    except UnicodeError:\n                        pass\n                else:\n                    assert False, 'utf-8 conversion failed'\n        self._chunks.append((s, charset))\n\n    def _split(self, s, charset, maxlinelen, splitchars):\n        # Split up a header safely for use with encode_chunks.\n        splittable = charset.to_splittable(s)\n        encoded = charset.from_splittable(splittable, True)\n        elen = charset.encoded_header_len(encoded)\n        # If the line's encoded length first, just return it\n        if elen <= maxlinelen:\n            return [(encoded, charset)]\n        # If we have undetermined raw 8bit characters sitting in a byte\n        # string, we really don't know what the right thing to do is.  We\n        # can't really split it because it might be multibyte data which we\n        # could break if we split it between pairs.  The least harm seems to\n        # be to not split the header at all, but that means they could go out\n        # longer than maxlinelen.\n        if charset == '8bit':\n            return [(s, charset)]\n        # BAW: I'm not sure what the right test here is.  What we're trying to\n        # do is be faithful to RFC 2822's recommendation that ($2.2.3):\n        #\n        # \"Note: Though structured field bodies are defined in such a way that\n        #  folding can take place between many of the lexical tokens (and even\n        #  within some of the lexical tokens), folding SHOULD be limited to\n        #  placing the CRLF at higher-level syntactic breaks.\"\n        #\n        # For now, I can only imagine doing this when the charset is us-ascii,\n        # although it's possible that other charsets may also benefit from the\n        # higher-level syntactic breaks.\n        elif charset == 'us-ascii':\n            return self._split_ascii(s, charset, maxlinelen, splitchars)\n        # BAW: should we use encoded?\n        elif elen == len(s):\n            # We can split on _maxlinelen boundaries because we know that the\n            # encoding won't change the size of the string\n            splitpnt = maxlinelen\n            first = charset.from_splittable(splittable[:splitpnt], False)\n            last = charset.from_splittable(splittable[splitpnt:], False)\n        else:\n            # Binary search for split point\n            first, last = _binsplit(splittable, charset, maxlinelen)\n        # first is of the proper length so just wrap it in the appropriate\n        # chrome.  last must be recursively split.\n        fsplittable = charset.to_splittable(first)\n        fencoded = charset.from_splittable(fsplittable, True)\n        chunk = [(fencoded, charset)]\n        return chunk + self._split(last, charset, self._maxlinelen, splitchars)\n\n    def _split_ascii(self, s, charset, firstlen, splitchars):\n        chunks = _split_ascii(s, firstlen, self._maxlinelen,\n                              self._continuation_ws, splitchars)\n        return zip(chunks, [charset]*len(chunks))\n\n    def _encode_chunks(self, newchunks, maxlinelen):\n        # MIME-encode a header with many different charsets and/or encodings.\n        #\n        # Given a list of pairs (string, charset), return a MIME-encoded\n        # string suitable for use in a header field.  Each pair may have\n        # different charsets and/or encodings, and the resulting header will\n        # accurately reflect each setting.\n        #\n        # Each encoding can be email.utils.QP (quoted-printable, for\n        # ASCII-like character sets like iso-8859-1), email.utils.BASE64\n        # (Base64, for non-ASCII like character sets like KOI8-R and\n        # iso-2022-jp), or None (no encoding).\n        #\n        # Each pair will be represented on a separate line; the resulting\n        # string will be in the format:\n        #\n        # =?charset1?q?Mar=EDa_Gonz=E1lez_Alonso?=\\n\n        #  =?charset2?b?SvxyZ2VuIEL2aW5n?=\"\n        chunks = []\n        for header, charset in newchunks:\n            if not header:\n                continue\n            if charset is None or charset.header_encoding is None:\n                s = header\n            else:\n                s = charset.header_encode(header)\n            # Don't add more folding whitespace than necessary\n            if chunks and chunks[-1].endswith(' '):\n                extra = ''\n            else:\n                extra = ' '\n            _max_append(chunks, s, maxlinelen, extra)\n        joiner = NL + self._continuation_ws\n        return joiner.join(chunks)\n\n    def encode(self, splitchars=';, '):\n        \"\"\"Encode a message header into an RFC-compliant format.\n\n        There are many issues involved in converting a given string for use in\n        an email header.  Only certain character sets are readable in most\n        email clients, and as header strings can only contain a subset of\n        7-bit ASCII, care must be taken to properly convert and encode (with\n        Base64 or quoted-printable) header strings.  In addition, there is a\n        75-character length limit on any given encoded header field, so\n        line-wrapping must be performed, even with double-byte character sets.\n\n        This method will do its best to convert the string to the correct\n        character set used in email, and encode and line wrap it safely with\n        the appropriate scheme for that character set.\n\n        If the given charset is not known or an error occurs during\n        conversion, this function will return the header untouched.\n\n        Optional splitchars is a string containing characters to split long\n        ASCII lines on, in rough support of RFC 2822's `highest level\n        syntactic breaks'.  This doesn't affect RFC 2047 encoded lines.\n        \"\"\"\n        newchunks = []\n        maxlinelen = self._firstlinelen\n        lastlen = 0\n        for s, charset in self._chunks:\n            # The first bit of the next chunk should be just long enough to\n            # fill the next line.  Don't forget the space separating the\n            # encoded words.\n            targetlen = maxlinelen - lastlen - 1\n            if targetlen < charset.encoded_header_len(''):\n                # Stick it on the next line\n                targetlen = maxlinelen\n            newchunks += self._split(s, charset, targetlen, splitchars)\n            lastchunk, lastcharset = newchunks[-1]\n            lastlen = lastcharset.encoded_header_len(lastchunk)\n        value = self._encode_chunks(newchunks, maxlinelen)\n        if _embeded_header.search(value):\n            raise HeaderParseError(\"header value appears to contain \"\n                \"an embedded header: {!r}\".format(value))\n        return value\n\n\n\f\ndef _split_ascii(s, firstlen, restlen, continuation_ws, splitchars):\n    lines = []\n    maxlen = firstlen\n    for line in s.splitlines():\n        # Ignore any leading whitespace (i.e. continuation whitespace) already\n        # on the line, since we'll be adding our own.\n        line = line.lstrip()\n        if len(line) < maxlen:\n            lines.append(line)\n            maxlen = restlen\n            continue\n        # Attempt to split the line at the highest-level syntactic break\n        # possible.  Note that we don't have a lot of smarts about field\n        # syntax; we just try to break on semi-colons, then commas, then\n        # whitespace.\n        for ch in splitchars:\n            if ch in line:\n                break\n        else:\n            # There's nothing useful to split the line on, not even spaces, so\n            # just append this line unchanged\n            lines.append(line)\n            maxlen = restlen\n            continue\n        # Now split the line on the character plus trailing whitespace\n        cre = re.compile(r'%s\\s*' % ch)\n        if ch in ';,':\n            eol = ch\n        else:\n            eol = ''\n        joiner = eol + ' '\n        joinlen = len(joiner)\n        wslen = len(continuation_ws.replace('\\t', SPACE8))\n        this = []\n        linelen = 0\n        for part in cre.split(line):\n            curlen = linelen + max(0, len(this)-1) * joinlen\n            partlen = len(part)\n            onfirstline = not lines\n            # We don't want to split after the field name, if we're on the\n            # first line and the field name is present in the header string.\n            if ch == ' ' and onfirstline and \\\n                   len(this) == 1 and fcre.match(this[0]):\n                this.append(part)\n                linelen += partlen\n            elif curlen + partlen > maxlen:\n                if this:\n                    lines.append(joiner.join(this) + eol)\n                # If this part is longer than maxlen and we aren't already\n                # splitting on whitespace, try to recursively split this line\n                # on whitespace.\n                if partlen > maxlen and ch != ' ':\n                    subl = _split_ascii(part, maxlen, restlen,\n                                        continuation_ws, ' ')\n                    lines.extend(subl[:-1])\n                    this = [subl[-1]]\n                else:\n                    this = [part]\n                linelen = wslen + len(this[-1])\n                maxlen = restlen\n            else:\n                this.append(part)\n                linelen += partlen\n        # Put any left over parts on a line by themselves\n        if this:\n            lines.append(joiner.join(this))\n    return lines\n\n\n\f\ndef _binsplit(splittable, charset, maxlinelen):\n    i = 0\n    j = len(splittable)\n    while i < j:\n        # Invariants:\n        # 1. splittable[:k] fits for all k <= i (note that we *assume*,\n        #    at the start, that splittable[:0] fits).\n        # 2. splittable[:k] does not fit for any k > j (at the start,\n        #    this means we shouldn't look at any k > len(splittable)).\n        # 3. We don't know about splittable[:k] for k in i+1..j.\n        # 4. We want to set i to the largest k that fits, with i <= k <= j.\n        #\n        m = (i+j+1) >> 1  # ceiling((i+j)/2); i < m <= j\n        chunk = charset.from_splittable(splittable[:m], True)\n        chunklen = charset.encoded_header_len(chunk)\n        if chunklen <= maxlinelen:\n            # m is acceptable, so is a new lower bound.\n            i = m\n        else:\n            # m is not acceptable, so final i must be < m.\n            j = m - 1\n    # i == j.  Invariant #1 implies that splittable[:i] fits, and\n    # invariant #2 implies that splittable[:i+1] does not fit, so i\n    # is what we're looking for.\n    first = charset.from_splittable(splittable[:i], False)\n    last  = charset.from_splittable(splittable[i:], False)\n    return first, last\n", 
    "email.iterators": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Various types of useful iterators and generators.\"\"\"\n\n__all__ = [\n    'body_line_iterator',\n    'typed_subpart_iterator',\n    'walk',\n    # Do not include _structure() since it's part of the debugging API.\n    ]\n\nimport sys\nfrom cStringIO import StringIO\n\n\n\f\n# This function will become a method of the Message class\ndef walk(self):\n    \"\"\"Walk over the message tree, yielding each subpart.\n\n    The walk is performed in depth-first order.  This method is a\n    generator.\n    \"\"\"\n    yield self\n    if self.is_multipart():\n        for subpart in self.get_payload():\n            for subsubpart in subpart.walk():\n                yield subsubpart\n\n\n\f\n# These two functions are imported into the Iterators.py interface module.\ndef body_line_iterator(msg, decode=False):\n    \"\"\"Iterate over the parts, returning string payloads line-by-line.\n\n    Optional decode (default False) is passed through to .get_payload().\n    \"\"\"\n    for subpart in msg.walk():\n        payload = subpart.get_payload(decode=decode)\n        if isinstance(payload, basestring):\n            for line in StringIO(payload):\n                yield line\n\n\ndef typed_subpart_iterator(msg, maintype='text', subtype=None):\n    \"\"\"Iterate over the subparts with a given MIME type.\n\n    Use `maintype' as the main MIME type to match against; this defaults to\n    \"text\".  Optional `subtype' is the MIME subtype to match against; if\n    omitted, only the main type is matched.\n    \"\"\"\n    for subpart in msg.walk():\n        if subpart.get_content_maintype() == maintype:\n            if subtype is None or subpart.get_content_subtype() == subtype:\n                yield subpart\n\n\n\f\ndef _structure(msg, fp=None, level=0, include_default=False):\n    \"\"\"A handy debugging aid\"\"\"\n    if fp is None:\n        fp = sys.stdout\n    tab = ' ' * (level * 4)\n    print >> fp, tab + msg.get_content_type(),\n    if include_default:\n        print >> fp, '[%s]' % msg.get_default_type()\n    else:\n        print >> fp\n    if msg.is_multipart():\n        for subpart in msg.get_payload():\n            _structure(subpart, fp, level+1, include_default)\n", 
    "email.message": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Basic message object for the email package object model.\"\"\"\n\n__all__ = ['Message']\n\nimport re\nimport uu\nimport binascii\nimport warnings\nfrom cStringIO import StringIO\n\n# Intrapackage imports\nimport email.charset\nfrom email import utils\nfrom email import errors\n\nSEMISPACE = '; '\n\n# Regular expression that matches `special' characters in parameters, the\n# existence of which force quoting of the parameter value.\ntspecials = re.compile(r'[ \\(\\)<>@,;:\\\\\"/\\[\\]\\?=]')\n\n\n# Helper functions\ndef _splitparam(param):\n    # Split header parameters.  BAW: this may be too simple.  It isn't\n    # strictly RFC 2045 (section 5.1) compliant, but it catches most headers\n    # found in the wild.  We may eventually need a full fledged parser\n    # eventually.\n    a, sep, b = param.partition(';')\n    if not sep:\n        return a.strip(), None\n    return a.strip(), b.strip()\n\f\ndef _formatparam(param, value=None, quote=True):\n    \"\"\"Convenience function to format and return a key=value pair.\n\n    This will quote the value if needed or if quote is true.  If value is a\n    three tuple (charset, language, value), it will be encoded according\n    to RFC2231 rules.\n    \"\"\"\n    if value is not None and len(value) > 0:\n        # A tuple is used for RFC 2231 encoded parameter values where items\n        # are (charset, language, value).  charset is a string, not a Charset\n        # instance.\n        if isinstance(value, tuple):\n            # Encode as per RFC 2231\n            param += '*'\n            value = utils.encode_rfc2231(value[2], value[0], value[1])\n        # BAW: Please check this.  I think that if quote is set it should\n        # force quoting even if not necessary.\n        if quote or tspecials.search(value):\n            return '%s=\"%s\"' % (param, utils.quote(value))\n        else:\n            return '%s=%s' % (param, value)\n    else:\n        return param\n\ndef _parseparam(s):\n    plist = []\n    while s[:1] == ';':\n        s = s[1:]\n        end = s.find(';')\n        while end > 0 and (s.count('\"', 0, end) - s.count('\\\\\"', 0, end)) % 2:\n            end = s.find(';', end + 1)\n        if end < 0:\n            end = len(s)\n        f = s[:end]\n        if '=' in f:\n            i = f.index('=')\n            f = f[:i].strip().lower() + '=' + f[i+1:].strip()\n        plist.append(f.strip())\n        s = s[end:]\n    return plist\n\n\ndef _unquotevalue(value):\n    # This is different than utils.collapse_rfc2231_value() because it doesn't\n    # try to convert the value to a unicode.  Message.get_param() and\n    # Message.get_params() are both currently defined to return the tuple in\n    # the face of RFC 2231 parameters.\n    if isinstance(value, tuple):\n        return value[0], value[1], utils.unquote(value[2])\n    else:\n        return utils.unquote(value)\n\n\n\f\nclass Message:\n    \"\"\"Basic message object.\n\n    A message object is defined as something that has a bunch of RFC 2822\n    headers and a payload.  It may optionally have an envelope header\n    (a.k.a. Unix-From or From_ header).  If the message is a container (i.e. a\n    multipart or a message/rfc822), then the payload is a list of Message\n    objects, otherwise it is a string.\n\n    Message objects implement part of the `mapping' interface, which assumes\n    there is exactly one occurrence of the header per message.  Some headers\n    do in fact appear multiple times (e.g. Received) and for those headers,\n    you must use the explicit API to set or get all the headers.  Not all of\n    the mapping methods are implemented.\n    \"\"\"\n    def __init__(self):\n        self._headers = []\n        self._unixfrom = None\n        self._payload = None\n        self._charset = None\n        # Defaults for multipart messages\n        self.preamble = self.epilogue = None\n        self.defects = []\n        # Default content type\n        self._default_type = 'text/plain'\n\n    def __str__(self):\n        \"\"\"Return the entire formatted message as a string.\n        This includes the headers, body, and envelope header.\n        \"\"\"\n        return self.as_string(unixfrom=True)\n\n    def as_string(self, unixfrom=False):\n        \"\"\"Return the entire formatted message as a string.\n        Optional `unixfrom' when True, means include the Unix From_ envelope\n        header.\n\n        This is a convenience method and may not generate the message exactly\n        as you intend because by default it mangles lines that begin with\n        \"From \".  For more flexibility, use the flatten() method of a\n        Generator instance.\n        \"\"\"\n        from email.generator import Generator\n        fp = StringIO()\n        g = Generator(fp)\n        g.flatten(self, unixfrom=unixfrom)\n        return fp.getvalue()\n\n    def is_multipart(self):\n        \"\"\"Return True if the message consists of multiple parts.\"\"\"\n        return isinstance(self._payload, list)\n\n    #\n    # Unix From_ line\n    #\n    def set_unixfrom(self, unixfrom):\n        self._unixfrom = unixfrom\n\n    def get_unixfrom(self):\n        return self._unixfrom\n\n    #\n    # Payload manipulation.\n    #\n    def attach(self, payload):\n        \"\"\"Add the given payload to the current payload.\n\n        The current payload will always be a list of objects after this method\n        is called.  If you want to set the payload to a scalar object, use\n        set_payload() instead.\n        \"\"\"\n        if self._payload is None:\n            self._payload = [payload]\n        else:\n            self._payload.append(payload)\n\n    def get_payload(self, i=None, decode=False):\n        \"\"\"Return a reference to the payload.\n\n        The payload will either be a list object or a string.  If you mutate\n        the list object, you modify the message's payload in place.  Optional\n        i returns that index into the payload.\n\n        Optional decode is a flag indicating whether the payload should be\n        decoded or not, according to the Content-Transfer-Encoding header\n        (default is False).\n\n        When True and the message is not a multipart, the payload will be\n        decoded if this header's value is `quoted-printable' or `base64'.  If\n        some other encoding is used, or the header is missing, or if the\n        payload has bogus data (i.e. bogus base64 or uuencoded data), the\n        payload is returned as-is.\n\n        If the message is a multipart and the decode flag is True, then None\n        is returned.\n        \"\"\"\n        if i is None:\n            payload = self._payload\n        elif not isinstance(self._payload, list):\n            raise TypeError('Expected list, got %s' % type(self._payload))\n        else:\n            payload = self._payload[i]\n        if decode:\n            if self.is_multipart():\n                return None\n            cte = self.get('content-transfer-encoding', '').lower()\n            if cte == 'quoted-printable':\n                return utils._qdecode(payload)\n            elif cte == 'base64':\n                try:\n                    return utils._bdecode(payload)\n                except binascii.Error:\n                    # Incorrect padding\n                    return payload\n            elif cte in ('x-uuencode', 'uuencode', 'uue', 'x-uue'):\n                sfp = StringIO()\n                try:\n                    uu.decode(StringIO(payload+'\\n'), sfp, quiet=True)\n                    payload = sfp.getvalue()\n                except uu.Error:\n                    # Some decoding problem\n                    return payload\n        # Everything else, including encodings with 8bit or 7bit are returned\n        # unchanged.\n        return payload\n\n    def set_payload(self, payload, charset=None):\n        \"\"\"Set the payload to the given value.\n\n        Optional charset sets the message's default character set.  See\n        set_charset() for details.\n        \"\"\"\n        self._payload = payload\n        if charset is not None:\n            self.set_charset(charset)\n\n    def set_charset(self, charset):\n        \"\"\"Set the charset of the payload to a given character set.\n\n        charset can be a Charset instance, a string naming a character set, or\n        None.  If it is a string it will be converted to a Charset instance.\n        If charset is None, the charset parameter will be removed from the\n        Content-Type field.  Anything else will generate a TypeError.\n\n        The message will be assumed to be of type text/* encoded with\n        charset.input_charset.  It will be converted to charset.output_charset\n        and encoded properly, if needed, when generating the plain text\n        representation of the message.  MIME headers (MIME-Version,\n        Content-Type, Content-Transfer-Encoding) will be added as needed.\n\n        \"\"\"\n        if charset is None:\n            self.del_param('charset')\n            self._charset = None\n            return\n        if isinstance(charset, basestring):\n            charset = email.charset.Charset(charset)\n        if not isinstance(charset, email.charset.Charset):\n            raise TypeError(charset)\n        # BAW: should we accept strings that can serve as arguments to the\n        # Charset constructor?\n        self._charset = charset\n        if 'MIME-Version' not in self:\n            self.add_header('MIME-Version', '1.0')\n        if 'Content-Type' not in self:\n            self.add_header('Content-Type', 'text/plain',\n                            charset=charset.get_output_charset())\n        else:\n            self.set_param('charset', charset.get_output_charset())\n        if isinstance(self._payload, unicode):\n            self._payload = self._payload.encode(charset.output_charset)\n        if str(charset) != charset.get_output_charset():\n            self._payload = charset.body_encode(self._payload)\n        if 'Content-Transfer-Encoding' not in self:\n            cte = charset.get_body_encoding()\n            try:\n                cte(self)\n            except TypeError:\n                self._payload = charset.body_encode(self._payload)\n                self.add_header('Content-Transfer-Encoding', cte)\n\n    def get_charset(self):\n        \"\"\"Return the Charset instance associated with the message's payload.\n        \"\"\"\n        return self._charset\n\n    #\n    # MAPPING INTERFACE (partial)\n    #\n    def __len__(self):\n        \"\"\"Return the total number of headers, including duplicates.\"\"\"\n        return len(self._headers)\n\n    def __getitem__(self, name):\n        \"\"\"Get a header value.\n\n        Return None if the header is missing instead of raising an exception.\n\n        Note that if the header appeared multiple times, exactly which\n        occurrence gets returned is undefined.  Use get_all() to get all\n        the values matching a header field name.\n        \"\"\"\n        return self.get(name)\n\n    def __setitem__(self, name, val):\n        \"\"\"Set the value of a header.\n\n        Note: this does not overwrite an existing header with the same field\n        name.  Use __delitem__() first to delete any existing headers.\n        \"\"\"\n        self._headers.append((name, val))\n\n    def __delitem__(self, name):\n        \"\"\"Delete all occurrences of a header, if present.\n\n        Does not raise an exception if the header is missing.\n        \"\"\"\n        name = name.lower()\n        newheaders = []\n        for k, v in self._headers:\n            if k.lower() != name:\n                newheaders.append((k, v))\n        self._headers = newheaders\n\n    def __contains__(self, name):\n        return name.lower() in [k.lower() for k, v in self._headers]\n\n    def has_key(self, name):\n        \"\"\"Return true if the message contains the header.\"\"\"\n        missing = object()\n        return self.get(name, missing) is not missing\n\n    def keys(self):\n        \"\"\"Return a list of all the message's header field names.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return [k for k, v in self._headers]\n\n    def values(self):\n        \"\"\"Return a list of all the message's header values.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return [v for k, v in self._headers]\n\n    def items(self):\n        \"\"\"Get all the message's header fields and values.\n\n        These will be sorted in the order they appeared in the original\n        message, or were added to the message, and may contain duplicates.\n        Any fields deleted and re-inserted are always appended to the header\n        list.\n        \"\"\"\n        return self._headers[:]\n\n    def get(self, name, failobj=None):\n        \"\"\"Get a header value.\n\n        Like __getitem__() but return failobj instead of None when the field\n        is missing.\n        \"\"\"\n        name = name.lower()\n        for k, v in self._headers:\n            if k.lower() == name:\n                return v\n        return failobj\n\n    #\n    # Additional useful stuff\n    #\n\n    def get_all(self, name, failobj=None):\n        \"\"\"Return a list of all the values for the named field.\n\n        These will be sorted in the order they appeared in the original\n        message, and may contain duplicates.  Any fields deleted and\n        re-inserted are always appended to the header list.\n\n        If no such fields exist, failobj is returned (defaults to None).\n        \"\"\"\n        values = []\n        name = name.lower()\n        for k, v in self._headers:\n            if k.lower() == name:\n                values.append(v)\n        if not values:\n            return failobj\n        return values\n\n    def add_header(self, _name, _value, **_params):\n        \"\"\"Extended header setting.\n\n        name is the header field to add.  keyword arguments can be used to set\n        additional parameters for the header field, with underscores converted\n        to dashes.  Normally the parameter will be added as key=\"value\" unless\n        value is None, in which case only the key will be added.  If a\n        parameter value contains non-ASCII characters it must be specified as a\n        three-tuple of (charset, language, value), in which case it will be\n        encoded according to RFC2231 rules.\n\n        Example:\n\n        msg.add_header('content-disposition', 'attachment', filename='bud.gif')\n        \"\"\"\n        parts = []\n        for k, v in _params.items():\n            if v is None:\n                parts.append(k.replace('_', '-'))\n            else:\n                parts.append(_formatparam(k.replace('_', '-'), v))\n        if _value is not None:\n            parts.insert(0, _value)\n        self._headers.append((_name, SEMISPACE.join(parts)))\n\n    def replace_header(self, _name, _value):\n        \"\"\"Replace a header.\n\n        Replace the first matching header found in the message, retaining\n        header order and case.  If no matching header was found, a KeyError is\n        raised.\n        \"\"\"\n        _name = _name.lower()\n        for i, (k, v) in zip(range(len(self._headers)), self._headers):\n            if k.lower() == _name:\n                self._headers[i] = (k, _value)\n                break\n        else:\n            raise KeyError(_name)\n\n    #\n    # Use these three methods instead of the three above.\n    #\n\n    def get_content_type(self):\n        \"\"\"Return the message's content type.\n\n        The returned string is coerced to lower case of the form\n        `maintype/subtype'.  If there was no Content-Type header in the\n        message, the default type as given by get_default_type() will be\n        returned.  Since according to RFC 2045, messages always have a default\n        type this will always return a value.\n\n        RFC 2045 defines a message's default type to be text/plain unless it\n        appears inside a multipart/digest container, in which case it would be\n        message/rfc822.\n        \"\"\"\n        missing = object()\n        value = self.get('content-type', missing)\n        if value is missing:\n            # This should have no parameters\n            return self.get_default_type()\n        ctype = _splitparam(value)[0].lower()\n        # RFC 2045, section 5.2 says if its invalid, use text/plain\n        if ctype.count('/') != 1:\n            return 'text/plain'\n        return ctype\n\n    def get_content_maintype(self):\n        \"\"\"Return the message's main content type.\n\n        This is the `maintype' part of the string returned by\n        get_content_type().\n        \"\"\"\n        ctype = self.get_content_type()\n        return ctype.split('/')[0]\n\n    def get_content_subtype(self):\n        \"\"\"Returns the message's sub-content type.\n\n        This is the `subtype' part of the string returned by\n        get_content_type().\n        \"\"\"\n        ctype = self.get_content_type()\n        return ctype.split('/')[1]\n\n    def get_default_type(self):\n        \"\"\"Return the `default' content type.\n\n        Most messages have a default content type of text/plain, except for\n        messages that are subparts of multipart/digest containers.  Such\n        subparts have a default content type of message/rfc822.\n        \"\"\"\n        return self._default_type\n\n    def set_default_type(self, ctype):\n        \"\"\"Set the `default' content type.\n\n        ctype should be either \"text/plain\" or \"message/rfc822\", although this\n        is not enforced.  The default content type is not stored in the\n        Content-Type header.\n        \"\"\"\n        self._default_type = ctype\n\n    def _get_params_preserve(self, failobj, header):\n        # Like get_params() but preserves the quoting of values.  BAW:\n        # should this be part of the public interface?\n        missing = object()\n        value = self.get(header, missing)\n        if value is missing:\n            return failobj\n        params = []\n        for p in _parseparam(';' + value):\n            try:\n                name, val = p.split('=', 1)\n                name = name.strip()\n                val = val.strip()\n            except ValueError:\n                # Must have been a bare attribute\n                name = p.strip()\n                val = ''\n            params.append((name, val))\n        params = utils.decode_params(params)\n        return params\n\n    def get_params(self, failobj=None, header='content-type', unquote=True):\n        \"\"\"Return the message's Content-Type parameters, as a list.\n\n        The elements of the returned list are 2-tuples of key/value pairs, as\n        split on the `=' sign.  The left hand side of the `=' is the key,\n        while the right hand side is the value.  If there is no `=' sign in\n        the parameter the value is the empty string.  The value is as\n        described in the get_param() method.\n\n        Optional failobj is the object to return if there is no Content-Type\n        header.  Optional header is the header to search instead of\n        Content-Type.  If unquote is True, the value is unquoted.\n        \"\"\"\n        missing = object()\n        params = self._get_params_preserve(missing, header)\n        if params is missing:\n            return failobj\n        if unquote:\n            return [(k, _unquotevalue(v)) for k, v in params]\n        else:\n            return params\n\n    def get_param(self, param, failobj=None, header='content-type',\n                  unquote=True):\n        \"\"\"Return the parameter value if found in the Content-Type header.\n\n        Optional failobj is the object to return if there is no Content-Type\n        header, or the Content-Type header has no such parameter.  Optional\n        header is the header to search instead of Content-Type.\n\n        Parameter keys are always compared case insensitively.  The return\n        value can either be a string, or a 3-tuple if the parameter was RFC\n        2231 encoded.  When it's a 3-tuple, the elements of the value are of\n        the form (CHARSET, LANGUAGE, VALUE).  Note that both CHARSET and\n        LANGUAGE can be None, in which case you should consider VALUE to be\n        encoded in the us-ascii charset.  You can usually ignore LANGUAGE.\n\n        Your application should be prepared to deal with 3-tuple return\n        values, and can convert the parameter to a Unicode string like so:\n\n            param = msg.get_param('foo')\n            if isinstance(param, tuple):\n                param = unicode(param[2], param[0] or 'us-ascii')\n\n        In any case, the parameter value (either the returned string, or the\n        VALUE item in the 3-tuple) is always unquoted, unless unquote is set\n        to False.\n        \"\"\"\n        if header not in self:\n            return failobj\n        for k, v in self._get_params_preserve(failobj, header):\n            if k.lower() == param.lower():\n                if unquote:\n                    return _unquotevalue(v)\n                else:\n                    return v\n        return failobj\n\n    def set_param(self, param, value, header='Content-Type', requote=True,\n                  charset=None, language=''):\n        \"\"\"Set a parameter in the Content-Type header.\n\n        If the parameter already exists in the header, its value will be\n        replaced with the new value.\n\n        If header is Content-Type and has not yet been defined for this\n        message, it will be set to \"text/plain\" and the new parameter and\n        value will be appended as per RFC 2045.\n\n        An alternate header can specified in the header argument, and all\n        parameters will be quoted as necessary unless requote is False.\n\n        If charset is specified, the parameter will be encoded according to RFC\n        2231.  Optional language specifies the RFC 2231 language, defaulting\n        to the empty string.  Both charset and language should be strings.\n        \"\"\"\n        if not isinstance(value, tuple) and charset:\n            value = (charset, language, value)\n\n        if header not in self and header.lower() == 'content-type':\n            ctype = 'text/plain'\n        else:\n            ctype = self.get(header)\n        if not self.get_param(param, header=header):\n            if not ctype:\n                ctype = _formatparam(param, value, requote)\n            else:\n                ctype = SEMISPACE.join(\n                    [ctype, _formatparam(param, value, requote)])\n        else:\n            ctype = ''\n            for old_param, old_value in self.get_params(header=header,\n                                                        unquote=requote):\n                append_param = ''\n                if old_param.lower() == param.lower():\n                    append_param = _formatparam(param, value, requote)\n                else:\n                    append_param = _formatparam(old_param, old_value, requote)\n                if not ctype:\n                    ctype = append_param\n                else:\n                    ctype = SEMISPACE.join([ctype, append_param])\n        if ctype != self.get(header):\n            del self[header]\n            self[header] = ctype\n\n    def del_param(self, param, header='content-type', requote=True):\n        \"\"\"Remove the given parameter completely from the Content-Type header.\n\n        The header will be re-written in place without the parameter or its\n        value. All values will be quoted as necessary unless requote is\n        False.  Optional header specifies an alternative to the Content-Type\n        header.\n        \"\"\"\n        if header not in self:\n            return\n        new_ctype = ''\n        for p, v in self.get_params(header=header, unquote=requote):\n            if p.lower() != param.lower():\n                if not new_ctype:\n                    new_ctype = _formatparam(p, v, requote)\n                else:\n                    new_ctype = SEMISPACE.join([new_ctype,\n                                                _formatparam(p, v, requote)])\n        if new_ctype != self.get(header):\n            del self[header]\n            self[header] = new_ctype\n\n    def set_type(self, type, header='Content-Type', requote=True):\n        \"\"\"Set the main type and subtype for the Content-Type header.\n\n        type must be a string in the form \"maintype/subtype\", otherwise a\n        ValueError is raised.\n\n        This method replaces the Content-Type header, keeping all the\n        parameters in place.  If requote is False, this leaves the existing\n        header's quoting as is.  Otherwise, the parameters will be quoted (the\n        default).\n\n        An alternative header can be specified in the header argument.  When\n        the Content-Type header is set, we'll always also add a MIME-Version\n        header.\n        \"\"\"\n        # BAW: should we be strict?\n        if not type.count('/') == 1:\n            raise ValueError\n        # Set the Content-Type, you get a MIME-Version\n        if header.lower() == 'content-type':\n            del self['mime-version']\n            self['MIME-Version'] = '1.0'\n        if header not in self:\n            self[header] = type\n            return\n        params = self.get_params(header=header, unquote=requote)\n        del self[header]\n        self[header] = type\n        # Skip the first param; it's the old type.\n        for p, v in params[1:]:\n            self.set_param(p, v, header, requote)\n\n    def get_filename(self, failobj=None):\n        \"\"\"Return the filename associated with the payload if present.\n\n        The filename is extracted from the Content-Disposition header's\n        `filename' parameter, and it is unquoted.  If that header is missing\n        the `filename' parameter, this method falls back to looking for the\n        `name' parameter.\n        \"\"\"\n        missing = object()\n        filename = self.get_param('filename', missing, 'content-disposition')\n        if filename is missing:\n            filename = self.get_param('name', missing, 'content-type')\n        if filename is missing:\n            return failobj\n        return utils.collapse_rfc2231_value(filename).strip()\n\n    def get_boundary(self, failobj=None):\n        \"\"\"Return the boundary associated with the payload if present.\n\n        The boundary is extracted from the Content-Type header's `boundary'\n        parameter, and it is unquoted.\n        \"\"\"\n        missing = object()\n        boundary = self.get_param('boundary', missing)\n        if boundary is missing:\n            return failobj\n        # RFC 2046 says that boundaries may begin but not end in w/s\n        return utils.collapse_rfc2231_value(boundary).rstrip()\n\n    def set_boundary(self, boundary):\n        \"\"\"Set the boundary parameter in Content-Type to 'boundary'.\n\n        This is subtly different than deleting the Content-Type header and\n        adding a new one with a new boundary parameter via add_header().  The\n        main difference is that using the set_boundary() method preserves the\n        order of the Content-Type header in the original message.\n\n        HeaderParseError is raised if the message has no Content-Type header.\n        \"\"\"\n        missing = object()\n        params = self._get_params_preserve(missing, 'content-type')\n        if params is missing:\n            # There was no Content-Type header, and we don't know what type\n            # to set it to, so raise an exception.\n            raise errors.HeaderParseError('No Content-Type header found')\n        newparams = []\n        foundp = False\n        for pk, pv in params:\n            if pk.lower() == 'boundary':\n                newparams.append(('boundary', '\"%s\"' % boundary))\n                foundp = True\n            else:\n                newparams.append((pk, pv))\n        if not foundp:\n            # The original Content-Type header had no boundary attribute.\n            # Tack one on the end.  BAW: should we raise an exception\n            # instead???\n            newparams.append(('boundary', '\"%s\"' % boundary))\n        # Replace the existing Content-Type header with the new value\n        newheaders = []\n        for h, v in self._headers:\n            if h.lower() == 'content-type':\n                parts = []\n                for k, v in newparams:\n                    if v == '':\n                        parts.append(k)\n                    else:\n                        parts.append('%s=%s' % (k, v))\n                newheaders.append((h, SEMISPACE.join(parts)))\n\n            else:\n                newheaders.append((h, v))\n        self._headers = newheaders\n\n    def get_content_charset(self, failobj=None):\n        \"\"\"Return the charset parameter of the Content-Type header.\n\n        The returned string is always coerced to lower case.  If there is no\n        Content-Type header, or if that header has no charset parameter,\n        failobj is returned.\n        \"\"\"\n        missing = object()\n        charset = self.get_param('charset', missing)\n        if charset is missing:\n            return failobj\n        if isinstance(charset, tuple):\n            # RFC 2231 encoded, so decode it, and it better end up as ascii.\n            pcharset = charset[0] or 'us-ascii'\n            try:\n                # LookupError will be raised if the charset isn't known to\n                # Python.  UnicodeError will be raised if the encoded text\n                # contains a character not in the charset.\n                charset = unicode(charset[2], pcharset).encode('us-ascii')\n            except (LookupError, UnicodeError):\n                charset = charset[2]\n        # charset character must be in us-ascii range\n        try:\n            if isinstance(charset, str):\n                charset = unicode(charset, 'us-ascii')\n            charset = charset.encode('us-ascii')\n        except UnicodeError:\n            return failobj\n        # RFC 2046, $4.1.2 says charsets are not case sensitive\n        return charset.lower()\n\n    def get_charsets(self, failobj=None):\n        \"\"\"Return a list containing the charset(s) used in this message.\n\n        The returned list of items describes the Content-Type headers'\n        charset parameter for this message and all the subparts in its\n        payload.\n\n        Each item will either be a string (the value of the charset parameter\n        in the Content-Type header of that part) or the value of the\n        'failobj' parameter (defaults to None), if the part does not have a\n        main MIME type of \"text\", or the charset is not defined.\n\n        The list will contain one string for each part of the message, plus\n        one for the container message (i.e. self), so that a non-multipart\n        message will still return a list of length 1.\n        \"\"\"\n        return [part.get_content_charset(failobj) for part in self.walk()]\n\n    # I.e. def walk(self): ...\n    from email.iterators import walk\n", 
    "email.mime.__init__": "", 
    "email.parser": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Barry Warsaw, Thomas Wouters, Anthony Baxter\n# Contact: email-sig@python.org\n\n\"\"\"A parser of RFC 2822 and MIME email messages.\"\"\"\n\n__all__ = ['Parser', 'HeaderParser']\n\nimport warnings\nfrom cStringIO import StringIO\n\nfrom email.feedparser import FeedParser\nfrom email.message import Message\n\n\n\f\nclass Parser:\n    def __init__(self, *args, **kws):\n        \"\"\"Parser of RFC 2822 and MIME email messages.\n\n        Creates an in-memory object tree representing the email message, which\n        can then be manipulated and turned over to a Generator to return the\n        textual representation of the message.\n\n        The string must be formatted as a block of RFC 2822 headers and header\n        continuation lines, optionally preceeded by a `Unix-from' header.  The\n        header block is terminated either by the end of the string or by a\n        blank line.\n\n        _class is the class to instantiate for new message objects when they\n        must be created.  This class must have a constructor that can take\n        zero arguments.  Default is Message.Message.\n        \"\"\"\n        if len(args) >= 1:\n            if '_class' in kws:\n                raise TypeError(\"Multiple values for keyword arg '_class'\")\n            kws['_class'] = args[0]\n        if len(args) == 2:\n            if 'strict' in kws:\n                raise TypeError(\"Multiple values for keyword arg 'strict'\")\n            kws['strict'] = args[1]\n        if len(args) > 2:\n            raise TypeError('Too many arguments')\n        if '_class' in kws:\n            self._class = kws['_class']\n            del kws['_class']\n        else:\n            self._class = Message\n        if 'strict' in kws:\n            warnings.warn(\"'strict' argument is deprecated (and ignored)\",\n                          DeprecationWarning, 2)\n            del kws['strict']\n        if kws:\n            raise TypeError('Unexpected keyword arguments')\n\n    def parse(self, fp, headersonly=False):\n        \"\"\"Create a message structure from the data in a file.\n\n        Reads all the data from the file and returns the root of the message\n        structure.  Optional headersonly is a flag specifying whether to stop\n        parsing after reading the headers or not.  The default is False,\n        meaning it parses the entire contents of the file.\n        \"\"\"\n        feedparser = FeedParser(self._class)\n        if headersonly:\n            feedparser._set_headersonly()\n        while True:\n            data = fp.read(8192)\n            if not data:\n                break\n            feedparser.feed(data)\n        return feedparser.close()\n\n    def parsestr(self, text, headersonly=False):\n        \"\"\"Create a message structure from a string.\n\n        Returns the root of the message structure.  Optional headersonly is a\n        flag specifying whether to stop parsing after reading the headers or\n        not.  The default is False, meaning it parses the entire contents of\n        the file.\n        \"\"\"\n        return self.parse(StringIO(text), headersonly=headersonly)\n\n\n\f\nclass HeaderParser(Parser):\n    def parse(self, fp, headersonly=True):\n        return Parser.parse(self, fp, True)\n\n    def parsestr(self, text, headersonly=True):\n        return Parser.parsestr(self, text, True)\n", 
    "email.quoprimime": "# Copyright (C) 2001-2006 Python Software Foundation\n# Author: Ben Gertzfield\n# Contact: email-sig@python.org\n\n\"\"\"Quoted-printable content transfer encoding per RFCs 2045-2047.\n\nThis module handles the content transfer encoding method defined in RFC 2045\nto encode US ASCII-like 8-bit data called `quoted-printable'.  It is used to\nsafely encode text that is in a character set similar to the 7-bit US ASCII\ncharacter set, but that includes some 8-bit characters that are normally not\nallowed in email bodies or headers.\n\nQuoted-printable is very space-inefficient for encoding binary files; use the\nemail.base64mime module for that instead.\n\nThis module provides an interface to encode and decode both headers and bodies\nwith quoted-printable encoding.\n\nRFC 2045 defines a method for including character set information in an\n`encoded-word' in a header.  This method is commonly used for 8-bit real names\nin To:/From:/Cc: etc. fields, as well as Subject: lines.\n\nThis module does not do the line wrapping or end-of-line character\nconversion necessary for proper internationalized headers; it only\ndoes dumb encoding and decoding.  To deal with the various line\nwrapping issues, use the email.header module.\n\"\"\"\n\n__all__ = [\n    'body_decode',\n    'body_encode',\n    'body_quopri_check',\n    'body_quopri_len',\n    'decode',\n    'decodestring',\n    'encode',\n    'encodestring',\n    'header_decode',\n    'header_encode',\n    'header_quopri_check',\n    'header_quopri_len',\n    'quote',\n    'unquote',\n    ]\n\nimport re\n\nfrom string import hexdigits\nfrom email.utils import fix_eols\n\nCRLF = '\\r\\n'\nNL = '\\n'\n\n# See also Charset.py\nMISC_LEN = 7\n\nhqre = re.compile(r'[^-a-zA-Z0-9!*+/ ]')\nbqre = re.compile(r'[^ !-<>-~\\t]')\n\n\n\f\n# Helpers\ndef header_quopri_check(c):\n    \"\"\"Return True if the character should be escaped with header quopri.\"\"\"\n    return bool(hqre.match(c))\n\n\ndef body_quopri_check(c):\n    \"\"\"Return True if the character should be escaped with body quopri.\"\"\"\n    return bool(bqre.match(c))\n\n\ndef header_quopri_len(s):\n    \"\"\"Return the length of str when it is encoded with header quopri.\"\"\"\n    count = 0\n    for c in s:\n        if hqre.match(c):\n            count += 3\n        else:\n            count += 1\n    return count\n\n\ndef body_quopri_len(str):\n    \"\"\"Return the length of str when it is encoded with body quopri.\"\"\"\n    count = 0\n    for c in str:\n        if bqre.match(c):\n            count += 3\n        else:\n            count += 1\n    return count\n\n\ndef _max_append(L, s, maxlen, extra=''):\n    if not L:\n        L.append(s.lstrip())\n    elif len(L[-1]) + len(s) <= maxlen:\n        L[-1] += extra + s\n    else:\n        L.append(s.lstrip())\n\n\ndef unquote(s):\n    \"\"\"Turn a string in the form =AB to the ASCII character with value 0xab\"\"\"\n    return chr(int(s[1:3], 16))\n\n\ndef quote(c):\n    return \"=%02X\" % ord(c)\n\n\n\f\ndef header_encode(header, charset=\"iso-8859-1\", keep_eols=False,\n                  maxlinelen=76, eol=NL):\n    \"\"\"Encode a single header line with quoted-printable (like) encoding.\n\n    Defined in RFC 2045, this `Q' encoding is similar to quoted-printable, but\n    used specifically for email header fields to allow charsets with mostly 7\n    bit characters (and some 8 bit) to remain more or less readable in non-RFC\n    2045 aware mail clients.\n\n    charset names the character set to use to encode the header.  It defaults\n    to iso-8859-1.\n\n    The resulting string will be in the form:\n\n    \"=?charset?q?I_f=E2rt_in_your_g=E8n=E8ral_dire=E7tion?\\\\n\n      =?charset?q?Silly_=C8nglish_Kn=EEghts?=\"\n\n    with each line wrapped safely at, at most, maxlinelen characters (defaults\n    to 76 characters).  If maxlinelen is None, the entire string is encoded in\n    one chunk with no splitting.\n\n    End-of-line characters (\\\\r, \\\\n, \\\\r\\\\n) will be automatically converted\n    to the canonical email line separator \\\\r\\\\n unless the keep_eols\n    parameter is True (the default is False).\n\n    Each line of the header will be terminated in the value of eol, which\n    defaults to \"\\\\n\".  Set this to \"\\\\r\\\\n\" if you are using the result of\n    this function directly in email.\n    \"\"\"\n    # Return empty headers unchanged\n    if not header:\n        return header\n\n    if not keep_eols:\n        header = fix_eols(header)\n\n    # Quopri encode each line, in encoded chunks no greater than maxlinelen in\n    # length, after the RFC chrome is added in.\n    quoted = []\n    if maxlinelen is None:\n        # An obnoxiously large number that's good enough\n        max_encoded = 100000\n    else:\n        max_encoded = maxlinelen - len(charset) - MISC_LEN - 1\n\n    for c in header:\n        # Space may be represented as _ instead of =20 for readability\n        if c == ' ':\n            _max_append(quoted, '_', max_encoded)\n        # These characters can be included verbatim\n        elif not hqre.match(c):\n            _max_append(quoted, c, max_encoded)\n        # Otherwise, replace with hex value like =E2\n        else:\n            _max_append(quoted, \"=%02X\" % ord(c), max_encoded)\n\n    # Now add the RFC chrome to each encoded chunk and glue the chunks\n    # together.  BAW: should we be able to specify the leading whitespace in\n    # the joiner?\n    joiner = eol + ' '\n    return joiner.join(['=?%s?q?%s?=' % (charset, line) for line in quoted])\n\n\n\f\ndef encode(body, binary=False, maxlinelen=76, eol=NL):\n    \"\"\"Encode with quoted-printable, wrapping at maxlinelen characters.\n\n    If binary is False (the default), end-of-line characters will be converted\n    to the canonical email end-of-line sequence \\\\r\\\\n.  Otherwise they will\n    be left verbatim.\n\n    Each line of encoded text will end with eol, which defaults to \"\\\\n\".  Set\n    this to \"\\\\r\\\\n\" if you will be using the result of this function directly\n    in an email.\n\n    Each line will be wrapped at, at most, maxlinelen characters (defaults to\n    76 characters).  Long lines will have the `soft linefeed' quoted-printable\n    character \"=\" appended to them, so the decoded text will be identical to\n    the original text.\n    \"\"\"\n    if not body:\n        return body\n\n    if not binary:\n        body = fix_eols(body)\n\n    # BAW: We're accumulating the body text by string concatenation.  That\n    # can't be very efficient, but I don't have time now to rewrite it.  It\n    # just feels like this algorithm could be more efficient.\n    encoded_body = ''\n    lineno = -1\n    # Preserve line endings here so we can check later to see an eol needs to\n    # be added to the output later.\n    lines = body.splitlines(1)\n    for line in lines:\n        # But strip off line-endings for processing this line.\n        if line.endswith(CRLF):\n            line = line[:-2]\n        elif line[-1] in CRLF:\n            line = line[:-1]\n\n        lineno += 1\n        encoded_line = ''\n        prev = None\n        linelen = len(line)\n        # Now we need to examine every character to see if it needs to be\n        # quopri encoded.  BAW: again, string concatenation is inefficient.\n        for j in range(linelen):\n            c = line[j]\n            prev = c\n            if bqre.match(c):\n                c = quote(c)\n            elif j+1 == linelen:\n                # Check for whitespace at end of line; special case\n                if c not in ' \\t':\n                    encoded_line += c\n                prev = c\n                continue\n            # Check to see to see if the line has reached its maximum length\n            if len(encoded_line) + len(c) >= maxlinelen:\n                encoded_body += encoded_line + '=' + eol\n                encoded_line = ''\n            encoded_line += c\n        # Now at end of line..\n        if prev and prev in ' \\t':\n            # Special case for whitespace at end of file\n            if lineno + 1 == len(lines):\n                prev = quote(prev)\n                if len(encoded_line) + len(prev) > maxlinelen:\n                    encoded_body += encoded_line + '=' + eol + prev\n                else:\n                    encoded_body += encoded_line + prev\n            # Just normal whitespace at end of line\n            else:\n                encoded_body += encoded_line + prev + '=' + eol\n            encoded_line = ''\n        # Now look at the line we just finished and it has a line ending, we\n        # need to add eol to the end of the line.\n        if lines[lineno].endswith(CRLF) or lines[lineno][-1] in CRLF:\n            encoded_body += encoded_line + eol\n        else:\n            encoded_body += encoded_line\n        encoded_line = ''\n    return encoded_body\n\n\n# For convenience and backwards compatibility w/ standard base64 module\nbody_encode = encode\nencodestring = encode\n\n\n\f\n# BAW: I'm not sure if the intent was for the signature of this function to be\n# the same as base64MIME.decode() or not...\ndef decode(encoded, eol=NL):\n    \"\"\"Decode a quoted-printable string.\n\n    Lines are separated with eol, which defaults to \\\\n.\n    \"\"\"\n    if not encoded:\n        return encoded\n    # BAW: see comment in encode() above.  Again, we're building up the\n    # decoded string with string concatenation, which could be done much more\n    # efficiently.\n    decoded = ''\n\n    for line in encoded.splitlines():\n        line = line.rstrip()\n        if not line:\n            decoded += eol\n            continue\n\n        i = 0\n        n = len(line)\n        while i < n:\n            c = line[i]\n            if c != '=':\n                decoded += c\n                i += 1\n            # Otherwise, c == \"=\".  Are we at the end of the line?  If so, add\n            # a soft line break.\n            elif i+1 == n:\n                i += 1\n                continue\n            # Decode if in form =AB\n            elif i+2 < n and line[i+1] in hexdigits and line[i+2] in hexdigits:\n                decoded += unquote(line[i:i+3])\n                i += 3\n            # Otherwise, not in form =AB, pass literally\n            else:\n                decoded += c\n                i += 1\n\n            if i == n:\n                decoded += eol\n    # Special case if original string did not end with eol\n    if not encoded.endswith(eol) and decoded.endswith(eol):\n        decoded = decoded[:-1]\n    return decoded\n\n\n# For convenience and backwards compatibility w/ standard base64 module\nbody_decode = decode\ndecodestring = decode\n\n\n\f\ndef _unquote_match(match):\n    \"\"\"Turn a match in the form =AB to the ASCII character with value 0xab\"\"\"\n    s = match.group(0)\n    return unquote(s)\n\n\n# Header decoding is done a bit differently\ndef header_decode(s):\n    \"\"\"Decode a string encoded with RFC 2045 MIME header `Q' encoding.\n\n    This function does not parse a full MIME header value encoded with\n    quoted-printable (like =?iso-8895-1?q?Hello_World?=) -- please use\n    the high level email.header class for that functionality.\n    \"\"\"\n    s = s.replace('_', ' ')\n    return re.sub(r'=[a-fA-F0-9]{2}', _unquote_match, s)\n", 
    "email.utils": "# Copyright (C) 2001-2010 Python Software Foundation\n# Author: Barry Warsaw\n# Contact: email-sig@python.org\n\n\"\"\"Miscellaneous utilities.\"\"\"\n\n__all__ = [\n    'collapse_rfc2231_value',\n    'decode_params',\n    'decode_rfc2231',\n    'encode_rfc2231',\n    'formataddr',\n    'formatdate',\n    'getaddresses',\n    'make_msgid',\n    'mktime_tz',\n    'parseaddr',\n    'parsedate',\n    'parsedate_tz',\n    'unquote',\n    ]\n\nimport os\nimport re\nimport time\nimport base64\nimport random\nimport socket\nimport urllib\nimport warnings\n\nfrom email._parseaddr import quote\nfrom email._parseaddr import AddressList as _AddressList\nfrom email._parseaddr import mktime_tz\n\n# We need wormarounds for bugs in these methods in older Pythons (see below)\nfrom email._parseaddr import parsedate as _parsedate\nfrom email._parseaddr import parsedate_tz as _parsedate_tz\n\nfrom quopri import decodestring as _qdecode\n\n# Intrapackage imports\nfrom email.encoders import _bencode, _qencode\n\nCOMMASPACE = ', '\nEMPTYSTRING = ''\nUEMPTYSTRING = u''\nCRLF = '\\r\\n'\nTICK = \"'\"\n\nspecialsre = re.compile(r'[][\\\\()<>@,:;\".]')\nescapesre = re.compile(r'[][\\\\()\"]')\n\n\n\f\n# Helpers\n\ndef _identity(s):\n    return s\n\n\ndef _bdecode(s):\n    \"\"\"Decodes a base64 string.\n\n    This function is equivalent to base64.decodestring and it's retained only\n    for backward compatibility. It used to remove the last \\\\n of the decoded\n    string, if it had any (see issue 7143).\n    \"\"\"\n    if not s:\n        return s\n    return base64.decodestring(s)\n\n\n\f\ndef fix_eols(s):\n    \"\"\"Replace all line-ending characters with \\\\r\\\\n.\"\"\"\n    # Fix newlines with no preceding carriage return\n    s = re.sub(r'(?<!\\r)\\n', CRLF, s)\n    # Fix carriage returns with no following newline\n    s = re.sub(r'\\r(?!\\n)', CRLF, s)\n    return s\n\n\n\f\ndef formataddr(pair):\n    \"\"\"The inverse of parseaddr(), this takes a 2-tuple of the form\n    (realname, email_address) and returns the string value suitable\n    for an RFC 2822 From, To or Cc header.\n\n    If the first element of pair is false, then the second element is\n    returned unmodified.\n    \"\"\"\n    name, address = pair\n    if name:\n        quotes = ''\n        if specialsre.search(name):\n            quotes = '\"'\n        name = escapesre.sub(r'\\\\\\g<0>', name)\n        return '%s%s%s <%s>' % (quotes, name, quotes, address)\n    return address\n\n\n\f\ndef getaddresses(fieldvalues):\n    \"\"\"Return a list of (REALNAME, EMAIL) for each fieldvalue.\"\"\"\n    all = COMMASPACE.join(fieldvalues)\n    a = _AddressList(all)\n    return a.addresslist\n\n\n\f\necre = re.compile(r'''\n  =\\?                   # literal =?\n  (?P<charset>[^?]*?)   # non-greedy up to the next ? is the charset\n  \\?                    # literal ?\n  (?P<encoding>[qb])    # either a \"q\" or a \"b\", case insensitive\n  \\?                    # literal ?\n  (?P<atom>.*?)         # non-greedy up to the next ?= is the atom\n  \\?=                   # literal ?=\n  ''', re.VERBOSE | re.IGNORECASE)\n\n\n\f\ndef formatdate(timeval=None, localtime=False, usegmt=False):\n    \"\"\"Returns a date string as specified by RFC 2822, e.g.:\n\n    Fri, 09 Nov 2001 01:08:47 -0000\n\n    Optional timeval if given is a floating point time value as accepted by\n    gmtime() and localtime(), otherwise the current time is used.\n\n    Optional localtime is a flag that when True, interprets timeval, and\n    returns a date relative to the local timezone instead of UTC, properly\n    taking daylight savings time into account.\n\n    Optional argument usegmt means that the timezone is written out as\n    an ascii string, not numeric one (so \"GMT\" instead of \"+0000\"). This\n    is needed for HTTP, and is only used when localtime==False.\n    \"\"\"\n    # Note: we cannot use strftime() because that honors the locale and RFC\n    # 2822 requires that day and month names be the English abbreviations.\n    if timeval is None:\n        timeval = time.time()\n    if localtime:\n        now = time.localtime(timeval)\n        # Calculate timezone offset, based on whether the local zone has\n        # daylight savings time, and whether DST is in effect.\n        if time.daylight and now[-1]:\n            offset = time.altzone\n        else:\n            offset = time.timezone\n        hours, minutes = divmod(abs(offset), 3600)\n        # Remember offset is in seconds west of UTC, but the timezone is in\n        # minutes east of UTC, so the signs differ.\n        if offset > 0:\n            sign = '-'\n        else:\n            sign = '+'\n        zone = '%s%02d%02d' % (sign, hours, minutes // 60)\n    else:\n        now = time.gmtime(timeval)\n        # Timezone offset is always -0000\n        if usegmt:\n            zone = 'GMT'\n        else:\n            zone = '-0000'\n    return '%s, %02d %s %04d %02d:%02d:%02d %s' % (\n        ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'][now[6]],\n        now[2],\n        ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n         'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'][now[1] - 1],\n        now[0], now[3], now[4], now[5],\n        zone)\n\n\n\f\ndef make_msgid(idstring=None):\n    \"\"\"Returns a string suitable for RFC 2822 compliant Message-ID, e.g:\n\n    <20020201195627.33539.96671@nightshade.la.mastaler.com>\n\n    Optional idstring if given is a string used to strengthen the\n    uniqueness of the message id.\n    \"\"\"\n    timeval = time.time()\n    utcdate = time.strftime('%Y%m%d%H%M%S', time.gmtime(timeval))\n    pid = os.getpid()\n    randint = random.randrange(100000)\n    if idstring is None:\n        idstring = ''\n    else:\n        idstring = '.' + idstring\n    idhost = socket.getfqdn()\n    msgid = '<%s.%s.%s%s@%s>' % (utcdate, pid, randint, idstring, idhost)\n    return msgid\n\n\n\f\n# These functions are in the standalone mimelib version only because they've\n# subsequently been fixed in the latest Python versions.  We use this to worm\n# around broken older Pythons.\ndef parsedate(data):\n    if not data:\n        return None\n    return _parsedate(data)\n\n\ndef parsedate_tz(data):\n    if not data:\n        return None\n    return _parsedate_tz(data)\n\n\ndef parseaddr(addr):\n    addrs = _AddressList(addr).addresslist\n    if not addrs:\n        return '', ''\n    return addrs[0]\n\n\n# rfc822.unquote() doesn't properly de-backslash-ify in Python pre-2.3.\ndef unquote(str):\n    \"\"\"Remove quotes from a string.\"\"\"\n    if len(str) > 1:\n        if str.startswith('\"') and str.endswith('\"'):\n            return str[1:-1].replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n        if str.startswith('<') and str.endswith('>'):\n            return str[1:-1]\n    return str\n\n\n\f\n# RFC2231-related functions - parameter encoding and decoding\ndef decode_rfc2231(s):\n    \"\"\"Decode string according to RFC 2231\"\"\"\n    parts = s.split(TICK, 2)\n    if len(parts) <= 2:\n        return None, None, s\n    return parts\n\n\ndef encode_rfc2231(s, charset=None, language=None):\n    \"\"\"Encode string according to RFC 2231.\n\n    If neither charset nor language is given, then s is returned as-is.  If\n    charset is given but not language, the string is encoded using the empty\n    string for language.\n    \"\"\"\n    import urllib\n    s = urllib.quote(s, safe='')\n    if charset is None and language is None:\n        return s\n    if language is None:\n        language = ''\n    return \"%s'%s'%s\" % (charset, language, s)\n\n\nrfc2231_continuation = re.compile(r'^(?P<name>\\w+)\\*((?P<num>[0-9]+)\\*?)?$')\n\ndef decode_params(params):\n    \"\"\"Decode parameters list according to RFC 2231.\n\n    params is a sequence of 2-tuples containing (param name, string value).\n    \"\"\"\n    # Copy params so we don't mess with the original\n    params = params[:]\n    new_params = []\n    # Map parameter's name to a list of continuations.  The values are a\n    # 3-tuple of the continuation number, the string value, and a flag\n    # specifying whether a particular segment is %-encoded.\n    rfc2231_params = {}\n    name, value = params.pop(0)\n    new_params.append((name, value))\n    while params:\n        name, value = params.pop(0)\n        if name.endswith('*'):\n            encoded = True\n        else:\n            encoded = False\n        value = unquote(value)\n        mo = rfc2231_continuation.match(name)\n        if mo:\n            name, num = mo.group('name', 'num')\n            if num is not None:\n                num = int(num)\n            rfc2231_params.setdefault(name, []).append((num, value, encoded))\n        else:\n            new_params.append((name, '\"%s\"' % quote(value)))\n    if rfc2231_params:\n        for name, continuations in rfc2231_params.items():\n            value = []\n            extended = False\n            # Sort by number\n            continuations.sort()\n            # And now append all values in numerical order, converting\n            # %-encodings for the encoded segments.  If any of the\n            # continuation names ends in a *, then the entire string, after\n            # decoding segments and concatenating, must have the charset and\n            # language specifiers at the beginning of the string.\n            for num, s, encoded in continuations:\n                if encoded:\n                    s = urllib.unquote(s)\n                    extended = True\n                value.append(s)\n            value = quote(EMPTYSTRING.join(value))\n            if extended:\n                charset, language, value = decode_rfc2231(value)\n                new_params.append((name, (charset, language, '\"%s\"' % value)))\n            else:\n                new_params.append((name, '\"%s\"' % value))\n    return new_params\n\ndef collapse_rfc2231_value(value, errors='replace',\n                           fallback_charset='us-ascii'):\n    if isinstance(value, tuple):\n        rawval = unquote(value[2])\n        charset = value[0] or 'us-ascii'\n        try:\n            return unicode(rawval, charset, errors)\n        except LookupError:\n            # XXX charset is unknown to Python.\n            return unicode(rawval, fallback_charset, errors)\n    else:\n        return unquote(value)\n", 
    "encodings.__init__": "\"\"\" Standard \"encodings\" Package\n\n    Standard Python encoding modules are stored in this package\n    directory.\n\n    Codec modules must have names corresponding to normalized encoding\n    names as defined in the normalize_encoding() function below, e.g.\n    'utf-8' must be implemented by the module 'utf_8.py'.\n\n    Each codec module must export the following interface:\n\n    * getregentry() -> codecs.CodecInfo object\n    The getregentry() API must a CodecInfo object with encoder, decoder,\n    incrementalencoder, incrementaldecoder, streamwriter and streamreader\n    atttributes which adhere to the Python Codec Interface Standard.\n\n    In addition, a module may optionally also define the following\n    APIs which are then used by the package's codec search function:\n\n    * getaliases() -> sequence of encoding name strings to use as aliases\n\n    Alias names returned by getaliases() must be normalized encoding\n    names as defined by normalize_encoding().\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"#\"\n\nimport codecs\nfrom encodings import aliases\nimport __builtin__\n\n_cache = {}\n_unknown = '--unknown--'\n_import_tail = ['*']\n_norm_encoding_map = ('                                              . '\n                      '0123456789       ABCDEFGHIJKLMNOPQRSTUVWXYZ     '\n                      ' abcdefghijklmnopqrstuvwxyz                     '\n                      '                                                '\n                      '                                                '\n                      '                ')\n_aliases = aliases.aliases\n\nclass CodecRegistryError(LookupError, SystemError):\n    pass\n\ndef normalize_encoding(encoding):\n\n    \"\"\" Normalize an encoding name.\n\n        Normalization works as follows: all non-alphanumeric\n        characters except the dot used for Python package names are\n        collapsed and replaced with a single underscore, e.g. '  -;#'\n        becomes '_'. Leading and trailing underscores are removed.\n\n        Note that encoding names should be ASCII only; if they do use\n        non-ASCII characters, these must be Latin-1 compatible.\n\n    \"\"\"\n    # Make sure we have an 8-bit string, because .translate() works\n    # differently for Unicode strings.\n    if hasattr(__builtin__, \"unicode\") and isinstance(encoding, unicode):\n        # Note that .encode('latin-1') does *not* use the codec\n        # registry, so this call doesn't recurse. (See unicodeobject.c\n        # PyUnicode_AsEncodedString() for details)\n        encoding = encoding.encode('latin-1')\n    return '_'.join(encoding.translate(_norm_encoding_map).split())\n\ndef search_function(encoding):\n\n    # Cache lookup\n    entry = _cache.get(encoding, _unknown)\n    if entry is not _unknown:\n        return entry\n\n    # Import the module:\n    #\n    # First try to find an alias for the normalized encoding\n    # name and lookup the module using the aliased name, then try to\n    # lookup the module using the standard import scheme, i.e. first\n    # try in the encodings package, then at top-level.\n    #\n    norm_encoding = normalize_encoding(encoding)\n    aliased_encoding = _aliases.get(norm_encoding) or \\\n                       _aliases.get(norm_encoding.replace('.', '_'))\n    if aliased_encoding is not None:\n        modnames = [aliased_encoding,\n                    norm_encoding]\n    else:\n        modnames = [norm_encoding]\n    for modname in modnames:\n        if not modname or '.' in modname:\n            continue\n        try:\n            # Import is absolute to prevent the possibly malicious import of a\n            # module with side-effects that is not in the 'encodings' package.\n            mod = __import__('encodings.' + modname, fromlist=_import_tail,\n                             level=0)\n        except ImportError:\n            pass\n        else:\n            break\n    else:\n        mod = None\n\n    try:\n        getregentry = mod.getregentry\n    except AttributeError:\n        # Not a codec module\n        mod = None\n\n    if mod is None:\n        # Cache misses\n        _cache[encoding] = None\n        return None\n\n    # Now ask the module for the registry entry\n    entry = getregentry()\n    if not isinstance(entry, codecs.CodecInfo):\n        if not 4 <= len(entry) <= 7:\n            raise CodecRegistryError,\\\n                 'module \"%s\" (%s) failed to register' % \\\n                  (mod.__name__, mod.__file__)\n        if not hasattr(entry[0], '__call__') or \\\n           not hasattr(entry[1], '__call__') or \\\n           (entry[2] is not None and not hasattr(entry[2], '__call__')) or \\\n           (entry[3] is not None and not hasattr(entry[3], '__call__')) or \\\n           (len(entry) > 4 and entry[4] is not None and not hasattr(entry[4], '__call__')) or \\\n           (len(entry) > 5 and entry[5] is not None and not hasattr(entry[5], '__call__')):\n            raise CodecRegistryError,\\\n                'incompatible codecs in module \"%s\" (%s)' % \\\n                (mod.__name__, mod.__file__)\n        if len(entry)<7 or entry[6] is None:\n            entry += (None,)*(6-len(entry)) + (mod.__name__.split(\".\", 1)[1],)\n        entry = codecs.CodecInfo(*entry)\n\n    # Cache the codec registry entry\n    _cache[encoding] = entry\n\n    # Register its aliases (without overwriting previously registered\n    # aliases)\n    try:\n        codecaliases = mod.getaliases()\n    except AttributeError:\n        pass\n    else:\n        for alias in codecaliases:\n            if alias not in _aliases:\n                _aliases[alias] = modname\n\n    # Return the registry entry\n    return entry\n\n# Register the search_function in the Python codec registry\ncodecs.register(search_function)\n", 
    "encodings.aliases": "\"\"\" Encoding Aliases Support\n\n    This module is used by the encodings package search function to\n    map encodings names to module names.\n\n    Note that the search function normalizes the encoding names before\n    doing the lookup, so the mapping will have to map normalized\n    encoding names to module names.\n\n    Contents:\n\n        The following aliases dictionary contains mappings of all IANA\n        character set names for which the Python core library provides\n        codecs. In addition to these, a few Python specific codec\n        aliases have also been added.\n\n\"\"\"\naliases = {\n\n    # Please keep this list sorted alphabetically by value !\n\n    # ascii codec\n    '646'                : 'ascii',\n    'ansi_x3.4_1968'     : 'ascii',\n    'ansi_x3_4_1968'     : 'ascii', # some email headers use this non-standard name\n    'ansi_x3.4_1986'     : 'ascii',\n    'cp367'              : 'ascii',\n    'csascii'            : 'ascii',\n    'ibm367'             : 'ascii',\n    'iso646_us'          : 'ascii',\n    'iso_646.irv_1991'   : 'ascii',\n    'iso_ir_6'           : 'ascii',\n    'us'                 : 'ascii',\n    'us_ascii'           : 'ascii',\n\n    # base64_codec codec\n    'base64'             : 'base64_codec',\n    'base_64'            : 'base64_codec',\n\n    # big5 codec\n    'big5_tw'            : 'big5',\n    'csbig5'             : 'big5',\n\n    # big5hkscs codec\n    'big5_hkscs'         : 'big5hkscs',\n    'hkscs'              : 'big5hkscs',\n\n    # bz2_codec codec\n    'bz2'                : 'bz2_codec',\n\n    # cp037 codec\n    '037'                : 'cp037',\n    'csibm037'           : 'cp037',\n    'ebcdic_cp_ca'       : 'cp037',\n    'ebcdic_cp_nl'       : 'cp037',\n    'ebcdic_cp_us'       : 'cp037',\n    'ebcdic_cp_wt'       : 'cp037',\n    'ibm037'             : 'cp037',\n    'ibm039'             : 'cp037',\n\n    # cp1026 codec\n    '1026'               : 'cp1026',\n    'csibm1026'          : 'cp1026',\n    'ibm1026'            : 'cp1026',\n\n    # cp1140 codec\n    '1140'               : 'cp1140',\n    'ibm1140'            : 'cp1140',\n\n    # cp1250 codec\n    '1250'               : 'cp1250',\n    'windows_1250'       : 'cp1250',\n\n    # cp1251 codec\n    '1251'               : 'cp1251',\n    'windows_1251'       : 'cp1251',\n\n    # cp1252 codec\n    '1252'               : 'cp1252',\n    'windows_1252'       : 'cp1252',\n\n    # cp1253 codec\n    '1253'               : 'cp1253',\n    'windows_1253'       : 'cp1253',\n\n    # cp1254 codec\n    '1254'               : 'cp1254',\n    'windows_1254'       : 'cp1254',\n\n    # cp1255 codec\n    '1255'               : 'cp1255',\n    'windows_1255'       : 'cp1255',\n\n    # cp1256 codec\n    '1256'               : 'cp1256',\n    'windows_1256'       : 'cp1256',\n\n    # cp1257 codec\n    '1257'               : 'cp1257',\n    'windows_1257'       : 'cp1257',\n\n    # cp1258 codec\n    '1258'               : 'cp1258',\n    'windows_1258'       : 'cp1258',\n\n    # cp424 codec\n    '424'                : 'cp424',\n    'csibm424'           : 'cp424',\n    'ebcdic_cp_he'       : 'cp424',\n    'ibm424'             : 'cp424',\n\n    # cp437 codec\n    '437'                : 'cp437',\n    'cspc8codepage437'   : 'cp437',\n    'ibm437'             : 'cp437',\n\n    # cp500 codec\n    '500'                : 'cp500',\n    'csibm500'           : 'cp500',\n    'ebcdic_cp_be'       : 'cp500',\n    'ebcdic_cp_ch'       : 'cp500',\n    'ibm500'             : 'cp500',\n\n    # cp775 codec\n    '775'                : 'cp775',\n    'cspc775baltic'      : 'cp775',\n    'ibm775'             : 'cp775',\n\n    # cp850 codec\n    '850'                : 'cp850',\n    'cspc850multilingual' : 'cp850',\n    'ibm850'             : 'cp850',\n\n    # cp852 codec\n    '852'                : 'cp852',\n    'cspcp852'           : 'cp852',\n    'ibm852'             : 'cp852',\n\n    # cp855 codec\n    '855'                : 'cp855',\n    'csibm855'           : 'cp855',\n    'ibm855'             : 'cp855',\n\n    # cp857 codec\n    '857'                : 'cp857',\n    'csibm857'           : 'cp857',\n    'ibm857'             : 'cp857',\n\n    # cp858 codec\n    '858'                : 'cp858',\n    'csibm858'           : 'cp858',\n    'ibm858'             : 'cp858',\n\n    # cp860 codec\n    '860'                : 'cp860',\n    'csibm860'           : 'cp860',\n    'ibm860'             : 'cp860',\n\n    # cp861 codec\n    '861'                : 'cp861',\n    'cp_is'              : 'cp861',\n    'csibm861'           : 'cp861',\n    'ibm861'             : 'cp861',\n\n    # cp862 codec\n    '862'                : 'cp862',\n    'cspc862latinhebrew' : 'cp862',\n    'ibm862'             : 'cp862',\n\n    # cp863 codec\n    '863'                : 'cp863',\n    'csibm863'           : 'cp863',\n    'ibm863'             : 'cp863',\n\n    # cp864 codec\n    '864'                : 'cp864',\n    'csibm864'           : 'cp864',\n    'ibm864'             : 'cp864',\n\n    # cp865 codec\n    '865'                : 'cp865',\n    'csibm865'           : 'cp865',\n    'ibm865'             : 'cp865',\n\n    # cp866 codec\n    '866'                : 'cp866',\n    'csibm866'           : 'cp866',\n    'ibm866'             : 'cp866',\n\n    # cp869 codec\n    '869'                : 'cp869',\n    'cp_gr'              : 'cp869',\n    'csibm869'           : 'cp869',\n    'ibm869'             : 'cp869',\n\n    # cp932 codec\n    '932'                : 'cp932',\n    'ms932'              : 'cp932',\n    'mskanji'            : 'cp932',\n    'ms_kanji'           : 'cp932',\n\n    # cp949 codec\n    '949'                : 'cp949',\n    'ms949'              : 'cp949',\n    'uhc'                : 'cp949',\n\n    # cp950 codec\n    '950'                : 'cp950',\n    'ms950'              : 'cp950',\n\n    # euc_jis_2004 codec\n    'jisx0213'           : 'euc_jis_2004',\n    'eucjis2004'         : 'euc_jis_2004',\n    'euc_jis2004'        : 'euc_jis_2004',\n\n    # euc_jisx0213 codec\n    'eucjisx0213'        : 'euc_jisx0213',\n\n    # euc_jp codec\n    'eucjp'              : 'euc_jp',\n    'ujis'               : 'euc_jp',\n    'u_jis'              : 'euc_jp',\n\n    # euc_kr codec\n    'euckr'              : 'euc_kr',\n    'korean'             : 'euc_kr',\n    'ksc5601'            : 'euc_kr',\n    'ks_c_5601'          : 'euc_kr',\n    'ks_c_5601_1987'     : 'euc_kr',\n    'ksx1001'            : 'euc_kr',\n    'ks_x_1001'          : 'euc_kr',\n\n    # gb18030 codec\n    'gb18030_2000'       : 'gb18030',\n\n    # gb2312 codec\n    'chinese'            : 'gb2312',\n    'csiso58gb231280'    : 'gb2312',\n    'euc_cn'             : 'gb2312',\n    'euccn'              : 'gb2312',\n    'eucgb2312_cn'       : 'gb2312',\n    'gb2312_1980'        : 'gb2312',\n    'gb2312_80'          : 'gb2312',\n    'iso_ir_58'          : 'gb2312',\n\n    # gbk codec\n    '936'                : 'gbk',\n    'cp936'              : 'gbk',\n    'ms936'              : 'gbk',\n\n    # hex_codec codec\n    'hex'                : 'hex_codec',\n\n    # hp_roman8 codec\n    'roman8'             : 'hp_roman8',\n    'r8'                 : 'hp_roman8',\n    'csHPRoman8'         : 'hp_roman8',\n\n    # hz codec\n    'hzgb'               : 'hz',\n    'hz_gb'              : 'hz',\n    'hz_gb_2312'         : 'hz',\n\n    # iso2022_jp codec\n    'csiso2022jp'        : 'iso2022_jp',\n    'iso2022jp'          : 'iso2022_jp',\n    'iso_2022_jp'        : 'iso2022_jp',\n\n    # iso2022_jp_1 codec\n    'iso2022jp_1'        : 'iso2022_jp_1',\n    'iso_2022_jp_1'      : 'iso2022_jp_1',\n\n    # iso2022_jp_2 codec\n    'iso2022jp_2'        : 'iso2022_jp_2',\n    'iso_2022_jp_2'      : 'iso2022_jp_2',\n\n    # iso2022_jp_2004 codec\n    'iso_2022_jp_2004'   : 'iso2022_jp_2004',\n    'iso2022jp_2004'     : 'iso2022_jp_2004',\n\n    # iso2022_jp_3 codec\n    'iso2022jp_3'        : 'iso2022_jp_3',\n    'iso_2022_jp_3'      : 'iso2022_jp_3',\n\n    # iso2022_jp_ext codec\n    'iso2022jp_ext'      : 'iso2022_jp_ext',\n    'iso_2022_jp_ext'    : 'iso2022_jp_ext',\n\n    # iso2022_kr codec\n    'csiso2022kr'        : 'iso2022_kr',\n    'iso2022kr'          : 'iso2022_kr',\n    'iso_2022_kr'        : 'iso2022_kr',\n\n    # iso8859_10 codec\n    'csisolatin6'        : 'iso8859_10',\n    'iso_8859_10'        : 'iso8859_10',\n    'iso_8859_10_1992'   : 'iso8859_10',\n    'iso_ir_157'         : 'iso8859_10',\n    'l6'                 : 'iso8859_10',\n    'latin6'             : 'iso8859_10',\n\n    # iso8859_11 codec\n    'thai'               : 'iso8859_11',\n    'iso_8859_11'        : 'iso8859_11',\n    'iso_8859_11_2001'   : 'iso8859_11',\n\n    # iso8859_13 codec\n    'iso_8859_13'        : 'iso8859_13',\n    'l7'                 : 'iso8859_13',\n    'latin7'             : 'iso8859_13',\n\n    # iso8859_14 codec\n    'iso_8859_14'        : 'iso8859_14',\n    'iso_8859_14_1998'   : 'iso8859_14',\n    'iso_celtic'         : 'iso8859_14',\n    'iso_ir_199'         : 'iso8859_14',\n    'l8'                 : 'iso8859_14',\n    'latin8'             : 'iso8859_14',\n\n    # iso8859_15 codec\n    'iso_8859_15'        : 'iso8859_15',\n    'l9'                 : 'iso8859_15',\n    'latin9'             : 'iso8859_15',\n\n    # iso8859_16 codec\n    'iso_8859_16'        : 'iso8859_16',\n    'iso_8859_16_2001'   : 'iso8859_16',\n    'iso_ir_226'         : 'iso8859_16',\n    'l10'                : 'iso8859_16',\n    'latin10'            : 'iso8859_16',\n\n    # iso8859_2 codec\n    'csisolatin2'        : 'iso8859_2',\n    'iso_8859_2'         : 'iso8859_2',\n    'iso_8859_2_1987'    : 'iso8859_2',\n    'iso_ir_101'         : 'iso8859_2',\n    'l2'                 : 'iso8859_2',\n    'latin2'             : 'iso8859_2',\n\n    # iso8859_3 codec\n    'csisolatin3'        : 'iso8859_3',\n    'iso_8859_3'         : 'iso8859_3',\n    'iso_8859_3_1988'    : 'iso8859_3',\n    'iso_ir_109'         : 'iso8859_3',\n    'l3'                 : 'iso8859_3',\n    'latin3'             : 'iso8859_3',\n\n    # iso8859_4 codec\n    'csisolatin4'        : 'iso8859_4',\n    'iso_8859_4'         : 'iso8859_4',\n    'iso_8859_4_1988'    : 'iso8859_4',\n    'iso_ir_110'         : 'iso8859_4',\n    'l4'                 : 'iso8859_4',\n    'latin4'             : 'iso8859_4',\n\n    # iso8859_5 codec\n    'csisolatincyrillic' : 'iso8859_5',\n    'cyrillic'           : 'iso8859_5',\n    'iso_8859_5'         : 'iso8859_5',\n    'iso_8859_5_1988'    : 'iso8859_5',\n    'iso_ir_144'         : 'iso8859_5',\n\n    # iso8859_6 codec\n    'arabic'             : 'iso8859_6',\n    'asmo_708'           : 'iso8859_6',\n    'csisolatinarabic'   : 'iso8859_6',\n    'ecma_114'           : 'iso8859_6',\n    'iso_8859_6'         : 'iso8859_6',\n    'iso_8859_6_1987'    : 'iso8859_6',\n    'iso_ir_127'         : 'iso8859_6',\n\n    # iso8859_7 codec\n    'csisolatingreek'    : 'iso8859_7',\n    'ecma_118'           : 'iso8859_7',\n    'elot_928'           : 'iso8859_7',\n    'greek'              : 'iso8859_7',\n    'greek8'             : 'iso8859_7',\n    'iso_8859_7'         : 'iso8859_7',\n    'iso_8859_7_1987'    : 'iso8859_7',\n    'iso_ir_126'         : 'iso8859_7',\n\n    # iso8859_8 codec\n    'csisolatinhebrew'   : 'iso8859_8',\n    'hebrew'             : 'iso8859_8',\n    'iso_8859_8'         : 'iso8859_8',\n    'iso_8859_8_1988'    : 'iso8859_8',\n    'iso_ir_138'         : 'iso8859_8',\n\n    # iso8859_9 codec\n    'csisolatin5'        : 'iso8859_9',\n    'iso_8859_9'         : 'iso8859_9',\n    'iso_8859_9_1989'    : 'iso8859_9',\n    'iso_ir_148'         : 'iso8859_9',\n    'l5'                 : 'iso8859_9',\n    'latin5'             : 'iso8859_9',\n\n    # johab codec\n    'cp1361'             : 'johab',\n    'ms1361'             : 'johab',\n\n    # koi8_r codec\n    'cskoi8r'            : 'koi8_r',\n\n    # latin_1 codec\n    #\n    # Note that the latin_1 codec is implemented internally in C and a\n    # lot faster than the charmap codec iso8859_1 which uses the same\n    # encoding. This is why we discourage the use of the iso8859_1\n    # codec and alias it to latin_1 instead.\n    #\n    '8859'               : 'latin_1',\n    'cp819'              : 'latin_1',\n    'csisolatin1'        : 'latin_1',\n    'ibm819'             : 'latin_1',\n    'iso8859'            : 'latin_1',\n    'iso8859_1'          : 'latin_1',\n    'iso_8859_1'         : 'latin_1',\n    'iso_8859_1_1987'    : 'latin_1',\n    'iso_ir_100'         : 'latin_1',\n    'l1'                 : 'latin_1',\n    'latin'              : 'latin_1',\n    'latin1'             : 'latin_1',\n\n    # mac_cyrillic codec\n    'maccyrillic'        : 'mac_cyrillic',\n\n    # mac_greek codec\n    'macgreek'           : 'mac_greek',\n\n    # mac_iceland codec\n    'maciceland'         : 'mac_iceland',\n\n    # mac_latin2 codec\n    'maccentraleurope'   : 'mac_latin2',\n    'maclatin2'          : 'mac_latin2',\n\n    # mac_roman codec\n    'macroman'           : 'mac_roman',\n\n    # mac_turkish codec\n    'macturkish'         : 'mac_turkish',\n\n    # mbcs codec\n    'dbcs'               : 'mbcs',\n\n    # ptcp154 codec\n    'csptcp154'          : 'ptcp154',\n    'pt154'              : 'ptcp154',\n    'cp154'              : 'ptcp154',\n    'cyrillic_asian'     : 'ptcp154',\n\n    # quopri_codec codec\n    'quopri'             : 'quopri_codec',\n    'quoted_printable'   : 'quopri_codec',\n    'quotedprintable'    : 'quopri_codec',\n\n    # rot_13 codec\n    'rot13'              : 'rot_13',\n\n    # shift_jis codec\n    'csshiftjis'         : 'shift_jis',\n    'shiftjis'           : 'shift_jis',\n    'sjis'               : 'shift_jis',\n    's_jis'              : 'shift_jis',\n\n    # shift_jis_2004 codec\n    'shiftjis2004'       : 'shift_jis_2004',\n    'sjis_2004'          : 'shift_jis_2004',\n    's_jis_2004'         : 'shift_jis_2004',\n\n    # shift_jisx0213 codec\n    'shiftjisx0213'      : 'shift_jisx0213',\n    'sjisx0213'          : 'shift_jisx0213',\n    's_jisx0213'         : 'shift_jisx0213',\n\n    # tactis codec\n    'tis260'             : 'tactis',\n\n    # tis_620 codec\n    'tis620'             : 'tis_620',\n    'tis_620_0'          : 'tis_620',\n    'tis_620_2529_0'     : 'tis_620',\n    'tis_620_2529_1'     : 'tis_620',\n    'iso_ir_166'         : 'tis_620',\n\n    # utf_16 codec\n    'u16'                : 'utf_16',\n    'utf16'              : 'utf_16',\n\n    # utf_16_be codec\n    'unicodebigunmarked' : 'utf_16_be',\n    'utf_16be'           : 'utf_16_be',\n\n    # utf_16_le codec\n    'unicodelittleunmarked' : 'utf_16_le',\n    'utf_16le'           : 'utf_16_le',\n\n    # utf_32 codec\n    'u32'                : 'utf_32',\n    'utf32'              : 'utf_32',\n\n    # utf_32_be codec\n    'utf_32be'           : 'utf_32_be',\n\n    # utf_32_le codec\n    'utf_32le'           : 'utf_32_le',\n\n    # utf_7 codec\n    'u7'                 : 'utf_7',\n    'utf7'               : 'utf_7',\n    'unicode_1_1_utf_7'  : 'utf_7',\n\n    # utf_8 codec\n    'u8'                 : 'utf_8',\n    'utf'                : 'utf_8',\n    'utf8'               : 'utf_8',\n    'utf8_ucs2'          : 'utf_8',\n    'utf8_ucs4'          : 'utf_8',\n\n    # uu_codec codec\n    'uu'                 : 'uu_codec',\n\n    # zlib_codec codec\n    'zip'                : 'zlib_codec',\n    'zlib'               : 'zlib_codec',\n\n}\n", 
    "encodings.ascii": "\"\"\" Python 'ascii' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.ascii_encode\n    decode = codecs.ascii_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.ascii_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.ascii_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\nclass StreamConverter(StreamWriter,StreamReader):\n\n    encode = codecs.ascii_decode\n    decode = codecs.ascii_encode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='ascii',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.base64_codec": "\"\"\" Python 'base64_codec' Codec - base64 content transfer encoding\n\n    Unlike most of the other codecs which target Unicode, this codec\n    will return Python string objects for both encode and decode.\n\n    Written by Marc-Andre Lemburg (mal@lemburg.com).\n\n\"\"\"\nimport codecs, base64\n\n### Codec APIs\n\ndef base64_encode(input,errors='strict'):\n\n    \"\"\" Encodes the object input and returns a tuple (output\n        object, length consumed).\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = base64.encodestring(input)\n    return (output, len(input))\n\ndef base64_decode(input,errors='strict'):\n\n    \"\"\" Decodes the object input and returns a tuple (output\n        object, length consumed).\n\n        input must be an object which provides the bf_getreadbuf\n        buffer slot. Python strings, buffer objects and memory\n        mapped files are examples of objects providing this slot.\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = base64.decodestring(input)\n    return (output, len(input))\n\nclass Codec(codecs.Codec):\n\n    def encode(self, input,errors='strict'):\n        return base64_encode(input,errors)\n    def decode(self, input,errors='strict'):\n        return base64_decode(input,errors)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.encodestring(input)\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        assert self.errors == 'strict'\n        return base64.decodestring(input)\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='base64',\n        encode=base64_encode,\n        decode=base64_decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.hex_codec": "\"\"\" Python 'hex_codec' Codec - 2-digit hex content transfer encoding\n\n    Unlike most of the other codecs which target Unicode, this codec\n    will return Python string objects for both encode and decode.\n\n    Written by Marc-Andre Lemburg (mal@lemburg.com).\n\n\"\"\"\nimport codecs, binascii\n\n### Codec APIs\n\ndef hex_encode(input,errors='strict'):\n\n    \"\"\" Encodes the object input and returns a tuple (output\n        object, length consumed).\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = binascii.b2a_hex(input)\n    return (output, len(input))\n\ndef hex_decode(input,errors='strict'):\n\n    \"\"\" Decodes the object input and returns a tuple (output\n        object, length consumed).\n\n        input must be an object which provides the bf_getreadbuf\n        buffer slot. Python strings, buffer objects and memory\n        mapped files are examples of objects providing this slot.\n\n        errors defines the error handling to apply. It defaults to\n        'strict' handling which is the only currently supported\n        error handling for this codec.\n\n    \"\"\"\n    assert errors == 'strict'\n    output = binascii.a2b_hex(input)\n    return (output, len(input))\n\nclass Codec(codecs.Codec):\n\n    def encode(self, input,errors='strict'):\n        return hex_encode(input,errors)\n    def decode(self, input,errors='strict'):\n        return hex_decode(input,errors)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        assert self.errors == 'strict'\n        return binascii.b2a_hex(input)\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        assert self.errors == 'strict'\n        return binascii.a2b_hex(input)\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='hex',\n        encode=hex_encode,\n        decode=hex_decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.latin_1": "\"\"\" Python 'latin-1' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.latin_1_encode\n    decode = codecs.latin_1_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.latin_1_encode(input,self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.latin_1_decode(input,self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\nclass StreamConverter(StreamWriter,StreamReader):\n\n    encode = codecs.latin_1_decode\n    decode = codecs.latin_1_encode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='iso8859-1',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "encodings.raw_unicode_escape": "\"\"\" Python 'raw-unicode-escape' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.raw_unicode_escape_encode\n    decode = codecs.raw_unicode_escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.raw_unicode_escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.raw_unicode_escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='raw-unicode-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.string_escape": "# -*- coding: utf-8 -*-\n\"\"\" Python 'escape' Codec\n\n\nWritten by Martin v. L\u00f6wis (martin@v.loewis.de).\n\n\"\"\"\nimport codecs\n\nclass Codec(codecs.Codec):\n\n    encode = codecs.escape_encode\n    decode = codecs.escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='string-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.unicode_escape": "\"\"\" Python 'unicode-escape' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.unicode_escape_encode\n    decode = codecs.unicode_escape_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.unicode_escape_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.unicode_escape_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='unicode-escape',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.unicode_internal": "\"\"\" Python 'unicode-internal' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nclass Codec(codecs.Codec):\n\n    # Note: Binding these as C functions will result in the class not\n    # converting them to methods. This is intended.\n    encode = codecs.unicode_internal_encode\n    decode = codecs.unicode_internal_decode\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.unicode_internal_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.IncrementalDecoder):\n    def decode(self, input, final=False):\n        return codecs.unicode_internal_decode(input, self.errors)[0]\n\nclass StreamWriter(Codec,codecs.StreamWriter):\n    pass\n\nclass StreamReader(Codec,codecs.StreamReader):\n    pass\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='unicode-internal',\n        encode=Codec.encode,\n        decode=Codec.decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamwriter=StreamWriter,\n        streamreader=StreamReader,\n    )\n", 
    "encodings.utf_16": "\"\"\" Python 'utf-16' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs, sys\n\n### Codec APIs\n\nencode = codecs.utf_16_encode\n\ndef decode(input, errors='strict'):\n    return codecs.utf_16_decode(input, errors, True)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def __init__(self, errors='strict'):\n        codecs.IncrementalEncoder.__init__(self, errors)\n        self.encoder = None\n\n    def encode(self, input, final=False):\n        if self.encoder is None:\n            result = codecs.utf_16_encode(input, self.errors)[0]\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n            return result\n        return self.encoder(input, self.errors)[0]\n\n    def reset(self):\n        codecs.IncrementalEncoder.reset(self)\n        self.encoder = None\n\n    def getstate(self):\n        # state info we return to the caller:\n        # 0: stream is in natural order for this platform\n        # 2: endianness hasn't been determined yet\n        # (we're never writing in unnatural order)\n        return (2 if self.encoder is None else 0)\n\n    def setstate(self, state):\n        if state:\n            self.encoder = None\n        else:\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n\nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n    def __init__(self, errors='strict'):\n        codecs.BufferedIncrementalDecoder.__init__(self, errors)\n        self.decoder = None\n\n    def _buffer_decode(self, input, errors, final):\n        if self.decoder is None:\n            (output, consumed, byteorder) = \\\n                codecs.utf_16_ex_decode(input, errors, 0, final)\n            if byteorder == -1:\n                self.decoder = codecs.utf_16_le_decode\n            elif byteorder == 1:\n                self.decoder = codecs.utf_16_be_decode\n            elif consumed >= 2:\n                raise UnicodeError(\"UTF-16 stream does not start with BOM\")\n            return (output, consumed)\n        return self.decoder(input, self.errors, final)\n\n    def reset(self):\n        codecs.BufferedIncrementalDecoder.reset(self)\n        self.decoder = None\n\nclass StreamWriter(codecs.StreamWriter):\n    def __init__(self, stream, errors='strict'):\n        codecs.StreamWriter.__init__(self, stream, errors)\n        self.encoder = None\n\n    def reset(self):\n        codecs.StreamWriter.reset(self)\n        self.encoder = None\n\n    def encode(self, input, errors='strict'):\n        if self.encoder is None:\n            result = codecs.utf_16_encode(input, errors)\n            if sys.byteorder == 'little':\n                self.encoder = codecs.utf_16_le_encode\n            else:\n                self.encoder = codecs.utf_16_be_encode\n            return result\n        else:\n            return self.encoder(input, errors)\n\nclass StreamReader(codecs.StreamReader):\n\n    def reset(self):\n        codecs.StreamReader.reset(self)\n        try:\n            del self.decode\n        except AttributeError:\n            pass\n\n    def decode(self, input, errors='strict'):\n        (object, consumed, byteorder) = \\\n            codecs.utf_16_ex_decode(input, errors, 0, False)\n        if byteorder == -1:\n            self.decode = codecs.utf_16_le_decode\n        elif byteorder == 1:\n            self.decode = codecs.utf_16_be_decode\n        elif consumed>=2:\n            raise UnicodeError,\"UTF-16 stream does not start with BOM\"\n        return (object, consumed)\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='utf-16',\n        encode=encode,\n        decode=decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "encodings.utf_8": "\"\"\" Python 'utf-8' Codec\n\n\nWritten by Marc-Andre Lemburg (mal@lemburg.com).\n\n(c) Copyright CNRI, All Rights Reserved. NO WARRANTY.\n\n\"\"\"\nimport codecs\n\n### Codec APIs\n\nencode = codecs.utf_8_encode\n\ndef decode(input, errors='strict'):\n    return codecs.utf_8_decode(input, errors, True)\n\nclass IncrementalEncoder(codecs.IncrementalEncoder):\n    def encode(self, input, final=False):\n        return codecs.utf_8_encode(input, self.errors)[0]\n\nclass IncrementalDecoder(codecs.BufferedIncrementalDecoder):\n    _buffer_decode = codecs.utf_8_decode\n\nclass StreamWriter(codecs.StreamWriter):\n    encode = codecs.utf_8_encode\n\nclass StreamReader(codecs.StreamReader):\n    decode = codecs.utf_8_decode\n\n### encodings module API\n\ndef getregentry():\n    return codecs.CodecInfo(\n        name='utf-8',\n        encode=encode,\n        decode=decode,\n        incrementalencoder=IncrementalEncoder,\n        incrementaldecoder=IncrementalDecoder,\n        streamreader=StreamReader,\n        streamwriter=StreamWriter,\n    )\n", 
    "fnmatch": "\"\"\"Filename matching with shell patterns.\n\nfnmatch(FILENAME, PATTERN) matches according to the local convention.\nfnmatchcase(FILENAME, PATTERN) always takes case in account.\n\nThe functions operate by translating the pattern into a regular\nexpression.  They cache the compiled regular expressions for speed.\n\nThe function translate(PATTERN) returns a regular expression\ncorresponding to PATTERN.  (It does not compile it.)\n\"\"\"\n\nimport re\n\n__all__ = [\"filter\", \"fnmatch\", \"fnmatchcase\", \"translate\"]\n\n_cache = {}\n_MAXCACHE = 100\n\ndef _purge():\n    \"\"\"Clear the pattern cache\"\"\"\n    _cache.clear()\n\ndef fnmatch(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN.\n\n    Patterns are Unix shell style:\n\n    *       matches everything\n    ?       matches any single character\n    [seq]   matches any character in seq\n    [!seq]  matches any char not in seq\n\n    An initial period in FILENAME is not special.\n    Both FILENAME and PATTERN are first case-normalized\n    if the operating system requires it.\n    If you don't want this, use fnmatchcase(FILENAME, PATTERN).\n    \"\"\"\n\n    import os\n    name = os.path.normcase(name)\n    pat = os.path.normcase(pat)\n    return fnmatchcase(name, pat)\n\ndef filter(names, pat):\n    \"\"\"Return the subset of the list NAMES that match PAT\"\"\"\n    import os,posixpath\n    result=[]\n    pat=os.path.normcase(pat)\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    match=_cache[pat].match\n    if os.path is posixpath:\n        # normcase on posix is NOP. Optimize it away from the loop.\n        for name in names:\n            if match(name):\n                result.append(name)\n    else:\n        for name in names:\n            if match(os.path.normcase(name)):\n                result.append(name)\n    return result\n\ndef fnmatchcase(name, pat):\n    \"\"\"Test whether FILENAME matches PATTERN, including case.\n\n    This is a version of fnmatch() which doesn't case-normalize\n    its arguments.\n    \"\"\"\n\n    if not pat in _cache:\n        res = translate(pat)\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        _cache[pat] = re.compile(res)\n    return _cache[pat].match(name) is not None\n\ndef translate(pat):\n    \"\"\"Translate a shell PATTERN to a regular expression.\n\n    There is no way to quote meta-characters.\n    \"\"\"\n\n    i, n = 0, len(pat)\n    res = ''\n    while i < n:\n        c = pat[i]\n        i = i+1\n        if c == '*':\n            res = res + '.*'\n        elif c == '?':\n            res = res + '.'\n        elif c == '[':\n            j = i\n            if j < n and pat[j] == '!':\n                j = j+1\n            if j < n and pat[j] == ']':\n                j = j+1\n            while j < n and pat[j] != ']':\n                j = j+1\n            if j >= n:\n                res = res + '\\\\['\n            else:\n                stuff = pat[i:j].replace('\\\\','\\\\\\\\')\n                i = j+1\n                if stuff[0] == '!':\n                    stuff = '^' + stuff[1:]\n                elif stuff[0] == '^':\n                    stuff = '\\\\' + stuff\n                res = '%s[%s]' % (res, stuff)\n        else:\n            res = res + re.escape(c)\n    return res + '\\Z(?ms)'\n", 
    "ftplib": "\"\"\"An FTP client class and some helper functions.\n\nBased on RFC 959: File Transfer Protocol (FTP), by J. Postel and J. Reynolds\n\nExample:\n\n>>> from ftplib import FTP\n>>> ftp = FTP('ftp.python.org') # connect to host, default port\n>>> ftp.login() # default, i.e.: user anonymous, passwd anonymous@\n'230 Guest login ok, access restrictions apply.'\n>>> ftp.retrlines('LIST') # list directory contents\ntotal 9\ndrwxr-xr-x   8 root     wheel        1024 Jan  3  1994 .\ndrwxr-xr-x   8 root     wheel        1024 Jan  3  1994 ..\ndrwxr-xr-x   2 root     wheel        1024 Jan  3  1994 bin\ndrwxr-xr-x   2 root     wheel        1024 Jan  3  1994 etc\nd-wxrwxr-x   2 ftp      wheel        1024 Sep  5 13:43 incoming\ndrwxr-xr-x   2 root     wheel        1024 Nov 17  1993 lib\ndrwxr-xr-x   6 1094     wheel        1024 Sep 13 19:07 pub\ndrwxr-xr-x   3 root     wheel        1024 Jan  3  1994 usr\n-rw-r--r--   1 root     root          312 Aug  1  1994 welcome.msg\n'226 Transfer complete.'\n>>> ftp.quit()\n'221 Goodbye.'\n>>>\n\nA nice test that reveals some of the network dialogue would be:\npython ftplib.py -d localhost -l -p -l\n\"\"\"\n\n#\n# Changes and improvements suggested by Steve Majewski.\n# Modified by Jack to work on the mac.\n# Modified by Siebren to support docstrings and PASV.\n# Modified by Phil Schwartz to add storbinary and storlines callbacks.\n# Modified by Giampaolo Rodola' to add TLS support.\n#\n\nimport os\nimport sys\n\n# Import SOCKS module if it exists, else standard socket module socket\ntry:\n    import SOCKS; socket = SOCKS; del SOCKS # import SOCKS as socket\n    from socket import getfqdn; socket.getfqdn = getfqdn; del getfqdn\nexcept ImportError:\n    import socket\nfrom socket import _GLOBAL_DEFAULT_TIMEOUT\n\n__all__ = [\"FTP\",\"Netrc\"]\n\n# Magic number from <socket.h>\nMSG_OOB = 0x1                           # Process data out of band\n\n\n# The standard FTP server control port\nFTP_PORT = 21\n# The sizehint parameter passed to readline() calls\nMAXLINE = 8192\n\n\n# Exception raised when an error or invalid response is received\nclass Error(Exception): pass\nclass error_reply(Error): pass          # unexpected [123]xx reply\nclass error_temp(Error): pass           # 4xx errors\nclass error_perm(Error): pass           # 5xx errors\nclass error_proto(Error): pass          # response does not begin with [1-5]\n\n\n# All exceptions (hopefully) that may be raised here and that aren't\n# (always) programming errors on our side\nall_errors = (Error, IOError, EOFError)\n\n\n# Line terminators (we always output CRLF, but accept any of CRLF, CR, LF)\nCRLF = '\\r\\n'\n\n# The class itself\nclass FTP:\n\n    '''An FTP client class.\n\n    To create a connection, call the class using these arguments:\n            host, user, passwd, acct, timeout\n\n    The first four arguments are all strings, and have default value ''.\n    timeout must be numeric and defaults to None if not passed,\n    meaning that no timeout will be set on any ftp socket(s)\n    If a timeout is passed, then this is now the default timeout for all ftp\n    socket operations for this instance.\n\n    Then use self.connect() with optional host and port argument.\n\n    To download a file, use ftp.retrlines('RETR ' + filename),\n    or ftp.retrbinary() with slightly different arguments.\n    To upload a file, use ftp.storlines() or ftp.storbinary(),\n    which have an open file as argument (see their definitions\n    below for details).\n    The download/upload functions first issue appropriate TYPE\n    and PORT or PASV commands.\n'''\n\n    debugging = 0\n    host = ''\n    port = FTP_PORT\n    maxline = MAXLINE\n    sock = None\n    file = None\n    welcome = None\n    passiveserver = 1\n\n    # Initialization method (called by class instantiation).\n    # Initialize host to localhost, port to standard ftp port\n    # Optional arguments are host (for connect()),\n    # and user, passwd, acct (for login())\n    def __init__(self, host='', user='', passwd='', acct='',\n                 timeout=_GLOBAL_DEFAULT_TIMEOUT):\n        self.timeout = timeout\n        if host:\n            self.connect(host)\n            if user:\n                self.login(user, passwd, acct)\n\n    def connect(self, host='', port=0, timeout=-999):\n        '''Connect to host.  Arguments are:\n         - host: hostname to connect to (string, default previous host)\n         - port: port to connect to (integer, default previous port)\n        '''\n        if host != '':\n            self.host = host\n        if port > 0:\n            self.port = port\n        if timeout != -999:\n            self.timeout = timeout\n        self.sock = socket.create_connection((self.host, self.port), self.timeout)\n        self.af = self.sock.family\n        self.file = self.sock.makefile('rb')\n        self.welcome = self.getresp()\n        return self.welcome\n\n    def getwelcome(self):\n        '''Get the welcome message from the server.\n        (this is read and squirreled away by connect())'''\n        if self.debugging:\n            print '*welcome*', self.sanitize(self.welcome)\n        return self.welcome\n\n    def set_debuglevel(self, level):\n        '''Set the debugging level.\n        The required argument level means:\n        0: no debugging output (default)\n        1: print commands and responses but not body text etc.\n        2: also print raw lines read and sent before stripping CR/LF'''\n        self.debugging = level\n    debug = set_debuglevel\n\n    def set_pasv(self, val):\n        '''Use passive or active mode for data transfers.\n        With a false argument, use the normal PORT mode,\n        With a true argument, use the PASV command.'''\n        self.passiveserver = val\n\n    # Internal: \"sanitize\" a string for printing\n    def sanitize(self, s):\n        if s[:5] == 'pass ' or s[:5] == 'PASS ':\n            i = len(s)\n            while i > 5 and s[i-1] in '\\r\\n':\n                i = i-1\n            s = s[:5] + '*'*(i-5) + s[i:]\n        return repr(s)\n\n    # Internal: send one line to the server, appending CRLF\n    def putline(self, line):\n        line = line + CRLF\n        if self.debugging > 1: print '*put*', self.sanitize(line)\n        self.sock.sendall(line)\n\n    # Internal: send one command to the server (through putline())\n    def putcmd(self, line):\n        if self.debugging: print '*cmd*', self.sanitize(line)\n        self.putline(line)\n\n    # Internal: return one line from the server, stripping CRLF.\n    # Raise EOFError if the connection is closed\n    def getline(self):\n        line = self.file.readline(self.maxline + 1)\n        if len(line) > self.maxline:\n            raise Error(\"got more than %d bytes\" % self.maxline)\n        if self.debugging > 1:\n            print '*get*', self.sanitize(line)\n        if not line: raise EOFError\n        if line[-2:] == CRLF: line = line[:-2]\n        elif line[-1:] in CRLF: line = line[:-1]\n        return line\n\n    # Internal: get a response from the server, which may possibly\n    # consist of multiple lines.  Return a single string with no\n    # trailing CRLF.  If the response consists of multiple lines,\n    # these are separated by '\\n' characters in the string\n    def getmultiline(self):\n        line = self.getline()\n        if line[3:4] == '-':\n            code = line[:3]\n            while 1:\n                nextline = self.getline()\n                line = line + ('\\n' + nextline)\n                if nextline[:3] == code and \\\n                        nextline[3:4] != '-':\n                    break\n        return line\n\n    # Internal: get a response from the server.\n    # Raise various errors if the response indicates an error\n    def getresp(self):\n        resp = self.getmultiline()\n        if self.debugging: print '*resp*', self.sanitize(resp)\n        self.lastresp = resp[:3]\n        c = resp[:1]\n        if c in ('1', '2', '3'):\n            return resp\n        if c == '4':\n            raise error_temp, resp\n        if c == '5':\n            raise error_perm, resp\n        raise error_proto, resp\n\n    def voidresp(self):\n        \"\"\"Expect a response beginning with '2'.\"\"\"\n        resp = self.getresp()\n        if resp[:1] != '2':\n            raise error_reply, resp\n        return resp\n\n    def abort(self):\n        '''Abort a file transfer.  Uses out-of-band data.\n        This does not follow the procedure from the RFC to send Telnet\n        IP and Synch; that doesn't seem to work with the servers I've\n        tried.  Instead, just send the ABOR command as OOB data.'''\n        line = 'ABOR' + CRLF\n        if self.debugging > 1: print '*put urgent*', self.sanitize(line)\n        self.sock.sendall(line, MSG_OOB)\n        resp = self.getmultiline()\n        if resp[:3] not in ('426', '225', '226'):\n            raise error_proto, resp\n\n    def sendcmd(self, cmd):\n        '''Send a command and return the response.'''\n        self.putcmd(cmd)\n        return self.getresp()\n\n    def voidcmd(self, cmd):\n        \"\"\"Send a command and expect a response beginning with '2'.\"\"\"\n        self.putcmd(cmd)\n        return self.voidresp()\n\n    def sendport(self, host, port):\n        '''Send a PORT command with the current host and the given\n        port number.\n        '''\n        hbytes = host.split('.')\n        pbytes = [repr(port//256), repr(port%256)]\n        bytes = hbytes + pbytes\n        cmd = 'PORT ' + ','.join(bytes)\n        return self.voidcmd(cmd)\n\n    def sendeprt(self, host, port):\n        '''Send a EPRT command with the current host and the given port number.'''\n        af = 0\n        if self.af == socket.AF_INET:\n            af = 1\n        if self.af == socket.AF_INET6:\n            af = 2\n        if af == 0:\n            raise error_proto, 'unsupported address family'\n        fields = ['', repr(af), host, repr(port), '']\n        cmd = 'EPRT ' + '|'.join(fields)\n        return self.voidcmd(cmd)\n\n    def makeport(self):\n        '''Create a new socket and send a PORT command for it.'''\n        err = None\n        sock = None\n        for res in socket.getaddrinfo(None, 0, self.af, socket.SOCK_STREAM, 0, socket.AI_PASSIVE):\n            af, socktype, proto, canonname, sa = res\n            try:\n                sock = socket.socket(af, socktype, proto)\n                sock.bind(sa)\n            except socket.error, err:\n                if sock:\n                    sock.close()\n                sock = None\n                continue\n            break\n        if sock is None:\n            if err is not None:\n                raise err\n            else:\n                raise socket.error(\"getaddrinfo returns an empty list\")\n        sock.listen(1)\n        port = sock.getsockname()[1] # Get proper port\n        host = self.sock.getsockname()[0] # Get proper host\n        if self.af == socket.AF_INET:\n            resp = self.sendport(host, port)\n        else:\n            resp = self.sendeprt(host, port)\n        if self.timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n            sock.settimeout(self.timeout)\n        return sock\n\n    def makepasv(self):\n        if self.af == socket.AF_INET:\n            host, port = parse227(self.sendcmd('PASV'))\n        else:\n            host, port = parse229(self.sendcmd('EPSV'), self.sock.getpeername())\n        return host, port\n\n    def ntransfercmd(self, cmd, rest=None):\n        \"\"\"Initiate a transfer over the data connection.\n\n        If the transfer is active, send a port command and the\n        transfer command, and accept the connection.  If the server is\n        passive, send a pasv command, connect to it, and start the\n        transfer command.  Either way, return the socket for the\n        connection and the expected size of the transfer.  The\n        expected size may be None if it could not be determined.\n\n        Optional `rest' argument can be a string that is sent as the\n        argument to a REST command.  This is essentially a server\n        marker used to tell the server to skip over any data up to the\n        given marker.\n        \"\"\"\n        size = None\n        if self.passiveserver:\n            host, port = self.makepasv()\n            conn = socket.create_connection((host, port), self.timeout)\n            try:\n                if rest is not None:\n                    self.sendcmd(\"REST %s\" % rest)\n                resp = self.sendcmd(cmd)\n                # Some servers apparently send a 200 reply to\n                # a LIST or STOR command, before the 150 reply\n                # (and way before the 226 reply). This seems to\n                # be in violation of the protocol (which only allows\n                # 1xx or error messages for LIST), so we just discard\n                # this response.\n                if resp[0] == '2':\n                    resp = self.getresp()\n                if resp[0] != '1':\n                    raise error_reply, resp\n            except:\n                conn.close()\n                raise\n        else:\n            sock = self.makeport()\n            try:\n                if rest is not None:\n                    self.sendcmd(\"REST %s\" % rest)\n                resp = self.sendcmd(cmd)\n                # See above.\n                if resp[0] == '2':\n                    resp = self.getresp()\n                if resp[0] != '1':\n                    raise error_reply, resp\n                conn, sockaddr = sock.accept()\n                if self.timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n                    conn.settimeout(self.timeout)\n            finally:\n                sock.close()\n        if resp[:3] == '150':\n            # this is conditional in case we received a 125\n            size = parse150(resp)\n        return conn, size\n\n    def transfercmd(self, cmd, rest=None):\n        \"\"\"Like ntransfercmd() but returns only the socket.\"\"\"\n        return self.ntransfercmd(cmd, rest)[0]\n\n    def login(self, user = '', passwd = '', acct = ''):\n        '''Login, default anonymous.'''\n        if not user: user = 'anonymous'\n        if not passwd: passwd = ''\n        if not acct: acct = ''\n        if user == 'anonymous' and passwd in ('', '-'):\n            # If there is no anonymous ftp password specified\n            # then we'll just use anonymous@\n            # We don't send any other thing because:\n            # - We want to remain anonymous\n            # - We want to stop SPAM\n            # - We don't want to let ftp sites to discriminate by the user,\n            #   host or country.\n            passwd = passwd + 'anonymous@'\n        resp = self.sendcmd('USER ' + user)\n        if resp[0] == '3': resp = self.sendcmd('PASS ' + passwd)\n        if resp[0] == '3': resp = self.sendcmd('ACCT ' + acct)\n        if resp[0] != '2':\n            raise error_reply, resp\n        return resp\n\n    def retrbinary(self, cmd, callback, blocksize=8192, rest=None):\n        \"\"\"Retrieve data in binary mode.  A new port is created for you.\n\n        Args:\n          cmd: A RETR command.\n          callback: A single parameter callable to be called on each\n                    block of data read.\n          blocksize: The maximum number of bytes to read from the\n                     socket at one time.  [default: 8192]\n          rest: Passed to transfercmd().  [default: None]\n\n        Returns:\n          The response code.\n        \"\"\"\n        self.voidcmd('TYPE I')\n        conn = self.transfercmd(cmd, rest)\n        while 1:\n            data = conn.recv(blocksize)\n            if not data:\n                break\n            callback(data)\n        conn.close()\n        return self.voidresp()\n\n    def retrlines(self, cmd, callback = None):\n        \"\"\"Retrieve data in line mode.  A new port is created for you.\n\n        Args:\n          cmd: A RETR, LIST, NLST, or MLSD command.\n          callback: An optional single parameter callable that is called\n                    for each line with the trailing CRLF stripped.\n                    [default: print_line()]\n\n        Returns:\n          The response code.\n        \"\"\"\n        if callback is None: callback = print_line\n        resp = self.sendcmd('TYPE A')\n        conn = self.transfercmd(cmd)\n        fp = conn.makefile('rb')\n        while 1:\n            line = fp.readline(self.maxline + 1)\n            if len(line) > self.maxline:\n                raise Error(\"got more than %d bytes\" % self.maxline)\n            if self.debugging > 2: print '*retr*', repr(line)\n            if not line:\n                break\n            if line[-2:] == CRLF:\n                line = line[:-2]\n            elif line[-1:] == '\\n':\n                line = line[:-1]\n            callback(line)\n        fp.close()\n        conn.close()\n        return self.voidresp()\n\n    def storbinary(self, cmd, fp, blocksize=8192, callback=None, rest=None):\n        \"\"\"Store a file in binary mode.  A new port is created for you.\n\n        Args:\n          cmd: A STOR command.\n          fp: A file-like object with a read(num_bytes) method.\n          blocksize: The maximum data size to read from fp and send over\n                     the connection at once.  [default: 8192]\n          callback: An optional single parameter callable that is called on\n                    each block of data after it is sent.  [default: None]\n          rest: Passed to transfercmd().  [default: None]\n\n        Returns:\n          The response code.\n        \"\"\"\n        self.voidcmd('TYPE I')\n        conn = self.transfercmd(cmd, rest)\n        while 1:\n            buf = fp.read(blocksize)\n            if not buf: break\n            conn.sendall(buf)\n            if callback: callback(buf)\n        conn.close()\n        return self.voidresp()\n\n    def storlines(self, cmd, fp, callback=None):\n        \"\"\"Store a file in line mode.  A new port is created for you.\n\n        Args:\n          cmd: A STOR command.\n          fp: A file-like object with a readline() method.\n          callback: An optional single parameter callable that is called on\n                    each line after it is sent.  [default: None]\n\n        Returns:\n          The response code.\n        \"\"\"\n        self.voidcmd('TYPE A')\n        conn = self.transfercmd(cmd)\n        while 1:\n            buf = fp.readline(self.maxline + 1)\n            if len(buf) > self.maxline:\n                raise Error(\"got more than %d bytes\" % self.maxline)\n            if not buf: break\n            if buf[-2:] != CRLF:\n                if buf[-1] in CRLF: buf = buf[:-1]\n                buf = buf + CRLF\n            conn.sendall(buf)\n            if callback: callback(buf)\n        conn.close()\n        return self.voidresp()\n\n    def acct(self, password):\n        '''Send new account name.'''\n        cmd = 'ACCT ' + password\n        return self.voidcmd(cmd)\n\n    def nlst(self, *args):\n        '''Return a list of files in a given directory (default the current).'''\n        cmd = 'NLST'\n        for arg in args:\n            cmd = cmd + (' ' + arg)\n        files = []\n        self.retrlines(cmd, files.append)\n        return files\n\n    def dir(self, *args):\n        '''List a directory in long form.\n        By default list current directory to stdout.\n        Optional last argument is callback function; all\n        non-empty arguments before it are concatenated to the\n        LIST command.  (This *should* only be used for a pathname.)'''\n        cmd = 'LIST'\n        func = None\n        if args[-1:] and type(args[-1]) != type(''):\n            args, func = args[:-1], args[-1]\n        for arg in args:\n            if arg:\n                cmd = cmd + (' ' + arg)\n        self.retrlines(cmd, func)\n\n    def rename(self, fromname, toname):\n        '''Rename a file.'''\n        resp = self.sendcmd('RNFR ' + fromname)\n        if resp[0] != '3':\n            raise error_reply, resp\n        return self.voidcmd('RNTO ' + toname)\n\n    def delete(self, filename):\n        '''Delete a file.'''\n        resp = self.sendcmd('DELE ' + filename)\n        if resp[:3] in ('250', '200'):\n            return resp\n        else:\n            raise error_reply, resp\n\n    def cwd(self, dirname):\n        '''Change to a directory.'''\n        if dirname == '..':\n            try:\n                return self.voidcmd('CDUP')\n            except error_perm, msg:\n                if msg.args[0][:3] != '500':\n                    raise\n        elif dirname == '':\n            dirname = '.'  # does nothing, but could return error\n        cmd = 'CWD ' + dirname\n        return self.voidcmd(cmd)\n\n    def size(self, filename):\n        '''Retrieve the size of a file.'''\n        # The SIZE command is defined in RFC-3659\n        resp = self.sendcmd('SIZE ' + filename)\n        if resp[:3] == '213':\n            s = resp[3:].strip()\n            try:\n                return int(s)\n            except (OverflowError, ValueError):\n                return long(s)\n\n    def mkd(self, dirname):\n        '''Make a directory, return its full pathname.'''\n        resp = self.sendcmd('MKD ' + dirname)\n        return parse257(resp)\n\n    def rmd(self, dirname):\n        '''Remove a directory.'''\n        return self.voidcmd('RMD ' + dirname)\n\n    def pwd(self):\n        '''Return current working directory.'''\n        resp = self.sendcmd('PWD')\n        return parse257(resp)\n\n    def quit(self):\n        '''Quit, and close the connection.'''\n        resp = self.voidcmd('QUIT')\n        self.close()\n        return resp\n\n    def close(self):\n        '''Close the connection without assuming anything about it.'''\n        if self.file is not None:\n            self.file.close()\n        if self.sock is not None:\n            self.sock.close()\n        self.file = self.sock = None\n\ntry:\n    import ssl\nexcept ImportError:\n    pass\nelse:\n    class FTP_TLS(FTP):\n        '''A FTP subclass which adds TLS support to FTP as described\n        in RFC-4217.\n\n        Connect as usual to port 21 implicitly securing the FTP control\n        connection before authenticating.\n\n        Securing the data connection requires user to explicitly ask\n        for it by calling prot_p() method.\n\n        Usage example:\n        >>> from ftplib import FTP_TLS\n        >>> ftps = FTP_TLS('ftp.python.org')\n        >>> ftps.login()  # login anonymously previously securing control channel\n        '230 Guest login ok, access restrictions apply.'\n        >>> ftps.prot_p()  # switch to secure data connection\n        '200 Protection level set to P'\n        >>> ftps.retrlines('LIST')  # list directory content securely\n        total 9\n        drwxr-xr-x   8 root     wheel        1024 Jan  3  1994 .\n        drwxr-xr-x   8 root     wheel        1024 Jan  3  1994 ..\n        drwxr-xr-x   2 root     wheel        1024 Jan  3  1994 bin\n        drwxr-xr-x   2 root     wheel        1024 Jan  3  1994 etc\n        d-wxrwxr-x   2 ftp      wheel        1024 Sep  5 13:43 incoming\n        drwxr-xr-x   2 root     wheel        1024 Nov 17  1993 lib\n        drwxr-xr-x   6 1094     wheel        1024 Sep 13 19:07 pub\n        drwxr-xr-x   3 root     wheel        1024 Jan  3  1994 usr\n        -rw-r--r--   1 root     root          312 Aug  1  1994 welcome.msg\n        '226 Transfer complete.'\n        >>> ftps.quit()\n        '221 Goodbye.'\n        >>>\n        '''\n        ssl_version = ssl.PROTOCOL_TLSv1\n\n        def __init__(self, host='', user='', passwd='', acct='', keyfile=None,\n                     certfile=None, timeout=_GLOBAL_DEFAULT_TIMEOUT):\n            self.keyfile = keyfile\n            self.certfile = certfile\n            self._prot_p = False\n            FTP.__init__(self, host, user, passwd, acct, timeout)\n\n        def login(self, user='', passwd='', acct='', secure=True):\n            if secure and not isinstance(self.sock, ssl.SSLSocket):\n                self.auth()\n            return FTP.login(self, user, passwd, acct)\n\n        def auth(self):\n            '''Set up secure control connection by using TLS/SSL.'''\n            if isinstance(self.sock, ssl.SSLSocket):\n                raise ValueError(\"Already using TLS\")\n            if self.ssl_version == ssl.PROTOCOL_TLSv1:\n                resp = self.voidcmd('AUTH TLS')\n            else:\n                resp = self.voidcmd('AUTH SSL')\n            self.sock = ssl.wrap_socket(self.sock, self.keyfile, self.certfile,\n                                        ssl_version=self.ssl_version)\n            self.file = self.sock.makefile(mode='rb')\n            return resp\n\n        def prot_p(self):\n            '''Set up secure data connection.'''\n            # PROT defines whether or not the data channel is to be protected.\n            # Though RFC-2228 defines four possible protection levels,\n            # RFC-4217 only recommends two, Clear and Private.\n            # Clear (PROT C) means that no security is to be used on the\n            # data-channel, Private (PROT P) means that the data-channel\n            # should be protected by TLS.\n            # PBSZ command MUST still be issued, but must have a parameter of\n            # '0' to indicate that no buffering is taking place and the data\n            # connection should not be encapsulated.\n            self.voidcmd('PBSZ 0')\n            resp = self.voidcmd('PROT P')\n            self._prot_p = True\n            return resp\n\n        def prot_c(self):\n            '''Set up clear text data connection.'''\n            resp = self.voidcmd('PROT C')\n            self._prot_p = False\n            return resp\n\n        # --- Overridden FTP methods\n\n        def ntransfercmd(self, cmd, rest=None):\n            conn, size = FTP.ntransfercmd(self, cmd, rest)\n            if self._prot_p:\n                conn = ssl.wrap_socket(conn, self.keyfile, self.certfile,\n                                       ssl_version=self.ssl_version)\n            return conn, size\n\n        def retrbinary(self, cmd, callback, blocksize=8192, rest=None):\n            self.voidcmd('TYPE I')\n            conn = self.transfercmd(cmd, rest)\n            try:\n                while 1:\n                    data = conn.recv(blocksize)\n                    if not data:\n                        break\n                    callback(data)\n                # shutdown ssl layer\n                if isinstance(conn, ssl.SSLSocket):\n                    conn.unwrap()\n            finally:\n                conn.close()\n            return self.voidresp()\n\n        def retrlines(self, cmd, callback = None):\n            if callback is None: callback = print_line\n            resp = self.sendcmd('TYPE A')\n            conn = self.transfercmd(cmd)\n            fp = conn.makefile('rb')\n            try:\n                while 1:\n                    line = fp.readline(self.maxline + 1)\n                    if len(line) > self.maxline:\n                        raise Error(\"got more than %d bytes\" % self.maxline)\n                    if self.debugging > 2: print '*retr*', repr(line)\n                    if not line:\n                        break\n                    if line[-2:] == CRLF:\n                        line = line[:-2]\n                    elif line[-1:] == '\\n':\n                        line = line[:-1]\n                    callback(line)\n                # shutdown ssl layer\n                if isinstance(conn, ssl.SSLSocket):\n                    conn.unwrap()\n            finally:\n                fp.close()\n                conn.close()\n            return self.voidresp()\n\n        def storbinary(self, cmd, fp, blocksize=8192, callback=None, rest=None):\n            self.voidcmd('TYPE I')\n            conn = self.transfercmd(cmd, rest)\n            try:\n                while 1:\n                    buf = fp.read(blocksize)\n                    if not buf: break\n                    conn.sendall(buf)\n                    if callback: callback(buf)\n                # shutdown ssl layer\n                if isinstance(conn, ssl.SSLSocket):\n                    conn.unwrap()\n            finally:\n                conn.close()\n            return self.voidresp()\n\n        def storlines(self, cmd, fp, callback=None):\n            self.voidcmd('TYPE A')\n            conn = self.transfercmd(cmd)\n            try:\n                while 1:\n                    buf = fp.readline(self.maxline + 1)\n                    if len(buf) > self.maxline:\n                        raise Error(\"got more than %d bytes\" % self.maxline)\n                    if not buf: break\n                    if buf[-2:] != CRLF:\n                        if buf[-1] in CRLF: buf = buf[:-1]\n                        buf = buf + CRLF\n                    conn.sendall(buf)\n                    if callback: callback(buf)\n                # shutdown ssl layer\n                if isinstance(conn, ssl.SSLSocket):\n                    conn.unwrap()\n            finally:\n                conn.close()\n            return self.voidresp()\n\n    __all__.append('FTP_TLS')\n    all_errors = (Error, IOError, EOFError, ssl.SSLError)\n\n\n_150_re = None\n\ndef parse150(resp):\n    '''Parse the '150' response for a RETR request.\n    Returns the expected transfer size or None; size is not guaranteed to\n    be present in the 150 message.\n    '''\n    if resp[:3] != '150':\n        raise error_reply, resp\n    global _150_re\n    if _150_re is None:\n        import re\n        _150_re = re.compile(\"150 .* \\((\\d+) bytes\\)\", re.IGNORECASE)\n    m = _150_re.match(resp)\n    if not m:\n        return None\n    s = m.group(1)\n    try:\n        return int(s)\n    except (OverflowError, ValueError):\n        return long(s)\n\n\n_227_re = None\n\ndef parse227(resp):\n    '''Parse the '227' response for a PASV request.\n    Raises error_proto if it does not contain '(h1,h2,h3,h4,p1,p2)'\n    Return ('host.addr.as.numbers', port#) tuple.'''\n\n    if resp[:3] != '227':\n        raise error_reply, resp\n    global _227_re\n    if _227_re is None:\n        import re\n        _227_re = re.compile(r'(\\d+),(\\d+),(\\d+),(\\d+),(\\d+),(\\d+)')\n    m = _227_re.search(resp)\n    if not m:\n        raise error_proto, resp\n    numbers = m.groups()\n    host = '.'.join(numbers[:4])\n    port = (int(numbers[4]) << 8) + int(numbers[5])\n    return host, port\n\n\ndef parse229(resp, peer):\n    '''Parse the '229' response for a EPSV request.\n    Raises error_proto if it does not contain '(|||port|)'\n    Return ('host.addr.as.numbers', port#) tuple.'''\n\n    if resp[:3] != '229':\n        raise error_reply, resp\n    left = resp.find('(')\n    if left < 0: raise error_proto, resp\n    right = resp.find(')', left + 1)\n    if right < 0:\n        raise error_proto, resp # should contain '(|||port|)'\n    if resp[left + 1] != resp[right - 1]:\n        raise error_proto, resp\n    parts = resp[left + 1:right].split(resp[left+1])\n    if len(parts) != 5:\n        raise error_proto, resp\n    host = peer[0]\n    port = int(parts[3])\n    return host, port\n\n\ndef parse257(resp):\n    '''Parse the '257' response for a MKD or PWD request.\n    This is a response to a MKD or PWD request: a directory name.\n    Returns the directoryname in the 257 reply.'''\n\n    if resp[:3] != '257':\n        raise error_reply, resp\n    if resp[3:5] != ' \"':\n        return '' # Not compliant to RFC 959, but UNIX ftpd does this\n    dirname = ''\n    i = 5\n    n = len(resp)\n    while i < n:\n        c = resp[i]\n        i = i+1\n        if c == '\"':\n            if i >= n or resp[i] != '\"':\n                break\n            i = i+1\n        dirname = dirname + c\n    return dirname\n\n\ndef print_line(line):\n    '''Default retrlines callback to print a line.'''\n    print line\n\n\ndef ftpcp(source, sourcename, target, targetname = '', type = 'I'):\n    '''Copy file from one FTP-instance to another.'''\n    if not targetname: targetname = sourcename\n    type = 'TYPE ' + type\n    source.voidcmd(type)\n    target.voidcmd(type)\n    sourcehost, sourceport = parse227(source.sendcmd('PASV'))\n    target.sendport(sourcehost, sourceport)\n    # RFC 959: the user must \"listen\" [...] BEFORE sending the\n    # transfer request.\n    # So: STOR before RETR, because here the target is a \"user\".\n    treply = target.sendcmd('STOR ' + targetname)\n    if treply[:3] not in ('125', '150'): raise error_proto  # RFC 959\n    sreply = source.sendcmd('RETR ' + sourcename)\n    if sreply[:3] not in ('125', '150'): raise error_proto  # RFC 959\n    source.voidresp()\n    target.voidresp()\n\n\nclass Netrc:\n    \"\"\"Class to parse & provide access to 'netrc' format files.\n\n    See the netrc(4) man page for information on the file format.\n\n    WARNING: This class is obsolete -- use module netrc instead.\n\n    \"\"\"\n    __defuser = None\n    __defpasswd = None\n    __defacct = None\n\n    def __init__(self, filename=None):\n        if filename is None:\n            if \"HOME\" in os.environ:\n                filename = os.path.join(os.environ[\"HOME\"],\n                                        \".netrc\")\n            else:\n                raise IOError, \\\n                      \"specify file to load or set $HOME\"\n        self.__hosts = {}\n        self.__macros = {}\n        fp = open(filename, \"r\")\n        in_macro = 0\n        while 1:\n            line = fp.readline(self.maxline + 1)\n            if len(line) > self.maxline:\n                raise Error(\"got more than %d bytes\" % self.maxline)\n            if not line: break\n            if in_macro and line.strip():\n                macro_lines.append(line)\n                continue\n            elif in_macro:\n                self.__macros[macro_name] = tuple(macro_lines)\n                in_macro = 0\n            words = line.split()\n            host = user = passwd = acct = None\n            default = 0\n            i = 0\n            while i < len(words):\n                w1 = words[i]\n                if i+1 < len(words):\n                    w2 = words[i + 1]\n                else:\n                    w2 = None\n                if w1 == 'default':\n                    default = 1\n                elif w1 == 'machine' and w2:\n                    host = w2.lower()\n                    i = i + 1\n                elif w1 == 'login' and w2:\n                    user = w2\n                    i = i + 1\n                elif w1 == 'password' and w2:\n                    passwd = w2\n                    i = i + 1\n                elif w1 == 'account' and w2:\n                    acct = w2\n                    i = i + 1\n                elif w1 == 'macdef' and w2:\n                    macro_name = w2\n                    macro_lines = []\n                    in_macro = 1\n                    break\n                i = i + 1\n            if default:\n                self.__defuser = user or self.__defuser\n                self.__defpasswd = passwd or self.__defpasswd\n                self.__defacct = acct or self.__defacct\n            if host:\n                if host in self.__hosts:\n                    ouser, opasswd, oacct = \\\n                           self.__hosts[host]\n                    user = user or ouser\n                    passwd = passwd or opasswd\n                    acct = acct or oacct\n                self.__hosts[host] = user, passwd, acct\n        fp.close()\n\n    def get_hosts(self):\n        \"\"\"Return a list of hosts mentioned in the .netrc file.\"\"\"\n        return self.__hosts.keys()\n\n    def get_account(self, host):\n        \"\"\"Returns login information for the named host.\n\n        The return value is a triple containing userid,\n        password, and the accounting field.\n\n        \"\"\"\n        host = host.lower()\n        user = passwd = acct = None\n        if host in self.__hosts:\n            user, passwd, acct = self.__hosts[host]\n        user = user or self.__defuser\n        passwd = passwd or self.__defpasswd\n        acct = acct or self.__defacct\n        return user, passwd, acct\n\n    def get_macros(self):\n        \"\"\"Return a list of all defined macro names.\"\"\"\n        return self.__macros.keys()\n\n    def get_macro(self, macro):\n        \"\"\"Return a sequence of lines which define a named macro.\"\"\"\n        return self.__macros[macro]\n\n\n\ndef test():\n    '''Test program.\n    Usage: ftp [-d] [-r[file]] host [-l[dir]] [-d[dir]] [-p] [file] ...\n\n    -d dir\n    -l list\n    -p password\n    '''\n\n    if len(sys.argv) < 2:\n        print test.__doc__\n        sys.exit(0)\n\n    debugging = 0\n    rcfile = None\n    while sys.argv[1] == '-d':\n        debugging = debugging+1\n        del sys.argv[1]\n    if sys.argv[1][:2] == '-r':\n        # get name of alternate ~/.netrc file:\n        rcfile = sys.argv[1][2:]\n        del sys.argv[1]\n    host = sys.argv[1]\n    ftp = FTP(host)\n    ftp.set_debuglevel(debugging)\n    userid = passwd = acct = ''\n    try:\n        netrc = Netrc(rcfile)\n    except IOError:\n        if rcfile is not None:\n            sys.stderr.write(\"Could not open account file\"\n                             \" -- using anonymous login.\")\n    else:\n        try:\n            userid, passwd, acct = netrc.get_account(host)\n        except KeyError:\n            # no account for host\n            sys.stderr.write(\n                    \"No account -- using anonymous login.\")\n    ftp.login(userid, passwd, acct)\n    for file in sys.argv[2:]:\n        if file[:2] == '-l':\n            ftp.dir(file[2:])\n        elif file[:2] == '-d':\n            cmd = 'CWD'\n            if file[2:]: cmd = cmd + ' ' + file[2:]\n            resp = ftp.sendcmd(cmd)\n        elif file == '-p':\n            ftp.set_pasv(not ftp.passiveserver)\n        else:\n            ftp.retrbinary('RETR ' + file, \\\n                           sys.stdout.write, 1024)\n    ftp.quit()\n\n\nif __name__ == '__main__':\n    test()\n", 
    "functools": "\"\"\"functools.py - Tools for working with functions and callable objects\n\"\"\"\n# Python module wrapper for _functools C module\n# to allow utilities written in Python to be added\n# to the functools module.\n# Written by Nick Coghlan <ncoghlan at gmail.com>\n#   Copyright (C) 2006 Python Software Foundation.\n# See C source code for _functools credits/copyright\n\nfrom _functools import partial, reduce\n\n# update_wrapper() and wraps() are tools to help write\n# wrapper functions that can handle naive introspection\n\nWRAPPER_ASSIGNMENTS = ('__module__', '__name__', '__doc__')\nWRAPPER_UPDATES = ('__dict__',)\ndef update_wrapper(wrapper,\n                   wrapped,\n                   assigned = WRAPPER_ASSIGNMENTS,\n                   updated = WRAPPER_UPDATES):\n    \"\"\"Update a wrapper function to look like the wrapped function\n\n       wrapper is the function to be updated\n       wrapped is the original function\n       assigned is a tuple naming the attributes assigned directly\n       from the wrapped function to the wrapper function (defaults to\n       functools.WRAPPER_ASSIGNMENTS)\n       updated is a tuple naming the attributes of the wrapper that\n       are updated with the corresponding attribute from the wrapped\n       function (defaults to functools.WRAPPER_UPDATES)\n    \"\"\"\n    for attr in assigned:\n        setattr(wrapper, attr, getattr(wrapped, attr))\n    for attr in updated:\n        getattr(wrapper, attr).update(getattr(wrapped, attr, {}))\n    # Return the wrapper so this can be used as a decorator via partial()\n    return wrapper\n\ndef wraps(wrapped,\n          assigned = WRAPPER_ASSIGNMENTS,\n          updated = WRAPPER_UPDATES):\n    \"\"\"Decorator factory to apply update_wrapper() to a wrapper function\n\n       Returns a decorator that invokes update_wrapper() with the decorated\n       function as the wrapper argument and the arguments to wraps() as the\n       remaining arguments. Default arguments are as for update_wrapper().\n       This is a convenience function to simplify applying partial() to\n       update_wrapper().\n    \"\"\"\n    return partial(update_wrapper, wrapped=wrapped,\n                   assigned=assigned, updated=updated)\n\ndef total_ordering(cls):\n    \"\"\"Class decorator that fills in missing ordering methods\"\"\"\n    convert = {\n        '__lt__': [('__gt__', lambda self, other: not (self < other or self == other)),\n                   ('__le__', lambda self, other: self < other or self == other),\n                   ('__ge__', lambda self, other: not self < other)],\n        '__le__': [('__ge__', lambda self, other: not self <= other or self == other),\n                   ('__lt__', lambda self, other: self <= other and not self == other),\n                   ('__gt__', lambda self, other: not self <= other)],\n        '__gt__': [('__lt__', lambda self, other: not (self > other or self == other)),\n                   ('__ge__', lambda self, other: self > other or self == other),\n                   ('__le__', lambda self, other: not self > other)],\n        '__ge__': [('__le__', lambda self, other: (not self >= other) or self == other),\n                   ('__gt__', lambda self, other: self >= other and not self == other),\n                   ('__lt__', lambda self, other: not self >= other)]\n    }\n    roots = set(dir(cls)) & set(convert)\n    if not roots:\n        raise ValueError('must define at least one ordering operation: < > <= >=')\n    root = max(roots)       # prefer __lt__ to __le__ to __gt__ to __ge__\n    for opname, opfunc in convert[root]:\n        if opname not in roots:\n            opfunc.__name__ = opname\n            opfunc.__doc__ = getattr(int, opname).__doc__\n            setattr(cls, opname, opfunc)\n    return cls\n\ndef cmp_to_key(mycmp):\n    \"\"\"Convert a cmp= function into a key= function\"\"\"\n    class K(object):\n        __slots__ = ['obj']\n        def __init__(self, obj, *args):\n            self.obj = obj\n        def __lt__(self, other):\n            return mycmp(self.obj, other.obj) < 0\n        def __gt__(self, other):\n            return mycmp(self.obj, other.obj) > 0\n        def __eq__(self, other):\n            return mycmp(self.obj, other.obj) == 0\n        def __le__(self, other):\n            return mycmp(self.obj, other.obj) <= 0\n        def __ge__(self, other):\n            return mycmp(self.obj, other.obj) >= 0\n        def __ne__(self, other):\n            return mycmp(self.obj, other.obj) != 0\n        def __hash__(self):\n            raise TypeError('hash not implemented')\n    return K\n", 
    "genericpath": "\"\"\"\nPath operations common to more than one OS\nDo not use directly.  The OS specific modules import the appropriate\nfunctions from this module themselves.\n\"\"\"\nimport os\nimport stat\n\n__all__ = ['commonprefix', 'exists', 'getatime', 'getctime', 'getmtime',\n           'getsize', 'isdir', 'isfile']\n\n\n# Does a path exist?\n# This is false for dangling symbolic links on systems that support them.\ndef exists(path):\n    \"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\n    try:\n        os.stat(path)\n    except os.error:\n        return False\n    return True\n\n\n# This follows symbolic links, so both islink() and isdir() can be true\n# for the same path on systems that support symlinks\ndef isfile(path):\n    \"\"\"Test whether a path is a regular file\"\"\"\n    try:\n        st = os.stat(path)\n    except os.error:\n        return False\n    return stat.S_ISREG(st.st_mode)\n\n\n# Is a path a directory?\n# This follows symbolic links, so both islink() and isdir()\n# can be true for the same path on systems that support symlinks\ndef isdir(s):\n    \"\"\"Return true if the pathname refers to an existing directory.\"\"\"\n    try:\n        st = os.stat(s)\n    except os.error:\n        return False\n    return stat.S_ISDIR(st.st_mode)\n\n\ndef getsize(filename):\n    \"\"\"Return the size of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_size\n\n\ndef getmtime(filename):\n    \"\"\"Return the last modification time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_mtime\n\n\ndef getatime(filename):\n    \"\"\"Return the last access time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_atime\n\n\ndef getctime(filename):\n    \"\"\"Return the metadata change time of a file, reported by os.stat().\"\"\"\n    return os.stat(filename).st_ctime\n\n\n# Return the longest prefix of all list elements.\ndef commonprefix(m):\n    \"Given a list of pathnames, returns the longest common leading component\"\n    if not m: return ''\n    s1 = min(m)\n    s2 = max(m)\n    for i, c in enumerate(s1):\n        if c != s2[i]:\n            return s1[:i]\n    return s1\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\n# Generic implementation of splitext, to be parametrized with\n# the separators\ndef _splitext(p, sep, altsep, extsep):\n    \"\"\"Split the extension from a pathname.\n\n    Extension is everything from the last dot to the end, ignoring\n    leading dots.  Returns \"(root, ext)\"; ext may be empty.\"\"\"\n\n    sepIndex = p.rfind(sep)\n    if altsep:\n        altsepIndex = p.rfind(altsep)\n        sepIndex = max(sepIndex, altsepIndex)\n\n    dotIndex = p.rfind(extsep)\n    if dotIndex > sepIndex:\n        # skip all leading dots\n        filenameIndex = sepIndex + 1\n        while filenameIndex < dotIndex:\n            if p[filenameIndex] != extsep:\n                return p[:dotIndex], p[dotIndex:]\n            filenameIndex += 1\n\n    return p, ''\n", 
    "getopt": "\"\"\"Parser for command line options.\n\nThis module helps scripts to parse the command line arguments in\nsys.argv.  It supports the same conventions as the Unix getopt()\nfunction (including the special meanings of arguments of the form `-'\nand `--').  Long options similar to those supported by GNU software\nmay be used as well via an optional third argument.  This module\nprovides two functions and an exception:\n\ngetopt() -- Parse command line options\ngnu_getopt() -- Like getopt(), but allow option and non-option arguments\nto be intermixed.\nGetoptError -- exception (class) raised with 'opt' attribute, which is the\noption involved with the exception.\n\"\"\"\n\n# Long option support added by Lars Wirzenius <liw@iki.fi>.\n#\n# Gerrit Holl <gerrit@nl.linux.org> moved the string-based exceptions\n# to class-based exceptions.\n#\n# Peter Astrand <astrand@lysator.liu.se> added gnu_getopt().\n#\n# TODO for gnu_getopt():\n#\n# - GNU getopt_long_only mechanism\n# - allow the caller to specify ordering\n# - RETURN_IN_ORDER option\n# - GNU extension with '-' as first character of option string\n# - optional arguments, specified by double colons\n# - a option string with a W followed by semicolon should\n#   treat \"-W foo\" as \"--foo\"\n\n__all__ = [\"GetoptError\",\"error\",\"getopt\",\"gnu_getopt\"]\n\nimport os\n\nclass GetoptError(Exception):\n    opt = ''\n    msg = ''\n    def __init__(self, msg, opt=''):\n        self.msg = msg\n        self.opt = opt\n        Exception.__init__(self, msg, opt)\n\n    def __str__(self):\n        return self.msg\n\nerror = GetoptError # backward compatibility\n\ndef getopt(args, shortopts, longopts = []):\n    \"\"\"getopt(args, options[, long_options]) -> opts, args\n\n    Parses command line options and parameter list.  args is the\n    argument list to be parsed, without the leading reference to the\n    running program.  Typically, this means \"sys.argv[1:]\".  shortopts\n    is the string of option letters that the script wants to\n    recognize, with options that require an argument followed by a\n    colon (i.e., the same format that Unix getopt() uses).  If\n    specified, longopts is a list of strings with the names of the\n    long options which should be supported.  The leading '--'\n    characters should not be included in the option name.  Options\n    which require an argument should be followed by an equal sign\n    ('=').\n\n    The return value consists of two elements: the first is a list of\n    (option, value) pairs; the second is the list of program arguments\n    left after the option list was stripped (this is a trailing slice\n    of the first argument).  Each option-and-value pair returned has\n    the option as its first element, prefixed with a hyphen (e.g.,\n    '-x'), and the option argument as its second element, or an empty\n    string if the option has no argument.  The options occur in the\n    list in the same order in which they were found, thus allowing\n    multiple occurrences.  Long and short options may be mixed.\n\n    \"\"\"\n\n    opts = []\n    if type(longopts) == type(\"\"):\n        longopts = [longopts]\n    else:\n        longopts = list(longopts)\n    while args and args[0].startswith('-') and args[0] != '-':\n        if args[0] == '--':\n            args = args[1:]\n            break\n        if args[0].startswith('--'):\n            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])\n        else:\n            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n\n    return opts, args\n\ndef gnu_getopt(args, shortopts, longopts = []):\n    \"\"\"getopt(args, options[, long_options]) -> opts, args\n\n    This function works like getopt(), except that GNU style scanning\n    mode is used by default. This means that option and non-option\n    arguments may be intermixed. The getopt() function stops\n    processing options as soon as a non-option argument is\n    encountered.\n\n    If the first character of the option string is `+', or if the\n    environment variable POSIXLY_CORRECT is set, then option\n    processing stops as soon as a non-option argument is encountered.\n\n    \"\"\"\n\n    opts = []\n    prog_args = []\n    if isinstance(longopts, str):\n        longopts = [longopts]\n    else:\n        longopts = list(longopts)\n\n    # Allow options after non-option arguments?\n    if shortopts.startswith('+'):\n        shortopts = shortopts[1:]\n        all_options_first = True\n    elif os.environ.get(\"POSIXLY_CORRECT\"):\n        all_options_first = True\n    else:\n        all_options_first = False\n\n    while args:\n        if args[0] == '--':\n            prog_args += args[1:]\n            break\n\n        if args[0][:2] == '--':\n            opts, args = do_longs(opts, args[0][2:], longopts, args[1:])\n        elif args[0][:1] == '-' and args[0] != '-':\n            opts, args = do_shorts(opts, args[0][1:], shortopts, args[1:])\n        else:\n            if all_options_first:\n                prog_args += args\n                break\n            else:\n                prog_args.append(args[0])\n                args = args[1:]\n\n    return opts, prog_args\n\ndef do_longs(opts, opt, longopts, args):\n    try:\n        i = opt.index('=')\n    except ValueError:\n        optarg = None\n    else:\n        opt, optarg = opt[:i], opt[i+1:]\n\n    has_arg, opt = long_has_args(opt, longopts)\n    if has_arg:\n        if optarg is None:\n            if not args:\n                raise GetoptError('option --%s requires argument' % opt, opt)\n            optarg, args = args[0], args[1:]\n    elif optarg is not None:\n        raise GetoptError('option --%s must not have an argument' % opt, opt)\n    opts.append(('--' + opt, optarg or ''))\n    return opts, args\n\n# Return:\n#   has_arg?\n#   full option name\ndef long_has_args(opt, longopts):\n    possibilities = [o for o in longopts if o.startswith(opt)]\n    if not possibilities:\n        raise GetoptError('option --%s not recognized' % opt, opt)\n    # Is there an exact match?\n    if opt in possibilities:\n        return False, opt\n    elif opt + '=' in possibilities:\n        return True, opt\n    # No exact match, so better be unique.\n    if len(possibilities) > 1:\n        # XXX since possibilities contains all valid continuations, might be\n        # nice to work them into the error msg\n        raise GetoptError('option --%s not a unique prefix' % opt, opt)\n    assert len(possibilities) == 1\n    unique_match = possibilities[0]\n    has_arg = unique_match.endswith('=')\n    if has_arg:\n        unique_match = unique_match[:-1]\n    return has_arg, unique_match\n\ndef do_shorts(opts, optstring, shortopts, args):\n    while optstring != '':\n        opt, optstring = optstring[0], optstring[1:]\n        if short_has_arg(opt, shortopts):\n            if optstring == '':\n                if not args:\n                    raise GetoptError('option -%s requires argument' % opt,\n                                      opt)\n                optstring, args = args[0], args[1:]\n            optarg, optstring = optstring, ''\n        else:\n            optarg = ''\n        opts.append(('-' + opt, optarg))\n    return opts, args\n\ndef short_has_arg(opt, shortopts):\n    for i in range(len(shortopts)):\n        if opt == shortopts[i] != ':':\n            return shortopts.startswith(':', i+1)\n    raise GetoptError('option -%s not recognized' % opt, opt)\n\nif __name__ == '__main__':\n    import sys\n    print getopt(sys.argv[1:], \"a:b\", [\"alpha=\", \"beta\"])\n", 
    "getpass": "\"\"\"Utilities to get a password and/or the current user name.\n\ngetpass(prompt[, stream]) - Prompt for a password, with echo turned off.\ngetuser() - Get the user name from the environment or password database.\n\nGetPassWarning - This UserWarning is issued when getpass() cannot prevent\n                 echoing of the password contents while reading.\n\nOn Windows, the msvcrt module will be used.\nOn the Mac EasyDialogs.AskPassword is used, if available.\n\n\"\"\"\n\n# Authors: Piers Lauder (original)\n#          Guido van Rossum (Windows support and cleanup)\n#          Gregory P. Smith (tty support & GetPassWarning)\n\nimport os, sys, warnings\n\n__all__ = [\"getpass\",\"getuser\",\"GetPassWarning\"]\n\n\nclass GetPassWarning(UserWarning): pass\n\n\ndef unix_getpass(prompt='Password: ', stream=None):\n    \"\"\"Prompt for a password, with echo turned off.\n\n    Args:\n      prompt: Written on stream to ask for the input.  Default: 'Password: '\n      stream: A writable file object to display the prompt.  Defaults to\n              the tty.  If no tty is available defaults to sys.stderr.\n    Returns:\n      The seKr3t input.\n    Raises:\n      EOFError: If our input tty or stdin was closed.\n      GetPassWarning: When we were unable to turn echo off on the input.\n\n    Always restores terminal settings before returning.\n    \"\"\"\n    fd = None\n    tty = None\n    try:\n        # Always try reading and writing directly on the tty first.\n        fd = os.open('/dev/tty', os.O_RDWR|os.O_NOCTTY)\n        tty = os.fdopen(fd, 'w+', 1)\n        input = tty\n        if not stream:\n            stream = tty\n    except EnvironmentError, e:\n        # If that fails, see if stdin can be controlled.\n        try:\n            fd = sys.stdin.fileno()\n        except (AttributeError, ValueError):\n            passwd = fallback_getpass(prompt, stream)\n        input = sys.stdin\n        if not stream:\n            stream = sys.stderr\n\n    if fd is not None:\n        passwd = None\n        try:\n            old = termios.tcgetattr(fd)     # a copy to save\n            new = old[:]\n            new[3] &= ~termios.ECHO  # 3 == 'lflags'\n            tcsetattr_flags = termios.TCSAFLUSH\n            if hasattr(termios, 'TCSASOFT'):\n                tcsetattr_flags |= termios.TCSASOFT\n            try:\n                termios.tcsetattr(fd, tcsetattr_flags, new)\n                passwd = _raw_input(prompt, stream, input=input)\n            finally:\n                termios.tcsetattr(fd, tcsetattr_flags, old)\n                stream.flush()  # issue7208\n        except termios.error, e:\n            if passwd is not None:\n                # _raw_input succeeded.  The final tcsetattr failed.  Reraise\n                # instead of leaving the terminal in an unknown state.\n                raise\n            # We can't control the tty or stdin.  Give up and use normal IO.\n            # fallback_getpass() raises an appropriate warning.\n            del input, tty  # clean up unused file objects before blocking\n            passwd = fallback_getpass(prompt, stream)\n\n    stream.write('\\n')\n    return passwd\n\n\ndef win_getpass(prompt='Password: ', stream=None):\n    \"\"\"Prompt for password with echo off, using Windows getch().\"\"\"\n    if sys.stdin is not sys.__stdin__:\n        return fallback_getpass(prompt, stream)\n    import msvcrt\n    for c in prompt:\n        msvcrt.putch(c)\n    pw = \"\"\n    while 1:\n        c = msvcrt.getch()\n        if c == '\\r' or c == '\\n':\n            break\n        if c == '\\003':\n            raise KeyboardInterrupt\n        if c == '\\b':\n            pw = pw[:-1]\n        else:\n            pw = pw + c\n    msvcrt.putch('\\r')\n    msvcrt.putch('\\n')\n    return pw\n\n\ndef fallback_getpass(prompt='Password: ', stream=None):\n    warnings.warn(\"Can not control echo on the terminal.\", GetPassWarning,\n                  stacklevel=2)\n    if not stream:\n        stream = sys.stderr\n    print >>stream, \"Warning: Password input may be echoed.\"\n    return _raw_input(prompt, stream)\n\n\ndef _raw_input(prompt=\"\", stream=None, input=None):\n    # A raw_input() replacement that doesn't save the string in the\n    # GNU readline history.\n    if not stream:\n        stream = sys.stderr\n    if not input:\n        input = sys.stdin\n    prompt = str(prompt)\n    if prompt:\n        stream.write(prompt)\n        stream.flush()\n    # NOTE: The Python C API calls flockfile() (and unlock) during readline.\n    line = input.readline()\n    if not line:\n        raise EOFError\n    if line[-1] == '\\n':\n        line = line[:-1]\n    return line\n\n\ndef getuser():\n    \"\"\"Get the username from the environment or password database.\n\n    First try various environment variables, then the password\n    database.  This works on Windows as long as USERNAME is set.\n\n    \"\"\"\n\n    import os\n\n    for name in ('LOGNAME', 'USER', 'LNAME', 'USERNAME'):\n        user = os.environ.get(name)\n        if user:\n            return user\n\n    # If this fails, the exception will \"explain\" why\n    import pwd\n    return pwd.getpwuid(os.getuid())[0]\n\n# Bind the name getpass to the appropriate function\ntry:\n    import termios\n    # it's possible there is an incompatible termios from the\n    # McMillan Installer, make sure we have a UNIX-compatible termios\n    termios.tcgetattr, termios.tcsetattr\nexcept (ImportError, AttributeError):\n    try:\n        import msvcrt\n    except ImportError:\n        try:\n            from EasyDialogs import AskPassword\n        except ImportError:\n            getpass = fallback_getpass\n        else:\n            getpass = AskPassword\n    else:\n        getpass = win_getpass\nelse:\n    getpass = unix_getpass\n", 
    "gettext": "\"\"\"Internationalization and localization support.\n\nThis module provides internationalization (I18N) and localization (L10N)\nsupport for your Python programs by providing an interface to the GNU gettext\nmessage catalog library.\n\nI18N refers to the operation by which a program is made aware of multiple\nlanguages.  L10N refers to the adaptation of your program, once\ninternationalized, to the local language and cultural habits.\n\n\"\"\"\n\n# This module represents the integration of work, contributions, feedback, and\n# suggestions from the following people:\n#\n# Martin von Loewis, who wrote the initial implementation of the underlying\n# C-based libintlmodule (later renamed _gettext), along with a skeletal\n# gettext.py implementation.\n#\n# Peter Funk, who wrote fintl.py, a fairly complete wrapper around intlmodule,\n# which also included a pure-Python implementation to read .mo files if\n# intlmodule wasn't available.\n#\n# James Henstridge, who also wrote a gettext.py module, which has some\n# interesting, but currently unsupported experimental features: the notion of\n# a Catalog class and instances, and the ability to add to a catalog file via\n# a Python API.\n#\n# Barry Warsaw integrated these modules, wrote the .install() API and code,\n# and conformed all C and Python code to Python's coding standards.\n#\n# Francois Pinard and Marc-Andre Lemburg also contributed valuably to this\n# module.\n#\n# J. David Ibanez implemented plural forms. Bruno Haible fixed some bugs.\n#\n# TODO:\n# - Lazy loading of .mo files.  Currently the entire catalog is loaded into\n#   memory, but that's probably bad for large translated programs.  Instead,\n#   the lexical sort of original strings in GNU .mo files should be exploited\n#   to do binary searches and lazy initializations.  Or you might want to use\n#   the undocumented double-hash algorithm for .mo files with hash tables, but\n#   you'll need to study the GNU gettext code to do this.\n#\n# - Support Solaris .mo file formats.  Unfortunately, we've been unable to\n#   find this format documented anywhere.\n\n\nimport locale, copy, os, re, struct, sys\nfrom errno import ENOENT\n\n\n__all__ = ['NullTranslations', 'GNUTranslations', 'Catalog',\n           'find', 'translation', 'install', 'textdomain', 'bindtextdomain',\n           'dgettext', 'dngettext', 'gettext', 'ngettext',\n           ]\n\n_default_localedir = os.path.join(sys.prefix, 'share', 'locale')\n\n\ndef test(condition, true, false):\n    \"\"\"\n    Implements the C expression:\n\n      condition ? true : false\n\n    Required to correctly interpret plural forms.\n    \"\"\"\n    if condition:\n        return true\n    else:\n        return false\n\n\ndef c2py(plural):\n    \"\"\"Gets a C expression as used in PO files for plural forms and returns a\n    Python lambda function that implements an equivalent expression.\n    \"\"\"\n    # Security check, allow only the \"n\" identifier\n    try:\n        from cStringIO import StringIO\n    except ImportError:\n        from StringIO import StringIO\n    import token, tokenize\n    tokens = tokenize.generate_tokens(StringIO(plural).readline)\n    try:\n        danger = [x for x in tokens if x[0] == token.NAME and x[1] != 'n']\n    except tokenize.TokenError:\n        raise ValueError, \\\n              'plural forms expression error, maybe unbalanced parenthesis'\n    else:\n        if danger:\n            raise ValueError, 'plural forms expression could be dangerous'\n\n    # Replace some C operators by their Python equivalents\n    plural = plural.replace('&&', ' and ')\n    plural = plural.replace('||', ' or ')\n\n    expr = re.compile(r'\\!([^=])')\n    plural = expr.sub(' not \\\\1', plural)\n\n    # Regular expression and replacement function used to transform\n    # \"a?b:c\" to \"test(a,b,c)\".\n    expr = re.compile(r'(.*?)\\?(.*?):(.*)')\n    def repl(x):\n        return \"test(%s, %s, %s)\" % (x.group(1), x.group(2),\n                                     expr.sub(repl, x.group(3)))\n\n    # Code to transform the plural expression, taking care of parentheses\n    stack = ['']\n    for c in plural:\n        if c == '(':\n            stack.append('')\n        elif c == ')':\n            if len(stack) == 1:\n                # Actually, we never reach this code, because unbalanced\n                # parentheses get caught in the security check at the\n                # beginning.\n                raise ValueError, 'unbalanced parenthesis in plural form'\n            s = expr.sub(repl, stack.pop())\n            stack[-1] += '(%s)' % s\n        else:\n            stack[-1] += c\n    plural = expr.sub(repl, stack.pop())\n\n    return eval('lambda n: int(%s)' % plural)\n\n\n\ndef _expand_lang(locale):\n    from locale import normalize\n    locale = normalize(locale)\n    COMPONENT_CODESET   = 1 << 0\n    COMPONENT_TERRITORY = 1 << 1\n    COMPONENT_MODIFIER  = 1 << 2\n    # split up the locale into its base components\n    mask = 0\n    pos = locale.find('@')\n    if pos >= 0:\n        modifier = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_MODIFIER\n    else:\n        modifier = ''\n    pos = locale.find('.')\n    if pos >= 0:\n        codeset = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_CODESET\n    else:\n        codeset = ''\n    pos = locale.find('_')\n    if pos >= 0:\n        territory = locale[pos:]\n        locale = locale[:pos]\n        mask |= COMPONENT_TERRITORY\n    else:\n        territory = ''\n    language = locale\n    ret = []\n    for i in range(mask+1):\n        if not (i & ~mask):  # if all components for this combo exist ...\n            val = language\n            if i & COMPONENT_TERRITORY: val += territory\n            if i & COMPONENT_CODESET:   val += codeset\n            if i & COMPONENT_MODIFIER:  val += modifier\n            ret.append(val)\n    ret.reverse()\n    return ret\n\n\n\nclass NullTranslations:\n    def __init__(self, fp=None):\n        self._info = {}\n        self._charset = None\n        self._output_charset = None\n        self._fallback = None\n        if fp is not None:\n            self._parse(fp)\n\n    def _parse(self, fp):\n        pass\n\n    def add_fallback(self, fallback):\n        if self._fallback:\n            self._fallback.add_fallback(fallback)\n        else:\n            self._fallback = fallback\n\n    def gettext(self, message):\n        if self._fallback:\n            return self._fallback.gettext(message)\n        return message\n\n    def lgettext(self, message):\n        if self._fallback:\n            return self._fallback.lgettext(message)\n        return message\n\n    def ngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.lngettext(msgid1, msgid2, n)\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n\n    def ugettext(self, message):\n        if self._fallback:\n            return self._fallback.ugettext(message)\n        return unicode(message)\n\n    def ungettext(self, msgid1, msgid2, n):\n        if self._fallback:\n            return self._fallback.ungettext(msgid1, msgid2, n)\n        if n == 1:\n            return unicode(msgid1)\n        else:\n            return unicode(msgid2)\n\n    def info(self):\n        return self._info\n\n    def charset(self):\n        return self._charset\n\n    def output_charset(self):\n        return self._output_charset\n\n    def set_output_charset(self, charset):\n        self._output_charset = charset\n\n    def install(self, unicode=False, names=None):\n        import __builtin__\n        __builtin__.__dict__['_'] = unicode and self.ugettext or self.gettext\n        if hasattr(names, \"__contains__\"):\n            if \"gettext\" in names:\n                __builtin__.__dict__['gettext'] = __builtin__.__dict__['_']\n            if \"ngettext\" in names:\n                __builtin__.__dict__['ngettext'] = (unicode and self.ungettext\n                                                             or self.ngettext)\n            if \"lgettext\" in names:\n                __builtin__.__dict__['lgettext'] = self.lgettext\n            if \"lngettext\" in names:\n                __builtin__.__dict__['lngettext'] = self.lngettext\n\n\nclass GNUTranslations(NullTranslations):\n    # Magic number of .mo files\n    LE_MAGIC = 0x950412deL\n    BE_MAGIC = 0xde120495L\n\n    def _parse(self, fp):\n        \"\"\"Override this method to support alternative .mo formats.\"\"\"\n        unpack = struct.unpack\n        filename = getattr(fp, 'name', '')\n        # Parse the .mo file header, which consists of 5 little endian 32\n        # bit words.\n        self._catalog = catalog = {}\n        self.plural = lambda n: int(n != 1) # germanic plural by default\n        buf = fp.read()\n        buflen = len(buf)\n        # Are we big endian or little endian?\n        magic = unpack('<I', buf[:4])[0]\n        if magic == self.LE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('<4I', buf[4:20])\n            ii = '<II'\n        elif magic == self.BE_MAGIC:\n            version, msgcount, masteridx, transidx = unpack('>4I', buf[4:20])\n            ii = '>II'\n        else:\n            raise IOError(0, 'Bad magic number', filename)\n        # Now put all messages from the .mo file buffer into the catalog\n        # dictionary.\n        for i in xrange(0, msgcount):\n            mlen, moff = unpack(ii, buf[masteridx:masteridx+8])\n            mend = moff + mlen\n            tlen, toff = unpack(ii, buf[transidx:transidx+8])\n            tend = toff + tlen\n            if mend < buflen and tend < buflen:\n                msg = buf[moff:mend]\n                tmsg = buf[toff:tend]\n            else:\n                raise IOError(0, 'File is corrupt', filename)\n            # See if we're looking at GNU .mo conventions for metadata\n            if mlen == 0:\n                # Catalog description\n                lastk = k = None\n                for item in tmsg.splitlines():\n                    item = item.strip()\n                    if not item:\n                        continue\n                    if ':' in item:\n                        k, v = item.split(':', 1)\n                        k = k.strip().lower()\n                        v = v.strip()\n                        self._info[k] = v\n                        lastk = k\n                    elif lastk:\n                        self._info[lastk] += '\\n' + item\n                    if k == 'content-type':\n                        self._charset = v.split('charset=')[1]\n                    elif k == 'plural-forms':\n                        v = v.split(';')\n                        plural = v[1].split('plural=')[1]\n                        self.plural = c2py(plural)\n            # Note: we unconditionally convert both msgids and msgstrs to\n            # Unicode using the character encoding specified in the charset\n            # parameter of the Content-Type header.  The gettext documentation\n            # strongly encourages msgids to be us-ascii, but some applications\n            # require alternative encodings (e.g. Zope's ZCML and ZPT).  For\n            # traditional gettext applications, the msgid conversion will\n            # cause no problems since us-ascii should always be a subset of\n            # the charset encoding.  We may want to fall back to 8-bit msgids\n            # if the Unicode conversion fails.\n            if '\\x00' in msg:\n                # Plural forms\n                msgid1, msgid2 = msg.split('\\x00')\n                tmsg = tmsg.split('\\x00')\n                if self._charset:\n                    msgid1 = unicode(msgid1, self._charset)\n                    tmsg = [unicode(x, self._charset) for x in tmsg]\n                for i in range(len(tmsg)):\n                    catalog[(msgid1, i)] = tmsg[i]\n            else:\n                if self._charset:\n                    msg = unicode(msg, self._charset)\n                    tmsg = unicode(tmsg, self._charset)\n                catalog[msg] = tmsg\n            # advance to next entry in the seek tables\n            masteridx += 8\n            transidx += 8\n\n    def gettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.gettext(message)\n            return message\n        # Encode the Unicode tmsg back to an 8-bit string, if possible\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        elif self._charset:\n            return tmsg.encode(self._charset)\n        return tmsg\n\n    def lgettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.lgettext(message)\n            return message\n        if self._output_charset:\n            return tmsg.encode(self._output_charset)\n        return tmsg.encode(locale.getpreferredencoding())\n\n    def ngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            elif self._charset:\n                return tmsg.encode(self._charset)\n            return tmsg\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def lngettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n            if self._output_charset:\n                return tmsg.encode(self._output_charset)\n            return tmsg.encode(locale.getpreferredencoding())\n        except KeyError:\n            if self._fallback:\n                return self._fallback.lngettext(msgid1, msgid2, n)\n            if n == 1:\n                return msgid1\n            else:\n                return msgid2\n\n    def ugettext(self, message):\n        missing = object()\n        tmsg = self._catalog.get(message, missing)\n        if tmsg is missing:\n            if self._fallback:\n                return self._fallback.ugettext(message)\n            return unicode(message)\n        return tmsg\n\n    def ungettext(self, msgid1, msgid2, n):\n        try:\n            tmsg = self._catalog[(msgid1, self.plural(n))]\n        except KeyError:\n            if self._fallback:\n                return self._fallback.ungettext(msgid1, msgid2, n)\n            if n == 1:\n                tmsg = unicode(msgid1)\n            else:\n                tmsg = unicode(msgid2)\n        return tmsg\n\n\n# Locate a .mo file using the gettext strategy\ndef find(domain, localedir=None, languages=None, all=0):\n    # Get some reasonable defaults for arguments that were not supplied\n    if localedir is None:\n        localedir = _default_localedir\n    if languages is None:\n        languages = []\n        for envar in ('LANGUAGE', 'LC_ALL', 'LC_MESSAGES', 'LANG'):\n            val = os.environ.get(envar)\n            if val:\n                languages = val.split(':')\n                break\n        if 'C' not in languages:\n            languages.append('C')\n    # now normalize and expand the languages\n    nelangs = []\n    for lang in languages:\n        for nelang in _expand_lang(lang):\n            if nelang not in nelangs:\n                nelangs.append(nelang)\n    # select a language\n    if all:\n        result = []\n    else:\n        result = None\n    for lang in nelangs:\n        if lang == 'C':\n            break\n        mofile = os.path.join(localedir, lang, 'LC_MESSAGES', '%s.mo' % domain)\n        if os.path.exists(mofile):\n            if all:\n                result.append(mofile)\n            else:\n                return mofile\n    return result\n\n\n\n# a mapping between absolute .mo file path and Translation object\n_translations = {}\n\ndef translation(domain, localedir=None, languages=None,\n                class_=None, fallback=False, codeset=None):\n    if class_ is None:\n        class_ = GNUTranslations\n    mofiles = find(domain, localedir, languages, all=1)\n    if not mofiles:\n        if fallback:\n            return NullTranslations()\n        raise IOError(ENOENT, 'No translation file found for domain', domain)\n    # Avoid opening, reading, and parsing the .mo file after it's been done\n    # once.\n    result = None\n    for mofile in mofiles:\n        key = (class_, os.path.abspath(mofile))\n        t = _translations.get(key)\n        if t is None:\n            with open(mofile, 'rb') as fp:\n                t = _translations.setdefault(key, class_(fp))\n        # Copy the translation object to allow setting fallbacks and\n        # output charset. All other instance data is shared with the\n        # cached object.\n        t = copy.copy(t)\n        if codeset:\n            t.set_output_charset(codeset)\n        if result is None:\n            result = t\n        else:\n            result.add_fallback(t)\n    return result\n\n\ndef install(domain, localedir=None, unicode=False, codeset=None, names=None):\n    t = translation(domain, localedir, fallback=True, codeset=codeset)\n    t.install(unicode, names)\n\n\n\n# a mapping b/w domains and locale directories\n_localedirs = {}\n# a mapping b/w domains and codesets\n_localecodesets = {}\n# current global domain, `messages' used for compatibility w/ GNU gettext\n_current_domain = 'messages'\n\n\ndef textdomain(domain=None):\n    global _current_domain\n    if domain is not None:\n        _current_domain = domain\n    return _current_domain\n\n\ndef bindtextdomain(domain, localedir=None):\n    global _localedirs\n    if localedir is not None:\n        _localedirs[domain] = localedir\n    return _localedirs.get(domain, _default_localedir)\n\n\ndef bind_textdomain_codeset(domain, codeset=None):\n    global _localecodesets\n    if codeset is not None:\n        _localecodesets[domain] = codeset\n    return _localecodesets.get(domain)\n\n\ndef dgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.gettext(message)\n\ndef ldgettext(domain, message):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        return message\n    return t.lgettext(message)\n\ndef dngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.ngettext(msgid1, msgid2, n)\n\ndef ldngettext(domain, msgid1, msgid2, n):\n    try:\n        t = translation(domain, _localedirs.get(domain, None),\n                        codeset=_localecodesets.get(domain))\n    except IOError:\n        if n == 1:\n            return msgid1\n        else:\n            return msgid2\n    return t.lngettext(msgid1, msgid2, n)\n\ndef gettext(message):\n    return dgettext(_current_domain, message)\n\ndef lgettext(message):\n    return ldgettext(_current_domain, message)\n\ndef ngettext(msgid1, msgid2, n):\n    return dngettext(_current_domain, msgid1, msgid2, n)\n\ndef lngettext(msgid1, msgid2, n):\n    return ldngettext(_current_domain, msgid1, msgid2, n)\n\n# dcgettext() has been deemed unnecessary and is not implemented.\n\n# James Henstridge's Catalog constructor from GNOME gettext.  Documented usage\n# was:\n#\n#    import gettext\n#    cat = gettext.Catalog(PACKAGE, localedir=LOCALEDIR)\n#    _ = cat.gettext\n#    print _('Hello World')\n\n# The resulting catalog object currently don't support access through a\n# dictionary API, which was supported (but apparently unused) in GNOME\n# gettext.\n\nCatalog = translation\n", 
    "glob": "\"\"\"Filename globbing utility.\"\"\"\n\nimport sys\nimport os\nimport re\nimport fnmatch\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n__all__ = [\"glob\", \"iglob\"]\n\ndef glob(pathname):\n    \"\"\"Return a list of paths matching a pathname pattern.\n\n    The pattern may contain simple shell-style wildcards a la\n    fnmatch. However, unlike fnmatch, filenames starting with a\n    dot are special cases that are not matched by '*' and '?'\n    patterns.\n\n    \"\"\"\n    return list(iglob(pathname))\n\ndef iglob(pathname):\n    \"\"\"Return an iterator which yields the paths matching a pathname pattern.\n\n    The pattern may contain simple shell-style wildcards a la\n    fnmatch. However, unlike fnmatch, filenames starting with a\n    dot are special cases that are not matched by '*' and '?'\n    patterns.\n\n    \"\"\"\n    dirname, basename = os.path.split(pathname)\n    if not has_magic(pathname):\n        if basename:\n            if os.path.lexists(pathname):\n                yield pathname\n        else:\n            # Patterns ending with a slash should match only directories\n            if os.path.isdir(dirname):\n                yield pathname\n        return\n    if not dirname:\n        for name in glob1(os.curdir, basename):\n            yield name\n        return\n    # `os.path.split()` returns the argument itself as a dirname if it is a\n    # drive or UNC path.  Prevent an infinite recursion if a drive or UNC path\n    # contains magic characters (i.e. r'\\\\?\\C:').\n    if dirname != pathname and has_magic(dirname):\n        dirs = iglob(dirname)\n    else:\n        dirs = [dirname]\n    if has_magic(basename):\n        glob_in_dir = glob1\n    else:\n        glob_in_dir = glob0\n    for dirname in dirs:\n        for name in glob_in_dir(dirname, basename):\n            yield os.path.join(dirname, name)\n\n# These 2 helper functions non-recursively glob inside a literal directory.\n# They return a list of basenames. `glob1` accepts a pattern while `glob0`\n# takes a literal basename (so it only has to check for its existence).\n\ndef glob1(dirname, pattern):\n    if not dirname:\n        dirname = os.curdir\n    if isinstance(pattern, _unicode) and not isinstance(dirname, unicode):\n        dirname = unicode(dirname, sys.getfilesystemencoding() or\n                                   sys.getdefaultencoding())\n    try:\n        names = os.listdir(dirname)\n    except os.error:\n        return []\n    if pattern[0] != '.':\n        names = filter(lambda x: x[0] != '.', names)\n    return fnmatch.filter(names, pattern)\n\ndef glob0(dirname, basename):\n    if basename == '':\n        # `os.path.split()` returns an empty basename for paths ending with a\n        # directory separator.  'q*x/' should match only directories.\n        if os.path.isdir(dirname):\n            return [basename]\n    else:\n        if os.path.lexists(os.path.join(dirname, basename)):\n            return [basename]\n    return []\n\n\nmagic_check = re.compile('[*?[]')\n\ndef has_magic(s):\n    return magic_check.search(s) is not None\n", 
    "grp": "\n\"\"\" This module provides ctypes version of cpython's grp module\n\"\"\"\n\nfrom _pwdgrp_cffi import ffi, lib\nimport _structseq\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nclass struct_group:\n    __metaclass__ = _structseq.structseqtype\n    name = \"grp.struct_group\"\n\n    gr_name   = _structseq.structseqfield(0)\n    gr_passwd = _structseq.structseqfield(1)\n    gr_gid    = _structseq.structseqfield(2)\n    gr_mem    = _structseq.structseqfield(3)\n\n\ndef _group_from_gstruct(res):\n    i = 0\n    members = []\n    while res.gr_mem[i]:\n        members.append(ffi.string(res.gr_mem[i]))\n        i += 1\n    return struct_group([\n        ffi.string(res.gr_name),\n        ffi.string(res.gr_passwd),\n        res.gr_gid,\n        members])\n\n@builtinify\ndef getgrgid(gid):\n    res = lib.getgrgid(gid)\n    if not res:\n        # XXX maybe check error eventually\n        raise KeyError(gid)\n    return _group_from_gstruct(res)\n\n@builtinify\ndef getgrnam(name):\n    if not isinstance(name, basestring):\n        raise TypeError(\"expected string\")\n    name = str(name)\n    res = lib.getgrnam(name)\n    if not res:\n        raise KeyError(\"'getgrnam(): name not found: %s'\" % name)\n    return _group_from_gstruct(res)\n\n@builtinify\ndef getgrall():\n    lib.setgrent()\n    lst = []\n    while 1:\n        p = lib.getgrent()\n        if not p:\n            break\n        lst.append(_group_from_gstruct(p))\n    lib.endgrent()\n    return lst\n\n__all__ = ('struct_group', 'getgrgid', 'getgrnam', 'getgrall')\n\nif __name__ == \"__main__\":\n    from os import getgid\n    gid = getgid()\n    pw = getgrgid(gid)\n    print(\"gid %s: %s\" % (pw.gr_gid, pw))\n    name = pw.gr_name\n    print(\"name %r: %s\" % (name, getgrnam(name)))\n    print(\"All:\")\n    for pw in getgrall():\n        print(pw)\n", 
    "gzip": "\"\"\"Functions that read and write gzipped files.\n\nThe user of the file doesn't have to worry about the compression,\nbut random access is not allowed.\"\"\"\n\n# based on Andrew Kuchling's minigzip.py distributed with the zlib module\n\nimport struct, sys, time, os\nimport zlib\nimport io\nimport __builtin__\n\n__all__ = [\"GzipFile\",\"open\"]\n\nFTEXT, FHCRC, FEXTRA, FNAME, FCOMMENT = 1, 2, 4, 8, 16\n\nREAD, WRITE = 1, 2\n\ndef write32u(output, value):\n    # The L format writes the bit pattern correctly whether signed\n    # or unsigned.\n    output.write(struct.pack(\"<L\", value))\n\ndef read32(input):\n    return struct.unpack(\"<I\", input.read(4))[0]\n\ndef open(filename, mode=\"rb\", compresslevel=9):\n    \"\"\"Shorthand for GzipFile(filename, mode, compresslevel).\n\n    The filename argument is required; mode defaults to 'rb'\n    and compresslevel defaults to 9.\n\n    \"\"\"\n    return GzipFile(filename, mode, compresslevel)\n\nclass GzipFile(io.BufferedIOBase):\n    \"\"\"The GzipFile class simulates most of the methods of a file object with\n    the exception of the readinto() and truncate() methods.\n\n    \"\"\"\n\n    myfileobj = None\n    max_read_chunk = 10 * 1024 * 1024   # 10Mb\n\n    def __init__(self, filename=None, mode=None,\n                 compresslevel=9, fileobj=None, mtime=None):\n        \"\"\"Constructor for the GzipFile class.\n\n        At least one of fileobj and filename must be given a\n        non-trivial value.\n\n        The new class instance is based on fileobj, which can be a regular\n        file, a StringIO object, or any other object which simulates a file.\n        It defaults to None, in which case filename is opened to provide\n        a file object.\n\n        When fileobj is not None, the filename argument is only used to be\n        included in the gzip file header, which may includes the original\n        filename of the uncompressed file.  It defaults to the filename of\n        fileobj, if discernible; otherwise, it defaults to the empty string,\n        and in this case the original filename is not included in the header.\n\n        The mode argument can be any of 'r', 'rb', 'a', 'ab', 'w', or 'wb',\n        depending on whether the file will be read or written.  The default\n        is the mode of fileobj if discernible; otherwise, the default is 'rb'.\n        Be aware that only the 'rb', 'ab', and 'wb' values should be used\n        for cross-platform portability.\n\n        The compresslevel argument is an integer from 0 to 9 controlling the\n        level of compression; 1 is fastest and produces the least compression,\n        and 9 is slowest and produces the most compression. 0 is no compression\n        at all. The default is 9.\n\n        The mtime argument is an optional numeric timestamp to be written\n        to the stream when compressing.  All gzip compressed streams\n        are required to contain a timestamp.  If omitted or None, the\n        current time is used.  This module ignores the timestamp when\n        decompressing; however, some programs, such as gunzip, make use\n        of it.  The format of the timestamp is the same as that of the\n        return value of time.time() and of the st_mtime member of the\n        object returned by os.stat().\n\n        \"\"\"\n\n        # Make sure we don't inadvertently enable universal newlines on the\n        # underlying file object - in read mode, this causes data corruption.\n        if mode:\n            mode = mode.replace('U', '')\n        # guarantee the file is opened in binary mode on platforms\n        # that care about that sort of thing\n        if mode and 'b' not in mode:\n            mode += 'b'\n        if fileobj is None:\n            fileobj = self.myfileobj = __builtin__.open(filename, mode or 'rb')\n        if filename is None:\n            # Issue #13781: os.fdopen() creates a fileobj with a bogus name\n            # attribute. Avoid saving this in the gzip header's filename field.\n            if hasattr(fileobj, 'name') and fileobj.name != '<fdopen>':\n                filename = fileobj.name\n            else:\n                filename = ''\n        if mode is None:\n            if hasattr(fileobj, 'mode'): mode = fileobj.mode\n            else: mode = 'rb'\n\n        if mode[0:1] == 'r':\n            self.mode = READ\n            # Set flag indicating start of a new member\n            self._new_member = True\n            # Buffer data read from gzip file. extrastart is offset in\n            # stream where buffer starts. extrasize is number of\n            # bytes remaining in buffer from current stream position.\n            self.extrabuf = \"\"\n            self.extrasize = 0\n            self.extrastart = 0\n            self.name = filename\n            # Starts small, scales exponentially\n            self.min_readsize = 100\n\n        elif mode[0:1] == 'w' or mode[0:1] == 'a':\n            self.mode = WRITE\n            self._init_write(filename)\n            self.compress = zlib.compressobj(compresslevel,\n                                             zlib.DEFLATED,\n                                             -zlib.MAX_WBITS,\n                                             zlib.DEF_MEM_LEVEL,\n                                             0)\n        else:\n            raise IOError, \"Mode \" + mode + \" not supported\"\n\n        self.fileobj = fileobj\n        self.offset = 0\n        self.mtime = mtime\n\n        if self.mode == WRITE:\n            self._write_gzip_header()\n\n    @property\n    def filename(self):\n        import warnings\n        warnings.warn(\"use the name attribute\", DeprecationWarning, 2)\n        if self.mode == WRITE and self.name[-3:] != \".gz\":\n            return self.name + \".gz\"\n        return self.name\n\n    def __repr__(self):\n        s = repr(self.fileobj)\n        return '<gzip ' + s[1:-1] + ' ' + hex(id(self)) + '>'\n\n    def _check_closed(self):\n        \"\"\"Raises a ValueError if the underlying file object has been closed.\n\n        \"\"\"\n        if self.closed:\n            raise ValueError('I/O operation on closed file.')\n\n    def _init_write(self, filename):\n        self.name = filename\n        self.crc = zlib.crc32(\"\") & 0xffffffffL\n        self.size = 0\n        self.writebuf = []\n        self.bufsize = 0\n\n    def _write_gzip_header(self):\n        self.fileobj.write('\\037\\213')             # magic header\n        self.fileobj.write('\\010')                 # compression method\n        try:\n            # RFC 1952 requires the FNAME field to be Latin-1. Do not\n            # include filenames that cannot be represented that way.\n            fname = os.path.basename(self.name)\n            if not isinstance(fname, str):\n                fname = fname.encode('latin-1')\n            if fname.endswith('.gz'):\n                fname = fname[:-3]\n        except UnicodeEncodeError:\n            fname = ''\n        flags = 0\n        if fname:\n            flags = FNAME\n        self.fileobj.write(chr(flags))\n        mtime = self.mtime\n        if mtime is None:\n            mtime = time.time()\n        write32u(self.fileobj, long(mtime))\n        self.fileobj.write('\\002')\n        self.fileobj.write('\\377')\n        if fname:\n            self.fileobj.write(fname + '\\000')\n\n    def _init_read(self):\n        self.crc = zlib.crc32(\"\") & 0xffffffffL\n        self.size = 0\n\n    def _read_gzip_header(self):\n        magic = self.fileobj.read(2)\n        if magic != '\\037\\213':\n            raise IOError, 'Not a gzipped file'\n        method = ord( self.fileobj.read(1) )\n        if method != 8:\n            raise IOError, 'Unknown compression method'\n        flag = ord( self.fileobj.read(1) )\n        self.mtime = read32(self.fileobj)\n        # extraflag = self.fileobj.read(1)\n        # os = self.fileobj.read(1)\n        self.fileobj.read(2)\n\n        if flag & FEXTRA:\n            # Read & discard the extra field, if present\n            xlen = ord(self.fileobj.read(1))\n            xlen = xlen + 256*ord(self.fileobj.read(1))\n            self.fileobj.read(xlen)\n        if flag & FNAME:\n            # Read and discard a null-terminated string containing the filename\n            while True:\n                s = self.fileobj.read(1)\n                if not s or s=='\\000':\n                    break\n        if flag & FCOMMENT:\n            # Read and discard a null-terminated string containing a comment\n            while True:\n                s = self.fileobj.read(1)\n                if not s or s=='\\000':\n                    break\n        if flag & FHCRC:\n            self.fileobj.read(2)     # Read & discard the 16-bit header CRC\n\n    def write(self,data):\n        self._check_closed()\n        if self.mode != WRITE:\n            import errno\n            raise IOError(errno.EBADF, \"write() on read-only GzipFile object\")\n\n        if self.fileobj is None:\n            raise ValueError, \"write() on closed GzipFile object\"\n\n        # Convert data type if called by io.BufferedWriter.\n        if isinstance(data, memoryview):\n            data = data.tobytes()\n\n        if len(data) > 0:\n            self.size = self.size + len(data)\n            self.crc = zlib.crc32(data, self.crc) & 0xffffffffL\n            self.fileobj.write( self.compress.compress(data) )\n            self.offset += len(data)\n\n        return len(data)\n\n    def read(self, size=-1):\n        self._check_closed()\n        if self.mode != READ:\n            import errno\n            raise IOError(errno.EBADF, \"read() on write-only GzipFile object\")\n\n        if self.extrasize <= 0 and self.fileobj is None:\n            return ''\n\n        readsize = 1024\n        if size < 0:        # get the whole thing\n            try:\n                while True:\n                    self._read(readsize)\n                    readsize = min(self.max_read_chunk, readsize * 2)\n            except EOFError:\n                size = self.extrasize\n        else:               # just get some more of it\n            try:\n                while size > self.extrasize:\n                    self._read(readsize)\n                    readsize = min(self.max_read_chunk, readsize * 2)\n            except EOFError:\n                if size > self.extrasize:\n                    size = self.extrasize\n\n        offset = self.offset - self.extrastart\n        chunk = self.extrabuf[offset: offset + size]\n        self.extrasize = self.extrasize - size\n\n        self.offset += size\n        return chunk\n\n    def _unread(self, buf):\n        self.extrasize = len(buf) + self.extrasize\n        self.offset -= len(buf)\n\n    def _read(self, size=1024):\n        if self.fileobj is None:\n            raise EOFError, \"Reached EOF\"\n\n        if self._new_member:\n            # If the _new_member flag is set, we have to\n            # jump to the next member, if there is one.\n            #\n            # First, check if we're at the end of the file;\n            # if so, it's time to stop; no more members to read.\n            pos = self.fileobj.tell()   # Save current position\n            self.fileobj.seek(0, 2)     # Seek to end of file\n            if pos == self.fileobj.tell():\n                raise EOFError, \"Reached EOF\"\n            else:\n                self.fileobj.seek( pos ) # Return to original position\n\n            self._init_read()\n            self._read_gzip_header()\n            self.decompress = zlib.decompressobj(-zlib.MAX_WBITS)\n            self._new_member = False\n\n        # Read a chunk of data from the file\n        buf = self.fileobj.read(size)\n\n        # If the EOF has been reached, flush the decompression object\n        # and mark this object as finished.\n\n        if buf == \"\":\n            uncompress = self.decompress.flush()\n            self._read_eof()\n            self._add_read_data( uncompress )\n            raise EOFError, 'Reached EOF'\n\n        uncompress = self.decompress.decompress(buf)\n        self._add_read_data( uncompress )\n\n        if self.decompress.unused_data != \"\":\n            # Ending case: we've come to the end of a member in the file,\n            # so seek back to the start of the unused data, finish up\n            # this member, and read a new gzip header.\n            # (The number of bytes to seek back is the length of the unused\n            # data, minus 8 because _read_eof() will rewind a further 8 bytes)\n            self.fileobj.seek( -len(self.decompress.unused_data)+8, 1)\n\n            # Check the CRC and file size, and set the flag so we read\n            # a new member on the next call\n            self._read_eof()\n            self._new_member = True\n\n    def _add_read_data(self, data):\n        self.crc = zlib.crc32(data, self.crc) & 0xffffffffL\n        offset = self.offset - self.extrastart\n        self.extrabuf = self.extrabuf[offset:] + data\n        self.extrasize = self.extrasize + len(data)\n        self.extrastart = self.offset\n        self.size = self.size + len(data)\n\n    def _read_eof(self):\n        # We've read to the end of the file, so we have to rewind in order\n        # to reread the 8 bytes containing the CRC and the file size.\n        # We check the that the computed CRC and size of the\n        # uncompressed data matches the stored values.  Note that the size\n        # stored is the true file size mod 2**32.\n        self.fileobj.seek(-8, 1)\n        crc32 = read32(self.fileobj)\n        isize = read32(self.fileobj)  # may exceed 2GB\n        if crc32 != self.crc:\n            raise IOError(\"CRC check failed %s != %s\" % (hex(crc32),\n                                                         hex(self.crc)))\n        elif isize != (self.size & 0xffffffffL):\n            raise IOError, \"Incorrect length of data produced\"\n\n        # Gzip files can be padded with zeroes and still have archives.\n        # Consume all zero bytes and set the file position to the first\n        # non-zero byte. See http://www.gzip.org/#faq8\n        c = \"\\x00\"\n        while c == \"\\x00\":\n            c = self.fileobj.read(1)\n        if c:\n            self.fileobj.seek(-1, 1)\n\n    @property\n    def closed(self):\n        return self.fileobj is None\n\n    def close(self):\n        if self.fileobj is None:\n            return\n        if self.mode == WRITE:\n            self.fileobj.write(self.compress.flush())\n            write32u(self.fileobj, self.crc)\n            # self.size may exceed 2GB, or even 4GB\n            write32u(self.fileobj, self.size & 0xffffffffL)\n            self.fileobj = None\n        elif self.mode == READ:\n            self.fileobj = None\n        if self.myfileobj:\n            self.myfileobj.close()\n            self.myfileobj = None\n\n    def flush(self,zlib_mode=zlib.Z_SYNC_FLUSH):\n        self._check_closed()\n        if self.mode == WRITE:\n            # Ensure the compressor's buffer is flushed\n            self.fileobj.write(self.compress.flush(zlib_mode))\n            self.fileobj.flush()\n\n    def fileno(self):\n        \"\"\"Invoke the underlying file object's fileno() method.\n\n        This will raise AttributeError if the underlying file object\n        doesn't support fileno().\n        \"\"\"\n        return self.fileobj.fileno()\n\n    def rewind(self):\n        '''Return the uncompressed stream file position indicator to the\n        beginning of the file'''\n        if self.mode != READ:\n            raise IOError(\"Can't rewind in write mode\")\n        self.fileobj.seek(0)\n        self._new_member = True\n        self.extrabuf = \"\"\n        self.extrasize = 0\n        self.extrastart = 0\n        self.offset = 0\n\n    def readable(self):\n        return self.mode == READ\n\n    def writable(self):\n        return self.mode == WRITE\n\n    def seekable(self):\n        return True\n\n    def seek(self, offset, whence=0):\n        if whence:\n            if whence == 1:\n                offset = self.offset + offset\n            else:\n                raise ValueError('Seek from end not supported')\n        if self.mode == WRITE:\n            if offset < self.offset:\n                raise IOError('Negative seek in write mode')\n            count = offset - self.offset\n            for i in xrange(count // 1024):\n                self.write(1024 * '\\0')\n            self.write((count % 1024) * '\\0')\n        elif self.mode == READ:\n            if offset < self.offset:\n                # for negative seek, rewind and do positive seek\n                self.rewind()\n            count = offset - self.offset\n            for i in xrange(count // 1024):\n                self.read(1024)\n            self.read(count % 1024)\n\n        return self.offset\n\n    def readline(self, size=-1):\n        if size < 0:\n            # Shortcut common case - newline found in buffer.\n            offset = self.offset - self.extrastart\n            i = self.extrabuf.find('\\n', offset) + 1\n            if i > 0:\n                self.extrasize -= i - offset\n                self.offset += i - offset\n                return self.extrabuf[offset: i]\n\n            size = sys.maxint\n            readsize = self.min_readsize\n        else:\n            readsize = size\n        bufs = []\n        while size != 0:\n            c = self.read(readsize)\n            i = c.find('\\n')\n\n            # We set i=size to break out of the loop under two\n            # conditions: 1) there's no newline, and the chunk is\n            # larger than size, or 2) there is a newline, but the\n            # resulting line would be longer than 'size'.\n            if (size <= i) or (i == -1 and len(c) > size):\n                i = size - 1\n\n            if i >= 0 or c == '':\n                bufs.append(c[:i + 1])    # Add portion of last chunk\n                self._unread(c[i + 1:])   # Push back rest of chunk\n                break\n\n            # Append chunk to list, decrease 'size',\n            bufs.append(c)\n            size = size - len(c)\n            readsize = min(size, readsize * 2)\n        if readsize > self.min_readsize:\n            self.min_readsize = min(readsize, self.min_readsize * 2, 512)\n        return ''.join(bufs) # Return resulting line\n\n\ndef _test():\n    # Act like gzip; with -d, act like gunzip.\n    # The input file is not deleted, however, nor are any other gzip\n    # options or features supported.\n    args = sys.argv[1:]\n    decompress = args and args[0] == \"-d\"\n    if decompress:\n        args = args[1:]\n    if not args:\n        args = [\"-\"]\n    for arg in args:\n        if decompress:\n            if arg == \"-\":\n                f = GzipFile(filename=\"\", mode=\"rb\", fileobj=sys.stdin)\n                g = sys.stdout\n            else:\n                if arg[-3:] != \".gz\":\n                    print \"filename doesn't end in .gz:\", repr(arg)\n                    continue\n                f = open(arg, \"rb\")\n                g = __builtin__.open(arg[:-3], \"wb\")\n        else:\n            if arg == \"-\":\n                f = sys.stdin\n                g = GzipFile(filename=\"\", mode=\"wb\", fileobj=sys.stdout)\n            else:\n                f = __builtin__.open(arg, \"rb\")\n                g = open(arg + \".gz\", \"wb\")\n        while True:\n            chunk = f.read(1024)\n            if not chunk:\n                break\n            g.write(chunk)\n        if g is not sys.stdout:\n            g.close()\n        if f is not sys.stdin:\n            f.close()\n\nif __name__ == '__main__':\n    _test()\n", 
    "hashlib": "# $Id$\n#\n#  Copyright (C) 2005   Gregory P. Smith (greg@krypto.org)\n#  Licensed to PSF under a Contributor Agreement.\n#\n\n__doc__ = \"\"\"hashlib module - A common interface to many hash functions.\n\nnew(name, string='') - returns a new hash object implementing the\n                       given hash function; initializing the hash\n                       using the given string data.\n\nNamed constructor functions are also available, these are much faster\nthan using new():\n\nmd5(), sha1(), sha224(), sha256(), sha384(), and sha512()\n\nMore algorithms may be available on your platform but the above are guaranteed\nto exist.  See the algorithms_guaranteed and algorithms_available attributes\nto find out what algorithm names can be passed to new().\n\nNOTE: If you want the adler32 or crc32 hash functions they are available in\nthe zlib module.\n\nChoose your hash function wisely.  Some have known collision weaknesses.\nsha384 and sha512 will be slow on 32 bit platforms.\n\nHash objects have these methods:\n - update(arg): Update the hash object with the string arg. Repeated calls\n                are equivalent to a single call with the concatenation of all\n                the arguments.\n - digest():    Return the digest of the strings passed to the update() method\n                so far. This may contain non-ASCII characters, including\n                NUL bytes.\n - hexdigest(): Like digest() except the digest is returned as a string of\n                double length, containing only hexadecimal digits.\n - copy():      Return a copy (clone) of the hash object. This can be used to\n                efficiently compute the digests of strings that share a common\n                initial substring.\n\nFor example, to obtain the digest of the string 'Nobody inspects the\nspammish repetition':\n\n    >>> import hashlib\n    >>> m = hashlib.md5()\n    >>> m.update(\"Nobody inspects\")\n    >>> m.update(\" the spammish repetition\")\n    >>> m.digest()\n    '\\\\xbbd\\\\x9c\\\\x83\\\\xdd\\\\x1e\\\\xa5\\\\xc9\\\\xd9\\\\xde\\\\xc9\\\\xa1\\\\x8d\\\\xf0\\\\xff\\\\xe9'\n\nMore condensed:\n\n    >>> hashlib.sha224(\"Nobody inspects the spammish repetition\").hexdigest()\n    'a4337bc45a8fc544c03f52dc550cd6e1e87021bc896588bd79e901e2'\n\n\"\"\"\n\n# This tuple and __get_builtin_constructor() must be modified if a new\n# always available algorithm is added.\n__always_supported = ('md5', 'sha1', 'sha224', 'sha256', 'sha384', 'sha512')\n\nalgorithms_guaranteed = set(__always_supported)\nalgorithms_available = set(__always_supported)\n\nalgorithms = __always_supported\n\n__all__ = __always_supported + ('new', 'algorithms_guaranteed',\n                                'algorithms_available', 'algorithms',\n                                'pbkdf2_hmac')\n\n\ndef __get_builtin_constructor(name):\n    try:\n        if name in ('SHA1', 'sha1'):\n            import _sha\n            return _sha.new\n        elif name in ('MD5', 'md5'):\n            import _md5\n            return _md5.new\n        elif name in ('SHA256', 'sha256', 'SHA224', 'sha224'):\n            import _sha256\n            bs = name[3:]\n            if bs == '256':\n                return _sha256.sha256\n            elif bs == '224':\n                return _sha256.sha224\n        elif name in ('SHA512', 'sha512', 'SHA384', 'sha384'):\n            import _sha512\n            bs = name[3:]\n            if bs == '512':\n                return _sha512.sha512\n            elif bs == '384':\n                return _sha512.sha384\n    except ImportError:\n        pass  # no extension module, this hash is unsupported.\n\n    raise ValueError('unsupported hash type ' + name)\n\n\ndef __get_openssl_constructor(name):\n    try:\n        f = getattr(_hashlib, 'openssl_' + name)\n        # Allow the C module to raise ValueError.  The function will be\n        # defined but the hash not actually available thanks to OpenSSL.\n        f()\n        # Use the C function directly (very fast)\n        return f\n    except (AttributeError, ValueError):\n        return __get_builtin_constructor(name)\n\n\ndef __py_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    return __get_builtin_constructor(name)(string)\n\n\ndef __hash_new(name, string=''):\n    \"\"\"new(name, string='') - Return a new hashing object using the named algorithm;\n    optionally initialized with a string.\n    \"\"\"\n    try:\n        return _hashlib.new(name, string)\n    except ValueError:\n        # If the _hashlib module (OpenSSL) doesn't support the named\n        # hash, try using our builtin implementations.\n        # This allows for SHA224/256 and SHA384/512 support even though\n        # the OpenSSL library prior to 0.9.8 doesn't provide them.\n        return __get_builtin_constructor(name)(string)\n\n\ntry:\n    import _hashlib\n    new = __hash_new\n    __get_hash = __get_openssl_constructor\n    algorithms_available = algorithms_available.union(\n        _hashlib.openssl_md_meth_names)\nexcept ImportError:\n    new = __py_new\n    __get_hash = __get_builtin_constructor\n\nfor __func_name in __always_supported:\n    # try them all, some may not work due to the OpenSSL\n    # version not supporting that algorithm.\n    try:\n        globals()[__func_name] = __get_hash(__func_name)\n    except ValueError:\n        import logging\n        logging.exception('code for hash %s was not found.', __func_name)\n\n\ntry:\n    # OpenSSL's PKCS5_PBKDF2_HMAC requires OpenSSL 1.0+ with HMAC and SHA\n    from _hashlib import pbkdf2_hmac\nexcept ImportError:\n    import binascii\n    import struct\n\n    _trans_5C = b\"\".join(chr(x ^ 0x5C) for x in range(256))\n    _trans_36 = b\"\".join(chr(x ^ 0x36) for x in range(256))\n\n    def pbkdf2_hmac(hash_name, password, salt, iterations, dklen=None):\n        \"\"\"Password based key derivation function 2 (PKCS #5 v2.0)\n\n        This Python implementations based on the hmac module about as fast\n        as OpenSSL's PKCS5_PBKDF2_HMAC for short passwords and much faster\n        for long passwords.\n        \"\"\"\n        if not isinstance(hash_name, str):\n            raise TypeError(hash_name)\n\n        if not isinstance(password, (bytes, bytearray)):\n            password = bytes(buffer(password))\n        if not isinstance(salt, (bytes, bytearray)):\n            salt = bytes(buffer(salt))\n\n        # Fast inline HMAC implementation\n        inner = new(hash_name)\n        outer = new(hash_name)\n        blocksize = getattr(inner, 'block_size', 64)\n        if len(password) > blocksize:\n            password = new(hash_name, password).digest()\n        password = password + b'\\x00' * (blocksize - len(password))\n        inner.update(password.translate(_trans_36))\n        outer.update(password.translate(_trans_5C))\n\n        def prf(msg, inner=inner, outer=outer):\n            # PBKDF2_HMAC uses the password as key. We can re-use the same\n            # digest objects and and just update copies to skip initialization.\n            icpy = inner.copy()\n            ocpy = outer.copy()\n            icpy.update(msg)\n            ocpy.update(icpy.digest())\n            return ocpy.digest()\n\n        if iterations < 1:\n            raise ValueError(iterations)\n        if dklen is None:\n            dklen = outer.digest_size\n        if dklen < 1:\n            raise ValueError(dklen)\n\n        hex_format_string = \"%%0%ix\" % (new(hash_name).digest_size * 2)\n\n        dkey = b''\n        loop = 1\n        while len(dkey) < dklen:\n            prev = prf(salt + struct.pack(b'>I', loop))\n            rkey = int(binascii.hexlify(prev), 16)\n            for i in xrange(iterations - 1):\n                prev = prf(prev)\n                rkey ^= int(binascii.hexlify(prev), 16)\n            loop += 1\n            dkey += binascii.unhexlify(hex_format_string % rkey)\n\n        return dkey[:dklen]\n\n# Cleanup locals()\ndel __always_supported, __func_name, __get_hash\ndel __py_new, __hash_new, __get_openssl_constructor\n", 
    "heapq": "# -*- coding: utf-8 -*-\n\n\"\"\"Heap queue algorithm (a.k.a. priority queue).\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nUsage:\n\nheap = []            # creates an empty heap\nheappush(heap, item) # pushes a new item on the heap\nitem = heappop(heap) # pops the smallest item from the heap\nitem = heap[0]       # smallest item on the heap without popping it\nheapify(x)           # transforms list into a heap, in-place, in linear time\nitem = heapreplace(heap, item) # pops and returns smallest item, and adds\n                               # new item; the heap size is unchanged\n\nOur API differs from textbook heap algorithms as follows:\n\n- We use 0-based indexing.  This makes the relationship between the\n  index for a node and the indexes for its children slightly less\n  obvious, but is more suitable since Python uses 0-based indexing.\n\n- Our heappop() method returns the smallest item, not the largest.\n\nThese two make it possible to view the heap as a regular Python list\nwithout surprises: heap[0] is the smallest item, and heap.sort()\nmaintains the heap invariant!\n\"\"\"\n\n# Original code by Kevin O'Connor, augmented by Tim Peters and Raymond Hettinger\n\n__about__ = \"\"\"Heap queues\n\n[explanation by Fran\u00e7ois Pinard]\n\nHeaps are arrays for which a[k] <= a[2*k+1] and a[k] <= a[2*k+2] for\nall k, counting elements from 0.  For the sake of comparison,\nnon-existing elements are considered to be infinite.  The interesting\nproperty of a heap is that a[0] is always its smallest element.\n\nThe strange invariant above is meant to be an efficient memory\nrepresentation for a tournament.  The numbers below are `k', not a[k]:\n\n                                   0\n\n                  1                                 2\n\n          3               4                5               6\n\n      7       8       9       10      11      12      13      14\n\n    15 16   17 18   19 20   21 22   23 24   25 26   27 28   29 30\n\n\nIn the tree above, each cell `k' is topping `2*k+1' and `2*k+2'.  In\nan usual binary tournament we see in sports, each cell is the winner\nover the two cells it tops, and we can trace the winner down the tree\nto see all opponents s/he had.  However, in many computer applications\nof such tournaments, we do not need to trace the history of a winner.\nTo be more memory efficient, when a winner is promoted, we try to\nreplace it by something else at a lower level, and the rule becomes\nthat a cell and the two cells it tops contain three different items,\nbut the top cell \"wins\" over the two topped cells.\n\nIf this heap invariant is protected at all time, index 0 is clearly\nthe overall winner.  The simplest algorithmic way to remove it and\nfind the \"next\" winner is to move some loser (let's say cell 30 in the\ndiagram above) into the 0 position, and then percolate this new 0 down\nthe tree, exchanging values, until the invariant is re-established.\nThis is clearly logarithmic on the total number of items in the tree.\nBy iterating over all items, you get an O(n ln n) sort.\n\nA nice feature of this sort is that you can efficiently insert new\nitems while the sort is going on, provided that the inserted items are\nnot \"better\" than the last 0'th element you extracted.  This is\nespecially useful in simulation contexts, where the tree holds all\nincoming events, and the \"win\" condition means the smallest scheduled\ntime.  When an event schedule other events for execution, they are\nscheduled into the future, so they can easily go into the heap.  So, a\nheap is a good structure for implementing schedulers (this is what I\nused for my MIDI sequencer :-).\n\nVarious structures for implementing schedulers have been extensively\nstudied, and heaps are good for this, as they are reasonably speedy,\nthe speed is almost constant, and the worst case is not much different\nthan the average case.  However, there are other representations which\nare more efficient overall, yet the worst cases might be terrible.\n\nHeaps are also very useful in big disk sorts.  You most probably all\nknow that a big sort implies producing \"runs\" (which are pre-sorted\nsequences, which size is usually related to the amount of CPU memory),\nfollowed by a merging passes for these runs, which merging is often\nvery cleverly organised[1].  It is very important that the initial\nsort produces the longest runs possible.  Tournaments are a good way\nto that.  If, using all the memory available to hold a tournament, you\nreplace and percolate items that happen to fit the current run, you'll\nproduce runs which are twice the size of the memory for random input,\nand much better for input fuzzily ordered.\n\nMoreover, if you output the 0'th item on disk and get an input which\nmay not fit in the current tournament (because the value \"wins\" over\nthe last output value), it cannot fit in the heap, so the size of the\nheap decreases.  The freed memory could be cleverly reused immediately\nfor progressively building a second heap, which grows at exactly the\nsame rate the first heap is melting.  When the first heap completely\nvanishes, you switch heaps and start a new run.  Clever and quite\neffective!\n\nIn a word, heaps are useful memory structures to know.  I use them in\na few applications, and I think it is good to keep a `heap' module\naround. :-)\n\n--------------------\n[1] The disk balancing algorithms which are current, nowadays, are\nmore annoying than clever, and this is a consequence of the seeking\ncapabilities of the disks.  On devices which cannot seek, like big\ntape drives, the story was quite different, and one had to be very\nclever to ensure (far in advance) that each tape movement will be the\nmost effective possible (that is, will best participate at\n\"progressing\" the merge).  Some tapes were even able to read\nbackwards, and this was also used to avoid the rewinding time.\nBelieve me, real good tape sorts were quite spectacular to watch!\nFrom all times, sorting has always been a Great Art! :-)\n\"\"\"\n\n__all__ = ['heappush', 'heappop', 'heapify', 'heapreplace', 'merge',\n           'nlargest', 'nsmallest', 'heappushpop']\n\nfrom itertools import islice, count, imap, izip, tee, chain\nfrom operator import itemgetter\n\ndef cmp_lt(x, y):\n    # Use __lt__ if available; otherwise, try __le__.\n    # In Py3.x, only __lt__ will be called.\n    return (x < y) if hasattr(x, '__lt__') else (not y <= x)\n\ndef heappush(heap, item):\n    \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\"\n    heap.append(item)\n    _siftdown(heap, 0, len(heap)-1)\n\ndef heappop(heap):\n    \"\"\"Pop the smallest item off the heap, maintaining the heap invariant.\"\"\"\n    lastelt = heap.pop()    # raises appropriate IndexError if heap is empty\n    if heap:\n        returnitem = heap[0]\n        heap[0] = lastelt\n        _siftup(heap, 0)\n    else:\n        returnitem = lastelt\n    return returnitem\n\ndef heapreplace(heap, item):\n    \"\"\"Pop and return the current smallest value, and add the new item.\n\n    This is more efficient than heappop() followed by heappush(), and can be\n    more appropriate when using a fixed-size heap.  Note that the value\n    returned may be larger than item!  That constrains reasonable uses of\n    this routine unless written as part of a conditional replacement:\n\n        if item > heap[0]:\n            item = heapreplace(heap, item)\n    \"\"\"\n    returnitem = heap[0]    # raises appropriate IndexError if heap is empty\n    heap[0] = item\n    _siftup(heap, 0)\n    return returnitem\n\ndef heappushpop(heap, item):\n    \"\"\"Fast version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(heap[0], item):\n        item, heap[0] = heap[0], item\n        _siftup(heap, 0)\n    return item\n\ndef heapify(x):\n    \"\"\"Transform list into a heap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    # Transform bottom-up.  The largest index there's any point to looking at\n    # is the largest with a child index in-range, so must have 2*i + 1 < n,\n    # or i < (n-1)/2.  If n is even = 2*j, this is (2*j-1)/2 = j-1/2 so\n    # j-1 is the largest, which is n//2 - 1.  If n is odd = 2*j+1, this is\n    # (2*j+1-1)/2 = j so j-1 is the largest, and that's again n//2-1.\n    for i in reversed(xrange(n//2)):\n        _siftup(x, i)\n\ndef _heappushpop_max(heap, item):\n    \"\"\"Maxheap version of a heappush followed by a heappop.\"\"\"\n    if heap and cmp_lt(item, heap[0]):\n        item, heap[0] = heap[0], item\n        _siftup_max(heap, 0)\n    return item\n\ndef _heapify_max(x):\n    \"\"\"Transform list into a maxheap, in-place, in O(len(x)) time.\"\"\"\n    n = len(x)\n    for i in reversed(range(n//2)):\n        _siftup_max(x, i)\n\ndef nlargest(n, iterable):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, reverse=True)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    heapify(result)\n    _heappushpop = heappushpop\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort(reverse=True)\n    return result\n\ndef nsmallest(n, iterable):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable)[:n]\n    \"\"\"\n    if n < 0:\n        return []\n    it = iter(iterable)\n    result = list(islice(it, n))\n    if not result:\n        return result\n    _heapify_max(result)\n    _heappushpop = _heappushpop_max\n    for elem in it:\n        _heappushpop(result, elem)\n    result.sort()\n    return result\n\n# 'heap' is a heap at all indices >= startpos, except possibly for pos.  pos\n# is the index of a leaf with a possibly out-of-order value.  Restore the\n# heap invariant.\ndef _siftdown(heap, startpos, pos):\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(newitem, parent):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\n# The child indices of heap index pos are already heaps, and we want to make\n# a heap at index pos too.  We do this by bubbling the smaller child of\n# pos up (and so on with that child's children, etc) until hitting a leaf,\n# then using _siftdown to move the oddball originally at index pos into place.\n#\n# We *could* break out of the loop as soon as we find a pos where newitem <=\n# both its children, but turns out that's not a good idea, and despite that\n# many books write the algorithm that way.  During a heap pop, the last array\n# element is sifted in, and that tends to be large, so that comparing it\n# against values starting from the root usually doesn't pay (= usually doesn't\n# get us out of the loop early).  See Knuth, Volume 3, where this is\n# explained and quantified in an exercise.\n#\n# Cutting the # of comparisons is important, since these routines have no\n# way to extract \"the priority\" from an array element, so that intelligence\n# is likely to be hiding in custom __cmp__ methods, or in array elements\n# storing (priority, record) tuples.  Comparisons are thus potentially\n# expensive.\n#\n# On random arrays of length 1000, making this change cut the number of\n# comparisons made by heapify() a little, and those made by exhaustive\n# heappop() a lot, in accord with theory.  Here are typical results from 3\n# runs (3 just to demonstrate how small the variance is):\n#\n# Compares needed by heapify     Compares needed by 1000 heappops\n# --------------------------     --------------------------------\n# 1837 cut to 1663               14996 cut to 8680\n# 1855 cut to 1659               14966 cut to 8678\n# 1847 cut to 1660               15024 cut to 8703\n#\n# Building the heap by using heappush() 1000 times instead required\n# 2198, 2148, and 2219 compares:  heapify() is more efficient, when\n# you can use it.\n#\n# The total compares needed by list.sort() on the same lists were 8627,\n# 8627, and 8632 (this should be compared to the sum of heapify() and\n# heappop() compares):  list.sort() is (unsurprisingly!) more efficient\n# for sorting.\n\ndef _siftup(heap, pos):\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the smaller child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of smaller child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[childpos], heap[rightpos]):\n            childpos = rightpos\n        # Move the smaller child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown(heap, startpos, pos)\n\ndef _siftdown_max(heap, startpos, pos):\n    'Maxheap variant of _siftdown'\n    newitem = heap[pos]\n    # Follow the path to the root, moving parents down until finding a place\n    # newitem fits.\n    while pos > startpos:\n        parentpos = (pos - 1) >> 1\n        parent = heap[parentpos]\n        if cmp_lt(parent, newitem):\n            heap[pos] = parent\n            pos = parentpos\n            continue\n        break\n    heap[pos] = newitem\n\ndef _siftup_max(heap, pos):\n    'Maxheap variant of _siftup'\n    endpos = len(heap)\n    startpos = pos\n    newitem = heap[pos]\n    # Bubble up the larger child until hitting a leaf.\n    childpos = 2*pos + 1    # leftmost child position\n    while childpos < endpos:\n        # Set childpos to index of larger child.\n        rightpos = childpos + 1\n        if rightpos < endpos and not cmp_lt(heap[rightpos], heap[childpos]):\n            childpos = rightpos\n        # Move the larger child up.\n        heap[pos] = heap[childpos]\n        pos = childpos\n        childpos = 2*pos + 1\n    # The leaf at pos is empty now.  Put newitem there, and bubble it up\n    # to its final resting place (by sifting its parents down).\n    heap[pos] = newitem\n    _siftdown_max(heap, startpos, pos)\n\n# If available, use C implementation\ntry:\n    from _heapq import *\nexcept ImportError:\n    pass\n\ndef merge(*iterables):\n    '''Merge multiple sorted inputs into a single sorted output.\n\n    Similar to sorted(itertools.chain(*iterables)) but returns a generator,\n    does not pull the data into memory all at once, and assumes that each of\n    the input streams is already sorted (smallest to largest).\n\n    >>> list(merge([1,3,5,7], [0,2,4,8], [5,10,15,20], [], [25]))\n    [0, 1, 2, 3, 4, 5, 5, 7, 8, 10, 15, 20, 25]\n\n    '''\n    _heappop, _heapreplace, _StopIteration = heappop, heapreplace, StopIteration\n    _len = len\n\n    h = []\n    h_append = h.append\n    for itnum, it in enumerate(map(iter, iterables)):\n        try:\n            next = it.next\n            h_append([next(), itnum, next])\n        except _StopIteration:\n            pass\n    heapify(h)\n\n    while _len(h) > 1:\n        try:\n            while 1:\n                v, itnum, next = s = h[0]\n                yield v\n                s[0] = next()               # raises StopIteration when exhausted\n                _heapreplace(h, s)          # restore heap condition\n        except _StopIteration:\n            _heappop(h)                     # remove empty iterator\n    if h:\n        # fast case when only a single iterator remains\n        v, itnum, next = h[0]\n        yield v\n        for v in next.__self__:\n            yield v\n\n# Extend the implementations of nsmallest and nlargest to use a key= argument\n_nsmallest = nsmallest\ndef nsmallest(n, iterable, key=None):\n    \"\"\"Find the n smallest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key)[:n]\n    \"\"\"\n    # Short-cut for n==1 is to use min() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [min(chain(head, it))]\n        return [min(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count())                        # decorate\n        result = _nsmallest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(), in2)                 # decorate\n    result = _nsmallest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\n_nlargest = nlargest\ndef nlargest(n, iterable, key=None):\n    \"\"\"Find the n largest elements in a dataset.\n\n    Equivalent to:  sorted(iterable, key=key, reverse=True)[:n]\n    \"\"\"\n\n    # Short-cut for n==1 is to use max() when len(iterable)>0\n    if n == 1:\n        it = iter(iterable)\n        head = list(islice(it, 1))\n        if not head:\n            return []\n        if key is None:\n            return [max(chain(head, it))]\n        return [max(chain(head, it), key=key)]\n\n    # When n>=size, it's faster to use sorted()\n    try:\n        size = len(iterable)\n    except (TypeError, AttributeError):\n        pass\n    else:\n        if n >= size:\n            return sorted(iterable, key=key, reverse=True)[:n]\n\n    # When key is none, use simpler decoration\n    if key is None:\n        it = izip(iterable, count(0,-1))                    # decorate\n        result = _nlargest(n, it)\n        return map(itemgetter(0), result)                   # undecorate\n\n    # General case, slowest method\n    in1, in2 = tee(iterable)\n    it = izip(imap(key, in1), count(0,-1), in2)             # decorate\n    result = _nlargest(n, it)\n    return map(itemgetter(2), result)                       # undecorate\n\nif __name__ == \"__main__\":\n    # Simple sanity test\n    heap = []\n    data = [1, 3, 5, 7, 9, 2, 4, 6, 8, 0]\n    for item in data:\n        heappush(heap, item)\n    sort = []\n    while heap:\n        sort.append(heappop(heap))\n    print sort\n\n    import doctest\n    doctest.testmod()\n", 
    "hmac": "\"\"\"HMAC (Keyed-Hashing for Message Authentication) Python module.\n\nImplements the HMAC algorithm as described by RFC 2104.\n\"\"\"\n\nimport warnings as _warnings\n\nfrom operator import _compare_digest as compare_digest\n\n\ntrans_5C = \"\".join ([chr (x ^ 0x5C) for x in xrange(256)])\ntrans_36 = \"\".join ([chr (x ^ 0x36) for x in xrange(256)])\n\n# The size of the digests returned by HMAC depends on the underlying\n# hashing module used.  Use digest_size from the instance of HMAC instead.\ndigest_size = None\n\n# A unique object passed by HMAC.copy() to the HMAC constructor, in order\n# that the latter return very quickly.  HMAC(\"\") in contrast is quite\n# expensive.\n_secret_backdoor_key = []\n\nclass HMAC:\n    \"\"\"RFC 2104 HMAC class.  Also complies with RFC 4231.\n\n    This supports the API for Cryptographic Hash Functions (PEP 247).\n    \"\"\"\n    blocksize = 64  # 512-bit HMAC; can be changed in subclasses.\n\n    def __init__(self, key, msg = None, digestmod = None):\n        \"\"\"Create a new HMAC object.\n\n        key:       key for the keyed hash object.\n        msg:       Initial input for the hash, if provided.\n        digestmod: A module supporting PEP 247.  *OR*\n                   A hashlib constructor returning a new hash object.\n                   Defaults to hashlib.md5.\n        \"\"\"\n\n        if key is _secret_backdoor_key: # cheap\n            return\n\n        if digestmod is None:\n            import hashlib\n            digestmod = hashlib.md5\n\n        if hasattr(digestmod, '__call__'):\n            self.digest_cons = digestmod\n        else:\n            self.digest_cons = lambda d='': digestmod.new(d)\n\n        self.outer = self.digest_cons()\n        self.inner = self.digest_cons()\n        self.digest_size = self.inner.digest_size\n\n        if hasattr(self.inner, 'block_size'):\n            blocksize = self.inner.block_size\n            if blocksize < 16:\n                # Very low blocksize, most likely a legacy value like\n                # Lib/sha.py and Lib/md5.py have.\n                _warnings.warn('block_size of %d seems too small; using our '\n                               'default of %d.' % (blocksize, self.blocksize),\n                               RuntimeWarning, 2)\n                blocksize = self.blocksize\n        else:\n            _warnings.warn('No block_size attribute on given digest object; '\n                           'Assuming %d.' % (self.blocksize),\n                           RuntimeWarning, 2)\n            blocksize = self.blocksize\n\n        if len(key) > blocksize:\n            key = self.digest_cons(key).digest()\n\n        key = key + chr(0) * (blocksize - len(key))\n        self.outer.update(key.translate(trans_5C))\n        self.inner.update(key.translate(trans_36))\n        if msg is not None:\n            self.update(msg)\n\n##    def clear(self):\n##        raise NotImplementedError, \"clear() method not available in HMAC.\"\n\n    def update(self, msg):\n        \"\"\"Update this hashing object with the string msg.\n        \"\"\"\n        self.inner.update(msg)\n\n    def copy(self):\n        \"\"\"Return a separate copy of this hashing object.\n\n        An update to this copy won't affect the original object.\n        \"\"\"\n        other = self.__class__(_secret_backdoor_key)\n        other.digest_cons = self.digest_cons\n        other.digest_size = self.digest_size\n        other.inner = self.inner.copy()\n        other.outer = self.outer.copy()\n        return other\n\n    def _current(self):\n        \"\"\"Return a hash object for the current state.\n\n        To be used only internally with digest() and hexdigest().\n        \"\"\"\n        h = self.outer.copy()\n        h.update(self.inner.digest())\n        return h\n\n    def digest(self):\n        \"\"\"Return the hash value of this hashing object.\n\n        This returns a string containing 8-bit data.  The object is\n        not altered in any way by this function; you can continue\n        updating the object after calling this function.\n        \"\"\"\n        h = self._current()\n        return h.digest()\n\n    def hexdigest(self):\n        \"\"\"Like digest(), but returns a string of hexadecimal digits instead.\n        \"\"\"\n        h = self._current()\n        return h.hexdigest()\n\ndef new(key, msg = None, digestmod = None):\n    \"\"\"Create a new hashing object and return it.\n\n    key: The starting key for the hash.\n    msg: if available, will immediately be hashed into the object's starting\n    state.\n\n    You can now feed arbitrary strings into the object using its update()\n    method, and can ask for the hash value at any time by calling its digest()\n    method.\n    \"\"\"\n    return HMAC(key, msg, digestmod)\n", 
    "httplib": "r\"\"\"HTTP/1.1 client library\n\n<intro stuff goes here>\n<other stuff, too>\n\nHTTPConnection goes through a number of \"states\", which define when a client\nmay legally make another request or fetch the response for a particular\nrequest. This diagram details these state transitions:\n\n    (null)\n      |\n      | HTTPConnection()\n      v\n    Idle\n      |\n      | putrequest()\n      v\n    Request-started\n      |\n      | ( putheader() )*  endheaders()\n      v\n    Request-sent\n      |\n      | response = getresponse()\n      v\n    Unread-response   [Response-headers-read]\n      |\\____________________\n      |                     |\n      | response.read()     | putrequest()\n      v                     v\n    Idle                  Req-started-unread-response\n                     ______/|\n                   /        |\n   response.read() |        | ( putheader() )*  endheaders()\n                   v        v\n       Request-started    Req-sent-unread-response\n                            |\n                            | response.read()\n                            v\n                          Request-sent\n\nThis diagram presents the following rules:\n  -- a second request may not be started until {response-headers-read}\n  -- a response [object] cannot be retrieved until {request-sent}\n  -- there is no differentiation between an unread response body and a\n     partially read response body\n\nNote: this enforcement is applied by the HTTPConnection class. The\n      HTTPResponse class does not enforce this state machine, which\n      implies sophisticated clients may accelerate the request/response\n      pipeline. Caution should be taken, though: accelerating the states\n      beyond the above pattern may imply knowledge of the server's\n      connection-close behavior for certain requests. For example, it\n      is impossible to tell whether the server will close the connection\n      UNTIL the response headers have been read; this means that further\n      requests cannot be placed into the pipeline until it is known that\n      the server will NOT be closing the connection.\n\nLogical State                  __state            __response\n-------------                  -------            ----------\nIdle                           _CS_IDLE           None\nRequest-started                _CS_REQ_STARTED    None\nRequest-sent                   _CS_REQ_SENT       None\nUnread-response                _CS_IDLE           <response_class>\nReq-started-unread-response    _CS_REQ_STARTED    <response_class>\nReq-sent-unread-response       _CS_REQ_SENT       <response_class>\n\"\"\"\n\nfrom array import array\nimport os\nimport socket\nfrom sys import py3kwarning\nfrom urlparse import urlsplit\nimport warnings\nwith warnings.catch_warnings():\n    if py3kwarning:\n        warnings.filterwarnings(\"ignore\", \".*mimetools has been removed\",\n                                DeprecationWarning)\n    import mimetools\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\n__all__ = [\"HTTP\", \"HTTPResponse\", \"HTTPConnection\",\n           \"HTTPException\", \"NotConnected\", \"UnknownProtocol\",\n           \"UnknownTransferEncoding\", \"UnimplementedFileMode\",\n           \"IncompleteRead\", \"InvalidURL\", \"ImproperConnectionState\",\n           \"CannotSendRequest\", \"CannotSendHeader\", \"ResponseNotReady\",\n           \"BadStatusLine\", \"error\", \"responses\"]\n\nHTTP_PORT = 80\nHTTPS_PORT = 443\n\n_UNKNOWN = 'UNKNOWN'\n\n# connection states\n_CS_IDLE = 'Idle'\n_CS_REQ_STARTED = 'Request-started'\n_CS_REQ_SENT = 'Request-sent'\n\n# status codes\n# informational\nCONTINUE = 100\nSWITCHING_PROTOCOLS = 101\nPROCESSING = 102\n\n# successful\nOK = 200\nCREATED = 201\nACCEPTED = 202\nNON_AUTHORITATIVE_INFORMATION = 203\nNO_CONTENT = 204\nRESET_CONTENT = 205\nPARTIAL_CONTENT = 206\nMULTI_STATUS = 207\nIM_USED = 226\n\n# redirection\nMULTIPLE_CHOICES = 300\nMOVED_PERMANENTLY = 301\nFOUND = 302\nSEE_OTHER = 303\nNOT_MODIFIED = 304\nUSE_PROXY = 305\nTEMPORARY_REDIRECT = 307\n\n# client error\nBAD_REQUEST = 400\nUNAUTHORIZED = 401\nPAYMENT_REQUIRED = 402\nFORBIDDEN = 403\nNOT_FOUND = 404\nMETHOD_NOT_ALLOWED = 405\nNOT_ACCEPTABLE = 406\nPROXY_AUTHENTICATION_REQUIRED = 407\nREQUEST_TIMEOUT = 408\nCONFLICT = 409\nGONE = 410\nLENGTH_REQUIRED = 411\nPRECONDITION_FAILED = 412\nREQUEST_ENTITY_TOO_LARGE = 413\nREQUEST_URI_TOO_LONG = 414\nUNSUPPORTED_MEDIA_TYPE = 415\nREQUESTED_RANGE_NOT_SATISFIABLE = 416\nEXPECTATION_FAILED = 417\nUNPROCESSABLE_ENTITY = 422\nLOCKED = 423\nFAILED_DEPENDENCY = 424\nUPGRADE_REQUIRED = 426\n\n# server error\nINTERNAL_SERVER_ERROR = 500\nNOT_IMPLEMENTED = 501\nBAD_GATEWAY = 502\nSERVICE_UNAVAILABLE = 503\nGATEWAY_TIMEOUT = 504\nHTTP_VERSION_NOT_SUPPORTED = 505\nINSUFFICIENT_STORAGE = 507\nNOT_EXTENDED = 510\n\n# Mapping status codes to official W3C names\nresponses = {\n    100: 'Continue',\n    101: 'Switching Protocols',\n\n    200: 'OK',\n    201: 'Created',\n    202: 'Accepted',\n    203: 'Non-Authoritative Information',\n    204: 'No Content',\n    205: 'Reset Content',\n    206: 'Partial Content',\n\n    300: 'Multiple Choices',\n    301: 'Moved Permanently',\n    302: 'Found',\n    303: 'See Other',\n    304: 'Not Modified',\n    305: 'Use Proxy',\n    306: '(Unused)',\n    307: 'Temporary Redirect',\n\n    400: 'Bad Request',\n    401: 'Unauthorized',\n    402: 'Payment Required',\n    403: 'Forbidden',\n    404: 'Not Found',\n    405: 'Method Not Allowed',\n    406: 'Not Acceptable',\n    407: 'Proxy Authentication Required',\n    408: 'Request Timeout',\n    409: 'Conflict',\n    410: 'Gone',\n    411: 'Length Required',\n    412: 'Precondition Failed',\n    413: 'Request Entity Too Large',\n    414: 'Request-URI Too Long',\n    415: 'Unsupported Media Type',\n    416: 'Requested Range Not Satisfiable',\n    417: 'Expectation Failed',\n\n    500: 'Internal Server Error',\n    501: 'Not Implemented',\n    502: 'Bad Gateway',\n    503: 'Service Unavailable',\n    504: 'Gateway Timeout',\n    505: 'HTTP Version Not Supported',\n}\n\n# maximal amount of data to read at one time in _safe_read\nMAXAMOUNT = 1048576\n\n# maximal line length when calling readline().\n_MAXLINE = 65536\n\n# maximum amount of headers accepted\n_MAXHEADERS = 100\n\n\nclass HTTPMessage(mimetools.Message):\n\n    def addheader(self, key, value):\n        \"\"\"Add header for field key handling repeats.\"\"\"\n        prev = self.dict.get(key)\n        if prev is None:\n            self.dict[key] = value\n        else:\n            combined = \", \".join((prev, value))\n            self.dict[key] = combined\n\n    def addcontinue(self, key, more):\n        \"\"\"Add more field data from a continuation line.\"\"\"\n        prev = self.dict[key]\n        self.dict[key] = prev + \"\\n \" + more\n\n    def readheaders(self):\n        \"\"\"Read header lines.\n\n        Read header lines up to the entirely blank line that terminates them.\n        The (normally blank) line that ends the headers is skipped, but not\n        included in the returned list.  If a non-header line ends the headers,\n        (which is an error), an attempt is made to backspace over it; it is\n        never included in the returned list.\n\n        The variable self.status is set to the empty string if all went well,\n        otherwise it is an error message.  The variable self.headers is a\n        completely uninterpreted list of lines contained in the header (so\n        printing them will reproduce the header exactly as it appears in the\n        file).\n\n        If multiple header fields with the same name occur, they are combined\n        according to the rules in RFC 2616 sec 4.2:\n\n        Appending each subsequent field-value to the first, each separated\n        by a comma. The order in which header fields with the same field-name\n        are received is significant to the interpretation of the combined\n        field value.\n        \"\"\"\n        # XXX The implementation overrides the readheaders() method of\n        # rfc822.Message.  The base class design isn't amenable to\n        # customized behavior here so the method here is a copy of the\n        # base class code with a few small changes.\n\n        self.dict = {}\n        self.unixfrom = ''\n        self.headers = hlist = []\n        self.status = ''\n        headerseen = \"\"\n        firstline = 1\n        startofline = unread = tell = None\n        if hasattr(self.fp, 'unread'):\n            unread = self.fp.unread\n        elif self.seekable:\n            tell = self.fp.tell\n        while True:\n            if len(hlist) > _MAXHEADERS:\n                raise HTTPException(\"got more than %d headers\" % _MAXHEADERS)\n            if tell:\n                try:\n                    startofline = tell()\n                except IOError:\n                    startofline = tell = None\n                    self.seekable = 0\n            line = self.fp.readline(_MAXLINE + 1)\n            if len(line) > _MAXLINE:\n                raise LineTooLong(\"header line\")\n            if not line:\n                self.status = 'EOF in headers'\n                break\n            # Skip unix From name time lines\n            if firstline and line.startswith('From '):\n                self.unixfrom = self.unixfrom + line\n                continue\n            firstline = 0\n            if headerseen and line[0] in ' \\t':\n                # XXX Not sure if continuation lines are handled properly\n                # for http and/or for repeating headers\n                # It's a continuation line.\n                hlist.append(line)\n                self.addcontinue(headerseen, line.strip())\n                continue\n            elif self.iscomment(line):\n                # It's a comment.  Ignore it.\n                continue\n            elif self.islast(line):\n                # Note! No pushback here!  The delimiter line gets eaten.\n                break\n            headerseen = self.isheader(line)\n            if headerseen:\n                # It's a legal header line, save it.\n                hlist.append(line)\n                self.addheader(headerseen, line[len(headerseen)+1:].strip())\n                continue\n            else:\n                # It's not a header line; throw it back and stop here.\n                if not self.dict:\n                    self.status = 'No headers'\n                else:\n                    self.status = 'Non-header line where header expected'\n                # Try to undo the read.\n                if unread:\n                    unread(line)\n                elif tell:\n                    self.fp.seek(startofline)\n                else:\n                    self.status = self.status + '; bad seek'\n                break\n\nclass HTTPResponse:\n\n    # strict: If true, raise BadStatusLine if the status line can't be\n    # parsed as a valid HTTP/1.0 or 1.1 status line.  By default it is\n    # false because it prevents clients from talking to HTTP/0.9\n    # servers.  Note that a response with a sufficiently corrupted\n    # status line will look like an HTTP/0.9 response.\n\n    # See RFC 2616 sec 19.6 and RFC 1945 sec 6 for details.\n\n    def __init__(self, sock, debuglevel=0, strict=0, method=None, buffering=False):\n        if buffering:\n            # The caller won't be using any sock.recv() calls, so buffering\n            # is fine and recommended for performance.\n            self.fp = sock.makefile('rb')\n        else:\n            # The buffer size is specified as zero, because the headers of\n            # the response are read with readline().  If the reads were\n            # buffered the readline() calls could consume some of the\n            # response, which make be read via a recv() on the underlying\n            # socket.\n            self.fp = sock.makefile('rb', 0)\n        self.debuglevel = debuglevel\n        self.strict = strict\n        self._method = method\n\n        self.msg = None\n\n        # from the Status-Line of the response\n        self.version = _UNKNOWN # HTTP-Version\n        self.status = _UNKNOWN  # Status-Code\n        self.reason = _UNKNOWN  # Reason-Phrase\n\n        self.chunked = _UNKNOWN         # is \"chunked\" being used?\n        self.chunk_left = _UNKNOWN      # bytes left to read in current chunk\n        self.length = _UNKNOWN          # number of bytes left in response\n        self.will_close = _UNKNOWN      # conn will close at end of response\n\n    def _read_status(self):\n        # Initialize with Simple-Response defaults\n        line = self.fp.readline(_MAXLINE + 1)\n        if len(line) > _MAXLINE:\n            raise LineTooLong(\"header line\")\n        if self.debuglevel > 0:\n            print \"reply:\", repr(line)\n        if not line:\n            # Presumably, the server closed the connection before\n            # sending a valid response.\n            raise BadStatusLine(line)\n        try:\n            [version, status, reason] = line.split(None, 2)\n        except ValueError:\n            try:\n                [version, status] = line.split(None, 1)\n                reason = \"\"\n            except ValueError:\n                # empty version will cause next test to fail and status\n                # will be treated as 0.9 response.\n                version = \"\"\n        if not version.startswith('HTTP/'):\n            if self.strict:\n                self.close()\n                raise BadStatusLine(line)\n            else:\n                # assume it's a Simple-Response from an 0.9 server\n                self.fp = LineAndFileWrapper(line, self.fp)\n                return \"HTTP/0.9\", 200, \"\"\n\n        # The status code is a three-digit number\n        try:\n            status = int(status)\n            if status < 100 or status > 999:\n                raise BadStatusLine(line)\n        except ValueError:\n            raise BadStatusLine(line)\n        return version, status, reason\n\n    def begin(self):\n        if self.msg is not None:\n            # we've already started reading the response\n            return\n\n        # read until we get a non-100 response\n        while True:\n            version, status, reason = self._read_status()\n            if status != CONTINUE:\n                break\n            # skip the header from the 100 response\n            while True:\n                skip = self.fp.readline(_MAXLINE + 1)\n                if len(skip) > _MAXLINE:\n                    raise LineTooLong(\"header line\")\n                skip = skip.strip()\n                if not skip:\n                    break\n                if self.debuglevel > 0:\n                    print \"header:\", skip\n\n        self.status = status\n        self.reason = reason.strip()\n        if version == 'HTTP/1.0':\n            self.version = 10\n        elif version.startswith('HTTP/1.'):\n            self.version = 11   # use HTTP/1.1 code for HTTP/1.x where x>=1\n        elif version == 'HTTP/0.9':\n            self.version = 9\n        else:\n            raise UnknownProtocol(version)\n\n        if self.version == 9:\n            self.length = None\n            self.chunked = 0\n            self.will_close = 1\n            self.msg = HTTPMessage(StringIO())\n            return\n\n        self.msg = HTTPMessage(self.fp, 0)\n        if self.debuglevel > 0:\n            for hdr in self.msg.headers:\n                print \"header:\", hdr,\n\n        # don't let the msg keep an fp\n        self.msg.fp = None\n\n        # are we using the chunked-style of transfer encoding?\n        tr_enc = self.msg.getheader('transfer-encoding')\n        if tr_enc and tr_enc.lower() == \"chunked\":\n            self.chunked = 1\n            self.chunk_left = None\n        else:\n            self.chunked = 0\n\n        # will the connection close at the end of the response?\n        self.will_close = self._check_close()\n\n        # do we have a Content-Length?\n        # NOTE: RFC 2616, S4.4, #3 says we ignore this if tr_enc is \"chunked\"\n        length = self.msg.getheader('content-length')\n        if length and not self.chunked:\n            try:\n                self.length = int(length)\n            except ValueError:\n                self.length = None\n            else:\n                if self.length < 0:  # ignore nonsensical negative lengths\n                    self.length = None\n        else:\n            self.length = None\n\n        # does the body have a fixed length? (of zero)\n        if (status == NO_CONTENT or status == NOT_MODIFIED or\n            100 <= status < 200 or      # 1xx codes\n            self._method == 'HEAD'):\n            self.length = 0\n\n        # if the connection remains open, and we aren't using chunked, and\n        # a content-length was not provided, then assume that the connection\n        # WILL close.\n        if not self.will_close and \\\n           not self.chunked and \\\n           self.length is None:\n            self.will_close = 1\n\n    def _check_close(self):\n        conn = self.msg.getheader('connection')\n        if self.version == 11:\n            # An HTTP/1.1 proxy is assumed to stay open unless\n            # explicitly closed.\n            conn = self.msg.getheader('connection')\n            if conn and \"close\" in conn.lower():\n                return True\n            return False\n\n        # Some HTTP/1.0 implementations have support for persistent\n        # connections, using rules different than HTTP/1.1.\n\n        # For older HTTP, Keep-Alive indicates persistent connection.\n        if self.msg.getheader('keep-alive'):\n            return False\n\n        # At least Akamai returns a \"Connection: Keep-Alive\" header,\n        # which was supposed to be sent by the client.\n        if conn and \"keep-alive\" in conn.lower():\n            return False\n\n        # Proxy-Connection is a netscape hack.\n        pconn = self.msg.getheader('proxy-connection')\n        if pconn and \"keep-alive\" in pconn.lower():\n            return False\n\n        # otherwise, assume it will close\n        return True\n\n    def close(self):\n        if self.fp:\n            self.fp.close()\n            self.fp = None\n\n    def isclosed(self):\n        # NOTE: it is possible that we will not ever call self.close(). This\n        #       case occurs when will_close is TRUE, length is None, and we\n        #       read up to the last byte, but NOT past it.\n        #\n        # IMPLIES: if will_close is FALSE, then self.close() will ALWAYS be\n        #          called, meaning self.isclosed() is meaningful.\n        return self.fp is None\n\n    # XXX It would be nice to have readline and __iter__ for this, too.\n\n    def read(self, amt=None):\n        if self.fp is None:\n            return ''\n\n        if self._method == 'HEAD':\n            self.close()\n            return ''\n\n        if self.chunked:\n            return self._read_chunked(amt)\n\n        if amt is None:\n            # unbounded read\n            if self.length is None:\n                s = self.fp.read()\n            else:\n                try:\n                    s = self._safe_read(self.length)\n                except IncompleteRead:\n                    self.close()\n                    raise\n                self.length = 0\n            self.close()        # we read everything\n            return s\n\n        if self.length is not None:\n            if amt > self.length:\n                # clip the read to the \"end of response\"\n                amt = self.length\n\n        # we do not use _safe_read() here because this may be a .will_close\n        # connection, and the user is reading more bytes than will be provided\n        # (for example, reading in 1k chunks)\n        s = self.fp.read(amt)\n        if not s and amt:\n            # Ideally, we would raise IncompleteRead if the content-length\n            # wasn't satisfied, but it might break compatibility.\n            self.close()\n        if self.length is not None:\n            self.length -= len(s)\n            if not self.length:\n                self.close()\n\n        return s\n\n    def _read_chunked(self, amt):\n        assert self.chunked != _UNKNOWN\n        chunk_left = self.chunk_left\n        value = []\n        while True:\n            if chunk_left is None:\n                line = self.fp.readline(_MAXLINE + 1)\n                if len(line) > _MAXLINE:\n                    raise LineTooLong(\"chunk size\")\n                i = line.find(';')\n                if i >= 0:\n                    line = line[:i] # strip chunk-extensions\n                try:\n                    chunk_left = int(line, 16)\n                except ValueError:\n                    # close the connection as protocol synchronisation is\n                    # probably lost\n                    self.close()\n                    raise IncompleteRead(''.join(value))\n                if chunk_left == 0:\n                    break\n            if amt is None:\n                value.append(self._safe_read(chunk_left))\n            elif amt < chunk_left:\n                value.append(self._safe_read(amt))\n                self.chunk_left = chunk_left - amt\n                return ''.join(value)\n            elif amt == chunk_left:\n                value.append(self._safe_read(amt))\n                self._safe_read(2)  # toss the CRLF at the end of the chunk\n                self.chunk_left = None\n                return ''.join(value)\n            else:\n                value.append(self._safe_read(chunk_left))\n                amt -= chunk_left\n\n            # we read the whole chunk, get another\n            self._safe_read(2)      # toss the CRLF at the end of the chunk\n            chunk_left = None\n\n        # read and discard trailer up to the CRLF terminator\n        ### note: we shouldn't have any trailers!\n        while True:\n            line = self.fp.readline(_MAXLINE + 1)\n            if len(line) > _MAXLINE:\n                raise LineTooLong(\"trailer line\")\n            if not line:\n                # a vanishingly small number of sites EOF without\n                # sending the trailer\n                break\n            if line == '\\r\\n':\n                break\n\n        # we read everything; close the \"file\"\n        self.close()\n\n        return ''.join(value)\n\n    def _safe_read(self, amt):\n        \"\"\"Read the number of bytes requested, compensating for partial reads.\n\n        Normally, we have a blocking socket, but a read() can be interrupted\n        by a signal (resulting in a partial read).\n\n        Note that we cannot distinguish between EOF and an interrupt when zero\n        bytes have been read. IncompleteRead() will be raised in this\n        situation.\n\n        This function should be used when <amt> bytes \"should\" be present for\n        reading. If the bytes are truly not available (due to EOF), then the\n        IncompleteRead exception can be used to detect the problem.\n        \"\"\"\n        # NOTE(gps): As of svn r74426 socket._fileobject.read(x) will never\n        # return less than x bytes unless EOF is encountered.  It now handles\n        # signal interruptions (socket.error EINTR) internally.  This code\n        # never caught that exception anyways.  It seems largely pointless.\n        # self.fp.read(amt) will work fine.\n        s = []\n        while amt > 0:\n            chunk = self.fp.read(min(amt, MAXAMOUNT))\n            if not chunk:\n                raise IncompleteRead(''.join(s), amt)\n            s.append(chunk)\n            amt -= len(chunk)\n        return ''.join(s)\n\n    def fileno(self):\n        return self.fp.fileno()\n\n    def getheader(self, name, default=None):\n        if self.msg is None:\n            raise ResponseNotReady()\n        return self.msg.getheader(name, default)\n\n    def getheaders(self):\n        \"\"\"Return list of (header, value) tuples.\"\"\"\n        if self.msg is None:\n            raise ResponseNotReady()\n        return self.msg.items()\n\n\nclass HTTPConnection:\n\n    _http_vsn = 11\n    _http_vsn_str = 'HTTP/1.1'\n\n    response_class = HTTPResponse\n    default_port = HTTP_PORT\n    auto_open = 1\n    debuglevel = 0\n    strict = 0\n\n    def __init__(self, host, port=None, strict=None,\n                 timeout=socket._GLOBAL_DEFAULT_TIMEOUT, source_address=None):\n        self.timeout = timeout\n        self.source_address = source_address\n        self.sock = None\n        self._buffer = []\n        self.__response = None\n        self.__state = _CS_IDLE\n        self._method = None\n        self._tunnel_host = None\n        self._tunnel_port = None\n        self._tunnel_headers = {}\n        if strict is not None:\n            self.strict = strict\n\n        (self.host, self.port) = self._get_hostport(host, port)\n\n        # This is stored as an instance variable to allow unittests\n        # to replace with a suitable mock\n        self._create_connection = socket.create_connection\n\n    def set_tunnel(self, host, port=None, headers=None):\n        \"\"\" Set up host and port for HTTP CONNECT tunnelling.\n\n        In a connection that uses HTTP Connect tunneling, the host passed to the\n        constructor is used as proxy server that relays all communication to the\n        endpoint passed to set_tunnel. This is done by sending a HTTP CONNECT\n        request to the proxy server when the connection is established.\n\n        This method must be called before the HTML connection has been\n        established.\n\n        The headers argument should be a mapping of extra HTTP headers\n        to send with the CONNECT request.\n        \"\"\"\n        # Verify if this is required.\n        if self.sock:\n            raise RuntimeError(\"Can't setup tunnel for established connection.\")\n\n        self._tunnel_host = host\n        self._tunnel_port = port\n        if headers:\n            self._tunnel_headers = headers\n        else:\n            self._tunnel_headers.clear()\n\n    def _get_hostport(self, host, port):\n        if port is None:\n            i = host.rfind(':')\n            j = host.rfind(']')         # ipv6 addresses have [...]\n            if i > j:\n                try:\n                    port = int(host[i+1:])\n                except ValueError:\n                    if host[i+1:] == \"\":  # http://foo.com:/ == http://foo.com/\n                        port = self.default_port\n                    else:\n                        raise InvalidURL(\"nonnumeric port: '%s'\" % host[i+1:])\n                host = host[:i]\n            else:\n                port = self.default_port\n            if host and host[0] == '[' and host[-1] == ']':\n                host = host[1:-1]\n        return (host, port)\n\n    def set_debuglevel(self, level):\n        self.debuglevel = level\n\n    def _tunnel(self):\n        (host, port) = self._get_hostport(self._tunnel_host, self._tunnel_port)\n        self.send(\"CONNECT %s:%d HTTP/1.0\\r\\n\" % (host, port))\n        for header, value in self._tunnel_headers.iteritems():\n            self.send(\"%s: %s\\r\\n\" % (header, value))\n        self.send(\"\\r\\n\")\n        response = self.response_class(self.sock, strict = self.strict,\n                                       method = self._method)\n        (version, code, message) = response._read_status()\n\n        if code != 200:\n            self.close()\n            raise socket.error(\"Tunnel connection failed: %d %s\" % (code,\n                                                                    message.strip()))\n        while True:\n            line = response.fp.readline(_MAXLINE + 1)\n            if len(line) > _MAXLINE:\n                raise LineTooLong(\"header line\")\n            if not line:\n                # for sites which EOF without sending trailer\n                break\n            if line == '\\r\\n':\n                break\n\n\n    def connect(self):\n        \"\"\"Connect to the host and port specified in __init__.\"\"\"\n        self.sock = self._create_connection((self.host,self.port),\n                                           self.timeout, self.source_address)\n\n        if self._tunnel_host:\n            self._tunnel()\n\n    def close(self):\n        \"\"\"Close the connection to the HTTP server.\"\"\"\n        if self.sock:\n            self.sock.close()   # close it manually... there may be other refs\n            self.sock = None\n        if self.__response:\n            self.__response.close()\n            self.__response = None\n        self.__state = _CS_IDLE\n\n    def send(self, data):\n        \"\"\"Send `data' to the server.\"\"\"\n        if self.sock is None:\n            if self.auto_open:\n                self.connect()\n            else:\n                raise NotConnected()\n\n        if self.debuglevel > 0:\n            print \"send:\", repr(data)\n        blocksize = 8192\n        if hasattr(data,'read') and not isinstance(data, array):\n            if self.debuglevel > 0: print \"sendIng a read()able\"\n            datablock = data.read(blocksize)\n            while datablock:\n                self.sock.sendall(datablock)\n                datablock = data.read(blocksize)\n        else:\n            self.sock.sendall(data)\n\n    def _output(self, s):\n        \"\"\"Add a line of output to the current request buffer.\n\n        Assumes that the line does *not* end with \\\\r\\\\n.\n        \"\"\"\n        self._buffer.append(s)\n\n    def _send_output(self, message_body=None):\n        \"\"\"Send the currently buffered request and clear the buffer.\n\n        Appends an extra \\\\r\\\\n to the buffer.\n        A message_body may be specified, to be appended to the request.\n        \"\"\"\n        self._buffer.extend((\"\", \"\"))\n        msg = \"\\r\\n\".join(self._buffer)\n        del self._buffer[:]\n        # If msg and message_body are sent in a single send() call,\n        # it will avoid performance problems caused by the interaction\n        # between delayed ack and the Nagle algorithm.\n        if isinstance(message_body, str):\n            msg += message_body\n            message_body = None\n        self.send(msg)\n        if message_body is not None:\n            #message_body was not a string (i.e. it is a file) and\n            #we must run the risk of Nagle\n            self.send(message_body)\n\n    def putrequest(self, method, url, skip_host=0, skip_accept_encoding=0):\n        \"\"\"Send a request to the server.\n\n        `method' specifies an HTTP request method, e.g. 'GET'.\n        `url' specifies the object being requested, e.g. '/index.html'.\n        `skip_host' if True does not add automatically a 'Host:' header\n        `skip_accept_encoding' if True does not add automatically an\n           'Accept-Encoding:' header\n        \"\"\"\n\n        # if a prior response has been completed, then forget about it.\n        if self.__response and self.__response.isclosed():\n            self.__response = None\n\n\n        # in certain cases, we cannot issue another request on this connection.\n        # this occurs when:\n        #   1) we are in the process of sending a request.   (_CS_REQ_STARTED)\n        #   2) a response to a previous request has signalled that it is going\n        #      to close the connection upon completion.\n        #   3) the headers for the previous response have not been read, thus\n        #      we cannot determine whether point (2) is true.   (_CS_REQ_SENT)\n        #\n        # if there is no prior response, then we can request at will.\n        #\n        # if point (2) is true, then we will have passed the socket to the\n        # response (effectively meaning, \"there is no prior response\"), and\n        # will open a new one when a new request is made.\n        #\n        # Note: if a prior response exists, then we *can* start a new request.\n        #       We are not allowed to begin fetching the response to this new\n        #       request, however, until that prior response is complete.\n        #\n        if self.__state == _CS_IDLE:\n            self.__state = _CS_REQ_STARTED\n        else:\n            raise CannotSendRequest()\n\n        # Save the method we use, we need it later in the response phase\n        self._method = method\n        if not url:\n            url = '/'\n        hdr = '%s %s %s' % (method, url, self._http_vsn_str)\n\n        self._output(hdr)\n\n        if self._http_vsn == 11:\n            # Issue some standard headers for better HTTP/1.1 compliance\n\n            if not skip_host:\n                # this header is issued *only* for HTTP/1.1\n                # connections. more specifically, this means it is\n                # only issued when the client uses the new\n                # HTTPConnection() class. backwards-compat clients\n                # will be using HTTP/1.0 and those clients may be\n                # issuing this header themselves. we should NOT issue\n                # it twice; some web servers (such as Apache) barf\n                # when they see two Host: headers\n\n                # If we need a non-standard port,include it in the\n                # header.  If the request is going through a proxy,\n                # but the host of the actual URL, not the host of the\n                # proxy.\n\n                netloc = ''\n                if url.startswith('http'):\n                    nil, netloc, nil, nil, nil = urlsplit(url)\n\n                if netloc:\n                    try:\n                        netloc_enc = netloc.encode(\"ascii\")\n                    except UnicodeEncodeError:\n                        netloc_enc = netloc.encode(\"idna\")\n                    self.putheader('Host', netloc_enc)\n                else:\n                    if self._tunnel_host:\n                        host = self._tunnel_host\n                        port = self._tunnel_port\n                    else:\n                        host = self.host\n                        port = self.port\n\n                    try:\n                        host_enc = host.encode(\"ascii\")\n                    except UnicodeEncodeError:\n                        host_enc = host.encode(\"idna\")\n                    # Wrap the IPv6 Host Header with [] (RFC 2732)\n                    if host_enc.find(':') >= 0:\n                        host_enc = \"[\" + host_enc + \"]\"\n                    if port == self.default_port:\n                        self.putheader('Host', host_enc)\n                    else:\n                        self.putheader('Host', \"%s:%s\" % (host_enc, port))\n\n            # note: we are assuming that clients will not attempt to set these\n            #       headers since *this* library must deal with the\n            #       consequences. this also means that when the supporting\n            #       libraries are updated to recognize other forms, then this\n            #       code should be changed (removed or updated).\n\n            # we only want a Content-Encoding of \"identity\" since we don't\n            # support encodings such as x-gzip or x-deflate.\n            if not skip_accept_encoding:\n                self.putheader('Accept-Encoding', 'identity')\n\n            # we can accept \"chunked\" Transfer-Encodings, but no others\n            # NOTE: no TE header implies *only* \"chunked\"\n            #self.putheader('TE', 'chunked')\n\n            # if TE is supplied in the header, then it must appear in a\n            # Connection header.\n            #self.putheader('Connection', 'TE')\n\n        else:\n            # For HTTP/1.0, the server will assume \"not chunked\"\n            pass\n\n    def putheader(self, header, *values):\n        \"\"\"Send a request header line to the server.\n\n        For example: h.putheader('Accept', 'text/html')\n        \"\"\"\n        if self.__state != _CS_REQ_STARTED:\n            raise CannotSendHeader()\n\n        hdr = '%s: %s' % (header, '\\r\\n\\t'.join([str(v) for v in values]))\n        self._output(hdr)\n\n    def endheaders(self, message_body=None):\n        \"\"\"Indicate that the last header line has been sent to the server.\n\n        This method sends the request to the server.  The optional\n        message_body argument can be used to pass a message body\n        associated with the request.  The message body will be sent in\n        the same packet as the message headers if it is string, otherwise it is\n        sent as a separate packet.\n        \"\"\"\n        if self.__state == _CS_REQ_STARTED:\n            self.__state = _CS_REQ_SENT\n        else:\n            raise CannotSendHeader()\n        self._send_output(message_body)\n\n    def request(self, method, url, body=None, headers={}):\n        \"\"\"Send a complete request to the server.\"\"\"\n        self._send_request(method, url, body, headers)\n\n    def _set_content_length(self, body):\n        # Set the content-length based on the body.\n        thelen = None\n        try:\n            thelen = str(len(body))\n        except TypeError, te:\n            # If this is a file-like object, try to\n            # fstat its file descriptor\n            try:\n                thelen = str(os.fstat(body.fileno()).st_size)\n            except (AttributeError, OSError):\n                # Don't send a length if this failed\n                if self.debuglevel > 0: print \"Cannot stat!!\"\n\n        if thelen is not None:\n            self.putheader('Content-Length', thelen)\n\n    def _send_request(self, method, url, body, headers):\n        # Honor explicitly requested Host: and Accept-Encoding: headers.\n        header_names = dict.fromkeys([k.lower() for k in headers])\n        skips = {}\n        if 'host' in header_names:\n            skips['skip_host'] = 1\n        if 'accept-encoding' in header_names:\n            skips['skip_accept_encoding'] = 1\n\n        self.putrequest(method, url, **skips)\n\n        if body is not None and 'content-length' not in header_names:\n            self._set_content_length(body)\n        for hdr, value in headers.iteritems():\n            self.putheader(hdr, value)\n        self.endheaders(body)\n\n    def getresponse(self, buffering=False):\n        \"Get the response from the server.\"\n\n        # if a prior response has been completed, then forget about it.\n        if self.__response and self.__response.isclosed():\n            self.__response = None\n\n        #\n        # if a prior response exists, then it must be completed (otherwise, we\n        # cannot read this response's header to determine the connection-close\n        # behavior)\n        #\n        # note: if a prior response existed, but was connection-close, then the\n        # socket and response were made independent of this HTTPConnection\n        # object since a new request requires that we open a whole new\n        # connection\n        #\n        # this means the prior response had one of two states:\n        #   1) will_close: this connection was reset and the prior socket and\n        #                  response operate independently\n        #   2) persistent: the response was retained and we await its\n        #                  isclosed() status to become true.\n        #\n        if self.__state != _CS_REQ_SENT or self.__response:\n            raise ResponseNotReady()\n\n        args = (self.sock,)\n        kwds = {\"strict\":self.strict, \"method\":self._method}\n        if self.debuglevel > 0:\n            args += (self.debuglevel,)\n        if buffering:\n            #only add this keyword if non-default, for compatibility with\n            #other response_classes.\n            kwds[\"buffering\"] = True;\n        response = self.response_class(*args, **kwds)\n\n        try:\n            response.begin()\n        except:\n            response.close()\n            raise\n        assert response.will_close != _UNKNOWN\n        self.__state = _CS_IDLE\n\n        if response.will_close:\n            # this effectively passes the connection to the response\n            self.close()\n        else:\n            # remember this, so we can tell when it is complete\n            self.__response = response\n\n        return response\n\n\nclass HTTP:\n    \"Compatibility class with httplib.py from 1.5.\"\n\n    _http_vsn = 10\n    _http_vsn_str = 'HTTP/1.0'\n\n    debuglevel = 0\n\n    _connection_class = HTTPConnection\n\n    def __init__(self, host='', port=None, strict=None):\n        \"Provide a default host, since the superclass requires one.\"\n\n        # some joker passed 0 explicitly, meaning default port\n        if port == 0:\n            port = None\n\n        # Note that we may pass an empty string as the host; this will raise\n        # an error when we attempt to connect. Presumably, the client code\n        # will call connect before then, with a proper host.\n        self._setup(self._connection_class(host, port, strict))\n\n    def _setup(self, conn):\n        self._conn = conn\n\n        # set up delegation to flesh out interface\n        self.send = conn.send\n        self.putrequest = conn.putrequest\n        self.putheader = conn.putheader\n        self.endheaders = conn.endheaders\n        self.set_debuglevel = conn.set_debuglevel\n\n        conn._http_vsn = self._http_vsn\n        conn._http_vsn_str = self._http_vsn_str\n\n        self.file = None\n\n    def connect(self, host=None, port=None):\n        \"Accept arguments to set the host/port, since the superclass doesn't.\"\n\n        if host is not None:\n            self._conn._set_hostport(host, port)\n        self._conn.connect()\n\n    def getfile(self):\n        \"Provide a getfile, since the superclass' does not use this concept.\"\n        return self.file\n\n    def getreply(self, buffering=False):\n        \"\"\"Compat definition since superclass does not define it.\n\n        Returns a tuple consisting of:\n        - server status code (e.g. '200' if all goes well)\n        - server \"reason\" corresponding to status code\n        - any RFC822 headers in the response from the server\n        \"\"\"\n        try:\n            if not buffering:\n                response = self._conn.getresponse()\n            else:\n                #only add this keyword if non-default for compatibility\n                #with other connection classes\n                response = self._conn.getresponse(buffering)\n        except BadStatusLine, e:\n            ### hmm. if getresponse() ever closes the socket on a bad request,\n            ### then we are going to have problems with self.sock\n\n            ### should we keep this behavior? do people use it?\n            # keep the socket open (as a file), and return it\n            self.file = self._conn.sock.makefile('rb', 0)\n\n            # close our socket -- we want to restart after any protocol error\n            self.close()\n\n            self.headers = None\n            return -1, e.line, None\n\n        self.headers = response.msg\n        self.file = response.fp\n        return response.status, response.reason, response.msg\n\n    def close(self):\n        self._conn.close()\n\n        # note that self.file == response.fp, which gets closed by the\n        # superclass. just clear the object ref here.\n        ### hmm. messy. if status==-1, then self.file is owned by us.\n        ### well... we aren't explicitly closing, but losing this ref will\n        ### do it\n        self.file = None\n\ntry:\n    import ssl\nexcept ImportError:\n    pass\nelse:\n    class HTTPSConnection(HTTPConnection):\n        \"This class allows communication via SSL.\"\n\n        default_port = HTTPS_PORT\n\n        def __init__(self, host, port=None, key_file=None, cert_file=None,\n                     strict=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n                     source_address=None, context=None):\n            HTTPConnection.__init__(self, host, port, strict, timeout,\n                                    source_address)\n            self.key_file = key_file\n            self.cert_file = cert_file\n            if context is None:\n                context = ssl._create_default_https_context()\n            if key_file or cert_file:\n                context.load_cert_chain(cert_file, key_file)\n            self._context = context\n\n        def connect(self):\n            \"Connect to a host on a given (SSL) port.\"\n\n            HTTPConnection.connect(self)\n\n            if self._tunnel_host:\n                server_hostname = self._tunnel_host\n            else:\n                server_hostname = self.host\n\n            self.sock = self._context.wrap_socket(self.sock,\n                                                  server_hostname=server_hostname)\n\n    __all__.append(\"HTTPSConnection\")\n\n    class HTTPS(HTTP):\n        \"\"\"Compatibility with 1.5 httplib interface\n\n        Python 1.5.2 did not have an HTTPS class, but it defined an\n        interface for sending http requests that is also useful for\n        https.\n        \"\"\"\n\n        _connection_class = HTTPSConnection\n\n        def __init__(self, host='', port=None, key_file=None, cert_file=None,\n                     strict=None, context=None):\n            # provide a default host, pass the X509 cert info\n\n            # urf. compensate for bad input.\n            if port == 0:\n                port = None\n            self._setup(self._connection_class(host, port, key_file,\n                                               cert_file, strict,\n                                               context=context))\n\n            # we never actually use these for anything, but we keep them\n            # here for compatibility with post-1.5.2 CVS.\n            self.key_file = key_file\n            self.cert_file = cert_file\n\n\n    def FakeSocket (sock, sslobj):\n        warnings.warn(\"FakeSocket is deprecated, and won't be in 3.x.  \" +\n                      \"Use the result of ssl.wrap_socket() directly instead.\",\n                      DeprecationWarning, stacklevel=2)\n        return sslobj\n\n\nclass HTTPException(Exception):\n    # Subclasses that define an __init__ must call Exception.__init__\n    # or define self.args.  Otherwise, str() will fail.\n    pass\n\nclass NotConnected(HTTPException):\n    pass\n\nclass InvalidURL(HTTPException):\n    pass\n\nclass UnknownProtocol(HTTPException):\n    def __init__(self, version):\n        self.args = version,\n        self.version = version\n\nclass UnknownTransferEncoding(HTTPException):\n    pass\n\nclass UnimplementedFileMode(HTTPException):\n    pass\n\nclass IncompleteRead(HTTPException):\n    def __init__(self, partial, expected=None):\n        self.args = partial,\n        self.partial = partial\n        self.expected = expected\n    def __repr__(self):\n        if self.expected is not None:\n            e = ', %i more expected' % self.expected\n        else:\n            e = ''\n        return 'IncompleteRead(%i bytes read%s)' % (len(self.partial), e)\n    def __str__(self):\n        return repr(self)\n\nclass ImproperConnectionState(HTTPException):\n    pass\n\nclass CannotSendRequest(ImproperConnectionState):\n    pass\n\nclass CannotSendHeader(ImproperConnectionState):\n    pass\n\nclass ResponseNotReady(ImproperConnectionState):\n    pass\n\nclass BadStatusLine(HTTPException):\n    def __init__(self, line):\n        if not line:\n            line = repr(line)\n        self.args = line,\n        self.line = line\n\nclass LineTooLong(HTTPException):\n    def __init__(self, line_type):\n        HTTPException.__init__(self, \"got more than %d bytes when reading %s\"\n                                     % (_MAXLINE, line_type))\n\n# for backwards compatibility\nerror = HTTPException\n\nclass LineAndFileWrapper:\n    \"\"\"A limited file-like object for HTTP/0.9 responses.\"\"\"\n\n    # The status-line parsing code calls readline(), which normally\n    # get the HTTP status line.  For a 0.9 response, however, this is\n    # actually the first line of the body!  Clients need to get a\n    # readable file object that contains that line.\n\n    def __init__(self, line, file):\n        self._line = line\n        self._file = file\n        self._line_consumed = 0\n        self._line_offset = 0\n        self._line_left = len(line)\n\n    def __getattr__(self, attr):\n        return getattr(self._file, attr)\n\n    def _done(self):\n        # called when the last byte is read from the line.  After the\n        # call, all read methods are delegated to the underlying file\n        # object.\n        self._line_consumed = 1\n        self.read = self._file.read\n        self.readline = self._file.readline\n        self.readlines = self._file.readlines\n\n    def read(self, amt=None):\n        if self._line_consumed:\n            return self._file.read(amt)\n        assert self._line_left\n        if amt is None or amt > self._line_left:\n            s = self._line[self._line_offset:]\n            self._done()\n            if amt is None:\n                return s + self._file.read()\n            else:\n                return s + self._file.read(amt - len(s))\n        else:\n            assert amt <= self._line_left\n            i = self._line_offset\n            j = i + amt\n            s = self._line[i:j]\n            self._line_offset = j\n            self._line_left -= amt\n            if self._line_left == 0:\n                self._done()\n            return s\n\n    def readline(self):\n        if self._line_consumed:\n            return self._file.readline()\n        assert self._line_left\n        s = self._line[self._line_offset:]\n        self._done()\n        return s\n\n    def readlines(self, size=None):\n        if self._line_consumed:\n            return self._file.readlines(size)\n        assert self._line_left\n        L = [self._line[self._line_offset:]]\n        self._done()\n        if size is None:\n            return L + self._file.readlines()\n        else:\n            return L + self._file.readlines(size)\n", 
    "inspect": "# -*- coding: utf-8 -*-\n\"\"\"Get useful information from live Python objects.\n\nThis module encapsulates the interface provided by the internal special\nattributes (func_*, co_*, im_*, tb_*, etc.) in a friendlier fashion.\nIt also provides some help for examining source code and class layout.\n\nHere are some of the useful functions provided by this module:\n\n    ismodule(), isclass(), ismethod(), isfunction(), isgeneratorfunction(),\n        isgenerator(), istraceback(), isframe(), iscode(), isbuiltin(),\n        isroutine() - check object types\n    getmembers() - get members of an object that satisfy a given condition\n\n    getfile(), getsourcefile(), getsource() - find an object's source code\n    getdoc(), getcomments() - get documentation on an object\n    getmodule() - determine the module that an object came from\n    getclasstree() - arrange classes so as to represent their hierarchy\n\n    getargspec(), getargvalues(), getcallargs() - get info about function arguments\n    formatargspec(), formatargvalues() - format an argument spec\n    getouterframes(), getinnerframes() - get info about frames\n    currentframe() - get the current stack frame\n    stack(), trace() - get info about frames on the stack or in a traceback\n\"\"\"\n\n# This module is in the public domain.  No warranties.\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__date__ = '1 Jan 2001'\n\nimport sys\nimport os\nimport types\nimport string\nimport re\nimport dis\nimport imp\nimport tokenize\nimport linecache\nfrom operator import attrgetter\nfrom collections import namedtuple\n\n# These constants are from Include/code.h.\nCO_OPTIMIZED, CO_NEWLOCALS, CO_VARARGS, CO_VARKEYWORDS = 0x1, 0x2, 0x4, 0x8\nCO_NESTED, CO_GENERATOR, CO_NOFREE = 0x10, 0x20, 0x40\n# See Include/object.h\nTPFLAGS_IS_ABSTRACT = 1 << 20\n\n# ----------------------------------------------------------- type-checking\ndef ismodule(object):\n    \"\"\"Return true if the object is a module.\n\n    Module objects provide these attributes:\n        __doc__         documentation string\n        __file__        filename (missing for built-in modules)\"\"\"\n    return isinstance(object, types.ModuleType)\n\ndef isclass(object):\n    \"\"\"Return true if the object is a class.\n\n    Class objects provide these attributes:\n        __doc__         documentation string\n        __module__      name of module in which this class was defined\"\"\"\n    return isinstance(object, (type, types.ClassType))\n\ndef ismethod(object):\n    \"\"\"Return true if the object is an instance method.\n\n    Instance method objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this method was defined\n        im_class        class object in which this method belongs\n        im_func         function object containing implementation of method\n        im_self         instance to which this method is bound, or None\"\"\"\n    return isinstance(object, types.MethodType)\n\ndef ismethoddescriptor(object):\n    \"\"\"Return true if the object is a method descriptor.\n\n    But not if ismethod() or isclass() or isfunction() are true.\n\n    This is new in Python 2.2, and, for example, is true of int.__add__.\n    An object passing this test has a __get__ attribute but not a __set__\n    attribute, but beyond that the set of attributes varies.  __name__ is\n    usually sensible, and __doc__ often is.\n\n    Methods implemented via descriptors that also pass one of the other\n    tests return false from the ismethoddescriptor() test, simply because\n    the other tests promise more -- you can, e.g., count on having the\n    im_func attribute (etc) when an object passes ismethod().\"\"\"\n    return (hasattr(object, \"__get__\")\n            and not hasattr(object, \"__set__\") # else it's a data descriptor\n            and not ismethod(object)           # mutual exclusion\n            and not isfunction(object)\n            and not isclass(object))\n\ndef isdatadescriptor(object):\n    \"\"\"Return true if the object is a data descriptor.\n\n    Data descriptors have both a __get__ and a __set__ attribute.  Examples are\n    properties (defined in Python) and getsets and members (defined in C).\n    Typically, data descriptors will also have __name__ and __doc__ attributes\n    (properties, getsets, and members have both of these attributes), but this\n    is not guaranteed.\"\"\"\n    return (hasattr(object, \"__set__\") and hasattr(object, \"__get__\"))\n\nif hasattr(types, 'MemberDescriptorType'):\n    # CPython and equivalent\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.MemberDescriptorType)\nelse:\n    # Other implementations\n    def ismemberdescriptor(object):\n        \"\"\"Return true if the object is a member descriptor.\n\n        Member descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\nif hasattr(types, 'GetSetDescriptorType'):\n    # CPython and equivalent\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return isinstance(object, types.GetSetDescriptorType)\nelse:\n    # Other implementations\n    def isgetsetdescriptor(object):\n        \"\"\"Return true if the object is a getset descriptor.\n\n        getset descriptors are specialized descriptors defined in extension\n        modules.\"\"\"\n        return False\n\ndef isfunction(object):\n    \"\"\"Return true if the object is a user-defined function.\n\n    Function objects provide these attributes:\n        __doc__         documentation string\n        __name__        name with which this function was defined\n        func_code       code object containing compiled function bytecode\n        func_defaults   tuple of any default values for arguments\n        func_doc        (same as __doc__)\n        func_globals    global namespace in which this function was defined\n        func_name       (same as __name__)\"\"\"\n    return isinstance(object, types.FunctionType)\n\ndef isgeneratorfunction(object):\n    \"\"\"Return true if the object is a user-defined generator function.\n\n    Generator function objects provides same attributes as functions.\n\n    See help(isfunction) for attributes listing.\"\"\"\n    return bool((isfunction(object) or ismethod(object)) and\n                object.func_code.co_flags & CO_GENERATOR)\n\ndef isgenerator(object):\n    \"\"\"Return true if the object is a generator.\n\n    Generator objects provide these attributes:\n        __iter__        defined to support iteration over container\n        close           raises a new GeneratorExit exception inside the\n                        generator to terminate the iteration\n        gi_code         code object\n        gi_frame        frame object or possibly None once the generator has\n                        been exhausted\n        gi_running      set to 1 when generator is executing, 0 otherwise\n        next            return the next item from the container\n        send            resumes the generator and \"sends\" a value that becomes\n                        the result of the current yield-expression\n        throw           used to raise an exception inside the generator\"\"\"\n    return isinstance(object, types.GeneratorType)\n\ndef istraceback(object):\n    \"\"\"Return true if the object is a traceback.\n\n    Traceback objects provide these attributes:\n        tb_frame        frame object at this level\n        tb_lasti        index of last attempted instruction in bytecode\n        tb_lineno       current line number in Python source code\n        tb_next         next inner traceback object (called by this level)\"\"\"\n    return isinstance(object, types.TracebackType)\n\ndef isframe(object):\n    \"\"\"Return true if the object is a frame object.\n\n    Frame objects provide these attributes:\n        f_back          next outer frame object (this frame's caller)\n        f_builtins      built-in namespace seen by this frame\n        f_code          code object being executed in this frame\n        f_exc_traceback traceback if raised in this frame, or None\n        f_exc_type      exception type if raised in this frame, or None\n        f_exc_value     exception value if raised in this frame, or None\n        f_globals       global namespace seen by this frame\n        f_lasti         index of last attempted instruction in bytecode\n        f_lineno        current line number in Python source code\n        f_locals        local namespace seen by this frame\n        f_restricted    0 or 1 if frame is in restricted execution mode\n        f_trace         tracing function for this frame, or None\"\"\"\n    return isinstance(object, types.FrameType)\n\ndef iscode(object):\n    \"\"\"Return true if the object is a code object.\n\n    Code objects provide these attributes:\n        co_argcount     number of arguments (not including * or ** args)\n        co_code         string of raw compiled bytecode\n        co_consts       tuple of constants used in the bytecode\n        co_filename     name of file in which this code object was created\n        co_firstlineno  number of first line in Python source code\n        co_flags        bitmap: 1=optimized | 2=newlocals | 4=*arg | 8=**arg\n        co_lnotab       encoded mapping of line numbers to bytecode indices\n        co_name         name with which this code object was defined\n        co_names        tuple of names of local variables\n        co_nlocals      number of local variables\n        co_stacksize    virtual machine stack space required\n        co_varnames     tuple of names of arguments and local variables\"\"\"\n    return isinstance(object, types.CodeType)\n\ndef isbuiltin(object):\n    \"\"\"Return true if the object is a built-in function or method.\n\n    Built-in functions and methods provide these attributes:\n        __doc__         documentation string\n        __name__        original name of this function or method\n        __self__        instance to which a method is bound, or None\"\"\"\n    return isinstance(object, types.BuiltinFunctionType)\n\ndef isroutine(object):\n    \"\"\"Return true if the object is any kind of function or method.\"\"\"\n    return (isbuiltin(object)\n            or isfunction(object)\n            or ismethod(object)\n            or ismethoddescriptor(object))\n\ndef isabstract(object):\n    \"\"\"Return true if the object is an abstract base class (ABC).\"\"\"\n    return bool(isinstance(object, type) and object.__flags__ & TPFLAGS_IS_ABSTRACT)\n\ndef getmembers(object, predicate=None):\n    \"\"\"Return all members of an object as (name, value) pairs sorted by name.\n    Optionally, only return members that satisfy a given predicate.\"\"\"\n    results = []\n    for key in dir(object):\n        try:\n            value = getattr(object, key)\n        except AttributeError:\n            continue\n        if not predicate or predicate(value):\n            results.append((key, value))\n    results.sort()\n    return results\n\nAttribute = namedtuple('Attribute', 'name kind defining_class object')\n\ndef classify_class_attrs(cls):\n    \"\"\"Return list of attribute-descriptor tuples.\n\n    For each name in dir(cls), the return list contains a 4-tuple\n    with these elements:\n\n        0. The name (a string).\n\n        1. The kind of attribute this is, one of these strings:\n               'class method'    created via classmethod()\n               'static method'   created via staticmethod()\n               'property'        created via property()\n               'method'          any other flavor of method\n               'data'            not a method\n\n        2. The class which defined this attribute (a class).\n\n        3. The object as obtained directly from the defining class's\n           __dict__, not via getattr.  This is especially important for\n           data attributes:  C.data is just a data object, but\n           C.__dict__['data'] may be a data descriptor with additional\n           info, like a __doc__ string.\n    \"\"\"\n\n    mro = getmro(cls)\n    names = dir(cls)\n    result = []\n    for name in names:\n        # Get the object associated with the name, and where it was defined.\n        # Getting an obj from the __dict__ sometimes reveals more than\n        # using getattr.  Static and class methods are dramatic examples.\n        # Furthermore, some objects may raise an Exception when fetched with\n        # getattr(). This is the case with some descriptors (bug #1785).\n        # Thus, we only use getattr() as a last resort.\n        homecls = None\n        for base in (cls,) + mro:\n            if name in base.__dict__:\n                obj = base.__dict__[name]\n                homecls = base\n                break\n        else:\n            obj = getattr(cls, name)\n            homecls = getattr(obj, \"__objclass__\", homecls)\n\n        # Classify the object.\n        if isinstance(obj, staticmethod):\n            kind = \"static method\"\n        elif isinstance(obj, classmethod):\n            kind = \"class method\"\n        elif isinstance(obj, property):\n            kind = \"property\"\n        elif ismethoddescriptor(obj):\n            kind = \"method\"\n        elif isdatadescriptor(obj):\n            kind = \"data\"\n        else:\n            obj_via_getattr = getattr(cls, name)\n            if (ismethod(obj_via_getattr) or\n                ismethoddescriptor(obj_via_getattr)):\n                kind = \"method\"\n            else:\n                kind = \"data\"\n            obj = obj_via_getattr\n\n        result.append(Attribute(name, kind, homecls, obj))\n\n    return result\n\n# ----------------------------------------------------------- class helpers\ndef _searchbases(cls, accum):\n    # Simulate the \"classic class\" search order.\n    if cls in accum:\n        return\n    accum.append(cls)\n    for base in cls.__bases__:\n        _searchbases(base, accum)\n\ndef getmro(cls):\n    \"Return tuple of base classes (including cls) in method resolution order.\"\n    if hasattr(cls, \"__mro__\"):\n        return cls.__mro__\n    else:\n        result = []\n        _searchbases(cls, result)\n        return tuple(result)\n\n# -------------------------------------------------- source code extraction\ndef indentsize(line):\n    \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\"\n    expline = string.expandtabs(line)\n    return len(expline) - len(string.lstrip(expline))\n\ndef getdoc(object):\n    \"\"\"Get the documentation string for an object.\n\n    All tabs are expanded to spaces.  To clean up docstrings that are\n    indented to line up with blocks of code, any whitespace than can be\n    uniformly removed from the second line onwards is removed.\"\"\"\n    try:\n        doc = object.__doc__\n    except AttributeError:\n        return None\n    if not isinstance(doc, types.StringTypes):\n        return None\n    return cleandoc(doc)\n\ndef cleandoc(doc):\n    \"\"\"Clean up indentation from docstrings.\n\n    Any whitespace that can be uniformly removed from the second line\n    onwards is removed.\"\"\"\n    try:\n        lines = string.split(string.expandtabs(doc), '\\n')\n    except UnicodeError:\n        return None\n    else:\n        # Find minimum indentation of any non-blank lines after first line.\n        margin = sys.maxint\n        for line in lines[1:]:\n            content = len(string.lstrip(line))\n            if content:\n                indent = len(line) - content\n                margin = min(margin, indent)\n        # Remove indentation.\n        if lines:\n            lines[0] = lines[0].lstrip()\n        if margin < sys.maxint:\n            for i in range(1, len(lines)): lines[i] = lines[i][margin:]\n        # Remove any trailing or leading blank lines.\n        while lines and not lines[-1]:\n            lines.pop()\n        while lines and not lines[0]:\n            lines.pop(0)\n        return string.join(lines, '\\n')\n\ndef getfile(object):\n    \"\"\"Work out which source or compiled file an object was defined in.\"\"\"\n    if ismodule(object):\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in module'.format(object))\n    if isclass(object):\n        object = sys.modules.get(object.__module__)\n        if hasattr(object, '__file__'):\n            return object.__file__\n        raise TypeError('{!r} is a built-in class'.format(object))\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        return object.co_filename\n    raise TypeError('{!r} is not a module, class, method, '\n                    'function, traceback, frame, or code object'.format(object))\n\nModuleInfo = namedtuple('ModuleInfo', 'name suffix mode module_type')\n\ndef getmoduleinfo(path):\n    \"\"\"Get the module name, suffix, mode, and module type for a given file.\"\"\"\n    filename = os.path.basename(path)\n    suffixes = map(lambda info:\n                   (-len(info[0]), info[0], info[1], info[2]),\n                    imp.get_suffixes())\n    suffixes.sort() # try longest suffixes first, in case they overlap\n    for neglen, suffix, mode, mtype in suffixes:\n        if filename[neglen:] == suffix:\n            return ModuleInfo(filename[:neglen], suffix, mode, mtype)\n\ndef getmodulename(path):\n    \"\"\"Return the module name for a given file, or None.\"\"\"\n    info = getmoduleinfo(path)\n    if info: return info[0]\n\ndef getsourcefile(object):\n    \"\"\"Return the filename that can be used to locate an object's source.\n    Return None if no way can be identified to get the source.\n    \"\"\"\n    filename = getfile(object)\n    if string.lower(filename[-4:]) in ('.pyc', '.pyo'):\n        filename = filename[:-4] + '.py'\n    for suffix, mode, kind in imp.get_suffixes():\n        if 'b' in mode and string.lower(filename[-len(suffix):]) == suffix:\n            # Looks like a binary file.  We want to only return a text file.\n            return None\n    if os.path.exists(filename):\n        return filename\n    # only return a non-existent filename if the module has a PEP 302 loader\n    if hasattr(getmodule(object, filename), '__loader__'):\n        return filename\n    # or it is in the linecache\n    if filename in linecache.cache:\n        return filename\n\ndef getabsfile(object, _filename=None):\n    \"\"\"Return an absolute path to the source or compiled file for an object.\n\n    The idea is for each object to have a unique origin, so this routine\n    normalizes the result as much as possible.\"\"\"\n    if _filename is None:\n        _filename = getsourcefile(object) or getfile(object)\n    return os.path.normcase(os.path.abspath(_filename))\n\nmodulesbyfile = {}\n_filesbymodname = {}\n\ndef getmodule(object, _filename=None):\n    \"\"\"Return the module an object was defined in, or None if not found.\"\"\"\n    if ismodule(object):\n        return object\n    if hasattr(object, '__module__'):\n        return sys.modules.get(object.__module__)\n    # Try the filename to modulename cache\n    if _filename is not None and _filename in modulesbyfile:\n        return sys.modules.get(modulesbyfile[_filename])\n    # Try the cache again with the absolute file name\n    try:\n        file = getabsfile(object, _filename)\n    except TypeError:\n        return None\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Update the filename to module name cache and check yet again\n    # Copy sys.modules in order to cope with changes while iterating\n    for modname, module in sys.modules.items():\n        if ismodule(module) and hasattr(module, '__file__'):\n            f = module.__file__\n            if f == _filesbymodname.get(modname, None):\n                # Have already mapped this module, so skip it\n                continue\n            _filesbymodname[modname] = f\n            f = getabsfile(module)\n            # Always map to the name the module knows itself by\n            modulesbyfile[f] = modulesbyfile[\n                os.path.realpath(f)] = module.__name__\n    if file in modulesbyfile:\n        return sys.modules.get(modulesbyfile[file])\n    # Check the main module\n    main = sys.modules['__main__']\n    if not hasattr(object, '__name__'):\n        return None\n    if hasattr(main, object.__name__):\n        mainobject = getattr(main, object.__name__)\n        if mainobject is object:\n            return main\n    # Check builtins\n    builtin = sys.modules['__builtin__']\n    if hasattr(builtin, object.__name__):\n        builtinobject = getattr(builtin, object.__name__)\n        if builtinobject is object:\n            return builtin\n\ndef findsource(object):\n    \"\"\"Return the entire source file and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of all the lines\n    in the file and the line number indexes a line in that list.  An IOError\n    is raised if the source code cannot be retrieved.\"\"\"\n\n    file = getfile(object)\n    sourcefile = getsourcefile(object)\n    if not sourcefile and file[:1] + file[-1:] != '<>':\n        raise IOError('source code not available')\n    file = sourcefile if sourcefile else file\n\n    module = getmodule(object, file)\n    if module:\n        lines = linecache.getlines(file, module.__dict__)\n    else:\n        lines = linecache.getlines(file)\n    if not lines:\n        raise IOError('could not get source code')\n\n    if ismodule(object):\n        return lines, 0\n\n    if isclass(object):\n        name = object.__name__\n        pat = re.compile(r'^(\\s*)class\\s*' + name + r'\\b')\n        # make some effort to find the best matching class definition:\n        # use the one with the least indentation, which is the one\n        # that's most probably not inside a function definition.\n        candidates = []\n        for i in range(len(lines)):\n            match = pat.match(lines[i])\n            if match:\n                # if it's at toplevel, it's already the best one\n                if lines[i][0] == 'c':\n                    return lines, i\n                # else add whitespace to candidate list\n                candidates.append((match.group(1), i))\n        if candidates:\n            # this will sort by whitespace, and by line number,\n            # less whitespace first\n            candidates.sort()\n            return lines, candidates[0][1]\n        else:\n            raise IOError('could not find class definition')\n\n    if ismethod(object):\n        object = object.im_func\n    if isfunction(object):\n        object = object.func_code\n    if istraceback(object):\n        object = object.tb_frame\n    if isframe(object):\n        object = object.f_code\n    if iscode(object):\n        if not hasattr(object, 'co_firstlineno'):\n            raise IOError('could not find function definition')\n        lnum = object.co_firstlineno - 1\n        pat = re.compile(r'^(\\s*def\\s)|(.*(?<!\\w)lambda(:|\\s))|^(\\s*@)')\n        while lnum > 0:\n            if pat.match(lines[lnum]): break\n            lnum = lnum - 1\n        return lines, lnum\n    raise IOError('could not find code object')\n\ndef getcomments(object):\n    \"\"\"Get lines of comments immediately preceding an object's source code.\n\n    Returns None when source can't be found.\n    \"\"\"\n    try:\n        lines, lnum = findsource(object)\n    except (IOError, TypeError):\n        return None\n\n    if ismodule(object):\n        # Look for a comment block at the top of the file.\n        start = 0\n        if lines and lines[0][:2] == '#!': start = 1\n        while start < len(lines) and string.strip(lines[start]) in ('', '#'):\n            start = start + 1\n        if start < len(lines) and lines[start][:1] == '#':\n            comments = []\n            end = start\n            while end < len(lines) and lines[end][:1] == '#':\n                comments.append(string.expandtabs(lines[end]))\n                end = end + 1\n            return string.join(comments, '')\n\n    # Look for a preceding block of comments at the same indentation.\n    elif lnum > 0:\n        indent = indentsize(lines[lnum])\n        end = lnum - 1\n        if end >= 0 and string.lstrip(lines[end])[:1] == '#' and \\\n            indentsize(lines[end]) == indent:\n            comments = [string.lstrip(string.expandtabs(lines[end]))]\n            if end > 0:\n                end = end - 1\n                comment = string.lstrip(string.expandtabs(lines[end]))\n                while comment[:1] == '#' and indentsize(lines[end]) == indent:\n                    comments[:0] = [comment]\n                    end = end - 1\n                    if end < 0: break\n                    comment = string.lstrip(string.expandtabs(lines[end]))\n            while comments and string.strip(comments[0]) == '#':\n                comments[:1] = []\n            while comments and string.strip(comments[-1]) == '#':\n                comments[-1:] = []\n            return string.join(comments, '')\n\nclass EndOfBlock(Exception): pass\n\nclass BlockFinder:\n    \"\"\"Provide a tokeneater() method to detect the end of a code block.\"\"\"\n    def __init__(self):\n        self.indent = 0\n        self.islambda = False\n        self.started = False\n        self.passline = False\n        self.last = 1\n\n    def tokeneater(self, type, token, srow_scol, erow_ecol, line):\n        srow, scol = srow_scol\n        erow, ecol = erow_ecol\n        if not self.started:\n            # look for the first \"def\", \"class\" or \"lambda\"\n            if token in (\"def\", \"class\", \"lambda\"):\n                if token == \"lambda\":\n                    self.islambda = True\n                self.started = True\n            self.passline = True    # skip to the end of the line\n        elif type == tokenize.NEWLINE:\n            self.passline = False   # stop skipping when a NEWLINE is seen\n            self.last = srow\n            if self.islambda:       # lambdas always end at the first NEWLINE\n                raise EndOfBlock\n        elif self.passline:\n            pass\n        elif type == tokenize.INDENT:\n            self.indent = self.indent + 1\n            self.passline = True\n        elif type == tokenize.DEDENT:\n            self.indent = self.indent - 1\n            # the end of matching indent/dedent pairs end a block\n            # (note that this only works for \"def\"/\"class\" blocks,\n            #  not e.g. for \"if: else:\" or \"try: finally:\" blocks)\n            if self.indent <= 0:\n                raise EndOfBlock\n        elif self.indent == 0 and type not in (tokenize.COMMENT, tokenize.NL):\n            # any other token on the same indentation level end the previous\n            # block as well, except the pseudo-tokens COMMENT and NL.\n            raise EndOfBlock\n\ndef getblock(lines):\n    \"\"\"Extract the block of code at the top of the given list of lines.\"\"\"\n    blockfinder = BlockFinder()\n    try:\n        tokenize.tokenize(iter(lines).next, blockfinder.tokeneater)\n    except (EndOfBlock, IndentationError):\n        pass\n    return lines[:blockfinder.last]\n\ndef getsourcelines(object):\n    \"\"\"Return a list of source lines and starting line number for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a list of the lines\n    corresponding to the object and the line number indicates where in the\n    original source file the first line of code was found.  An IOError is\n    raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = findsource(object)\n\n    if ismodule(object): return lines, 0\n    else: return getblock(lines[lnum:]), lnum + 1\n\ndef getsource(object):\n    \"\"\"Return the text of the source code for an object.\n\n    The argument may be a module, class, method, function, traceback, frame,\n    or code object.  The source code is returned as a single string.  An\n    IOError is raised if the source code cannot be retrieved.\"\"\"\n    lines, lnum = getsourcelines(object)\n    return string.join(lines, '')\n\n# --------------------------------------------------- class tree extraction\ndef walktree(classes, children, parent):\n    \"\"\"Recursive helper function for getclasstree().\"\"\"\n    results = []\n    classes.sort(key=attrgetter('__module__', '__name__'))\n    for c in classes:\n        results.append((c, c.__bases__))\n        if c in children:\n            results.append(walktree(children[c], children, c))\n    return results\n\ndef getclasstree(classes, unique=0):\n    \"\"\"Arrange the given list of classes into a hierarchy of nested lists.\n\n    Where a nested list appears, it contains classes derived from the class\n    whose entry immediately precedes the list.  Each entry is a 2-tuple\n    containing a class and a tuple of its base classes.  If the 'unique'\n    argument is true, exactly one entry appears in the returned structure\n    for each class in the given list.  Otherwise, classes using multiple\n    inheritance and their descendants will appear multiple times.\"\"\"\n    children = {}\n    roots = []\n    for c in classes:\n        if c.__bases__:\n            for parent in c.__bases__:\n                if not parent in children:\n                    children[parent] = []\n                if c not in children[parent]:\n                    children[parent].append(c)\n                if unique and parent in classes: break\n        elif c not in roots:\n            roots.append(c)\n    for parent in children:\n        if parent not in classes:\n            roots.append(parent)\n    return walktree(roots, children, None)\n\n# ------------------------------------------------ argument list extraction\nArguments = namedtuple('Arguments', 'args varargs keywords')\n\ndef getargs(co):\n    \"\"\"Get information about the arguments accepted by a code object.\n\n    Three things are returned: (args, varargs, varkw), where 'args' is\n    a list of argument names (possibly containing nested lists), and\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\"\"\"\n\n    if not iscode(co):\n        if hasattr(len, 'func_code') and type(co) is type(len.func_code):\n            # PyPy extension: built-in function objects have a func_code too.\n            # There is no co_code on it, but co_argcount and co_varnames and\n            # co_flags are present.\n            pass\n        else:\n            raise TypeError('{!r} is not a code object'.format(co))\n\n    code = getattr(co, 'co_code', '')\n    nargs = co.co_argcount\n    names = co.co_varnames\n    args = list(names[:nargs])\n    step = 0\n\n    # The following acrobatics are for anonymous (tuple) arguments.\n    for i in range(nargs):\n        if args[i][:1] in ('', '.'):\n            stack, remain, count = [], [], []\n            while step < len(code):\n                op = ord(code[step])\n                step = step + 1\n                if op >= dis.HAVE_ARGUMENT:\n                    opname = dis.opname[op]\n                    value = ord(code[step]) + ord(code[step+1])*256\n                    step = step + 2\n                    if opname in ('UNPACK_TUPLE', 'UNPACK_SEQUENCE'):\n                        remain.append(value)\n                        count.append(value)\n                    elif opname == 'STORE_FAST':\n                        stack.append(names[value])\n\n                        # Special case for sublists of length 1: def foo((bar))\n                        # doesn't generate the UNPACK_TUPLE bytecode, so if\n                        # `remain` is empty here, we have such a sublist.\n                        if not remain:\n                            stack[0] = [stack[0]]\n                            break\n                        else:\n                            remain[-1] = remain[-1] - 1\n                            while remain[-1] == 0:\n                                remain.pop()\n                                size = count.pop()\n                                stack[-size:] = [stack[-size:]]\n                                if not remain: break\n                                remain[-1] = remain[-1] - 1\n                            if not remain: break\n            args[i] = stack[0]\n\n    varargs = None\n    if co.co_flags & CO_VARARGS:\n        varargs = co.co_varnames[nargs]\n        nargs = nargs + 1\n    varkw = None\n    if co.co_flags & CO_VARKEYWORDS:\n        varkw = co.co_varnames[nargs]\n    return Arguments(args, varargs, varkw)\n\nArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')\n\ndef getargspec(func):\n    \"\"\"Get the names and default values of a function's arguments.\n\n    A tuple of four things is returned: (args, varargs, varkw, defaults).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'defaults' is an n-tuple of the default values of the last n arguments.\n    \"\"\"\n\n    if ismethod(func):\n        func = func.im_func\n    if not (isfunction(func) or\n            isbuiltin(func) and hasattr(func, 'func_code')):\n            # PyPy extension: this works for built-in functions too\n        raise TypeError('{!r} is not a Python function'.format(func))\n    args, varargs, varkw = getargs(func.func_code)\n    return ArgSpec(args, varargs, varkw, func.func_defaults)\n\nArgInfo = namedtuple('ArgInfo', 'args varargs keywords locals')\n\ndef getargvalues(frame):\n    \"\"\"Get information about arguments passed into a particular frame.\n\n    A tuple of four things is returned: (args, varargs, varkw, locals).\n    'args' is a list of the argument names (it may contain nested lists).\n    'varargs' and 'varkw' are the names of the * and ** arguments or None.\n    'locals' is the locals dictionary of the given frame.\"\"\"\n    args, varargs, varkw = getargs(frame.f_code)\n    return ArgInfo(args, varargs, varkw, frame.f_locals)\n\ndef joinseq(seq):\n    if len(seq) == 1:\n        return '(' + seq[0] + ',)'\n    else:\n        return '(' + string.join(seq, ', ') + ')'\n\ndef strseq(object, convert, join=joinseq):\n    \"\"\"Recursively walk a sequence, stringifying each element.\"\"\"\n    if type(object) in (list, tuple):\n        return join(map(lambda o, c=convert, j=join: strseq(o, c, j), object))\n    else:\n        return convert(object)\n\ndef formatargspec(args, varargs=None, varkw=None, defaults=None,\n                  formatarg=str,\n                  formatvarargs=lambda name: '*' + name,\n                  formatvarkw=lambda name: '**' + name,\n                  formatvalue=lambda value: '=' + repr(value),\n                  join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargspec.\n\n    The first four arguments are (args, varargs, varkw, defaults).  The\n    other four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    specs = []\n    if defaults:\n        firstdefault = len(args) - len(defaults)\n    for i, arg in enumerate(args):\n        spec = strseq(arg, formatarg, join)\n        if defaults and i >= firstdefault:\n            spec = spec + formatvalue(defaults[i - firstdefault])\n        specs.append(spec)\n    if varargs is not None:\n        specs.append(formatvarargs(varargs))\n    if varkw is not None:\n        specs.append(formatvarkw(varkw))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef formatargvalues(args, varargs, varkw, locals,\n                    formatarg=str,\n                    formatvarargs=lambda name: '*' + name,\n                    formatvarkw=lambda name: '**' + name,\n                    formatvalue=lambda value: '=' + repr(value),\n                    join=joinseq):\n    \"\"\"Format an argument spec from the 4 values returned by getargvalues.\n\n    The first four arguments are (args, varargs, varkw, locals).  The\n    next four arguments are the corresponding optional formatting functions\n    that are called to turn names and values into strings.  The ninth\n    argument is an optional function to format the sequence of arguments.\"\"\"\n    def convert(name, locals=locals,\n                formatarg=formatarg, formatvalue=formatvalue):\n        return formatarg(name) + formatvalue(locals[name])\n    specs = []\n    for i in range(len(args)):\n        specs.append(strseq(args[i], convert, join))\n    if varargs:\n        specs.append(formatvarargs(varargs) + formatvalue(locals[varargs]))\n    if varkw:\n        specs.append(formatvarkw(varkw) + formatvalue(locals[varkw]))\n    return '(' + string.join(specs, ', ') + ')'\n\ndef getcallargs(func, *positional, **named):\n    \"\"\"Get the mapping of arguments to values.\n\n    A dict is returned, with keys the function argument names (including the\n    names of the * and ** arguments, if any), and values the respective bound\n    values from 'positional' and 'named'.\"\"\"\n    args, varargs, varkw, defaults = getargspec(func)\n    f_name = func.__name__\n    arg2value = {}\n\n    # The following closures are basically because of tuple parameter unpacking.\n    assigned_tuple_params = []\n    def assign(arg, value):\n        if isinstance(arg, str):\n            arg2value[arg] = value\n        else:\n            assigned_tuple_params.append(arg)\n            value = iter(value)\n            for i, subarg in enumerate(arg):\n                try:\n                    subvalue = next(value)\n                except StopIteration:\n                    raise ValueError('need more than %d %s to unpack' %\n                                     (i, 'values' if i > 1 else 'value'))\n                assign(subarg,subvalue)\n            try:\n                next(value)\n            except StopIteration:\n                pass\n            else:\n                raise ValueError('too many values to unpack')\n    def is_assigned(arg):\n        if isinstance(arg,str):\n            return arg in arg2value\n        return arg in assigned_tuple_params\n    if ismethod(func) and func.im_self is not None:\n        # implicit 'self' (or 'cls' for classmethods) argument\n        positional = (func.im_self,) + positional\n    num_pos = len(positional)\n    num_total = num_pos + len(named)\n    num_args = len(args)\n    num_defaults = len(defaults) if defaults else 0\n    for arg, value in zip(args, positional):\n        assign(arg, value)\n    if varargs:\n        if num_pos > num_args:\n            assign(varargs, positional[-(num_pos-num_args):])\n        else:\n            assign(varargs, ())\n    elif 0 < num_args < num_pos:\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at most' if defaults else 'exactly', num_args,\n            'arguments' if num_args > 1 else 'argument', num_total))\n    elif num_args == 0 and num_total:\n        if varkw:\n            if num_pos:\n                # XXX: We should use num_pos, but Python also uses num_total:\n                raise TypeError('%s() takes exactly 0 arguments '\n                                '(%d given)' % (f_name, num_total))\n        else:\n            raise TypeError('%s() takes no arguments (%d given)' %\n                            (f_name, num_total))\n    for arg in args:\n        if isinstance(arg, str) and arg in named:\n            if is_assigned(arg):\n                raise TypeError(\"%s() got multiple values for keyword \"\n                                \"argument '%s'\" % (f_name, arg))\n            else:\n                assign(arg, named.pop(arg))\n    if defaults:    # fill in any missing values with the defaults\n        for arg, value in zip(args[-num_defaults:], defaults):\n            if not is_assigned(arg):\n                assign(arg, value)\n    if varkw:\n        assign(varkw, named)\n    elif named:\n        unexpected = next(iter(named))\n        if isinstance(unexpected, unicode):\n            unexpected = unexpected.encode(sys.getdefaultencoding(), 'replace')\n        raise TypeError(\"%s() got an unexpected keyword argument '%s'\" %\n                        (f_name, unexpected))\n    unassigned = num_args - len([arg for arg in args if is_assigned(arg)])\n    if unassigned:\n        num_required = num_args - num_defaults\n        raise TypeError('%s() takes %s %d %s (%d given)' % (\n            f_name, 'at least' if defaults else 'exactly', num_required,\n            'arguments' if num_required > 1 else 'argument', num_total))\n    return arg2value\n\n# -------------------------------------------------- stack frame extraction\n\nTraceback = namedtuple('Traceback', 'filename lineno function code_context index')\n\ndef getframeinfo(frame, context=1):\n    \"\"\"Get information about a frame or traceback object.\n\n    A tuple of five things is returned: the filename, the line number of\n    the current line, the function name, a list of lines of context from\n    the source code, and the index of the current line within that list.\n    The optional second argument specifies the number of lines of context\n    to return, which are centered around the current line.\"\"\"\n    if istraceback(frame):\n        lineno = frame.tb_lineno\n        frame = frame.tb_frame\n    else:\n        lineno = frame.f_lineno\n    if not isframe(frame):\n        raise TypeError('{!r} is not a frame or traceback object'.format(frame))\n\n    filename = getsourcefile(frame) or getfile(frame)\n    if context > 0:\n        start = lineno - 1 - context//2\n        try:\n            lines, lnum = findsource(frame)\n        except IOError:\n            lines = index = None\n        else:\n            start = max(start, 1)\n            start = max(0, min(start, len(lines) - context))\n            lines = lines[start:start+context]\n            index = lineno - 1 - start\n    else:\n        lines = index = None\n\n    return Traceback(filename, lineno, frame.f_code.co_name, lines, index)\n\ndef getlineno(frame):\n    \"\"\"Get the line number from a frame object, allowing for optimization.\"\"\"\n    # FrameType.f_lineno is now a descriptor that grovels co_lnotab\n    return frame.f_lineno\n\ndef getouterframes(frame, context=1):\n    \"\"\"Get a list of records for a frame and all higher (calling) frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while frame:\n        framelist.append((frame,) + getframeinfo(frame, context))\n        frame = frame.f_back\n    return framelist\n\ndef getinnerframes(tb, context=1):\n    \"\"\"Get a list of records for a traceback's frame and all lower frames.\n\n    Each record contains a frame object, filename, line number, function\n    name, a list of lines of context, and index within the context.\"\"\"\n    framelist = []\n    while tb:\n        framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n        tb = tb.tb_next\n    return framelist\n\nif hasattr(sys, '_getframe'):\n    currentframe = sys._getframe\nelse:\n    currentframe = lambda _=None: None\n\ndef stack(context=1):\n    \"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\n    return getouterframes(sys._getframe(1), context)\n\ndef trace(context=1):\n    \"\"\"Return a list of records for the stack below the current exception.\"\"\"\n    return getinnerframes(sys.exc_info()[2], context)\n", 
    "io": "\"\"\"The io module provides the Python interfaces to stream handling. The\nbuiltin open function is defined in this module.\n\nAt the top of the I/O hierarchy is the abstract base class IOBase. It\ndefines the basic interface to a stream. Note, however, that there is no\nseparation between reading and writing to streams; implementations are\nallowed to raise an IOError if they do not support a given operation.\n\nExtending IOBase is RawIOBase which deals simply with the reading and\nwriting of raw bytes to a stream. FileIO subclasses RawIOBase to provide\nan interface to OS files.\n\nBufferedIOBase deals with buffering on a raw byte stream (RawIOBase). Its\nsubclasses, BufferedWriter, BufferedReader, and BufferedRWPair buffer\nstreams that are readable, writable, and both respectively.\nBufferedRandom provides a buffered interface to random access\nstreams. BytesIO is a simple stream of in-memory bytes.\n\nAnother IOBase subclass, TextIOBase, deals with the encoding and decoding\nof streams into text. TextIOWrapper, which extends it, is a buffered text\ninterface to a buffered raw stream (`BufferedIOBase`). Finally, StringIO\nis a in-memory stream for text.\n\nArgument names are not part of the specification, and only the arguments\nof open() are intended to be used as keyword arguments.\n\ndata:\n\nDEFAULT_BUFFER_SIZE\n\n   An int containing the default buffer size used by the module's buffered\n   I/O classes. open() uses the file's blksize (as obtained by os.stat) if\n   possible.\n\"\"\"\n# New I/O library conforming to PEP 3116.\n\n__author__ = (\"Guido van Rossum <guido@python.org>, \"\n              \"Mike Verdone <mike.verdone@gmail.com>, \"\n              \"Mark Russell <mark.russell@zen.co.uk>, \"\n              \"Antoine Pitrou <solipsis@pitrou.net>, \"\n              \"Amaury Forgeot d'Arc <amauryfa@gmail.com>, \"\n              \"Benjamin Peterson <benjamin@python.org>\")\n\n__all__ = [\"BlockingIOError\", \"open\", \"IOBase\", \"RawIOBase\", \"FileIO\",\n           \"BytesIO\", \"StringIO\", \"BufferedIOBase\",\n           \"BufferedReader\", \"BufferedWriter\", \"BufferedRWPair\",\n           \"BufferedRandom\", \"TextIOBase\", \"TextIOWrapper\",\n           \"UnsupportedOperation\", \"SEEK_SET\", \"SEEK_CUR\", \"SEEK_END\"]\n\n\nimport _io\nimport abc\n\nfrom _io import (DEFAULT_BUFFER_SIZE, BlockingIOError, UnsupportedOperation,\n                 open, FileIO, BytesIO, StringIO, BufferedReader,\n                 BufferedWriter, BufferedRWPair, BufferedRandom,\n                 IncrementalNewlineDecoder, TextIOWrapper)\n\nOpenWrapper = _io.open # for compatibility with _pyio\n\n# for seek()\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n# Declaring ABCs in C is tricky so we do it here.\n# Method descriptions and default implementations are inherited from the C\n# version however.\nclass IOBase(_io._IOBase):\n    __metaclass__ = abc.ABCMeta\n    __doc__ = _io._IOBase.__doc__\n\nclass RawIOBase(_io._RawIOBase, IOBase):\n    __doc__ = _io._RawIOBase.__doc__\n\nclass BufferedIOBase(_io._BufferedIOBase, IOBase):\n    __doc__ = _io._BufferedIOBase.__doc__\n\nclass TextIOBase(_io._TextIOBase, IOBase):\n    __doc__ = _io._TextIOBase.__doc__\n\nRawIOBase.register(FileIO)\n\nfor klass in (BytesIO, BufferedReader, BufferedWriter, BufferedRandom,\n              BufferedRWPair):\n    BufferedIOBase.register(klass)\n\nfor klass in (StringIO, TextIOWrapper):\n    TextIOBase.register(klass)\ndel klass\n", 
    "keyword": "#! /usr/bin/env python\n\n\"\"\"Keywords (from \"graminit.c\")\n\nThis file is automatically generated; please don't muck it up!\n\nTo update the symbols in this file, 'cd' to the top directory of\nthe python source tree after building the interpreter and run:\n\n    ./python Lib/keyword.py\n\"\"\"\n\n__all__ = [\"iskeyword\", \"kwlist\"]\n\nkwlist = [\n#--start keywords--\n        'and',\n        'as',\n        'assert',\n        'break',\n        'class',\n        'continue',\n        'def',\n        'del',\n        'elif',\n        'else',\n        'except',\n        'exec',\n        'finally',\n        'for',\n        'from',\n        'global',\n        'if',\n        'import',\n        'in',\n        'is',\n        'lambda',\n        'not',\n        'or',\n        'pass',\n        'print',\n        'raise',\n        'return',\n        'try',\n        'while',\n        'with',\n        'yield',\n#--end keywords--\n        ]\n\niskeyword = frozenset(kwlist).__contains__\n\ndef main():\n    import sys, re\n\n    args = sys.argv[1:]\n    iptfile = args and args[0] or \"Python/graminit.c\"\n    if len(args) > 1: optfile = args[1]\n    else: optfile = \"Lib/keyword.py\"\n\n    # scan the source file for keywords\n    fp = open(iptfile)\n    strprog = re.compile('\"([^\"]+)\"')\n    lines = []\n    for line in fp:\n        if '{1, \"' in line:\n            match = strprog.search(line)\n            if match:\n                lines.append(\"        '\" + match.group(1) + \"',\\n\")\n    fp.close()\n    lines.sort()\n\n    # load the output skeleton from the target\n    fp = open(optfile)\n    format = fp.readlines()\n    fp.close()\n\n    # insert the lines of keywords\n    try:\n        start = format.index(\"#--start keywords--\\n\") + 1\n        end = format.index(\"#--end keywords--\\n\")\n        format[start:end] = lines\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\\n\")\n        sys.exit(1)\n\n    # write the output file\n    fp = open(optfile, 'w')\n    fp.write(''.join(format))\n    fp.close()\n\nif __name__ == \"__main__\":\n    main()\n", 
    "linecache": "\"\"\"Cache lines from files.\n\nThis is intended to read lines from modules imported -- hence if a filename\nis not found, it will look down the module search path for a file by\nthat name.\n\"\"\"\n\nimport sys\nimport os\n\n__all__ = [\"getline\", \"clearcache\", \"checkcache\"]\n\ndef getline(filename, lineno, module_globals=None):\n    lines = getlines(filename, module_globals)\n    if 1 <= lineno <= len(lines):\n        return lines[lineno-1]\n    else:\n        return ''\n\n\n# The cache\n\ncache = {} # The cache\n\n\ndef clearcache():\n    \"\"\"Clear the cache entirely.\"\"\"\n\n    global cache\n    cache = {}\n\n\ndef getlines(filename, module_globals=None):\n    \"\"\"Get the lines for a file from the cache.\n    Update the cache if it doesn't contain an entry for this file already.\"\"\"\n\n    if filename in cache:\n        return cache[filename][2]\n    else:\n        return updatecache(filename, module_globals)\n\n\ndef checkcache(filename=None):\n    \"\"\"Discard cache entries that are out of date.\n    (This is not checked upon each call!)\"\"\"\n\n    if filename is None:\n        filenames = cache.keys()\n    else:\n        if filename in cache:\n            filenames = [filename]\n        else:\n            return\n\n    for filename in filenames:\n        size, mtime, lines, fullname = cache[filename]\n        if mtime is None:\n            continue   # no-op for files loaded via a __loader__\n        try:\n            stat = os.stat(fullname)\n        except os.error:\n            del cache[filename]\n            continue\n        if size != stat.st_size or mtime != stat.st_mtime:\n            del cache[filename]\n\n\ndef updatecache(filename, module_globals=None):\n    \"\"\"Update a cache entry and return its list of lines.\n    If something's wrong, print a message, discard the cache entry,\n    and return an empty list.\"\"\"\n\n    if filename in cache:\n        del cache[filename]\n    if not filename or (filename.startswith('<') and filename.endswith('>')):\n        return []\n\n    fullname = filename\n    try:\n        stat = os.stat(fullname)\n    except OSError:\n        basename = filename\n\n        # Try for a __loader__, if available\n        if module_globals and '__loader__' in module_globals:\n            name = module_globals.get('__name__')\n            loader = module_globals['__loader__']\n            get_source = getattr(loader, 'get_source', None)\n\n            if name and get_source:\n                try:\n                    data = get_source(name)\n                except (ImportError, IOError):\n                    pass\n                else:\n                    if data is None:\n                        # No luck, the PEP302 loader cannot find the source\n                        # for this module.\n                        return []\n                    cache[filename] = (\n                        len(data), None,\n                        [line+'\\n' for line in data.splitlines()], fullname\n                    )\n                    return cache[filename][2]\n\n        # Try looking through the module search path, which is only useful\n        # when handling a relative filename.\n        if os.path.isabs(filename):\n            return []\n\n        for dirname in sys.path:\n            # When using imputil, sys.path may contain things other than\n            # strings; ignore them when it happens.\n            try:\n                fullname = os.path.join(dirname, basename)\n            except (TypeError, AttributeError):\n                # Not sufficiently string-like to do anything useful with.\n                continue\n            try:\n                stat = os.stat(fullname)\n                break\n            except os.error:\n                pass\n        else:\n            return []\n    try:\n        with open(fullname, 'rU') as fp:\n            lines = fp.readlines()\n    except IOError:\n        return []\n    if lines and not lines[-1].endswith('\\n'):\n        lines[-1] += '\\n'\n    size, mtime = stat.st_size, stat.st_mtime\n    cache[filename] = size, mtime, lines, fullname\n    return lines\n", 
    "locale": "\"\"\" Locale support.\n\n    The module provides low-level access to the C lib's locale APIs\n    and adds high level number formatting APIs as well as a locale\n    aliasing engine to complement these.\n\n    The aliasing engine includes support for many commonly used locale\n    names and maps them to values suitable for passing to the C lib's\n    setlocale() function. It also includes default encodings for all\n    supported locale names.\n\n\"\"\"\n\nimport sys\nimport encodings\nimport encodings.aliases\nimport re\nimport operator\nimport functools\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Try importing the _locale module.\n#\n# If this fails, fall back on a basic 'C' locale emulation.\n\n# Yuck:  LC_MESSAGES is non-standard:  can't tell whether it exists before\n# trying the import.  So __all__ is also fiddled at the end of the file.\n__all__ = [\"getlocale\", \"getdefaultlocale\", \"getpreferredencoding\", \"Error\",\n           \"setlocale\", \"resetlocale\", \"localeconv\", \"strcoll\", \"strxfrm\",\n           \"str\", \"atof\", \"atoi\", \"format\", \"format_string\", \"currency\",\n           \"normalize\", \"LC_CTYPE\", \"LC_COLLATE\", \"LC_TIME\", \"LC_MONETARY\",\n           \"LC_NUMERIC\", \"LC_ALL\", \"CHAR_MAX\"]\n\ntry:\n\n    from _locale import *\n\nexcept ImportError:\n\n    # Locale emulation\n\n    CHAR_MAX = 127\n    LC_ALL = 6\n    LC_COLLATE = 3\n    LC_CTYPE = 0\n    LC_MESSAGES = 5\n    LC_MONETARY = 4\n    LC_NUMERIC = 1\n    LC_TIME = 2\n    Error = ValueError\n\n    def localeconv():\n        \"\"\" localeconv() -> dict.\n            Returns numeric and monetary locale-specific parameters.\n        \"\"\"\n        # 'C' locale default values\n        return {'grouping': [127],\n                'currency_symbol': '',\n                'n_sign_posn': 127,\n                'p_cs_precedes': 127,\n                'n_cs_precedes': 127,\n                'mon_grouping': [],\n                'n_sep_by_space': 127,\n                'decimal_point': '.',\n                'negative_sign': '',\n                'positive_sign': '',\n                'p_sep_by_space': 127,\n                'int_curr_symbol': '',\n                'p_sign_posn': 127,\n                'thousands_sep': '',\n                'mon_thousands_sep': '',\n                'frac_digits': 127,\n                'mon_decimal_point': '',\n                'int_frac_digits': 127}\n\n    def setlocale(category, value=None):\n        \"\"\" setlocale(integer,string=None) -> string.\n            Activates/queries locale processing.\n        \"\"\"\n        if value not in (None, '', 'C'):\n            raise Error, '_locale emulation only supports \"C\" locale'\n        return 'C'\n\n    def strcoll(a,b):\n        \"\"\" strcoll(string,string) -> int.\n            Compares two strings according to the locale.\n        \"\"\"\n        return cmp(a,b)\n\n    def strxfrm(s):\n        \"\"\" strxfrm(string) -> string.\n            Returns a string that behaves for cmp locale-aware.\n        \"\"\"\n        return s\n\n\n_localeconv = localeconv\n\n# With this dict, you can override some items of localeconv's return value.\n# This is useful for testing purposes.\n_override_localeconv = {}\n\n@functools.wraps(_localeconv)\ndef localeconv():\n    d = _localeconv()\n    if _override_localeconv:\n        d.update(_override_localeconv)\n    return d\n\n\n### Number formatting APIs\n\n# Author: Martin von Loewis\n# improved by Georg Brandl\n\n# Iterate over grouping intervals\ndef _grouping_intervals(grouping):\n    last_interval = None\n    for interval in grouping:\n        # if grouping is -1, we are done\n        if interval == CHAR_MAX:\n            return\n        # 0: re-use last group ad infinitum\n        if interval == 0:\n            if last_interval is None:\n                raise ValueError(\"invalid grouping\")\n            while True:\n                yield last_interval\n        yield interval\n        last_interval = interval\n\n#perform the grouping from right to left\ndef _group(s, monetary=False):\n    conv = localeconv()\n    thousands_sep = conv[monetary and 'mon_thousands_sep' or 'thousands_sep']\n    grouping = conv[monetary and 'mon_grouping' or 'grouping']\n    if not grouping:\n        return (s, 0)\n    if s[-1] == ' ':\n        stripped = s.rstrip()\n        right_spaces = s[len(stripped):]\n        s = stripped\n    else:\n        right_spaces = ''\n    left_spaces = ''\n    groups = []\n    for interval in _grouping_intervals(grouping):\n        if not s or s[-1] not in \"0123456789\":\n            # only non-digit characters remain (sign, spaces)\n            left_spaces = s\n            s = ''\n            break\n        groups.append(s[-interval:])\n        s = s[:-interval]\n    if s:\n        groups.append(s)\n    groups.reverse()\n    return (\n        left_spaces + thousands_sep.join(groups) + right_spaces,\n        len(thousands_sep) * (len(groups) - 1)\n    )\n\n# Strip a given amount of excess padding from the given string\ndef _strip_padding(s, amount):\n    lpos = 0\n    while amount and s[lpos] == ' ':\n        lpos += 1\n        amount -= 1\n    rpos = len(s) - 1\n    while amount and s[rpos] == ' ':\n        rpos -= 1\n        amount -= 1\n    return s[lpos:rpos+1]\n\n_percent_re = re.compile(r'%(?:\\((?P<key>.*?)\\))?'\n                         r'(?P<modifiers>[-#0-9 +*.hlL]*?)[eEfFgGdiouxXcrs%]')\n\ndef format(percent, value, grouping=False, monetary=False, *additional):\n    \"\"\"Returns the locale-aware substitution of a %? specifier\n    (percent).\n\n    additional is for format strings which contain one or more\n    '*' modifiers.\"\"\"\n    # this is only for one-percent-specifier strings and this should be checked\n    match = _percent_re.match(percent)\n    if not match or len(match.group())!= len(percent):\n        raise ValueError((\"format() must be given exactly one %%char \"\n                         \"format specifier, %s not valid\") % repr(percent))\n    return _format(percent, value, grouping, monetary, *additional)\n\ndef _format(percent, value, grouping=False, monetary=False, *additional):\n    if additional:\n        formatted = percent % ((value,) + additional)\n    else:\n        formatted = percent % value\n    # floats and decimal ints need special action!\n    if percent[-1] in 'eEfFgG':\n        seps = 0\n        parts = formatted.split('.')\n        if grouping:\n            parts[0], seps = _group(parts[0], monetary=monetary)\n        decimal_point = localeconv()[monetary and 'mon_decimal_point'\n                                              or 'decimal_point']\n        formatted = decimal_point.join(parts)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    elif percent[-1] in 'diu':\n        seps = 0\n        if grouping:\n            formatted, seps = _group(formatted, monetary=monetary)\n        if seps:\n            formatted = _strip_padding(formatted, seps)\n    return formatted\n\ndef format_string(f, val, grouping=False):\n    \"\"\"Formats a string in the same way that the % formatting would use,\n    but takes the current locale into account.\n    Grouping is applied if the third parameter is true.\"\"\"\n    percents = list(_percent_re.finditer(f))\n    new_f = _percent_re.sub('%s', f)\n\n    if operator.isMappingType(val):\n        new_val = []\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                new_val.append(format(perc.group(), val, grouping))\n    else:\n        if not isinstance(val, tuple):\n            val = (val,)\n        new_val = []\n        i = 0\n        for perc in percents:\n            if perc.group()[-1]=='%':\n                new_val.append('%')\n            else:\n                starcount = perc.group('modifiers').count('*')\n                new_val.append(_format(perc.group(),\n                                      val[i],\n                                      grouping,\n                                      False,\n                                      *val[i+1:i+1+starcount]))\n                i += (1 + starcount)\n    val = tuple(new_val)\n\n    return new_f % val\n\ndef currency(val, symbol=True, grouping=False, international=False):\n    \"\"\"Formats val according to the currency settings\n    in the current locale.\"\"\"\n    conv = localeconv()\n\n    # check for illegal values\n    digits = conv[international and 'int_frac_digits' or 'frac_digits']\n    if digits == 127:\n        raise ValueError(\"Currency formatting is not possible using \"\n                         \"the 'C' locale.\")\n\n    s = format('%%.%if' % digits, abs(val), grouping, monetary=True)\n    # '<' and '>' are markers if the sign must be inserted between symbol and value\n    s = '<' + s + '>'\n\n    if symbol:\n        smb = conv[international and 'int_curr_symbol' or 'currency_symbol']\n        precedes = conv[val<0 and 'n_cs_precedes' or 'p_cs_precedes']\n        separated = conv[val<0 and 'n_sep_by_space' or 'p_sep_by_space']\n\n        if precedes:\n            s = smb + (separated and ' ' or '') + s\n        else:\n            s = s + (separated and ' ' or '') + smb\n\n    sign_pos = conv[val<0 and 'n_sign_posn' or 'p_sign_posn']\n    sign = conv[val<0 and 'negative_sign' or 'positive_sign']\n\n    if sign_pos == 0:\n        s = '(' + s + ')'\n    elif sign_pos == 1:\n        s = sign + s\n    elif sign_pos == 2:\n        s = s + sign\n    elif sign_pos == 3:\n        s = s.replace('<', sign)\n    elif sign_pos == 4:\n        s = s.replace('>', sign)\n    else:\n        # the default if nothing specified;\n        # this should be the most fitting sign position\n        s = sign + s\n\n    return s.replace('<', '').replace('>', '')\n\ndef str(val):\n    \"\"\"Convert float to integer, taking the locale into account.\"\"\"\n    return format(\"%.12g\", val)\n\ndef atof(string, func=float):\n    \"Parses a string as a float according to the locale settings.\"\n    #First, get rid of the grouping\n    ts = localeconv()['thousands_sep']\n    if ts:\n        string = string.replace(ts, '')\n    #next, replace the decimal point with a dot\n    dd = localeconv()['decimal_point']\n    if dd:\n        string = string.replace(dd, '.')\n    #finally, parse the string\n    return func(string)\n\ndef atoi(str):\n    \"Converts a string to an integer according to the locale settings.\"\n    return atof(str, int)\n\ndef _test():\n    setlocale(LC_ALL, \"\")\n    #do grouping\n    s1 = format(\"%d\", 123456789,1)\n    print s1, \"is\", atoi(s1)\n    #standard formatting\n    s1 = str(3.14)\n    print s1, \"is\", atof(s1)\n\n### Locale name aliasing engine\n\n# Author: Marc-Andre Lemburg, mal@lemburg.com\n# Various tweaks by Fredrik Lundh <fredrik@pythonware.com>\n\n# store away the low-level version of setlocale (it's\n# overridden below)\n_setlocale = setlocale\n\n# Avoid relying on the locale-dependent .lower() method\n# (see issue #1813).\n_ascii_lower_map = ''.join(\n    chr(x + 32 if x >= ord('A') and x <= ord('Z') else x)\n    for x in range(256)\n)\n\ndef _replace_encoding(code, encoding):\n    if '.' in code:\n        langname = code[:code.index('.')]\n    else:\n        langname = code\n    # Convert the encoding to a C lib compatible encoding string\n    norm_encoding = encodings.normalize_encoding(encoding)\n    #print('norm encoding: %r' % norm_encoding)\n    norm_encoding = encodings.aliases.aliases.get(norm_encoding,\n                                                  norm_encoding)\n    #print('aliased encoding: %r' % norm_encoding)\n    encoding = locale_encoding_alias.get(norm_encoding,\n                                         norm_encoding)\n    #print('found encoding %r' % encoding)\n    return langname + '.' + encoding\n\ndef normalize(localename):\n\n    \"\"\" Returns a normalized locale code for the given locale\n        name.\n\n        The returned locale code is formatted for use with\n        setlocale().\n\n        If normalization fails, the original name is returned\n        unchanged.\n\n        If the given encoding is not known, the function defaults to\n        the default encoding for the locale code just like setlocale()\n        does.\n\n    \"\"\"\n    # Normalize the locale name and extract the encoding and modifier\n    if isinstance(localename, _unicode):\n        localename = localename.encode('ascii')\n    code = localename.translate(_ascii_lower_map)\n    if ':' in code:\n        # ':' is sometimes used as encoding delimiter.\n        code = code.replace(':', '.')\n    if '@' in code:\n        code, modifier = code.split('@', 1)\n    else:\n        modifier = ''\n    if '.' in code:\n        langname, encoding = code.split('.')[:2]\n    else:\n        langname = code\n        encoding = ''\n\n    # First lookup: fullname (possibly with encoding and modifier)\n    lang_enc = langname\n    if encoding:\n        norm_encoding = encoding.replace('-', '')\n        norm_encoding = norm_encoding.replace('_', '')\n        lang_enc += '.' + norm_encoding\n    lookup_name = lang_enc\n    if modifier:\n        lookup_name += '@' + modifier\n    code = locale_alias.get(lookup_name, None)\n    if code is not None:\n        return code\n    #print('first lookup failed')\n\n    if modifier:\n        # Second try: fullname without modifier (possibly with encoding)\n        code = locale_alias.get(lang_enc, None)\n        if code is not None:\n            #print('lookup without modifier succeeded')\n            if '@' not in code:\n                return code + '@' + modifier\n            if code.split('@', 1)[1].translate(_ascii_lower_map) == modifier:\n                return code\n        #print('second lookup failed')\n\n    if encoding:\n        # Third try: langname (without encoding, possibly with modifier)\n        lookup_name = langname\n        if modifier:\n            lookup_name += '@' + modifier\n        code = locale_alias.get(lookup_name, None)\n        if code is not None:\n            #print('lookup without encoding succeeded')\n            if '@' not in code:\n                return _replace_encoding(code, encoding)\n            code, modifier = code.split('@', 1)\n            return _replace_encoding(code, encoding) + '@' + modifier\n\n        if modifier:\n            # Fourth try: langname (without encoding and modifier)\n            code = locale_alias.get(langname, None)\n            if code is not None:\n                #print('lookup without modifier and encoding succeeded')\n                if '@' not in code:\n                    return _replace_encoding(code, encoding) + '@' + modifier\n                code, defmod = code.split('@', 1)\n                if defmod.translate(_ascii_lower_map) == modifier:\n                    return _replace_encoding(code, encoding) + '@' + defmod\n\n    return localename\n\ndef _parse_localename(localename):\n\n    \"\"\" Parses the locale code for localename and returns the\n        result as tuple (language code, encoding).\n\n        The localename is normalized and passed through the locale\n        alias engine. A ValueError is raised in case the locale name\n        cannot be parsed.\n\n        The language code corresponds to RFC 1766.  code and encoding\n        can be None in case the values cannot be determined or are\n        unknown to this implementation.\n\n    \"\"\"\n    code = normalize(localename)\n    if '@' in code:\n        # Deal with locale modifiers\n        code, modifier = code.split('@', 1)\n        if modifier == 'euro' and '.' not in code:\n            # Assume Latin-9 for @euro locales. This is bogus,\n            # since some systems may use other encodings for these\n            # locales. Also, we ignore other modifiers.\n            return code, 'iso-8859-15'\n\n    if '.' in code:\n        return tuple(code.split('.')[:2])\n    elif code == 'C':\n        return None, None\n    raise ValueError, 'unknown locale: %s' % localename\n\ndef _build_localename(localetuple):\n\n    \"\"\" Builds a locale code from the given tuple (language code,\n        encoding).\n\n        No aliasing or normalizing takes place.\n\n    \"\"\"\n    language, encoding = localetuple\n    if language is None:\n        language = 'C'\n    if encoding is None:\n        return language\n    else:\n        return language + '.' + encoding\n\ndef getdefaultlocale(envvars=('LC_ALL', 'LC_CTYPE', 'LANG', 'LANGUAGE')):\n\n    \"\"\" Tries to determine the default locale settings and returns\n        them as tuple (language code, encoding).\n\n        According to POSIX, a program which has not called\n        setlocale(LC_ALL, \"\") runs using the portable 'C' locale.\n        Calling setlocale(LC_ALL, \"\") lets it use the default locale as\n        defined by the LANG variable. Since we don't want to interfere\n        with the current locale setting we thus emulate the behavior\n        in the way described above.\n\n        To maintain compatibility with other platforms, not only the\n        LANG variable is tested, but a list of variables given as\n        envvars parameter. The first found to be defined will be\n        used. envvars defaults to the search path used in GNU gettext;\n        it must always contain the variable name 'LANG'.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n\n    try:\n        # check if it's supported by the _locale module\n        import _locale\n        code, encoding = _locale._getdefaultlocale()\n    except (ImportError, AttributeError):\n        pass\n    else:\n        # make sure the code/encoding values are valid\n        if sys.platform == \"win32\" and code and code[:2] == \"0x\":\n            # map windows language identifier to language name\n            code = windows_locale.get(int(code, 0))\n        # ...add other platform-specific processing here, if\n        # necessary...\n        return code, encoding\n\n    # fall back on POSIX behaviour\n    import os\n    lookup = os.environ.get\n    for variable in envvars:\n        localename = lookup(variable,None)\n        if localename:\n            if variable == 'LANGUAGE':\n                localename = localename.split(':')[0]\n            break\n    else:\n        localename = 'C'\n    return _parse_localename(localename)\n\n\ndef getlocale(category=LC_CTYPE):\n\n    \"\"\" Returns the current setting for the given locale category as\n        tuple (language code, encoding).\n\n        category may be one of the LC_* value except LC_ALL. It\n        defaults to LC_CTYPE.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n    localename = _setlocale(category)\n    if category == LC_ALL and ';' in localename:\n        raise TypeError, 'category LC_ALL is not supported'\n    return _parse_localename(localename)\n\ndef setlocale(category, locale=None):\n\n    \"\"\" Set the locale for the given category.  The locale can be\n        a string, an iterable of two strings (language code and encoding),\n        or None.\n\n        Iterables are converted to strings using the locale aliasing\n        engine.  Locale strings are passed directly to the C lib.\n\n        category may be given as one of the LC_* values.\n\n    \"\"\"\n    if locale and type(locale) is not type(\"\"):\n        # convert to string\n        locale = normalize(_build_localename(locale))\n    return _setlocale(category, locale)\n\ndef resetlocale(category=LC_ALL):\n\n    \"\"\" Sets the locale for category to the default setting.\n\n        The default setting is determined by calling\n        getdefaultlocale(). category defaults to LC_ALL.\n\n    \"\"\"\n    _setlocale(category, _build_localename(getdefaultlocale()))\n\nif sys.platform.startswith(\"win\"):\n    # On Win32, this will return the ANSI code page\n    def getpreferredencoding(do_setlocale = True):\n        \"\"\"Return the charset that the user is likely using.\"\"\"\n        import _locale\n        return _locale._getdefaultlocale()[1]\nelse:\n    # On Unix, if CODESET is available, use that.\n    try:\n        CODESET\n    except NameError:\n        # Fall back to parsing environment variables :-(\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            by looking at environment variables.\"\"\"\n            return getdefaultlocale()[1]\n    else:\n        def getpreferredencoding(do_setlocale = True):\n            \"\"\"Return the charset that the user is likely using,\n            according to the system configuration.\"\"\"\n            if do_setlocale:\n                oldloc = setlocale(LC_CTYPE)\n                try:\n                    setlocale(LC_CTYPE, \"\")\n                except Error:\n                    pass\n                result = nl_langinfo(CODESET)\n                setlocale(LC_CTYPE, oldloc)\n                return result\n            else:\n                return nl_langinfo(CODESET)\n\n\n### Database\n#\n# The following data was extracted from the locale.alias file which\n# comes with X11 and then hand edited removing the explicit encoding\n# definitions and adding some more aliases. The file is usually\n# available as /usr/lib/X11/locale/locale.alias.\n#\n\n#\n# The local_encoding_alias table maps lowercase encoding alias names\n# to C locale encoding names (case-sensitive). Note that normalize()\n# first looks up the encoding in the encodings.aliases dictionary and\n# then applies this mapping to find the correct C lib name for the\n# encoding.\n#\nlocale_encoding_alias = {\n\n    # Mappings for non-standard encoding names used in locale names\n    '437':                          'C',\n    'c':                            'C',\n    'en':                           'ISO8859-1',\n    'jis':                          'JIS7',\n    'jis7':                         'JIS7',\n    'ajec':                         'eucJP',\n\n    # Mappings from Python codec names to C lib encoding names\n    'ascii':                        'ISO8859-1',\n    'latin_1':                      'ISO8859-1',\n    'iso8859_1':                    'ISO8859-1',\n    'iso8859_10':                   'ISO8859-10',\n    'iso8859_11':                   'ISO8859-11',\n    'iso8859_13':                   'ISO8859-13',\n    'iso8859_14':                   'ISO8859-14',\n    'iso8859_15':                   'ISO8859-15',\n    'iso8859_16':                   'ISO8859-16',\n    'iso8859_2':                    'ISO8859-2',\n    'iso8859_3':                    'ISO8859-3',\n    'iso8859_4':                    'ISO8859-4',\n    'iso8859_5':                    'ISO8859-5',\n    'iso8859_6':                    'ISO8859-6',\n    'iso8859_7':                    'ISO8859-7',\n    'iso8859_8':                    'ISO8859-8',\n    'iso8859_9':                    'ISO8859-9',\n    'iso2022_jp':                   'JIS7',\n    'shift_jis':                    'SJIS',\n    'tactis':                       'TACTIS',\n    'euc_jp':                       'eucJP',\n    'euc_kr':                       'eucKR',\n    'utf_8':                        'UTF-8',\n    'koi8_r':                       'KOI8-R',\n    'koi8_u':                       'KOI8-U',\n    # XXX This list is still incomplete. If you know more\n    # mappings, please file a bug report. Thanks.\n}\n\n#\n# The locale_alias table maps lowercase alias names to C locale names\n# (case-sensitive). Encodings are always separated from the locale\n# name using a dot ('.'); they should only be given in case the\n# language name is needed to interpret the given encoding alias\n# correctly (CJK codes often have this need).\n#\n# Note that the normalize() function which uses this tables\n# removes '_' and '-' characters from the encoding part of the\n# locale name before doing the lookup. This saves a lot of\n# space in the table.\n#\n# MAL 2004-12-10:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.4\n# and older):\n#\n#    updated 'bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bg_bg' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'bulgarian' -> 'bg_BG.ISO8859-5' to 'bg_BG.CP1251'\n#    updated 'cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'cz_cz' -> 'cz_CZ.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'czech' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'dutch' -> 'nl_BE.ISO8859-1' to 'nl_NL.ISO8859-1'\n#    updated 'et' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'et_ee' -> 'et_EE.ISO8859-4' to 'et_EE.ISO8859-15'\n#    updated 'fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'fi_fi' -> 'fi_FI.ISO8859-1' to 'fi_FI.ISO8859-15'\n#    updated 'iw' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'iw_il' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'japanese' -> 'ja_JP.SJIS' to 'ja_JP.eucJP'\n#    updated 'lt' -> 'lt_LT.ISO8859-4' to 'lt_LT.ISO8859-13'\n#    updated 'lv' -> 'lv_LV.ISO8859-4' to 'lv_LV.ISO8859-13'\n#    updated 'sl' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'slovene' -> 'sl_CS.ISO8859-2' to 'sl_SI.ISO8859-2'\n#    updated 'th_th' -> 'th_TH.TACTIS' to 'th_TH.ISO8859-11'\n#    updated 'zh_cn' -> 'zh_CN.eucCN' to 'zh_CN.gb2312'\n#    updated 'zh_cn.big5' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#    updated 'zh_tw' -> 'zh_TW.eucTW' to 'zh_TW.big5'\n#\n# MAL 2008-05-30:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.5\n# and older):\n#\n#    updated 'cs_cs.iso88592' -> 'cs_CZ.ISO8859-2' to 'cs_CS.ISO8859-2'\n#    updated 'serbocroatian' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_hr.iso88592' -> 'sh_HR.ISO8859-2' to 'hr_HR.ISO8859-2'\n#    updated 'sh_sp' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sh_yu' -> 'sh_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sp' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sp_yu' -> 'sp_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_sp' -> 'sr_SP.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.cp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.iso88592' -> 'sr_YU.ISO8859-2' to 'sr_CS.ISO8859-2'\n#    updated 'sr_yu.iso88595' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.iso88595@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#    updated 'sr_yu.microsoftcp1251@cyrillic' -> 'sr_YU.CP1251' to 'sr_CS.CP1251'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_YU.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_YU.ISO8859-5' to 'sr_CS.ISO8859-5'\n#\n# AP 2010-04-12:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.6.5\n# and older):\n#\n#    updated 'ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'ru_ru' -> 'ru_RU.ISO8859-5' to 'ru_RU.UTF-8'\n#    updated 'serbocroatian' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sh_yu' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#    updated 'sr@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs.utf8@latn' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_CS.ISO8859-2' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8@latin'\n#    updated 'sr_yu.utf8@cyrillic' -> 'sr_CS.UTF-8' to 'sr_RS.UTF-8'\n#    updated 'sr_yu@cyrillic' -> 'sr_CS.ISO8859-5' to 'sr_RS.UTF-8'\n#\n# SS 2013-12-20:\n# Updated alias mapping to most recent locale.alias file\n# from X.org distribution using makelocalealias.py.\n#\n# These are the differences compared to the old mapping (Python 2.7.6\n# and older):\n#\n#    updated 'a3' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'a3_az.koi8c' -> 'a3_AZ.KOI8-C' to 'az_AZ.KOI8-C'\n#    updated 'cs_cs.iso88592' -> 'cs_CS.ISO8859-2' to 'cs_CZ.ISO8859-2'\n#    updated 'hebrew' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'hebrew.iso88598' -> 'iw_IL.ISO8859-8' to 'he_IL.ISO8859-8'\n#    updated 'sd' -> 'sd_IN@devanagari.UTF-8' to 'sd_IN.UTF-8'\n#    updated 'sr@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs' -> 'sr_RS.UTF-8' to 'sr_CS.UTF-8'\n#    updated 'sr_cs.utf8@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#    updated 'sr_cs@latn' -> 'sr_RS.UTF-8@latin' to 'sr_CS.UTF-8@latin'\n#\n# SS 2014-10-01:\n# Updated alias mapping with glibc 2.19 supported locales.\n\nlocale_alias = {\n    'a3':                                   'az_AZ.KOI8-C',\n    'a3_az':                                'az_AZ.KOI8-C',\n    'a3_az.koi8c':                          'az_AZ.KOI8-C',\n    'a3_az.koic':                           'az_AZ.KOI8-C',\n    'aa_dj':                                'aa_DJ.ISO8859-1',\n    'aa_er':                                'aa_ER.UTF-8',\n    'aa_et':                                'aa_ET.UTF-8',\n    'af':                                   'af_ZA.ISO8859-1',\n    'af_za':                                'af_ZA.ISO8859-1',\n    'af_za.iso88591':                       'af_ZA.ISO8859-1',\n    'am':                                   'am_ET.UTF-8',\n    'am_et':                                'am_ET.UTF-8',\n    'american':                             'en_US.ISO8859-1',\n    'american.iso88591':                    'en_US.ISO8859-1',\n    'an_es':                                'an_ES.ISO8859-15',\n    'ar':                                   'ar_AA.ISO8859-6',\n    'ar_aa':                                'ar_AA.ISO8859-6',\n    'ar_aa.iso88596':                       'ar_AA.ISO8859-6',\n    'ar_ae':                                'ar_AE.ISO8859-6',\n    'ar_ae.iso88596':                       'ar_AE.ISO8859-6',\n    'ar_bh':                                'ar_BH.ISO8859-6',\n    'ar_bh.iso88596':                       'ar_BH.ISO8859-6',\n    'ar_dz':                                'ar_DZ.ISO8859-6',\n    'ar_dz.iso88596':                       'ar_DZ.ISO8859-6',\n    'ar_eg':                                'ar_EG.ISO8859-6',\n    'ar_eg.iso88596':                       'ar_EG.ISO8859-6',\n    'ar_in':                                'ar_IN.UTF-8',\n    'ar_iq':                                'ar_IQ.ISO8859-6',\n    'ar_iq.iso88596':                       'ar_IQ.ISO8859-6',\n    'ar_jo':                                'ar_JO.ISO8859-6',\n    'ar_jo.iso88596':                       'ar_JO.ISO8859-6',\n    'ar_kw':                                'ar_KW.ISO8859-6',\n    'ar_kw.iso88596':                       'ar_KW.ISO8859-6',\n    'ar_lb':                                'ar_LB.ISO8859-6',\n    'ar_lb.iso88596':                       'ar_LB.ISO8859-6',\n    'ar_ly':                                'ar_LY.ISO8859-6',\n    'ar_ly.iso88596':                       'ar_LY.ISO8859-6',\n    'ar_ma':                                'ar_MA.ISO8859-6',\n    'ar_ma.iso88596':                       'ar_MA.ISO8859-6',\n    'ar_om':                                'ar_OM.ISO8859-6',\n    'ar_om.iso88596':                       'ar_OM.ISO8859-6',\n    'ar_qa':                                'ar_QA.ISO8859-6',\n    'ar_qa.iso88596':                       'ar_QA.ISO8859-6',\n    'ar_sa':                                'ar_SA.ISO8859-6',\n    'ar_sa.iso88596':                       'ar_SA.ISO8859-6',\n    'ar_sd':                                'ar_SD.ISO8859-6',\n    'ar_sd.iso88596':                       'ar_SD.ISO8859-6',\n    'ar_sy':                                'ar_SY.ISO8859-6',\n    'ar_sy.iso88596':                       'ar_SY.ISO8859-6',\n    'ar_tn':                                'ar_TN.ISO8859-6',\n    'ar_tn.iso88596':                       'ar_TN.ISO8859-6',\n    'ar_ye':                                'ar_YE.ISO8859-6',\n    'ar_ye.iso88596':                       'ar_YE.ISO8859-6',\n    'arabic':                               'ar_AA.ISO8859-6',\n    'arabic.iso88596':                      'ar_AA.ISO8859-6',\n    'as':                                   'as_IN.UTF-8',\n    'as_in':                                'as_IN.UTF-8',\n    'ast_es':                               'ast_ES.ISO8859-15',\n    'ayc_pe':                               'ayc_PE.UTF-8',\n    'az':                                   'az_AZ.ISO8859-9E',\n    'az_az':                                'az_AZ.ISO8859-9E',\n    'az_az.iso88599e':                      'az_AZ.ISO8859-9E',\n    'be':                                   'be_BY.CP1251',\n    'be@latin':                             'be_BY.UTF-8@latin',\n    'be_bg.utf8':                           'bg_BG.UTF-8',\n    'be_by':                                'be_BY.CP1251',\n    'be_by.cp1251':                         'be_BY.CP1251',\n    'be_by.microsoftcp1251':                'be_BY.CP1251',\n    'be_by.utf8@latin':                     'be_BY.UTF-8@latin',\n    'be_by@latin':                          'be_BY.UTF-8@latin',\n    'bem_zm':                               'bem_ZM.UTF-8',\n    'ber_dz':                               'ber_DZ.UTF-8',\n    'ber_ma':                               'ber_MA.UTF-8',\n    'bg':                                   'bg_BG.CP1251',\n    'bg_bg':                                'bg_BG.CP1251',\n    'bg_bg.cp1251':                         'bg_BG.CP1251',\n    'bg_bg.iso88595':                       'bg_BG.ISO8859-5',\n    'bg_bg.koi8r':                          'bg_BG.KOI8-R',\n    'bg_bg.microsoftcp1251':                'bg_BG.CP1251',\n    'bho_in':                               'bho_IN.UTF-8',\n    'bn_bd':                                'bn_BD.UTF-8',\n    'bn_in':                                'bn_IN.UTF-8',\n    'bo_cn':                                'bo_CN.UTF-8',\n    'bo_in':                                'bo_IN.UTF-8',\n    'bokmal':                               'nb_NO.ISO8859-1',\n    'bokm\\xe5l':                            'nb_NO.ISO8859-1',\n    'br':                                   'br_FR.ISO8859-1',\n    'br_fr':                                'br_FR.ISO8859-1',\n    'br_fr.iso88591':                       'br_FR.ISO8859-1',\n    'br_fr.iso885914':                      'br_FR.ISO8859-14',\n    'br_fr.iso885915':                      'br_FR.ISO8859-15',\n    'br_fr.iso885915@euro':                 'br_FR.ISO8859-15',\n    'br_fr.utf8@euro':                      'br_FR.UTF-8',\n    'br_fr@euro':                           'br_FR.ISO8859-15',\n    'brx_in':                               'brx_IN.UTF-8',\n    'bs':                                   'bs_BA.ISO8859-2',\n    'bs_ba':                                'bs_BA.ISO8859-2',\n    'bs_ba.iso88592':                       'bs_BA.ISO8859-2',\n    'bulgarian':                            'bg_BG.CP1251',\n    'byn_er':                               'byn_ER.UTF-8',\n    'c':                                    'C',\n    'c-french':                             'fr_CA.ISO8859-1',\n    'c-french.iso88591':                    'fr_CA.ISO8859-1',\n    'c.ascii':                              'C',\n    'c.en':                                 'C',\n    'c.iso88591':                           'en_US.ISO8859-1',\n    'c.utf8':                               'en_US.UTF-8',\n    'c_c':                                  'C',\n    'c_c.c':                                'C',\n    'ca':                                   'ca_ES.ISO8859-1',\n    'ca_ad':                                'ca_AD.ISO8859-1',\n    'ca_ad.iso88591':                       'ca_AD.ISO8859-1',\n    'ca_ad.iso885915':                      'ca_AD.ISO8859-15',\n    'ca_ad.iso885915@euro':                 'ca_AD.ISO8859-15',\n    'ca_ad.utf8@euro':                      'ca_AD.UTF-8',\n    'ca_ad@euro':                           'ca_AD.ISO8859-15',\n    'ca_es':                                'ca_ES.ISO8859-1',\n    'ca_es.iso88591':                       'ca_ES.ISO8859-1',\n    'ca_es.iso885915':                      'ca_ES.ISO8859-15',\n    'ca_es.iso885915@euro':                 'ca_ES.ISO8859-15',\n    'ca_es.utf8@euro':                      'ca_ES.UTF-8',\n    'ca_es@valencia':                       'ca_ES.ISO8859-15@valencia',\n    'ca_es@euro':                           'ca_ES.ISO8859-15',\n    'ca_fr':                                'ca_FR.ISO8859-1',\n    'ca_fr.iso88591':                       'ca_FR.ISO8859-1',\n    'ca_fr.iso885915':                      'ca_FR.ISO8859-15',\n    'ca_fr.iso885915@euro':                 'ca_FR.ISO8859-15',\n    'ca_fr.utf8@euro':                      'ca_FR.UTF-8',\n    'ca_fr@euro':                           'ca_FR.ISO8859-15',\n    'ca_it':                                'ca_IT.ISO8859-1',\n    'ca_it.iso88591':                       'ca_IT.ISO8859-1',\n    'ca_it.iso885915':                      'ca_IT.ISO8859-15',\n    'ca_it.iso885915@euro':                 'ca_IT.ISO8859-15',\n    'ca_it.utf8@euro':                      'ca_IT.UTF-8',\n    'ca_it@euro':                           'ca_IT.ISO8859-15',\n    'catalan':                              'ca_ES.ISO8859-1',\n    'cextend':                              'en_US.ISO8859-1',\n    'cextend.en':                           'en_US.ISO8859-1',\n    'chinese-s':                            'zh_CN.eucCN',\n    'chinese-t':                            'zh_TW.eucTW',\n    'crh_ua':                               'crh_UA.UTF-8',\n    'croatian':                             'hr_HR.ISO8859-2',\n    'cs':                                   'cs_CZ.ISO8859-2',\n    'cs_cs':                                'cs_CZ.ISO8859-2',\n    'cs_cs.iso88592':                       'cs_CZ.ISO8859-2',\n    'cs_cz':                                'cs_CZ.ISO8859-2',\n    'cs_cz.iso88592':                       'cs_CZ.ISO8859-2',\n    'csb_pl':                               'csb_PL.UTF-8',\n    'cv_ru':                                'cv_RU.UTF-8',\n    'cy':                                   'cy_GB.ISO8859-1',\n    'cy_gb':                                'cy_GB.ISO8859-1',\n    'cy_gb.iso88591':                       'cy_GB.ISO8859-1',\n    'cy_gb.iso885914':                      'cy_GB.ISO8859-14',\n    'cy_gb.iso885915':                      'cy_GB.ISO8859-15',\n    'cy_gb@euro':                           'cy_GB.ISO8859-15',\n    'cz':                                   'cs_CZ.ISO8859-2',\n    'cz_cz':                                'cs_CZ.ISO8859-2',\n    'czech':                                'cs_CZ.ISO8859-2',\n    'da':                                   'da_DK.ISO8859-1',\n    'da.iso885915':                         'da_DK.ISO8859-15',\n    'da_dk':                                'da_DK.ISO8859-1',\n    'da_dk.88591':                          'da_DK.ISO8859-1',\n    'da_dk.885915':                         'da_DK.ISO8859-15',\n    'da_dk.iso88591':                       'da_DK.ISO8859-1',\n    'da_dk.iso885915':                      'da_DK.ISO8859-15',\n    'da_dk@euro':                           'da_DK.ISO8859-15',\n    'danish':                               'da_DK.ISO8859-1',\n    'danish.iso88591':                      'da_DK.ISO8859-1',\n    'dansk':                                'da_DK.ISO8859-1',\n    'de':                                   'de_DE.ISO8859-1',\n    'de.iso885915':                         'de_DE.ISO8859-15',\n    'de_at':                                'de_AT.ISO8859-1',\n    'de_at.iso88591':                       'de_AT.ISO8859-1',\n    'de_at.iso885915':                      'de_AT.ISO8859-15',\n    'de_at.iso885915@euro':                 'de_AT.ISO8859-15',\n    'de_at.utf8@euro':                      'de_AT.UTF-8',\n    'de_at@euro':                           'de_AT.ISO8859-15',\n    'de_be':                                'de_BE.ISO8859-1',\n    'de_be.iso88591':                       'de_BE.ISO8859-1',\n    'de_be.iso885915':                      'de_BE.ISO8859-15',\n    'de_be.iso885915@euro':                 'de_BE.ISO8859-15',\n    'de_be.utf8@euro':                      'de_BE.UTF-8',\n    'de_be@euro':                           'de_BE.ISO8859-15',\n    'de_ch':                                'de_CH.ISO8859-1',\n    'de_ch.iso88591':                       'de_CH.ISO8859-1',\n    'de_ch.iso885915':                      'de_CH.ISO8859-15',\n    'de_ch@euro':                           'de_CH.ISO8859-15',\n    'de_de':                                'de_DE.ISO8859-1',\n    'de_de.88591':                          'de_DE.ISO8859-1',\n    'de_de.885915':                         'de_DE.ISO8859-15',\n    'de_de.885915@euro':                    'de_DE.ISO8859-15',\n    'de_de.iso88591':                       'de_DE.ISO8859-1',\n    'de_de.iso885915':                      'de_DE.ISO8859-15',\n    'de_de.iso885915@euro':                 'de_DE.ISO8859-15',\n    'de_de.utf8@euro':                      'de_DE.UTF-8',\n    'de_de@euro':                           'de_DE.ISO8859-15',\n    'de_li.utf8':                           'de_LI.UTF-8',\n    'de_lu':                                'de_LU.ISO8859-1',\n    'de_lu.iso88591':                       'de_LU.ISO8859-1',\n    'de_lu.iso885915':                      'de_LU.ISO8859-15',\n    'de_lu.iso885915@euro':                 'de_LU.ISO8859-15',\n    'de_lu.utf8@euro':                      'de_LU.UTF-8',\n    'de_lu@euro':                           'de_LU.ISO8859-15',\n    'deutsch':                              'de_DE.ISO8859-1',\n    'doi_in':                               'doi_IN.UTF-8',\n    'dutch':                                'nl_NL.ISO8859-1',\n    'dutch.iso88591':                       'nl_BE.ISO8859-1',\n    'dv_mv':                                'dv_MV.UTF-8',\n    'dz_bt':                                'dz_BT.UTF-8',\n    'ee':                                   'ee_EE.ISO8859-4',\n    'ee_ee':                                'ee_EE.ISO8859-4',\n    'ee_ee.iso88594':                       'ee_EE.ISO8859-4',\n    'eesti':                                'et_EE.ISO8859-1',\n    'el':                                   'el_GR.ISO8859-7',\n    'el_cy':                                'el_CY.ISO8859-7',\n    'el_gr':                                'el_GR.ISO8859-7',\n    'el_gr.iso88597':                       'el_GR.ISO8859-7',\n    'el_gr@euro':                           'el_GR.ISO8859-15',\n    'en':                                   'en_US.ISO8859-1',\n    'en.iso88591':                          'en_US.ISO8859-1',\n    'en_ag':                                'en_AG.UTF-8',\n    'en_au':                                'en_AU.ISO8859-1',\n    'en_au.iso88591':                       'en_AU.ISO8859-1',\n    'en_be':                                'en_BE.ISO8859-1',\n    'en_be@euro':                           'en_BE.ISO8859-15',\n    'en_bw':                                'en_BW.ISO8859-1',\n    'en_bw.iso88591':                       'en_BW.ISO8859-1',\n    'en_ca':                                'en_CA.ISO8859-1',\n    'en_ca.iso88591':                       'en_CA.ISO8859-1',\n    'en_dk':                                'en_DK.ISO8859-1',\n    'en_dl.utf8':                           'en_DL.UTF-8',\n    'en_gb':                                'en_GB.ISO8859-1',\n    'en_gb.88591':                          'en_GB.ISO8859-1',\n    'en_gb.iso88591':                       'en_GB.ISO8859-1',\n    'en_gb.iso885915':                      'en_GB.ISO8859-15',\n    'en_gb@euro':                           'en_GB.ISO8859-15',\n    'en_hk':                                'en_HK.ISO8859-1',\n    'en_hk.iso88591':                       'en_HK.ISO8859-1',\n    'en_ie':                                'en_IE.ISO8859-1',\n    'en_ie.iso88591':                       'en_IE.ISO8859-1',\n    'en_ie.iso885915':                      'en_IE.ISO8859-15',\n    'en_ie.iso885915@euro':                 'en_IE.ISO8859-15',\n    'en_ie.utf8@euro':                      'en_IE.UTF-8',\n    'en_ie@euro':                           'en_IE.ISO8859-15',\n    'en_in':                                'en_IN.ISO8859-1',\n    'en_ng':                                'en_NG.UTF-8',\n    'en_nz':                                'en_NZ.ISO8859-1',\n    'en_nz.iso88591':                       'en_NZ.ISO8859-1',\n    'en_ph':                                'en_PH.ISO8859-1',\n    'en_ph.iso88591':                       'en_PH.ISO8859-1',\n    'en_sg':                                'en_SG.ISO8859-1',\n    'en_sg.iso88591':                       'en_SG.ISO8859-1',\n    'en_uk':                                'en_GB.ISO8859-1',\n    'en_us':                                'en_US.ISO8859-1',\n    'en_us.88591':                          'en_US.ISO8859-1',\n    'en_us.885915':                         'en_US.ISO8859-15',\n    'en_us.iso88591':                       'en_US.ISO8859-1',\n    'en_us.iso885915':                      'en_US.ISO8859-15',\n    'en_us.iso885915@euro':                 'en_US.ISO8859-15',\n    'en_us@euro':                           'en_US.ISO8859-15',\n    'en_us@euro@euro':                      'en_US.ISO8859-15',\n    'en_za':                                'en_ZA.ISO8859-1',\n    'en_za.88591':                          'en_ZA.ISO8859-1',\n    'en_za.iso88591':                       'en_ZA.ISO8859-1',\n    'en_za.iso885915':                      'en_ZA.ISO8859-15',\n    'en_za@euro':                           'en_ZA.ISO8859-15',\n    'en_zm':                                'en_ZM.UTF-8',\n    'en_zw':                                'en_ZW.ISO8859-1',\n    'en_zw.iso88591':                       'en_ZW.ISO8859-1',\n    'en_zw.utf8':                           'en_ZS.UTF-8',\n    'eng_gb':                               'en_GB.ISO8859-1',\n    'eng_gb.8859':                          'en_GB.ISO8859-1',\n    'english':                              'en_EN.ISO8859-1',\n    'english.iso88591':                     'en_EN.ISO8859-1',\n    'english_uk':                           'en_GB.ISO8859-1',\n    'english_uk.8859':                      'en_GB.ISO8859-1',\n    'english_united-states':                'en_US.ISO8859-1',\n    'english_united-states.437':            'C',\n    'english_us':                           'en_US.ISO8859-1',\n    'english_us.8859':                      'en_US.ISO8859-1',\n    'english_us.ascii':                     'en_US.ISO8859-1',\n    'eo':                                   'eo_XX.ISO8859-3',\n    'eo.utf8':                              'eo.UTF-8',\n    'eo_eo':                                'eo_EO.ISO8859-3',\n    'eo_eo.iso88593':                       'eo_EO.ISO8859-3',\n    'eo_us.utf8':                           'eo_US.UTF-8',\n    'eo_xx':                                'eo_XX.ISO8859-3',\n    'eo_xx.iso88593':                       'eo_XX.ISO8859-3',\n    'es':                                   'es_ES.ISO8859-1',\n    'es_ar':                                'es_AR.ISO8859-1',\n    'es_ar.iso88591':                       'es_AR.ISO8859-1',\n    'es_bo':                                'es_BO.ISO8859-1',\n    'es_bo.iso88591':                       'es_BO.ISO8859-1',\n    'es_cl':                                'es_CL.ISO8859-1',\n    'es_cl.iso88591':                       'es_CL.ISO8859-1',\n    'es_co':                                'es_CO.ISO8859-1',\n    'es_co.iso88591':                       'es_CO.ISO8859-1',\n    'es_cr':                                'es_CR.ISO8859-1',\n    'es_cr.iso88591':                       'es_CR.ISO8859-1',\n    'es_cu':                                'es_CU.UTF-8',\n    'es_do':                                'es_DO.ISO8859-1',\n    'es_do.iso88591':                       'es_DO.ISO8859-1',\n    'es_ec':                                'es_EC.ISO8859-1',\n    'es_ec.iso88591':                       'es_EC.ISO8859-1',\n    'es_es':                                'es_ES.ISO8859-1',\n    'es_es.88591':                          'es_ES.ISO8859-1',\n    'es_es.iso88591':                       'es_ES.ISO8859-1',\n    'es_es.iso885915':                      'es_ES.ISO8859-15',\n    'es_es.iso885915@euro':                 'es_ES.ISO8859-15',\n    'es_es.utf8@euro':                      'es_ES.UTF-8',\n    'es_es@euro':                           'es_ES.ISO8859-15',\n    'es_gt':                                'es_GT.ISO8859-1',\n    'es_gt.iso88591':                       'es_GT.ISO8859-1',\n    'es_hn':                                'es_HN.ISO8859-1',\n    'es_hn.iso88591':                       'es_HN.ISO8859-1',\n    'es_mx':                                'es_MX.ISO8859-1',\n    'es_mx.iso88591':                       'es_MX.ISO8859-1',\n    'es_ni':                                'es_NI.ISO8859-1',\n    'es_ni.iso88591':                       'es_NI.ISO8859-1',\n    'es_pa':                                'es_PA.ISO8859-1',\n    'es_pa.iso88591':                       'es_PA.ISO8859-1',\n    'es_pa.iso885915':                      'es_PA.ISO8859-15',\n    'es_pa@euro':                           'es_PA.ISO8859-15',\n    'es_pe':                                'es_PE.ISO8859-1',\n    'es_pe.iso88591':                       'es_PE.ISO8859-1',\n    'es_pe.iso885915':                      'es_PE.ISO8859-15',\n    'es_pe@euro':                           'es_PE.ISO8859-15',\n    'es_pr':                                'es_PR.ISO8859-1',\n    'es_pr.iso88591':                       'es_PR.ISO8859-1',\n    'es_py':                                'es_PY.ISO8859-1',\n    'es_py.iso88591':                       'es_PY.ISO8859-1',\n    'es_py.iso885915':                      'es_PY.ISO8859-15',\n    'es_py@euro':                           'es_PY.ISO8859-15',\n    'es_sv':                                'es_SV.ISO8859-1',\n    'es_sv.iso88591':                       'es_SV.ISO8859-1',\n    'es_sv.iso885915':                      'es_SV.ISO8859-15',\n    'es_sv@euro':                           'es_SV.ISO8859-15',\n    'es_us':                                'es_US.ISO8859-1',\n    'es_us.iso88591':                       'es_US.ISO8859-1',\n    'es_uy':                                'es_UY.ISO8859-1',\n    'es_uy.iso88591':                       'es_UY.ISO8859-1',\n    'es_uy.iso885915':                      'es_UY.ISO8859-15',\n    'es_uy@euro':                           'es_UY.ISO8859-15',\n    'es_ve':                                'es_VE.ISO8859-1',\n    'es_ve.iso88591':                       'es_VE.ISO8859-1',\n    'es_ve.iso885915':                      'es_VE.ISO8859-15',\n    'es_ve@euro':                           'es_VE.ISO8859-15',\n    'estonian':                             'et_EE.ISO8859-1',\n    'et':                                   'et_EE.ISO8859-15',\n    'et_ee':                                'et_EE.ISO8859-15',\n    'et_ee.iso88591':                       'et_EE.ISO8859-1',\n    'et_ee.iso885913':                      'et_EE.ISO8859-13',\n    'et_ee.iso885915':                      'et_EE.ISO8859-15',\n    'et_ee.iso88594':                       'et_EE.ISO8859-4',\n    'et_ee@euro':                           'et_EE.ISO8859-15',\n    'eu':                                   'eu_ES.ISO8859-1',\n    'eu_es':                                'eu_ES.ISO8859-1',\n    'eu_es.iso88591':                       'eu_ES.ISO8859-1',\n    'eu_es.iso885915':                      'eu_ES.ISO8859-15',\n    'eu_es.iso885915@euro':                 'eu_ES.ISO8859-15',\n    'eu_es.utf8@euro':                      'eu_ES.UTF-8',\n    'eu_es@euro':                           'eu_ES.ISO8859-15',\n    'eu_fr':                                'eu_FR.ISO8859-1',\n    'fa':                                   'fa_IR.UTF-8',\n    'fa_ir':                                'fa_IR.UTF-8',\n    'fa_ir.isiri3342':                      'fa_IR.ISIRI-3342',\n    'ff_sn':                                'ff_SN.UTF-8',\n    'fi':                                   'fi_FI.ISO8859-15',\n    'fi.iso885915':                         'fi_FI.ISO8859-15',\n    'fi_fi':                                'fi_FI.ISO8859-15',\n    'fi_fi.88591':                          'fi_FI.ISO8859-1',\n    'fi_fi.iso88591':                       'fi_FI.ISO8859-1',\n    'fi_fi.iso885915':                      'fi_FI.ISO8859-15',\n    'fi_fi.iso885915@euro':                 'fi_FI.ISO8859-15',\n    'fi_fi.utf8@euro':                      'fi_FI.UTF-8',\n    'fi_fi@euro':                           'fi_FI.ISO8859-15',\n    'fil_ph':                               'fil_PH.UTF-8',\n    'finnish':                              'fi_FI.ISO8859-1',\n    'finnish.iso88591':                     'fi_FI.ISO8859-1',\n    'fo':                                   'fo_FO.ISO8859-1',\n    'fo_fo':                                'fo_FO.ISO8859-1',\n    'fo_fo.iso88591':                       'fo_FO.ISO8859-1',\n    'fo_fo.iso885915':                      'fo_FO.ISO8859-15',\n    'fo_fo@euro':                           'fo_FO.ISO8859-15',\n    'fr':                                   'fr_FR.ISO8859-1',\n    'fr.iso885915':                         'fr_FR.ISO8859-15',\n    'fr_be':                                'fr_BE.ISO8859-1',\n    'fr_be.88591':                          'fr_BE.ISO8859-1',\n    'fr_be.iso88591':                       'fr_BE.ISO8859-1',\n    'fr_be.iso885915':                      'fr_BE.ISO8859-15',\n    'fr_be.iso885915@euro':                 'fr_BE.ISO8859-15',\n    'fr_be.utf8@euro':                      'fr_BE.UTF-8',\n    'fr_be@euro':                           'fr_BE.ISO8859-15',\n    'fr_ca':                                'fr_CA.ISO8859-1',\n    'fr_ca.88591':                          'fr_CA.ISO8859-1',\n    'fr_ca.iso88591':                       'fr_CA.ISO8859-1',\n    'fr_ca.iso885915':                      'fr_CA.ISO8859-15',\n    'fr_ca@euro':                           'fr_CA.ISO8859-15',\n    'fr_ch':                                'fr_CH.ISO8859-1',\n    'fr_ch.88591':                          'fr_CH.ISO8859-1',\n    'fr_ch.iso88591':                       'fr_CH.ISO8859-1',\n    'fr_ch.iso885915':                      'fr_CH.ISO8859-15',\n    'fr_ch@euro':                           'fr_CH.ISO8859-15',\n    'fr_fr':                                'fr_FR.ISO8859-1',\n    'fr_fr.88591':                          'fr_FR.ISO8859-1',\n    'fr_fr.iso88591':                       'fr_FR.ISO8859-1',\n    'fr_fr.iso885915':                      'fr_FR.ISO8859-15',\n    'fr_fr.iso885915@euro':                 'fr_FR.ISO8859-15',\n    'fr_fr.utf8@euro':                      'fr_FR.UTF-8',\n    'fr_fr@euro':                           'fr_FR.ISO8859-15',\n    'fr_lu':                                'fr_LU.ISO8859-1',\n    'fr_lu.88591':                          'fr_LU.ISO8859-1',\n    'fr_lu.iso88591':                       'fr_LU.ISO8859-1',\n    'fr_lu.iso885915':                      'fr_LU.ISO8859-15',\n    'fr_lu.iso885915@euro':                 'fr_LU.ISO8859-15',\n    'fr_lu.utf8@euro':                      'fr_LU.UTF-8',\n    'fr_lu@euro':                           'fr_LU.ISO8859-15',\n    'fran\\xe7ais':                          'fr_FR.ISO8859-1',\n    'fre_fr':                               'fr_FR.ISO8859-1',\n    'fre_fr.8859':                          'fr_FR.ISO8859-1',\n    'french':                               'fr_FR.ISO8859-1',\n    'french.iso88591':                      'fr_CH.ISO8859-1',\n    'french_france':                        'fr_FR.ISO8859-1',\n    'french_france.8859':                   'fr_FR.ISO8859-1',\n    'fur_it':                               'fur_IT.UTF-8',\n    'fy_de':                                'fy_DE.UTF-8',\n    'fy_nl':                                'fy_NL.UTF-8',\n    'ga':                                   'ga_IE.ISO8859-1',\n    'ga_ie':                                'ga_IE.ISO8859-1',\n    'ga_ie.iso88591':                       'ga_IE.ISO8859-1',\n    'ga_ie.iso885914':                      'ga_IE.ISO8859-14',\n    'ga_ie.iso885915':                      'ga_IE.ISO8859-15',\n    'ga_ie.iso885915@euro':                 'ga_IE.ISO8859-15',\n    'ga_ie.utf8@euro':                      'ga_IE.UTF-8',\n    'ga_ie@euro':                           'ga_IE.ISO8859-15',\n    'galego':                               'gl_ES.ISO8859-1',\n    'galician':                             'gl_ES.ISO8859-1',\n    'gd':                                   'gd_GB.ISO8859-1',\n    'gd_gb':                                'gd_GB.ISO8859-1',\n    'gd_gb.iso88591':                       'gd_GB.ISO8859-1',\n    'gd_gb.iso885914':                      'gd_GB.ISO8859-14',\n    'gd_gb.iso885915':                      'gd_GB.ISO8859-15',\n    'gd_gb@euro':                           'gd_GB.ISO8859-15',\n    'ger_de':                               'de_DE.ISO8859-1',\n    'ger_de.8859':                          'de_DE.ISO8859-1',\n    'german':                               'de_DE.ISO8859-1',\n    'german.iso88591':                      'de_CH.ISO8859-1',\n    'german_germany':                       'de_DE.ISO8859-1',\n    'german_germany.8859':                  'de_DE.ISO8859-1',\n    'gez_er':                               'gez_ER.UTF-8',\n    'gez_et':                               'gez_ET.UTF-8',\n    'gl':                                   'gl_ES.ISO8859-1',\n    'gl_es':                                'gl_ES.ISO8859-1',\n    'gl_es.iso88591':                       'gl_ES.ISO8859-1',\n    'gl_es.iso885915':                      'gl_ES.ISO8859-15',\n    'gl_es.iso885915@euro':                 'gl_ES.ISO8859-15',\n    'gl_es.utf8@euro':                      'gl_ES.UTF-8',\n    'gl_es@euro':                           'gl_ES.ISO8859-15',\n    'greek':                                'el_GR.ISO8859-7',\n    'greek.iso88597':                       'el_GR.ISO8859-7',\n    'gu_in':                                'gu_IN.UTF-8',\n    'gv':                                   'gv_GB.ISO8859-1',\n    'gv_gb':                                'gv_GB.ISO8859-1',\n    'gv_gb.iso88591':                       'gv_GB.ISO8859-1',\n    'gv_gb.iso885914':                      'gv_GB.ISO8859-14',\n    'gv_gb.iso885915':                      'gv_GB.ISO8859-15',\n    'gv_gb@euro':                           'gv_GB.ISO8859-15',\n    'ha_ng':                                'ha_NG.UTF-8',\n    'he':                                   'he_IL.ISO8859-8',\n    'he_il':                                'he_IL.ISO8859-8',\n    'he_il.cp1255':                         'he_IL.CP1255',\n    'he_il.iso88598':                       'he_IL.ISO8859-8',\n    'he_il.microsoftcp1255':                'he_IL.CP1255',\n    'hebrew':                               'he_IL.ISO8859-8',\n    'hebrew.iso88598':                      'he_IL.ISO8859-8',\n    'hi':                                   'hi_IN.ISCII-DEV',\n    'hi_in':                                'hi_IN.ISCII-DEV',\n    'hi_in.isciidev':                       'hi_IN.ISCII-DEV',\n    'hne':                                  'hne_IN.UTF-8',\n    'hne_in':                               'hne_IN.UTF-8',\n    'hr':                                   'hr_HR.ISO8859-2',\n    'hr_hr':                                'hr_HR.ISO8859-2',\n    'hr_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'hrvatski':                             'hr_HR.ISO8859-2',\n    'hsb_de':                               'hsb_DE.ISO8859-2',\n    'ht_ht':                                'ht_HT.UTF-8',\n    'hu':                                   'hu_HU.ISO8859-2',\n    'hu_hu':                                'hu_HU.ISO8859-2',\n    'hu_hu.iso88592':                       'hu_HU.ISO8859-2',\n    'hungarian':                            'hu_HU.ISO8859-2',\n    'hy_am':                                'hy_AM.UTF-8',\n    'hy_am.armscii8':                       'hy_AM.ARMSCII_8',\n    'ia':                                   'ia.UTF-8',\n    'ia_fr':                                'ia_FR.UTF-8',\n    'icelandic':                            'is_IS.ISO8859-1',\n    'icelandic.iso88591':                   'is_IS.ISO8859-1',\n    'id':                                   'id_ID.ISO8859-1',\n    'id_id':                                'id_ID.ISO8859-1',\n    'ig_ng':                                'ig_NG.UTF-8',\n    'ik_ca':                                'ik_CA.UTF-8',\n    'in':                                   'id_ID.ISO8859-1',\n    'in_id':                                'id_ID.ISO8859-1',\n    'is':                                   'is_IS.ISO8859-1',\n    'is_is':                                'is_IS.ISO8859-1',\n    'is_is.iso88591':                       'is_IS.ISO8859-1',\n    'is_is.iso885915':                      'is_IS.ISO8859-15',\n    'is_is@euro':                           'is_IS.ISO8859-15',\n    'iso-8859-1':                           'en_US.ISO8859-1',\n    'iso-8859-15':                          'en_US.ISO8859-15',\n    'iso8859-1':                            'en_US.ISO8859-1',\n    'iso8859-15':                           'en_US.ISO8859-15',\n    'iso_8859_1':                           'en_US.ISO8859-1',\n    'iso_8859_15':                          'en_US.ISO8859-15',\n    'it':                                   'it_IT.ISO8859-1',\n    'it.iso885915':                         'it_IT.ISO8859-15',\n    'it_ch':                                'it_CH.ISO8859-1',\n    'it_ch.iso88591':                       'it_CH.ISO8859-1',\n    'it_ch.iso885915':                      'it_CH.ISO8859-15',\n    'it_ch@euro':                           'it_CH.ISO8859-15',\n    'it_it':                                'it_IT.ISO8859-1',\n    'it_it.88591':                          'it_IT.ISO8859-1',\n    'it_it.iso88591':                       'it_IT.ISO8859-1',\n    'it_it.iso885915':                      'it_IT.ISO8859-15',\n    'it_it.iso885915@euro':                 'it_IT.ISO8859-15',\n    'it_it.utf8@euro':                      'it_IT.UTF-8',\n    'it_it@euro':                           'it_IT.ISO8859-15',\n    'italian':                              'it_IT.ISO8859-1',\n    'italian.iso88591':                     'it_IT.ISO8859-1',\n    'iu':                                   'iu_CA.NUNACOM-8',\n    'iu_ca':                                'iu_CA.NUNACOM-8',\n    'iu_ca.nunacom8':                       'iu_CA.NUNACOM-8',\n    'iw':                                   'he_IL.ISO8859-8',\n    'iw_il':                                'he_IL.ISO8859-8',\n    'iw_il.iso88598':                       'he_IL.ISO8859-8',\n    'iw_il.utf8':                           'iw_IL.UTF-8',\n    'ja':                                   'ja_JP.eucJP',\n    'ja.jis':                               'ja_JP.JIS7',\n    'ja.sjis':                              'ja_JP.SJIS',\n    'ja_jp':                                'ja_JP.eucJP',\n    'ja_jp.ajec':                           'ja_JP.eucJP',\n    'ja_jp.euc':                            'ja_JP.eucJP',\n    'ja_jp.eucjp':                          'ja_JP.eucJP',\n    'ja_jp.iso-2022-jp':                    'ja_JP.JIS7',\n    'ja_jp.iso2022jp':                      'ja_JP.JIS7',\n    'ja_jp.jis':                            'ja_JP.JIS7',\n    'ja_jp.jis7':                           'ja_JP.JIS7',\n    'ja_jp.mscode':                         'ja_JP.SJIS',\n    'ja_jp.pck':                            'ja_JP.SJIS',\n    'ja_jp.sjis':                           'ja_JP.SJIS',\n    'ja_jp.ujis':                           'ja_JP.eucJP',\n    'japan':                                'ja_JP.eucJP',\n    'japanese':                             'ja_JP.eucJP',\n    'japanese-euc':                         'ja_JP.eucJP',\n    'japanese.euc':                         'ja_JP.eucJP',\n    'japanese.sjis':                        'ja_JP.SJIS',\n    'jp_jp':                                'ja_JP.eucJP',\n    'ka':                                   'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge':                                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianacademy':                'ka_GE.GEORGIAN-ACADEMY',\n    'ka_ge.georgianps':                     'ka_GE.GEORGIAN-PS',\n    'ka_ge.georgianrs':                     'ka_GE.GEORGIAN-ACADEMY',\n    'kk_kz':                                'kk_KZ.RK1048',\n    'kl':                                   'kl_GL.ISO8859-1',\n    'kl_gl':                                'kl_GL.ISO8859-1',\n    'kl_gl.iso88591':                       'kl_GL.ISO8859-1',\n    'kl_gl.iso885915':                      'kl_GL.ISO8859-15',\n    'kl_gl@euro':                           'kl_GL.ISO8859-15',\n    'km_kh':                                'km_KH.UTF-8',\n    'kn':                                   'kn_IN.UTF-8',\n    'kn_in':                                'kn_IN.UTF-8',\n    'ko':                                   'ko_KR.eucKR',\n    'ko_kr':                                'ko_KR.eucKR',\n    'ko_kr.euc':                            'ko_KR.eucKR',\n    'ko_kr.euckr':                          'ko_KR.eucKR',\n    'kok_in':                               'kok_IN.UTF-8',\n    'korean':                               'ko_KR.eucKR',\n    'korean.euc':                           'ko_KR.eucKR',\n    'ks':                                   'ks_IN.UTF-8',\n    'ks_in':                                'ks_IN.UTF-8',\n    'ks_in@devanagari':                     'ks_IN.UTF-8@devanagari',\n    'ks_in@devanagari.utf8':                'ks_IN.UTF-8@devanagari',\n    'ku_tr':                                'ku_TR.ISO8859-9',\n    'kw':                                   'kw_GB.ISO8859-1',\n    'kw_gb':                                'kw_GB.ISO8859-1',\n    'kw_gb.iso88591':                       'kw_GB.ISO8859-1',\n    'kw_gb.iso885914':                      'kw_GB.ISO8859-14',\n    'kw_gb.iso885915':                      'kw_GB.ISO8859-15',\n    'kw_gb@euro':                           'kw_GB.ISO8859-15',\n    'ky':                                   'ky_KG.UTF-8',\n    'ky_kg':                                'ky_KG.UTF-8',\n    'lb_lu':                                'lb_LU.UTF-8',\n    'lg_ug':                                'lg_UG.ISO8859-10',\n    'li_be':                                'li_BE.UTF-8',\n    'li_nl':                                'li_NL.UTF-8',\n    'lij_it':                               'lij_IT.UTF-8',\n    'lithuanian':                           'lt_LT.ISO8859-13',\n    'lo':                                   'lo_LA.MULELAO-1',\n    'lo_la':                                'lo_LA.MULELAO-1',\n    'lo_la.cp1133':                         'lo_LA.IBM-CP1133',\n    'lo_la.ibmcp1133':                      'lo_LA.IBM-CP1133',\n    'lo_la.mulelao1':                       'lo_LA.MULELAO-1',\n    'lt':                                   'lt_LT.ISO8859-13',\n    'lt_lt':                                'lt_LT.ISO8859-13',\n    'lt_lt.iso885913':                      'lt_LT.ISO8859-13',\n    'lt_lt.iso88594':                       'lt_LT.ISO8859-4',\n    'lv':                                   'lv_LV.ISO8859-13',\n    'lv_lv':                                'lv_LV.ISO8859-13',\n    'lv_lv.iso885913':                      'lv_LV.ISO8859-13',\n    'lv_lv.iso88594':                       'lv_LV.ISO8859-4',\n    'mag_in':                               'mag_IN.UTF-8',\n    'mai':                                  'mai_IN.UTF-8',\n    'mai_in':                               'mai_IN.UTF-8',\n    'mg_mg':                                'mg_MG.ISO8859-15',\n    'mhr_ru':                               'mhr_RU.UTF-8',\n    'mi':                                   'mi_NZ.ISO8859-1',\n    'mi_nz':                                'mi_NZ.ISO8859-1',\n    'mi_nz.iso88591':                       'mi_NZ.ISO8859-1',\n    'mk':                                   'mk_MK.ISO8859-5',\n    'mk_mk':                                'mk_MK.ISO8859-5',\n    'mk_mk.cp1251':                         'mk_MK.CP1251',\n    'mk_mk.iso88595':                       'mk_MK.ISO8859-5',\n    'mk_mk.microsoftcp1251':                'mk_MK.CP1251',\n    'ml':                                   'ml_IN.UTF-8',\n    'ml_in':                                'ml_IN.UTF-8',\n    'mn_mn':                                'mn_MN.UTF-8',\n    'mni_in':                               'mni_IN.UTF-8',\n    'mr':                                   'mr_IN.UTF-8',\n    'mr_in':                                'mr_IN.UTF-8',\n    'ms':                                   'ms_MY.ISO8859-1',\n    'ms_my':                                'ms_MY.ISO8859-1',\n    'ms_my.iso88591':                       'ms_MY.ISO8859-1',\n    'mt':                                   'mt_MT.ISO8859-3',\n    'mt_mt':                                'mt_MT.ISO8859-3',\n    'mt_mt.iso88593':                       'mt_MT.ISO8859-3',\n    'my_mm':                                'my_MM.UTF-8',\n    'nan_tw@latin':                         'nan_TW.UTF-8@latin',\n    'nb':                                   'nb_NO.ISO8859-1',\n    'nb_no':                                'nb_NO.ISO8859-1',\n    'nb_no.88591':                          'nb_NO.ISO8859-1',\n    'nb_no.iso88591':                       'nb_NO.ISO8859-1',\n    'nb_no.iso885915':                      'nb_NO.ISO8859-15',\n    'nb_no@euro':                           'nb_NO.ISO8859-15',\n    'nds_de':                               'nds_DE.UTF-8',\n    'nds_nl':                               'nds_NL.UTF-8',\n    'ne_np':                                'ne_NP.UTF-8',\n    'nhn_mx':                               'nhn_MX.UTF-8',\n    'niu_nu':                               'niu_NU.UTF-8',\n    'niu_nz':                               'niu_NZ.UTF-8',\n    'nl':                                   'nl_NL.ISO8859-1',\n    'nl.iso885915':                         'nl_NL.ISO8859-15',\n    'nl_aw':                                'nl_AW.UTF-8',\n    'nl_be':                                'nl_BE.ISO8859-1',\n    'nl_be.88591':                          'nl_BE.ISO8859-1',\n    'nl_be.iso88591':                       'nl_BE.ISO8859-1',\n    'nl_be.iso885915':                      'nl_BE.ISO8859-15',\n    'nl_be.iso885915@euro':                 'nl_BE.ISO8859-15',\n    'nl_be.utf8@euro':                      'nl_BE.UTF-8',\n    'nl_be@euro':                           'nl_BE.ISO8859-15',\n    'nl_nl':                                'nl_NL.ISO8859-1',\n    'nl_nl.88591':                          'nl_NL.ISO8859-1',\n    'nl_nl.iso88591':                       'nl_NL.ISO8859-1',\n    'nl_nl.iso885915':                      'nl_NL.ISO8859-15',\n    'nl_nl.iso885915@euro':                 'nl_NL.ISO8859-15',\n    'nl_nl.utf8@euro':                      'nl_NL.UTF-8',\n    'nl_nl@euro':                           'nl_NL.ISO8859-15',\n    'nn':                                   'nn_NO.ISO8859-1',\n    'nn_no':                                'nn_NO.ISO8859-1',\n    'nn_no.88591':                          'nn_NO.ISO8859-1',\n    'nn_no.iso88591':                       'nn_NO.ISO8859-1',\n    'nn_no.iso885915':                      'nn_NO.ISO8859-15',\n    'nn_no@euro':                           'nn_NO.ISO8859-15',\n    'no':                                   'no_NO.ISO8859-1',\n    'no@nynorsk':                           'ny_NO.ISO8859-1',\n    'no_no':                                'no_NO.ISO8859-1',\n    'no_no.88591':                          'no_NO.ISO8859-1',\n    'no_no.iso88591':                       'no_NO.ISO8859-1',\n    'no_no.iso885915':                      'no_NO.ISO8859-15',\n    'no_no.iso88591@bokmal':                'no_NO.ISO8859-1',\n    'no_no.iso88591@nynorsk':               'no_NO.ISO8859-1',\n    'no_no@euro':                           'no_NO.ISO8859-15',\n    'norwegian':                            'no_NO.ISO8859-1',\n    'norwegian.iso88591':                   'no_NO.ISO8859-1',\n    'nr':                                   'nr_ZA.ISO8859-1',\n    'nr_za':                                'nr_ZA.ISO8859-1',\n    'nr_za.iso88591':                       'nr_ZA.ISO8859-1',\n    'nso':                                  'nso_ZA.ISO8859-15',\n    'nso_za':                               'nso_ZA.ISO8859-15',\n    'nso_za.iso885915':                     'nso_ZA.ISO8859-15',\n    'ny':                                   'ny_NO.ISO8859-1',\n    'ny_no':                                'ny_NO.ISO8859-1',\n    'ny_no.88591':                          'ny_NO.ISO8859-1',\n    'ny_no.iso88591':                       'ny_NO.ISO8859-1',\n    'ny_no.iso885915':                      'ny_NO.ISO8859-15',\n    'ny_no@euro':                           'ny_NO.ISO8859-15',\n    'nynorsk':                              'nn_NO.ISO8859-1',\n    'oc':                                   'oc_FR.ISO8859-1',\n    'oc_fr':                                'oc_FR.ISO8859-1',\n    'oc_fr.iso88591':                       'oc_FR.ISO8859-1',\n    'oc_fr.iso885915':                      'oc_FR.ISO8859-15',\n    'oc_fr@euro':                           'oc_FR.ISO8859-15',\n    'om_et':                                'om_ET.UTF-8',\n    'om_ke':                                'om_KE.ISO8859-1',\n    'or':                                   'or_IN.UTF-8',\n    'or_in':                                'or_IN.UTF-8',\n    'os_ru':                                'os_RU.UTF-8',\n    'pa':                                   'pa_IN.UTF-8',\n    'pa_in':                                'pa_IN.UTF-8',\n    'pa_pk':                                'pa_PK.UTF-8',\n    'pap_an':                               'pap_AN.UTF-8',\n    'pd':                                   'pd_US.ISO8859-1',\n    'pd_de':                                'pd_DE.ISO8859-1',\n    'pd_de.iso88591':                       'pd_DE.ISO8859-1',\n    'pd_de.iso885915':                      'pd_DE.ISO8859-15',\n    'pd_de@euro':                           'pd_DE.ISO8859-15',\n    'pd_us':                                'pd_US.ISO8859-1',\n    'pd_us.iso88591':                       'pd_US.ISO8859-1',\n    'pd_us.iso885915':                      'pd_US.ISO8859-15',\n    'pd_us@euro':                           'pd_US.ISO8859-15',\n    'ph':                                   'ph_PH.ISO8859-1',\n    'ph_ph':                                'ph_PH.ISO8859-1',\n    'ph_ph.iso88591':                       'ph_PH.ISO8859-1',\n    'pl':                                   'pl_PL.ISO8859-2',\n    'pl_pl':                                'pl_PL.ISO8859-2',\n    'pl_pl.iso88592':                       'pl_PL.ISO8859-2',\n    'polish':                               'pl_PL.ISO8859-2',\n    'portuguese':                           'pt_PT.ISO8859-1',\n    'portuguese.iso88591':                  'pt_PT.ISO8859-1',\n    'portuguese_brazil':                    'pt_BR.ISO8859-1',\n    'portuguese_brazil.8859':               'pt_BR.ISO8859-1',\n    'posix':                                'C',\n    'posix-utf2':                           'C',\n    'pp':                                   'pp_AN.ISO8859-1',\n    'pp_an':                                'pp_AN.ISO8859-1',\n    'pp_an.iso88591':                       'pp_AN.ISO8859-1',\n    'ps_af':                                'ps_AF.UTF-8',\n    'pt':                                   'pt_PT.ISO8859-1',\n    'pt.iso885915':                         'pt_PT.ISO8859-15',\n    'pt_br':                                'pt_BR.ISO8859-1',\n    'pt_br.88591':                          'pt_BR.ISO8859-1',\n    'pt_br.iso88591':                       'pt_BR.ISO8859-1',\n    'pt_br.iso885915':                      'pt_BR.ISO8859-15',\n    'pt_br@euro':                           'pt_BR.ISO8859-15',\n    'pt_pt':                                'pt_PT.ISO8859-1',\n    'pt_pt.88591':                          'pt_PT.ISO8859-1',\n    'pt_pt.iso88591':                       'pt_PT.ISO8859-1',\n    'pt_pt.iso885915':                      'pt_PT.ISO8859-15',\n    'pt_pt.iso885915@euro':                 'pt_PT.ISO8859-15',\n    'pt_pt.utf8@euro':                      'pt_PT.UTF-8',\n    'pt_pt@euro':                           'pt_PT.ISO8859-15',\n    'ro':                                   'ro_RO.ISO8859-2',\n    'ro_ro':                                'ro_RO.ISO8859-2',\n    'ro_ro.iso88592':                       'ro_RO.ISO8859-2',\n    'romanian':                             'ro_RO.ISO8859-2',\n    'ru':                                   'ru_RU.UTF-8',\n    'ru.koi8r':                             'ru_RU.KOI8-R',\n    'ru_ru':                                'ru_RU.UTF-8',\n    'ru_ru.cp1251':                         'ru_RU.CP1251',\n    'ru_ru.iso88595':                       'ru_RU.ISO8859-5',\n    'ru_ru.koi8r':                          'ru_RU.KOI8-R',\n    'ru_ru.microsoftcp1251':                'ru_RU.CP1251',\n    'ru_ua':                                'ru_UA.KOI8-U',\n    'ru_ua.cp1251':                         'ru_UA.CP1251',\n    'ru_ua.koi8u':                          'ru_UA.KOI8-U',\n    'ru_ua.microsoftcp1251':                'ru_UA.CP1251',\n    'rumanian':                             'ro_RO.ISO8859-2',\n    'russian':                              'ru_RU.ISO8859-5',\n    'rw':                                   'rw_RW.ISO8859-1',\n    'rw_rw':                                'rw_RW.ISO8859-1',\n    'rw_rw.iso88591':                       'rw_RW.ISO8859-1',\n    'sa_in':                                'sa_IN.UTF-8',\n    'sat_in':                               'sat_IN.UTF-8',\n    'sc_it':                                'sc_IT.UTF-8',\n    'sd':                                   'sd_IN.UTF-8',\n    'sd@devanagari':                        'sd_IN.UTF-8@devanagari',\n    'sd_in':                                'sd_IN.UTF-8',\n    'sd_in@devanagari':                     'sd_IN.UTF-8@devanagari',\n    'sd_in@devanagari.utf8':                'sd_IN.UTF-8@devanagari',\n    'sd_pk':                                'sd_PK.UTF-8',\n    'se_no':                                'se_NO.UTF-8',\n    'serbocroatian':                        'sr_RS.UTF-8@latin',\n    'sh':                                   'sr_RS.UTF-8@latin',\n    'sh_ba.iso88592@bosnia':                'sr_CS.ISO8859-2',\n    'sh_hr':                                'sh_HR.ISO8859-2',\n    'sh_hr.iso88592':                       'hr_HR.ISO8859-2',\n    'sh_sp':                                'sr_CS.ISO8859-2',\n    'sh_yu':                                'sr_RS.UTF-8@latin',\n    'shs_ca':                               'shs_CA.UTF-8',\n    'si':                                   'si_LK.UTF-8',\n    'si_lk':                                'si_LK.UTF-8',\n    'sid_et':                               'sid_ET.UTF-8',\n    'sinhala':                              'si_LK.UTF-8',\n    'sk':                                   'sk_SK.ISO8859-2',\n    'sk_sk':                                'sk_SK.ISO8859-2',\n    'sk_sk.iso88592':                       'sk_SK.ISO8859-2',\n    'sl':                                   'sl_SI.ISO8859-2',\n    'sl_cs':                                'sl_CS.ISO8859-2',\n    'sl_si':                                'sl_SI.ISO8859-2',\n    'sl_si.iso88592':                       'sl_SI.ISO8859-2',\n    'slovak':                               'sk_SK.ISO8859-2',\n    'slovene':                              'sl_SI.ISO8859-2',\n    'slovenian':                            'sl_SI.ISO8859-2',\n    'so_dj':                                'so_DJ.ISO8859-1',\n    'so_et':                                'so_ET.UTF-8',\n    'so_ke':                                'so_KE.ISO8859-1',\n    'so_so':                                'so_SO.ISO8859-1',\n    'sp':                                   'sr_CS.ISO8859-5',\n    'sp_yu':                                'sr_CS.ISO8859-5',\n    'spanish':                              'es_ES.ISO8859-1',\n    'spanish.iso88591':                     'es_ES.ISO8859-1',\n    'spanish_spain':                        'es_ES.ISO8859-1',\n    'spanish_spain.8859':                   'es_ES.ISO8859-1',\n    'sq':                                   'sq_AL.ISO8859-2',\n    'sq_al':                                'sq_AL.ISO8859-2',\n    'sq_al.iso88592':                       'sq_AL.ISO8859-2',\n    'sq_mk':                                'sq_MK.UTF-8',\n    'sr':                                   'sr_RS.UTF-8',\n    'sr@cyrillic':                          'sr_RS.UTF-8',\n    'sr@latin':                             'sr_RS.UTF-8@latin',\n    'sr@latn':                              'sr_CS.UTF-8@latin',\n    'sr_cs':                                'sr_CS.UTF-8',\n    'sr_cs.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_cs.iso88592@latn':                  'sr_CS.ISO8859-2',\n    'sr_cs.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_cs.utf8@latn':                      'sr_CS.UTF-8@latin',\n    'sr_cs@latn':                           'sr_CS.UTF-8@latin',\n    'sr_me':                                'sr_ME.UTF-8',\n    'sr_rs':                                'sr_RS.UTF-8',\n    'sr_rs@latin':                          'sr_RS.UTF-8@latin',\n    'sr_rs@latn':                           'sr_RS.UTF-8@latin',\n    'sr_sp':                                'sr_CS.ISO8859-2',\n    'sr_yu':                                'sr_RS.UTF-8@latin',\n    'sr_yu.cp1251@cyrillic':                'sr_CS.CP1251',\n    'sr_yu.iso88592':                       'sr_CS.ISO8859-2',\n    'sr_yu.iso88595':                       'sr_CS.ISO8859-5',\n    'sr_yu.iso88595@cyrillic':              'sr_CS.ISO8859-5',\n    'sr_yu.microsoftcp1251@cyrillic':       'sr_CS.CP1251',\n    'sr_yu.utf8':                           'sr_RS.UTF-8',\n    'sr_yu.utf8@cyrillic':                  'sr_RS.UTF-8',\n    'sr_yu@cyrillic':                       'sr_RS.UTF-8',\n    'ss':                                   'ss_ZA.ISO8859-1',\n    'ss_za':                                'ss_ZA.ISO8859-1',\n    'ss_za.iso88591':                       'ss_ZA.ISO8859-1',\n    'st':                                   'st_ZA.ISO8859-1',\n    'st_za':                                'st_ZA.ISO8859-1',\n    'st_za.iso88591':                       'st_ZA.ISO8859-1',\n    'sv':                                   'sv_SE.ISO8859-1',\n    'sv.iso885915':                         'sv_SE.ISO8859-15',\n    'sv_fi':                                'sv_FI.ISO8859-1',\n    'sv_fi.iso88591':                       'sv_FI.ISO8859-1',\n    'sv_fi.iso885915':                      'sv_FI.ISO8859-15',\n    'sv_fi.iso885915@euro':                 'sv_FI.ISO8859-15',\n    'sv_fi.utf8@euro':                      'sv_FI.UTF-8',\n    'sv_fi@euro':                           'sv_FI.ISO8859-15',\n    'sv_se':                                'sv_SE.ISO8859-1',\n    'sv_se.88591':                          'sv_SE.ISO8859-1',\n    'sv_se.iso88591':                       'sv_SE.ISO8859-1',\n    'sv_se.iso885915':                      'sv_SE.ISO8859-15',\n    'sv_se@euro':                           'sv_SE.ISO8859-15',\n    'sw_ke':                                'sw_KE.UTF-8',\n    'sw_tz':                                'sw_TZ.UTF-8',\n    'swedish':                              'sv_SE.ISO8859-1',\n    'swedish.iso88591':                     'sv_SE.ISO8859-1',\n    'szl_pl':                               'szl_PL.UTF-8',\n    'ta':                                   'ta_IN.TSCII-0',\n    'ta_in':                                'ta_IN.TSCII-0',\n    'ta_in.tscii':                          'ta_IN.TSCII-0',\n    'ta_in.tscii0':                         'ta_IN.TSCII-0',\n    'ta_lk':                                'ta_LK.UTF-8',\n    'te':                                   'te_IN.UTF-8',\n    'te_in':                                'te_IN.UTF-8',\n    'tg':                                   'tg_TJ.KOI8-C',\n    'tg_tj':                                'tg_TJ.KOI8-C',\n    'tg_tj.koi8c':                          'tg_TJ.KOI8-C',\n    'th':                                   'th_TH.ISO8859-11',\n    'th_th':                                'th_TH.ISO8859-11',\n    'th_th.iso885911':                      'th_TH.ISO8859-11',\n    'th_th.tactis':                         'th_TH.TIS620',\n    'th_th.tis620':                         'th_TH.TIS620',\n    'thai':                                 'th_TH.ISO8859-11',\n    'ti_er':                                'ti_ER.UTF-8',\n    'ti_et':                                'ti_ET.UTF-8',\n    'tig_er':                               'tig_ER.UTF-8',\n    'tk_tm':                                'tk_TM.UTF-8',\n    'tl':                                   'tl_PH.ISO8859-1',\n    'tl_ph':                                'tl_PH.ISO8859-1',\n    'tl_ph.iso88591':                       'tl_PH.ISO8859-1',\n    'tn':                                   'tn_ZA.ISO8859-15',\n    'tn_za':                                'tn_ZA.ISO8859-15',\n    'tn_za.iso885915':                      'tn_ZA.ISO8859-15',\n    'tr':                                   'tr_TR.ISO8859-9',\n    'tr_cy':                                'tr_CY.ISO8859-9',\n    'tr_tr':                                'tr_TR.ISO8859-9',\n    'tr_tr.iso88599':                       'tr_TR.ISO8859-9',\n    'ts':                                   'ts_ZA.ISO8859-1',\n    'ts_za':                                'ts_ZA.ISO8859-1',\n    'ts_za.iso88591':                       'ts_ZA.ISO8859-1',\n    'tt':                                   'tt_RU.TATAR-CYR',\n    'tt_ru':                                'tt_RU.TATAR-CYR',\n    'tt_ru.koi8c':                          'tt_RU.KOI8-C',\n    'tt_ru.tatarcyr':                       'tt_RU.TATAR-CYR',\n    'tt_ru@iqtelif':                        'tt_RU.UTF-8@iqtelif',\n    'turkish':                              'tr_TR.ISO8859-9',\n    'turkish.iso88599':                     'tr_TR.ISO8859-9',\n    'ug_cn':                                'ug_CN.UTF-8',\n    'uk':                                   'uk_UA.KOI8-U',\n    'uk_ua':                                'uk_UA.KOI8-U',\n    'uk_ua.cp1251':                         'uk_UA.CP1251',\n    'uk_ua.iso88595':                       'uk_UA.ISO8859-5',\n    'uk_ua.koi8u':                          'uk_UA.KOI8-U',\n    'uk_ua.microsoftcp1251':                'uk_UA.CP1251',\n    'univ':                                 'en_US.utf',\n    'universal':                            'en_US.utf',\n    'universal.utf8@ucs4':                  'en_US.UTF-8',\n    'unm_us':                               'unm_US.UTF-8',\n    'ur':                                   'ur_PK.CP1256',\n    'ur_in':                                'ur_IN.UTF-8',\n    'ur_pk':                                'ur_PK.CP1256',\n    'ur_pk.cp1256':                         'ur_PK.CP1256',\n    'ur_pk.microsoftcp1256':                'ur_PK.CP1256',\n    'uz':                                   'uz_UZ.UTF-8',\n    'uz_uz':                                'uz_UZ.UTF-8',\n    'uz_uz.iso88591':                       'uz_UZ.ISO8859-1',\n    'uz_uz.utf8@cyrillic':                  'uz_UZ.UTF-8',\n    'uz_uz@cyrillic':                       'uz_UZ.UTF-8',\n    've':                                   've_ZA.UTF-8',\n    've_za':                                've_ZA.UTF-8',\n    'vi':                                   'vi_VN.TCVN',\n    'vi_vn':                                'vi_VN.TCVN',\n    'vi_vn.tcvn':                           'vi_VN.TCVN',\n    'vi_vn.tcvn5712':                       'vi_VN.TCVN',\n    'vi_vn.viscii':                         'vi_VN.VISCII',\n    'vi_vn.viscii111':                      'vi_VN.VISCII',\n    'wa':                                   'wa_BE.ISO8859-1',\n    'wa_be':                                'wa_BE.ISO8859-1',\n    'wa_be.iso88591':                       'wa_BE.ISO8859-1',\n    'wa_be.iso885915':                      'wa_BE.ISO8859-15',\n    'wa_be.iso885915@euro':                 'wa_BE.ISO8859-15',\n    'wa_be@euro':                           'wa_BE.ISO8859-15',\n    'wae_ch':                               'wae_CH.UTF-8',\n    'wal_et':                               'wal_ET.UTF-8',\n    'wo_sn':                                'wo_SN.UTF-8',\n    'xh':                                   'xh_ZA.ISO8859-1',\n    'xh_za':                                'xh_ZA.ISO8859-1',\n    'xh_za.iso88591':                       'xh_ZA.ISO8859-1',\n    'yi':                                   'yi_US.CP1255',\n    'yi_us':                                'yi_US.CP1255',\n    'yi_us.cp1255':                         'yi_US.CP1255',\n    'yi_us.microsoftcp1255':                'yi_US.CP1255',\n    'yo_ng':                                'yo_NG.UTF-8',\n    'yue_hk':                               'yue_HK.UTF-8',\n    'zh':                                   'zh_CN.eucCN',\n    'zh_cn':                                'zh_CN.gb2312',\n    'zh_cn.big5':                           'zh_TW.big5',\n    'zh_cn.euc':                            'zh_CN.eucCN',\n    'zh_cn.gb18030':                        'zh_CN.gb18030',\n    'zh_cn.gb2312':                         'zh_CN.gb2312',\n    'zh_cn.gbk':                            'zh_CN.gbk',\n    'zh_hk':                                'zh_HK.big5hkscs',\n    'zh_hk.big5':                           'zh_HK.big5',\n    'zh_hk.big5hk':                         'zh_HK.big5hkscs',\n    'zh_hk.big5hkscs':                      'zh_HK.big5hkscs',\n    'zh_sg':                                'zh_SG.GB2312',\n    'zh_sg.gbk':                            'zh_SG.GBK',\n    'zh_tw':                                'zh_TW.big5',\n    'zh_tw.big5':                           'zh_TW.big5',\n    'zh_tw.euc':                            'zh_TW.eucTW',\n    'zh_tw.euctw':                          'zh_TW.eucTW',\n    'zu':                                   'zu_ZA.ISO8859-1',\n    'zu_za':                                'zu_ZA.ISO8859-1',\n    'zu_za.iso88591':                       'zu_ZA.ISO8859-1',\n}\n\n#\n# This maps Windows language identifiers to locale strings.\n#\n# This list has been updated from\n# http://msdn.microsoft.com/library/default.asp?url=/library/en-us/intl/nls_238z.asp\n# to include every locale up to Windows Vista.\n#\n# NOTE: this mapping is incomplete.  If your language is missing, please\n# submit a bug report to the Python bug tracker at http://bugs.python.org/\n# Make sure you include the missing language identifier and the suggested\n# locale code.\n#\n\nwindows_locale = {\n    0x0436: \"af_ZA\", # Afrikaans\n    0x041c: \"sq_AL\", # Albanian\n    0x0484: \"gsw_FR\",# Alsatian - France\n    0x045e: \"am_ET\", # Amharic - Ethiopia\n    0x0401: \"ar_SA\", # Arabic - Saudi Arabia\n    0x0801: \"ar_IQ\", # Arabic - Iraq\n    0x0c01: \"ar_EG\", # Arabic - Egypt\n    0x1001: \"ar_LY\", # Arabic - Libya\n    0x1401: \"ar_DZ\", # Arabic - Algeria\n    0x1801: \"ar_MA\", # Arabic - Morocco\n    0x1c01: \"ar_TN\", # Arabic - Tunisia\n    0x2001: \"ar_OM\", # Arabic - Oman\n    0x2401: \"ar_YE\", # Arabic - Yemen\n    0x2801: \"ar_SY\", # Arabic - Syria\n    0x2c01: \"ar_JO\", # Arabic - Jordan\n    0x3001: \"ar_LB\", # Arabic - Lebanon\n    0x3401: \"ar_KW\", # Arabic - Kuwait\n    0x3801: \"ar_AE\", # Arabic - United Arab Emirates\n    0x3c01: \"ar_BH\", # Arabic - Bahrain\n    0x4001: \"ar_QA\", # Arabic - Qatar\n    0x042b: \"hy_AM\", # Armenian\n    0x044d: \"as_IN\", # Assamese - India\n    0x042c: \"az_AZ\", # Azeri - Latin\n    0x082c: \"az_AZ\", # Azeri - Cyrillic\n    0x046d: \"ba_RU\", # Bashkir\n    0x042d: \"eu_ES\", # Basque - Russia\n    0x0423: \"be_BY\", # Belarusian\n    0x0445: \"bn_IN\", # Begali\n    0x201a: \"bs_BA\", # Bosnian - Cyrillic\n    0x141a: \"bs_BA\", # Bosnian - Latin\n    0x047e: \"br_FR\", # Breton - France\n    0x0402: \"bg_BG\", # Bulgarian\n#    0x0455: \"my_MM\", # Burmese - Not supported\n    0x0403: \"ca_ES\", # Catalan\n    0x0004: \"zh_CHS\",# Chinese - Simplified\n    0x0404: \"zh_TW\", # Chinese - Taiwan\n    0x0804: \"zh_CN\", # Chinese - PRC\n    0x0c04: \"zh_HK\", # Chinese - Hong Kong S.A.R.\n    0x1004: \"zh_SG\", # Chinese - Singapore\n    0x1404: \"zh_MO\", # Chinese - Macao S.A.R.\n    0x7c04: \"zh_CHT\",# Chinese - Traditional\n    0x0483: \"co_FR\", # Corsican - France\n    0x041a: \"hr_HR\", # Croatian\n    0x101a: \"hr_BA\", # Croatian - Bosnia\n    0x0405: \"cs_CZ\", # Czech\n    0x0406: \"da_DK\", # Danish\n    0x048c: \"gbz_AF\",# Dari - Afghanistan\n    0x0465: \"div_MV\",# Divehi - Maldives\n    0x0413: \"nl_NL\", # Dutch - The Netherlands\n    0x0813: \"nl_BE\", # Dutch - Belgium\n    0x0409: \"en_US\", # English - United States\n    0x0809: \"en_GB\", # English - United Kingdom\n    0x0c09: \"en_AU\", # English - Australia\n    0x1009: \"en_CA\", # English - Canada\n    0x1409: \"en_NZ\", # English - New Zealand\n    0x1809: \"en_IE\", # English - Ireland\n    0x1c09: \"en_ZA\", # English - South Africa\n    0x2009: \"en_JA\", # English - Jamaica\n    0x2409: \"en_CB\", # English - Carribbean\n    0x2809: \"en_BZ\", # English - Belize\n    0x2c09: \"en_TT\", # English - Trinidad\n    0x3009: \"en_ZW\", # English - Zimbabwe\n    0x3409: \"en_PH\", # English - Philippines\n    0x4009: \"en_IN\", # English - India\n    0x4409: \"en_MY\", # English - Malaysia\n    0x4809: \"en_IN\", # English - Singapore\n    0x0425: \"et_EE\", # Estonian\n    0x0438: \"fo_FO\", # Faroese\n    0x0464: \"fil_PH\",# Filipino\n    0x040b: \"fi_FI\", # Finnish\n    0x040c: \"fr_FR\", # French - France\n    0x080c: \"fr_BE\", # French - Belgium\n    0x0c0c: \"fr_CA\", # French - Canada\n    0x100c: \"fr_CH\", # French - Switzerland\n    0x140c: \"fr_LU\", # French - Luxembourg\n    0x180c: \"fr_MC\", # French - Monaco\n    0x0462: \"fy_NL\", # Frisian - Netherlands\n    0x0456: \"gl_ES\", # Galician\n    0x0437: \"ka_GE\", # Georgian\n    0x0407: \"de_DE\", # German - Germany\n    0x0807: \"de_CH\", # German - Switzerland\n    0x0c07: \"de_AT\", # German - Austria\n    0x1007: \"de_LU\", # German - Luxembourg\n    0x1407: \"de_LI\", # German - Liechtenstein\n    0x0408: \"el_GR\", # Greek\n    0x046f: \"kl_GL\", # Greenlandic - Greenland\n    0x0447: \"gu_IN\", # Gujarati\n    0x0468: \"ha_NG\", # Hausa - Latin\n    0x040d: \"he_IL\", # Hebrew\n    0x0439: \"hi_IN\", # Hindi\n    0x040e: \"hu_HU\", # Hungarian\n    0x040f: \"is_IS\", # Icelandic\n    0x0421: \"id_ID\", # Indonesian\n    0x045d: \"iu_CA\", # Inuktitut - Syllabics\n    0x085d: \"iu_CA\", # Inuktitut - Latin\n    0x083c: \"ga_IE\", # Irish - Ireland\n    0x0410: \"it_IT\", # Italian - Italy\n    0x0810: \"it_CH\", # Italian - Switzerland\n    0x0411: \"ja_JP\", # Japanese\n    0x044b: \"kn_IN\", # Kannada - India\n    0x043f: \"kk_KZ\", # Kazakh\n    0x0453: \"kh_KH\", # Khmer - Cambodia\n    0x0486: \"qut_GT\",# K'iche - Guatemala\n    0x0487: \"rw_RW\", # Kinyarwanda - Rwanda\n    0x0457: \"kok_IN\",# Konkani\n    0x0412: \"ko_KR\", # Korean\n    0x0440: \"ky_KG\", # Kyrgyz\n    0x0454: \"lo_LA\", # Lao - Lao PDR\n    0x0426: \"lv_LV\", # Latvian\n    0x0427: \"lt_LT\", # Lithuanian\n    0x082e: \"dsb_DE\",# Lower Sorbian - Germany\n    0x046e: \"lb_LU\", # Luxembourgish\n    0x042f: \"mk_MK\", # FYROM Macedonian\n    0x043e: \"ms_MY\", # Malay - Malaysia\n    0x083e: \"ms_BN\", # Malay - Brunei Darussalam\n    0x044c: \"ml_IN\", # Malayalam - India\n    0x043a: \"mt_MT\", # Maltese\n    0x0481: \"mi_NZ\", # Maori\n    0x047a: \"arn_CL\",# Mapudungun\n    0x044e: \"mr_IN\", # Marathi\n    0x047c: \"moh_CA\",# Mohawk - Canada\n    0x0450: \"mn_MN\", # Mongolian - Cyrillic\n    0x0850: \"mn_CN\", # Mongolian - PRC\n    0x0461: \"ne_NP\", # Nepali\n    0x0414: \"nb_NO\", # Norwegian - Bokmal\n    0x0814: \"nn_NO\", # Norwegian - Nynorsk\n    0x0482: \"oc_FR\", # Occitan - France\n    0x0448: \"or_IN\", # Oriya - India\n    0x0463: \"ps_AF\", # Pashto - Afghanistan\n    0x0429: \"fa_IR\", # Persian\n    0x0415: \"pl_PL\", # Polish\n    0x0416: \"pt_BR\", # Portuguese - Brazil\n    0x0816: \"pt_PT\", # Portuguese - Portugal\n    0x0446: \"pa_IN\", # Punjabi\n    0x046b: \"quz_BO\",# Quechua (Bolivia)\n    0x086b: \"quz_EC\",# Quechua (Ecuador)\n    0x0c6b: \"quz_PE\",# Quechua (Peru)\n    0x0418: \"ro_RO\", # Romanian - Romania\n    0x0417: \"rm_CH\", # Romansh\n    0x0419: \"ru_RU\", # Russian\n    0x243b: \"smn_FI\",# Sami Finland\n    0x103b: \"smj_NO\",# Sami Norway\n    0x143b: \"smj_SE\",# Sami Sweden\n    0x043b: \"se_NO\", # Sami Northern Norway\n    0x083b: \"se_SE\", # Sami Northern Sweden\n    0x0c3b: \"se_FI\", # Sami Northern Finland\n    0x203b: \"sms_FI\",# Sami Skolt\n    0x183b: \"sma_NO\",# Sami Southern Norway\n    0x1c3b: \"sma_SE\",# Sami Southern Sweden\n    0x044f: \"sa_IN\", # Sanskrit\n    0x0c1a: \"sr_SP\", # Serbian - Cyrillic\n    0x1c1a: \"sr_BA\", # Serbian - Bosnia Cyrillic\n    0x081a: \"sr_SP\", # Serbian - Latin\n    0x181a: \"sr_BA\", # Serbian - Bosnia Latin\n    0x045b: \"si_LK\", # Sinhala - Sri Lanka\n    0x046c: \"ns_ZA\", # Northern Sotho\n    0x0432: \"tn_ZA\", # Setswana - Southern Africa\n    0x041b: \"sk_SK\", # Slovak\n    0x0424: \"sl_SI\", # Slovenian\n    0x040a: \"es_ES\", # Spanish - Spain\n    0x080a: \"es_MX\", # Spanish - Mexico\n    0x0c0a: \"es_ES\", # Spanish - Spain (Modern)\n    0x100a: \"es_GT\", # Spanish - Guatemala\n    0x140a: \"es_CR\", # Spanish - Costa Rica\n    0x180a: \"es_PA\", # Spanish - Panama\n    0x1c0a: \"es_DO\", # Spanish - Dominican Republic\n    0x200a: \"es_VE\", # Spanish - Venezuela\n    0x240a: \"es_CO\", # Spanish - Colombia\n    0x280a: \"es_PE\", # Spanish - Peru\n    0x2c0a: \"es_AR\", # Spanish - Argentina\n    0x300a: \"es_EC\", # Spanish - Ecuador\n    0x340a: \"es_CL\", # Spanish - Chile\n    0x380a: \"es_UR\", # Spanish - Uruguay\n    0x3c0a: \"es_PY\", # Spanish - Paraguay\n    0x400a: \"es_BO\", # Spanish - Bolivia\n    0x440a: \"es_SV\", # Spanish - El Salvador\n    0x480a: \"es_HN\", # Spanish - Honduras\n    0x4c0a: \"es_NI\", # Spanish - Nicaragua\n    0x500a: \"es_PR\", # Spanish - Puerto Rico\n    0x540a: \"es_US\", # Spanish - United States\n#    0x0430: \"\", # Sutu - Not supported\n    0x0441: \"sw_KE\", # Swahili\n    0x041d: \"sv_SE\", # Swedish - Sweden\n    0x081d: \"sv_FI\", # Swedish - Finland\n    0x045a: \"syr_SY\",# Syriac\n    0x0428: \"tg_TJ\", # Tajik - Cyrillic\n    0x085f: \"tmz_DZ\",# Tamazight - Latin\n    0x0449: \"ta_IN\", # Tamil\n    0x0444: \"tt_RU\", # Tatar\n    0x044a: \"te_IN\", # Telugu\n    0x041e: \"th_TH\", # Thai\n    0x0851: \"bo_BT\", # Tibetan - Bhutan\n    0x0451: \"bo_CN\", # Tibetan - PRC\n    0x041f: \"tr_TR\", # Turkish\n    0x0442: \"tk_TM\", # Turkmen - Cyrillic\n    0x0480: \"ug_CN\", # Uighur - Arabic\n    0x0422: \"uk_UA\", # Ukrainian\n    0x042e: \"wen_DE\",# Upper Sorbian - Germany\n    0x0420: \"ur_PK\", # Urdu\n    0x0820: \"ur_IN\", # Urdu - India\n    0x0443: \"uz_UZ\", # Uzbek - Latin\n    0x0843: \"uz_UZ\", # Uzbek - Cyrillic\n    0x042a: \"vi_VN\", # Vietnamese\n    0x0452: \"cy_GB\", # Welsh\n    0x0488: \"wo_SN\", # Wolof - Senegal\n    0x0434: \"xh_ZA\", # Xhosa - South Africa\n    0x0485: \"sah_RU\",# Yakut - Cyrillic\n    0x0478: \"ii_CN\", # Yi - PRC\n    0x046a: \"yo_NG\", # Yoruba - Nigeria\n    0x0435: \"zu_ZA\", # Zulu\n}\n\ndef _print_locale():\n\n    \"\"\" Test function.\n    \"\"\"\n    categories = {}\n    def _init_categories(categories=categories):\n        for k,v in globals().items():\n            if k[:3] == 'LC_':\n                categories[k] = v\n    _init_categories()\n    del categories['LC_ALL']\n\n    print 'Locale defaults as determined by getdefaultlocale():'\n    print '-'*72\n    lang, enc = getdefaultlocale()\n    print 'Language: ', lang or '(undefined)'\n    print 'Encoding: ', enc or '(undefined)'\n    print\n\n    print 'Locale settings on startup:'\n    print '-'*72\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    print\n    print 'Locale settings after calling resetlocale():'\n    print '-'*72\n    resetlocale()\n    for name,category in categories.items():\n        print name, '...'\n        lang, enc = getlocale(category)\n        print '   Language: ', lang or '(undefined)'\n        print '   Encoding: ', enc or '(undefined)'\n        print\n\n    try:\n        setlocale(LC_ALL, \"\")\n    except:\n        print 'NOTE:'\n        print 'setlocale(LC_ALL, \"\") does not support the default locale'\n        print 'given in the OS environment variables.'\n    else:\n        print\n        print 'Locale settings after calling setlocale(LC_ALL, \"\"):'\n        print '-'*72\n        for name,category in categories.items():\n            print name, '...'\n            lang, enc = getlocale(category)\n            print '   Language: ', lang or '(undefined)'\n            print '   Encoding: ', enc or '(undefined)'\n            print\n\n###\n\ntry:\n    LC_MESSAGES\nexcept NameError:\n    pass\nelse:\n    __all__.append(\"LC_MESSAGES\")\n\nif __name__=='__main__':\n    print 'Locale aliasing:'\n    print\n    _print_locale()\n    print\n    print 'Number formatting:'\n    print\n    _test()\n", 
    "logging.__init__": "# Copyright 2001-2014 by Vinay Sajip. All Rights Reserved.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Vinay Sajip\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# VINAY SAJIP DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE, INCLUDING\n# ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL\n# VINAY SAJIP BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR\n# ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER\n# IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\n\"\"\"\nLogging package for Python. Based on PEP 282 and comments thereto in\ncomp.lang.python.\n\nCopyright (C) 2001-2014 Vinay Sajip. All Rights Reserved.\n\nTo use, simply 'import logging' and log away!\n\"\"\"\n\nimport sys, os, time, cStringIO, traceback, warnings, weakref, collections\n\n__all__ = ['BASIC_FORMAT', 'BufferingFormatter', 'CRITICAL', 'DEBUG', 'ERROR',\n           'FATAL', 'FileHandler', 'Filter', 'Formatter', 'Handler', 'INFO',\n           'LogRecord', 'Logger', 'LoggerAdapter', 'NOTSET', 'NullHandler',\n           'StreamHandler', 'WARN', 'WARNING', 'addLevelName', 'basicConfig',\n           'captureWarnings', 'critical', 'debug', 'disable', 'error',\n           'exception', 'fatal', 'getLevelName', 'getLogger', 'getLoggerClass',\n           'info', 'log', 'makeLogRecord', 'setLoggerClass', 'warn', 'warning']\n\ntry:\n    import codecs\nexcept ImportError:\n    codecs = None\n\ntry:\n    import thread\n    import threading\nexcept ImportError:\n    thread = None\n\n__author__  = \"Vinay Sajip <vinay_sajip@red-dove.com>\"\n__status__  = \"production\"\n# Note: the attributes below are no longer maintained.\n__version__ = \"0.5.1.2\"\n__date__    = \"07 February 2010\"\n\n#---------------------------------------------------------------------------\n#   Miscellaneous module data\n#---------------------------------------------------------------------------\ntry:\n    unicode\n    _unicode = True\nexcept NameError:\n    _unicode = False\n\n#\n# _srcfile is used when walking the stack to check when we've got the first\n# caller stack frame.\n#\nif hasattr(sys, 'frozen'): #support for py2exe\n    _srcfile = \"logging%s__init__%s\" % (os.sep, __file__[-4:])\nelif __file__[-4:].lower() in ['.pyc', '.pyo']:\n    _srcfile = __file__[:-4] + '.py'\nelse:\n    _srcfile = __file__\n_srcfile = os.path.normcase(_srcfile)\n\n# next bit filched from 1.5.2's inspect.py\ndef currentframe():\n    \"\"\"Return the frame object for the caller's stack frame.\"\"\"\n    try:\n        raise Exception\n    except:\n        return sys.exc_info()[2].tb_frame.f_back\n\nif hasattr(sys, '_getframe'): currentframe = lambda: sys._getframe(3)\n# done filching\n\n# _srcfile is only used in conjunction with sys._getframe().\n# To provide compatibility with older versions of Python, set _srcfile\n# to None if _getframe() is not available; this value will prevent\n# findCaller() from being called.\n#if not hasattr(sys, \"_getframe\"):\n#    _srcfile = None\n\n#\n#_startTime is used as the base when calculating the relative time of events\n#\n_startTime = time.time()\n\n#\n#raiseExceptions is used to see if exceptions during handling should be\n#propagated\n#\nraiseExceptions = 1\n\n#\n# If you don't want threading information in the log, set this to zero\n#\nlogThreads = 1\n\n#\n# If you don't want multiprocessing information in the log, set this to zero\n#\nlogMultiprocessing = 1\n\n#\n# If you don't want process information in the log, set this to zero\n#\nlogProcesses = 1\n\n#---------------------------------------------------------------------------\n#   Level related stuff\n#---------------------------------------------------------------------------\n#\n# Default levels and level names, these can be replaced with any positive set\n# of values having corresponding names. There is a pseudo-level, NOTSET, which\n# is only really there as a lower limit for user-defined levels. Handlers and\n# loggers are initialized with NOTSET so that they will log all messages, even\n# at user-defined levels.\n#\n\nCRITICAL = 50\nFATAL = CRITICAL\nERROR = 40\nWARNING = 30\nWARN = WARNING\nINFO = 20\nDEBUG = 10\nNOTSET = 0\n\n# NOTE(flaper87): This is different from\n# python's stdlib module since pypy's\n# dicts are much faster when their\n# keys are all of the same type.\n# Introduced in commit 9de7b40c586f\n_levelToName = {\n    CRITICAL: 'CRITICAL',\n    ERROR: 'ERROR',\n    WARNING: 'WARNING',\n    INFO: 'INFO',\n    DEBUG: 'DEBUG',\n    NOTSET: 'NOTSET',\n}\n_nameToLevel = {\n    'CRITICAL': CRITICAL,\n    'ERROR': ERROR,\n    'WARN': WARNING,\n    'WARNING': WARNING,\n    'INFO': INFO,\n    'DEBUG': DEBUG,\n    'NOTSET': NOTSET,\n}\n_levelNames = dict(_levelToName)\n_levelNames.update(_nameToLevel)   # backward compatibility\n\ndef getLevelName(level):\n    \"\"\"\n    Return the textual representation of logging level 'level'.\n\n    If the level is one of the predefined levels (CRITICAL, ERROR, WARNING,\n    INFO, DEBUG) then you get the corresponding string. If you have\n    associated levels with names using addLevelName then the name you have\n    associated with 'level' is returned.\n\n    If a numeric value corresponding to one of the defined levels is passed\n    in, the corresponding string representation is returned.\n\n    Otherwise, the string \"Level %s\" % level is returned.\n    \"\"\"\n\n    # NOTE(flaper87): Check also in _nameToLevel\n    # if value is None.\n    return (_levelToName.get(level) or\n            _nameToLevel.get(level, (\"Level %s\" % level)))\n\ndef addLevelName(level, levelName):\n    \"\"\"\n    Associate 'levelName' with 'level'.\n\n    This is used when converting levels to text during message formatting.\n    \"\"\"\n    _acquireLock()\n    try:    #unlikely to cause an exception, but you never know...\n        _levelToName[level] = levelName\n        _nameToLevel[levelName] = level\n    finally:\n        _releaseLock()\n\ndef _checkLevel(level):\n    if isinstance(level, (int, long)):\n        rv = level\n    elif str(level) == level:\n        if level not in _nameToLevel:\n            raise ValueError(\"Unknown level: %r\" % level)\n        rv = _nameToLevel[level]\n    else:\n        raise TypeError(\"Level not an integer or a valid string: %r\" % level)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Thread-related stuff\n#---------------------------------------------------------------------------\n\n#\n#_lock is used to serialize access to shared data structures in this module.\n#This needs to be an RLock because fileConfig() creates and configures\n#Handlers, and so might arbitrary user threads. Since Handler code updates the\n#shared dictionary _handlers, it needs to acquire the lock. But if configuring,\n#the lock would already have been acquired - so we need an RLock.\n#The same argument applies to Loggers and Manager.loggerDict.\n#\nif thread:\n    _lock = threading.RLock()\nelse:\n    _lock = None\n\ndef _acquireLock():\n    \"\"\"\n    Acquire the module-level lock for serializing access to shared data.\n\n    This should be released with _releaseLock().\n    \"\"\"\n    if _lock:\n        _lock.acquire()\n\ndef _releaseLock():\n    \"\"\"\n    Release the module-level lock acquired by calling _acquireLock().\n    \"\"\"\n    if _lock:\n        _lock.release()\n\n#---------------------------------------------------------------------------\n#   The logging record\n#---------------------------------------------------------------------------\n\nclass LogRecord(object):\n    \"\"\"\n    A LogRecord instance represents an event being logged.\n\n    LogRecord instances are created every time something is logged. They\n    contain all the information pertinent to the event being logged. The\n    main information passed in is in msg and args, which are combined\n    using str(msg) % args to create the message field of the record. The\n    record also includes information such as when the record was created,\n    the source line where the logging call was made, and any exception\n    information to be logged.\n    \"\"\"\n    def __init__(self, name, level, pathname, lineno,\n                 msg, args, exc_info, func=None):\n        \"\"\"\n        Initialize a logging record with interesting information.\n        \"\"\"\n        ct = time.time()\n        self.name = name\n        self.msg = msg\n        #\n        # The following statement allows passing of a dictionary as a sole\n        # argument, so that you can do something like\n        #  logging.debug(\"a %(a)d b %(b)s\", {'a':1, 'b':2})\n        # Suggested by Stefan Behnel.\n        # Note that without the test for args[0], we get a problem because\n        # during formatting, we test to see if the arg is present using\n        # 'if self.args:'. If the event being logged is e.g. 'Value is %d'\n        # and if the passed arg fails 'if self.args:' then no formatting\n        # is done. For example, logger.warn('Value is %d', 0) would log\n        # 'Value is %d' instead of 'Value is 0'.\n        # For the use case of passing a dictionary, this should not be a\n        # problem.\n        # Issue #21172: a request was made to relax the isinstance check\n        # to hasattr(args[0], '__getitem__'). However, the docs on string\n        # formatting still seem to suggest a mapping object is required.\n        # Thus, while not removing the isinstance check, it does now look\n        # for collections.Mapping rather than, as before, dict.\n        if (args and len(args) == 1 and isinstance(args[0], collections.Mapping)\n            and args[0]):\n            args = args[0]\n        self.args = args\n        self.levelname = getLevelName(level)\n        self.levelno = level\n        self.pathname = pathname\n        try:\n            self.filename = os.path.basename(pathname)\n            self.module = os.path.splitext(self.filename)[0]\n        except (TypeError, ValueError, AttributeError):\n            self.filename = pathname\n            self.module = \"Unknown module\"\n        self.exc_info = exc_info\n        self.exc_text = None      # used to cache the traceback text\n        self.lineno = lineno\n        self.funcName = func\n        self.created = ct\n        self.msecs = (ct - int(ct)) * 1000\n        self.relativeCreated = (self.created - _startTime) * 1000\n        if logThreads and thread:\n            self.thread = thread.get_ident()\n            self.threadName = threading.current_thread().name\n        else:\n            self.thread = None\n            self.threadName = None\n        if not logMultiprocessing:\n            self.processName = None\n        else:\n            self.processName = 'MainProcess'\n            mp = sys.modules.get('multiprocessing')\n            if mp is not None:\n                # Errors may occur if multiprocessing has not finished loading\n                # yet - e.g. if a custom import hook causes third-party code\n                # to run when multiprocessing calls import. See issue 8200\n                # for an example\n                try:\n                    self.processName = mp.current_process().name\n                except StandardError:\n                    pass\n        if logProcesses and hasattr(os, 'getpid'):\n            self.process = os.getpid()\n        else:\n            self.process = None\n\n    def __str__(self):\n        return '<LogRecord: %s, %s, %s, %s, \"%s\">'%(self.name, self.levelno,\n            self.pathname, self.lineno, self.msg)\n\n    def getMessage(self):\n        \"\"\"\n        Return the message for this LogRecord.\n\n        Return the message for this LogRecord after merging any user-supplied\n        arguments with the message.\n        \"\"\"\n        if not _unicode: #if no unicode support...\n            msg = str(self.msg)\n        else:\n            msg = self.msg\n            if not isinstance(msg, basestring):\n                try:\n                    msg = str(self.msg)\n                except UnicodeError:\n                    msg = self.msg      #Defer encoding till later\n        if self.args:\n            msg = msg % self.args\n        return msg\n\ndef makeLogRecord(dict):\n    \"\"\"\n    Make a LogRecord whose attributes are defined by the specified dictionary,\n    This function is useful for converting a logging event received over\n    a socket connection (which is sent as a dictionary) into a LogRecord\n    instance.\n    \"\"\"\n    rv = LogRecord(None, None, \"\", 0, \"\", (), None, None)\n    rv.__dict__.update(dict)\n    return rv\n\n#---------------------------------------------------------------------------\n#   Formatter classes and functions\n#---------------------------------------------------------------------------\n\nclass Formatter(object):\n    \"\"\"\n    Formatter instances are used to convert a LogRecord to text.\n\n    Formatters need to know how a LogRecord is constructed. They are\n    responsible for converting a LogRecord to (usually) a string which can\n    be interpreted by either a human or an external system. The base Formatter\n    allows a formatting string to be specified. If none is supplied, the\n    default value of \"%s(message)\\\\n\" is used.\n\n    The Formatter can be initialized with a format string which makes use of\n    knowledge of the LogRecord attributes - e.g. the default value mentioned\n    above makes use of the fact that the user's message and arguments are pre-\n    formatted into a LogRecord's message attribute. Currently, the useful\n    attributes in a LogRecord are described by:\n\n    %(name)s            Name of the logger (logging channel)\n    %(levelno)s         Numeric logging level for the message (DEBUG, INFO,\n                        WARNING, ERROR, CRITICAL)\n    %(levelname)s       Text logging level for the message (\"DEBUG\", \"INFO\",\n                        \"WARNING\", \"ERROR\", \"CRITICAL\")\n    %(pathname)s        Full pathname of the source file where the logging\n                        call was issued (if available)\n    %(filename)s        Filename portion of pathname\n    %(module)s          Module (name portion of filename)\n    %(lineno)d          Source line number where the logging call was issued\n                        (if available)\n    %(funcName)s        Function name\n    %(created)f         Time when the LogRecord was created (time.time()\n                        return value)\n    %(asctime)s         Textual time when the LogRecord was created\n    %(msecs)d           Millisecond portion of the creation time\n    %(relativeCreated)d Time in milliseconds when the LogRecord was created,\n                        relative to the time the logging module was loaded\n                        (typically at application startup time)\n    %(thread)d          Thread ID (if available)\n    %(threadName)s      Thread name (if available)\n    %(process)d         Process ID (if available)\n    %(message)s         The result of record.getMessage(), computed just as\n                        the record is emitted\n    \"\"\"\n\n    converter = time.localtime\n\n    def __init__(self, fmt=None, datefmt=None):\n        \"\"\"\n        Initialize the formatter with specified format strings.\n\n        Initialize the formatter either with the specified format string, or a\n        default as described above. Allow for specialized date formatting with\n        the optional datefmt argument (if omitted, you get the ISO8601 format).\n        \"\"\"\n        if fmt:\n            self._fmt = fmt\n        else:\n            self._fmt = \"%(message)s\"\n        self.datefmt = datefmt\n\n    def formatTime(self, record, datefmt=None):\n        \"\"\"\n        Return the creation time of the specified LogRecord as formatted text.\n\n        This method should be called from format() by a formatter which\n        wants to make use of a formatted time. This method can be overridden\n        in formatters to provide for any specific requirement, but the\n        basic behaviour is as follows: if datefmt (a string) is specified,\n        it is used with time.strftime() to format the creation time of the\n        record. Otherwise, the ISO8601 format is used. The resulting\n        string is returned. This function uses a user-configurable function\n        to convert the creation time to a tuple. By default, time.localtime()\n        is used; to change this for a particular formatter instance, set the\n        'converter' attribute to a function with the same signature as\n        time.localtime() or time.gmtime(). To change it for all formatters,\n        for example if you want all logging times to be shown in GMT,\n        set the 'converter' attribute in the Formatter class.\n        \"\"\"\n        ct = self.converter(record.created)\n        if datefmt:\n            s = time.strftime(datefmt, ct)\n        else:\n            t = time.strftime(\"%Y-%m-%d %H:%M:%S\", ct)\n            s = \"%s,%03d\" % (t, record.msecs)\n        return s\n\n    def formatException(self, ei):\n        \"\"\"\n        Format and return the specified exception information as a string.\n\n        This default implementation just uses\n        traceback.print_exception()\n        \"\"\"\n        sio = cStringIO.StringIO()\n        traceback.print_exception(ei[0], ei[1], ei[2], None, sio)\n        s = sio.getvalue()\n        sio.close()\n        if s[-1:] == \"\\n\":\n            s = s[:-1]\n        return s\n\n    def usesTime(self):\n        \"\"\"\n        Check if the format uses the creation time of the record.\n        \"\"\"\n        return self._fmt.find(\"%(asctime)\") >= 0\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record as text.\n\n        The record's attribute dictionary is used as the operand to a\n        string formatting operation which yields the returned string.\n        Before formatting the dictionary, a couple of preparatory steps\n        are carried out. The message attribute of the record is computed\n        using LogRecord.getMessage(). If the formatting string uses the\n        time (as determined by a call to usesTime(), formatTime() is\n        called to format the event time. If there is exception information,\n        it is formatted using formatException() and appended to the message.\n        \"\"\"\n        record.message = record.getMessage()\n        if self.usesTime():\n            record.asctime = self.formatTime(record, self.datefmt)\n        s = self._fmt % record.__dict__\n        if record.exc_info:\n            # Cache the traceback text to avoid converting it multiple times\n            # (it's constant anyway)\n            if not record.exc_text:\n                record.exc_text = self.formatException(record.exc_info)\n        if record.exc_text:\n            if s[-1:] != \"\\n\":\n                s = s + \"\\n\"\n            try:\n                s = s + record.exc_text\n            except UnicodeError:\n                # Sometimes filenames have non-ASCII chars, which can lead\n                # to errors when s is Unicode and record.exc_text is str\n                # See issue 8924.\n                # We also use replace for when there are multiple\n                # encodings, e.g. UTF-8 for the filesystem and latin-1\n                # for a script. See issue 13232.\n                s = s + record.exc_text.decode(sys.getfilesystemencoding(),\n                                               'replace')\n        return s\n\n#\n#   The default formatter to use when no other is specified\n#\n_defaultFormatter = Formatter()\n\nclass BufferingFormatter(object):\n    \"\"\"\n    A formatter suitable for formatting a number of records.\n    \"\"\"\n    def __init__(self, linefmt=None):\n        \"\"\"\n        Optionally specify a formatter which will be used to format each\n        individual record.\n        \"\"\"\n        if linefmt:\n            self.linefmt = linefmt\n        else:\n            self.linefmt = _defaultFormatter\n\n    def formatHeader(self, records):\n        \"\"\"\n        Return the header string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def formatFooter(self, records):\n        \"\"\"\n        Return the footer string for the specified records.\n        \"\"\"\n        return \"\"\n\n    def format(self, records):\n        \"\"\"\n        Format the specified records and return the result as a string.\n        \"\"\"\n        rv = \"\"\n        if len(records) > 0:\n            rv = rv + self.formatHeader(records)\n            for record in records:\n                rv = rv + self.linefmt.format(record)\n            rv = rv + self.formatFooter(records)\n        return rv\n\n#---------------------------------------------------------------------------\n#   Filter classes and functions\n#---------------------------------------------------------------------------\n\nclass Filter(object):\n    \"\"\"\n    Filter instances are used to perform arbitrary filtering of LogRecords.\n\n    Loggers and Handlers can optionally use Filter instances to filter\n    records as desired. The base filter class only allows events which are\n    below a certain point in the logger hierarchy. For example, a filter\n    initialized with \"A.B\" will allow events logged by loggers \"A.B\",\n    \"A.B.C\", \"A.B.C.D\", \"A.B.D\" etc. but not \"A.BB\", \"B.A.B\" etc. If\n    initialized with the empty string, all events are passed.\n    \"\"\"\n    def __init__(self, name=''):\n        \"\"\"\n        Initialize a filter.\n\n        Initialize with the name of the logger which, together with its\n        children, will have its events allowed through the filter. If no\n        name is specified, allow every event.\n        \"\"\"\n        self.name = name\n        self.nlen = len(name)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if the specified record is to be logged.\n\n        Is the specified record to be logged? Returns 0 for no, nonzero for\n        yes. If deemed appropriate, the record may be modified in-place.\n        \"\"\"\n        if self.nlen == 0:\n            return 1\n        elif self.name == record.name:\n            return 1\n        elif record.name.find(self.name, 0, self.nlen) != 0:\n            return 0\n        return (record.name[self.nlen] == \".\")\n\nclass Filterer(object):\n    \"\"\"\n    A base class for loggers and handlers which allows them to share\n    common code.\n    \"\"\"\n    def __init__(self):\n        \"\"\"\n        Initialize the list of filters to be an empty list.\n        \"\"\"\n        self.filters = []\n\n    def addFilter(self, filter):\n        \"\"\"\n        Add the specified filter to this handler.\n        \"\"\"\n        if not (filter in self.filters):\n            self.filters.append(filter)\n\n    def removeFilter(self, filter):\n        \"\"\"\n        Remove the specified filter from this handler.\n        \"\"\"\n        if filter in self.filters:\n            self.filters.remove(filter)\n\n    def filter(self, record):\n        \"\"\"\n        Determine if a record is loggable by consulting all the filters.\n\n        The default is to allow the record to be logged; any filter can veto\n        this and the record is then dropped. Returns a zero value if a record\n        is to be dropped, else non-zero.\n        \"\"\"\n        rv = 1\n        for f in self.filters:\n            if not f.filter(record):\n                rv = 0\n                break\n        return rv\n\n#---------------------------------------------------------------------------\n#   Handler classes and functions\n#---------------------------------------------------------------------------\n\n_handlers = weakref.WeakValueDictionary()  #map of handler names to handlers\n_handlerList = [] # added to allow handlers to be removed in reverse of order initialized\n\ndef _removeHandlerRef(wr):\n    \"\"\"\n    Remove a handler reference from the internal cleanup list.\n    \"\"\"\n    # This function can be called during module teardown, when globals are\n    # set to None. It can also be called from another thread. So we need to\n    # pre-emptively grab the necessary globals and check if they're None,\n    # to prevent race conditions and failures during interpreter shutdown.\n    acquire, release, handlers = _acquireLock, _releaseLock, _handlerList\n    if acquire and release and handlers:\n        acquire()\n        try:\n            if wr in handlers:\n                handlers.remove(wr)\n        finally:\n            release()\n\ndef _addHandlerRef(handler):\n    \"\"\"\n    Add a handler to the internal cleanup list using a weak reference.\n    \"\"\"\n    _acquireLock()\n    try:\n        _handlerList.append(weakref.ref(handler, _removeHandlerRef))\n    finally:\n        _releaseLock()\n\nclass Handler(Filterer):\n    \"\"\"\n    Handler instances dispatch logging events to specific destinations.\n\n    The base handler class. Acts as a placeholder which defines the Handler\n    interface. Handlers can optionally use Formatter instances to format\n    records as desired. By default, no formatter is specified; in this case,\n    the 'raw' message as determined by record.message is logged.\n    \"\"\"\n    def __init__(self, level=NOTSET):\n        \"\"\"\n        Initializes the instance - basically setting the formatter to None\n        and the filter list to empty.\n        \"\"\"\n        Filterer.__init__(self)\n        self._name = None\n        self.level = _checkLevel(level)\n        self.formatter = None\n        # Add the handler to the global _handlerList (for cleanup on shutdown)\n        _addHandlerRef(self)\n        self.createLock()\n\n    def get_name(self):\n        return self._name\n\n    def set_name(self, name):\n        _acquireLock()\n        try:\n            if self._name in _handlers:\n                del _handlers[self._name]\n            self._name = name\n            if name:\n                _handlers[name] = self\n        finally:\n            _releaseLock()\n\n    name = property(get_name, set_name)\n\n    def createLock(self):\n        \"\"\"\n        Acquire a thread lock for serializing access to the underlying I/O.\n        \"\"\"\n        if thread:\n            self.lock = threading.RLock()\n        else:\n            self.lock = None\n\n    def acquire(self):\n        \"\"\"\n        Acquire the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.acquire()\n\n    def release(self):\n        \"\"\"\n        Release the I/O thread lock.\n        \"\"\"\n        if self.lock:\n            self.lock.release()\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this handler.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def format(self, record):\n        \"\"\"\n        Format the specified record.\n\n        If a formatter is set, use it. Otherwise, use the default formatter\n        for the module.\n        \"\"\"\n        if self.formatter:\n            fmt = self.formatter\n        else:\n            fmt = _defaultFormatter\n        return fmt.format(record)\n\n    def emit(self, record):\n        \"\"\"\n        Do whatever it takes to actually log the specified logging record.\n\n        This version is intended to be implemented by subclasses and so\n        raises a NotImplementedError.\n        \"\"\"\n        raise NotImplementedError('emit must be implemented '\n                                  'by Handler subclasses')\n\n    def handle(self, record):\n        \"\"\"\n        Conditionally emit the specified logging record.\n\n        Emission depends on filters which may have been added to the handler.\n        Wrap the actual emission of the record with acquisition/release of\n        the I/O thread lock. Returns whether the filter passed the record for\n        emission.\n        \"\"\"\n        rv = self.filter(record)\n        if rv:\n            self.acquire()\n            try:\n                self.emit(record)\n            finally:\n                self.release()\n        return rv\n\n    def setFormatter(self, fmt):\n        \"\"\"\n        Set the formatter for this handler.\n        \"\"\"\n        self.formatter = fmt\n\n    def flush(self):\n        \"\"\"\n        Ensure all logging output has been flushed.\n\n        This version does nothing and is intended to be implemented by\n        subclasses.\n        \"\"\"\n        pass\n\n    def close(self):\n        \"\"\"\n        Tidy up any resources used by the handler.\n\n        This version removes the handler from an internal map of handlers,\n        _handlers, which is used for handler lookup by name. Subclasses\n        should ensure that this gets called from overridden close()\n        methods.\n        \"\"\"\n        #get the module data lock, as we're updating a shared structure.\n        _acquireLock()\n        try:    #unlikely to raise an exception, but you never know...\n            if self._name and self._name in _handlers:\n                del _handlers[self._name]\n        finally:\n            _releaseLock()\n\n    def handleError(self, record):\n        \"\"\"\n        Handle errors which occur during an emit() call.\n\n        This method should be called from handlers when an exception is\n        encountered during an emit() call. If raiseExceptions is false,\n        exceptions get silently ignored. This is what is mostly wanted\n        for a logging system - most users will not care about errors in\n        the logging system, they are more interested in application errors.\n        You could, however, replace this with a custom handler if you wish.\n        The record which was being processed is passed in to this method.\n        \"\"\"\n        if raiseExceptions and sys.stderr:  # see issue 13807\n            ei = sys.exc_info()\n            try:\n                traceback.print_exception(ei[0], ei[1], ei[2],\n                                          None, sys.stderr)\n                sys.stderr.write('Logged from file %s, line %s\\n' % (\n                                 record.filename, record.lineno))\n            except IOError:\n                pass    # see issue 5971\n            finally:\n                del ei\n\nclass StreamHandler(Handler):\n    \"\"\"\n    A handler class which writes logging records, appropriately formatted,\n    to a stream. Note that this class does not close the stream, as\n    sys.stdout or sys.stderr may be used.\n    \"\"\"\n\n    def __init__(self, stream=None):\n        \"\"\"\n        Initialize the handler.\n\n        If stream is not specified, sys.stderr is used.\n        \"\"\"\n        Handler.__init__(self)\n        if stream is None:\n            stream = sys.stderr\n        self.stream = stream\n\n    def flush(self):\n        \"\"\"\n        Flushes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream and hasattr(self.stream, \"flush\"):\n                self.stream.flush()\n        finally:\n            self.release()\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If a formatter is specified, it is used to format the record.\n        The record is then written to the stream with a trailing newline.  If\n        exception information is present, it is formatted using\n        traceback.print_exception and appended to the stream.  If the stream\n        has an 'encoding' attribute, it is used to determine how to do the\n        output to the stream.\n        \"\"\"\n        try:\n            msg = self.format(record)\n            stream = self.stream\n            fs = \"%s\\n\"\n            if not _unicode: #if no unicode support...\n                stream.write(fs % msg)\n            else:\n                try:\n                    if (isinstance(msg, unicode) and\n                        getattr(stream, 'encoding', None)):\n                        ufs = u'%s\\n'\n                        try:\n                            stream.write(ufs % msg)\n                        except UnicodeEncodeError:\n                            #Printing to terminals sometimes fails. For example,\n                            #with an encoding of 'cp1251', the above write will\n                            #work if written to a stream opened or wrapped by\n                            #the codecs module, but fail when writing to a\n                            #terminal even when the codepage is set to cp1251.\n                            #An extra encoding step seems to be needed.\n                            stream.write((ufs % msg).encode(stream.encoding))\n                    else:\n                        stream.write(fs % msg)\n                except UnicodeError:\n                    stream.write(fs % msg.encode(\"UTF-8\"))\n            self.flush()\n        except (KeyboardInterrupt, SystemExit):\n            raise\n        except:\n            self.handleError(record)\n\nclass FileHandler(StreamHandler):\n    \"\"\"\n    A handler class which writes formatted logging records to disk files.\n    \"\"\"\n    def __init__(self, filename, mode='a', encoding=None, delay=0):\n        \"\"\"\n        Open the specified file and use it as the stream for logging.\n        \"\"\"\n        #keep the absolute path, otherwise derived classes which use this\n        #may come a cropper when the current directory changes\n        if codecs is None:\n            encoding = None\n        self.baseFilename = os.path.abspath(filename)\n        self.mode = mode\n        self.encoding = encoding\n        self.delay = delay\n        if delay:\n            #We don't open the stream, but we still need to call the\n            #Handler constructor to set level, formatter, lock etc.\n            Handler.__init__(self)\n            self.stream = None\n        else:\n            StreamHandler.__init__(self, self._open())\n\n    def close(self):\n        \"\"\"\n        Closes the stream.\n        \"\"\"\n        self.acquire()\n        try:\n            if self.stream:\n                self.flush()\n                if hasattr(self.stream, \"close\"):\n                    self.stream.close()\n                self.stream = None\n            # Issue #19523: call unconditionally to\n            # prevent a handler leak when delay is set\n            StreamHandler.close(self)\n        finally:\n            self.release()\n\n    def _open(self):\n        \"\"\"\n        Open the current base file with the (original) mode and encoding.\n        Return the resulting stream.\n        \"\"\"\n        if self.encoding is None:\n            stream = open(self.baseFilename, self.mode)\n        else:\n            stream = codecs.open(self.baseFilename, self.mode, self.encoding)\n        return stream\n\n    def emit(self, record):\n        \"\"\"\n        Emit a record.\n\n        If the stream was not opened because 'delay' was specified in the\n        constructor, open it before calling the superclass's emit.\n        \"\"\"\n        if self.stream is None:\n            self.stream = self._open()\n        StreamHandler.emit(self, record)\n\n#---------------------------------------------------------------------------\n#   Manager classes and functions\n#---------------------------------------------------------------------------\n\nclass PlaceHolder(object):\n    \"\"\"\n    PlaceHolder instances are used in the Manager logger hierarchy to take\n    the place of nodes for which no loggers have been defined. This class is\n    intended for internal use only and not as part of the public API.\n    \"\"\"\n    def __init__(self, alogger):\n        \"\"\"\n        Initialize with the specified logger being a child of this placeholder.\n        \"\"\"\n        #self.loggers = [alogger]\n        self.loggerMap = { alogger : None }\n\n    def append(self, alogger):\n        \"\"\"\n        Add the specified logger as a child of this placeholder.\n        \"\"\"\n        #if alogger not in self.loggers:\n        if alogger not in self.loggerMap:\n            #self.loggers.append(alogger)\n            self.loggerMap[alogger] = None\n\n#\n#   Determine which class to use when instantiating loggers.\n#\n_loggerClass = None\n\ndef setLoggerClass(klass):\n    \"\"\"\n    Set the class to be used when instantiating a logger. The class should\n    define __init__() such that only a name argument is required, and the\n    __init__() should call Logger.__init__()\n    \"\"\"\n    if klass != Logger:\n        if not issubclass(klass, Logger):\n            raise TypeError(\"logger not derived from logging.Logger: \"\n                            + klass.__name__)\n    global _loggerClass\n    _loggerClass = klass\n\ndef getLoggerClass():\n    \"\"\"\n    Return the class to be used when instantiating a logger.\n    \"\"\"\n\n    return _loggerClass\n\nclass Manager(object):\n    \"\"\"\n    There is [under normal circumstances] just one Manager instance, which\n    holds the hierarchy of loggers.\n    \"\"\"\n    def __init__(self, rootnode):\n        \"\"\"\n        Initialize the manager with the root node of the logger hierarchy.\n        \"\"\"\n        self.root = rootnode\n        self.disable = 0\n        self.emittedNoHandlerWarning = 0\n        self.loggerDict = {}\n        self.loggerClass = None\n\n    def getLogger(self, name):\n        \"\"\"\n        Get a logger with the specified name (channel name), creating it\n        if it doesn't yet exist. This name is a dot-separated hierarchical\n        name, such as \"a\", \"a.b\", \"a.b.c\" or similar.\n\n        If a PlaceHolder existed for the specified name [i.e. the logger\n        didn't exist but a child of it did], replace it with the created\n        logger and fix up the parent/child references which pointed to the\n        placeholder to now point to the logger.\n        \"\"\"\n        rv = None\n        if not isinstance(name, basestring):\n            raise TypeError('A logger name must be string or Unicode')\n        if isinstance(name, unicode):\n            name = name.encode('utf-8')\n        _acquireLock()\n        try:\n            if name in self.loggerDict:\n                rv = self.loggerDict[name]\n                if isinstance(rv, PlaceHolder):\n                    ph = rv\n                    rv = (self.loggerClass or _loggerClass)(name)\n                    rv.manager = self\n                    self.loggerDict[name] = rv\n                    self._fixupChildren(ph, rv)\n                    self._fixupParents(rv)\n            else:\n                rv = (self.loggerClass or _loggerClass)(name)\n                rv.manager = self\n                self.loggerDict[name] = rv\n                self._fixupParents(rv)\n        finally:\n            _releaseLock()\n        return rv\n\n    def setLoggerClass(self, klass):\n        \"\"\"\n        Set the class to be used when instantiating a logger with this Manager.\n        \"\"\"\n        if klass != Logger:\n            if not issubclass(klass, Logger):\n                raise TypeError(\"logger not derived from logging.Logger: \"\n                                + klass.__name__)\n        self.loggerClass = klass\n\n    def _fixupParents(self, alogger):\n        \"\"\"\n        Ensure that there are either loggers or placeholders all the way\n        from the specified logger to the root of the logger hierarchy.\n        \"\"\"\n        name = alogger.name\n        i = name.rfind(\".\")\n        rv = None\n        while (i > 0) and not rv:\n            substr = name[:i]\n            if substr not in self.loggerDict:\n                self.loggerDict[substr] = PlaceHolder(alogger)\n            else:\n                obj = self.loggerDict[substr]\n                if isinstance(obj, Logger):\n                    rv = obj\n                else:\n                    assert isinstance(obj, PlaceHolder)\n                    obj.append(alogger)\n            i = name.rfind(\".\", 0, i - 1)\n        if not rv:\n            rv = self.root\n        alogger.parent = rv\n\n    def _fixupChildren(self, ph, alogger):\n        \"\"\"\n        Ensure that children of the placeholder ph are connected to the\n        specified logger.\n        \"\"\"\n        name = alogger.name\n        namelen = len(name)\n        for c in ph.loggerMap.keys():\n            #The if means ... if not c.parent.name.startswith(nm)\n            if c.parent.name[:namelen] != name:\n                alogger.parent = c.parent\n                c.parent = alogger\n\n#---------------------------------------------------------------------------\n#   Logger classes and functions\n#---------------------------------------------------------------------------\n\nclass Logger(Filterer):\n    \"\"\"\n    Instances of the Logger class represent a single logging channel. A\n    \"logging channel\" indicates an area of an application. Exactly how an\n    \"area\" is defined is up to the application developer. Since an\n    application can have any number of areas, logging channels are identified\n    by a unique string. Application areas can be nested (e.g. an area\n    of \"input processing\" might include sub-areas \"read CSV files\", \"read\n    XLS files\" and \"read Gnumeric files\"). To cater for this natural nesting,\n    channel names are organized into a namespace hierarchy where levels are\n    separated by periods, much like the Java or Python package namespace. So\n    in the instance given above, channel names might be \"input\" for the upper\n    level, and \"input.csv\", \"input.xls\" and \"input.gnu\" for the sub-levels.\n    There is no arbitrary limit to the depth of nesting.\n    \"\"\"\n    def __init__(self, name, level=NOTSET):\n        \"\"\"\n        Initialize the logger with a name and an optional level.\n        \"\"\"\n        Filterer.__init__(self)\n        self.name = name\n        self.level = _checkLevel(level)\n        self.parent = None\n        self.propagate = 1\n        self.handlers = []\n        self.disabled = 0\n\n    def setLevel(self, level):\n        \"\"\"\n        Set the logging level of this logger.\n        \"\"\"\n        self.level = _checkLevel(level)\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'DEBUG'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.debug(\"Houston, we have a %s\", \"thorny problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(DEBUG):\n            self._log(DEBUG, msg, args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'INFO'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.info(\"Houston, we have a %s\", \"interesting problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(INFO):\n            self._log(INFO, msg, args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'WARNING'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.warning(\"Houston, we have a %s\", \"bit of a problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(WARNING):\n            self._log(WARNING, msg, args, **kwargs)\n\n    warn = warning\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'ERROR'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.error(\"Houston, we have a %s\", \"major problem\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(ERROR):\n            self._log(ERROR, msg, args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Convenience method for logging an ERROR with exception information.\n        \"\"\"\n        kwargs['exc_info'] = 1\n        self.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with severity 'CRITICAL'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.critical(\"Houston, we have a %s\", \"major disaster\", exc_info=1)\n        \"\"\"\n        if self.isEnabledFor(CRITICAL):\n            self._log(CRITICAL, msg, args, **kwargs)\n\n    fatal = critical\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Log 'msg % args' with the integer severity 'level'.\n\n        To pass exception information, use the keyword argument exc_info with\n        a true value, e.g.\n\n        logger.log(level, \"We have a %s\", \"mysterious problem\", exc_info=1)\n        \"\"\"\n        if not isinstance(level, int):\n            if raiseExceptions:\n                raise TypeError(\"level must be an integer\")\n            else:\n                return\n        if self.isEnabledFor(level):\n            self._log(level, msg, args, **kwargs)\n\n    def findCaller(self):\n        \"\"\"\n        Find the stack frame of the caller so that we can note the source\n        file name, line number and function name.\n        \"\"\"\n        f = currentframe()\n        #On some versions of IronPython, currentframe() returns None if\n        #IronPython isn't run with -X:Frames.\n        if f is not None:\n            f = f.f_back\n        rv = \"(unknown file)\", 0, \"(unknown function)\"\n        while hasattr(f, \"f_code\"):\n            co = f.f_code\n            filename = os.path.normcase(co.co_filename)\n            if filename == _srcfile:\n                f = f.f_back\n                continue\n            rv = (co.co_filename, f.f_lineno, co.co_name)\n            break\n        return rv\n\n    def makeRecord(self, name, level, fn, lno, msg, args, exc_info, func=None, extra=None):\n        \"\"\"\n        A factory method which can be overridden in subclasses to create\n        specialized LogRecords.\n        \"\"\"\n        rv = LogRecord(name, level, fn, lno, msg, args, exc_info, func)\n        if extra is not None:\n            for key in extra:\n                if (key in [\"message\", \"asctime\"]) or (key in rv.__dict__):\n                    raise KeyError(\"Attempt to overwrite %r in LogRecord\" % key)\n                rv.__dict__[key] = extra[key]\n        return rv\n\n    def _log(self, level, msg, args, exc_info=None, extra=None):\n        \"\"\"\n        Low-level logging routine which creates a LogRecord and then calls\n        all the handlers of this logger to handle the record.\n        \"\"\"\n        if _srcfile:\n            #IronPython doesn't track Python frames, so findCaller raises an\n            #exception on some versions of IronPython. We trap it here so that\n            #IronPython can use logging.\n            try:\n                fn, lno, func = self.findCaller()\n            except ValueError:\n                fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        else:\n            fn, lno, func = \"(unknown file)\", 0, \"(unknown function)\"\n        if exc_info:\n            if not isinstance(exc_info, tuple):\n                exc_info = sys.exc_info()\n        record = self.makeRecord(self.name, level, fn, lno, msg, args, exc_info, func, extra)\n        self.handle(record)\n\n    def handle(self, record):\n        \"\"\"\n        Call the handlers for the specified record.\n\n        This method is used for unpickled records received from a socket, as\n        well as those created locally. Logger-level filtering is applied.\n        \"\"\"\n        if (not self.disabled) and self.filter(record):\n            self.callHandlers(record)\n\n    def addHandler(self, hdlr):\n        \"\"\"\n        Add the specified handler to this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if not (hdlr in self.handlers):\n                self.handlers.append(hdlr)\n        finally:\n            _releaseLock()\n\n    def removeHandler(self, hdlr):\n        \"\"\"\n        Remove the specified handler from this logger.\n        \"\"\"\n        _acquireLock()\n        try:\n            if hdlr in self.handlers:\n                self.handlers.remove(hdlr)\n        finally:\n            _releaseLock()\n\n    def callHandlers(self, record):\n        \"\"\"\n        Pass a record to all relevant handlers.\n\n        Loop through all handlers for this logger and its parents in the\n        logger hierarchy. If no handler was found, output a one-off error\n        message to sys.stderr. Stop searching up the hierarchy whenever a\n        logger with the \"propagate\" attribute set to zero is found - that\n        will be the last logger whose handlers are called.\n        \"\"\"\n        c = self\n        found = 0\n        while c:\n            for hdlr in c.handlers:\n                found = found + 1\n                if record.levelno >= hdlr.level:\n                    hdlr.handle(record)\n            if not c.propagate:\n                c = None    #break out\n            else:\n                c = c.parent\n        if (found == 0) and raiseExceptions and not self.manager.emittedNoHandlerWarning:\n            sys.stderr.write(\"No handlers could be found for logger\"\n                             \" \\\"%s\\\"\\n\" % self.name)\n            self.manager.emittedNoHandlerWarning = 1\n\n    def getEffectiveLevel(self):\n        \"\"\"\n        Get the effective level for this logger.\n\n        Loop through this logger and its parents in the logger hierarchy,\n        looking for a non-zero logging level. Return the first one found.\n        \"\"\"\n        logger = self\n        while logger:\n            if logger.level:\n                return logger.level\n            logger = logger.parent\n        return NOTSET\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        Is this logger enabled for level 'level'?\n        \"\"\"\n        if self.manager.disable >= level:\n            return 0\n        return level >= self.getEffectiveLevel()\n\n    def getChild(self, suffix):\n        \"\"\"\n        Get a logger which is a descendant to this one.\n\n        This is a convenience method, such that\n\n        logging.getLogger('abc').getChild('def.ghi')\n\n        is the same as\n\n        logging.getLogger('abc.def.ghi')\n\n        It's useful, for example, when the parent logger is named using\n        __name__ rather than a literal string.\n        \"\"\"\n        if self.root is not self:\n            suffix = '.'.join((self.name, suffix))\n        return self.manager.getLogger(suffix)\n\nclass RootLogger(Logger):\n    \"\"\"\n    A root logger is not that different to any other logger, except that\n    it must have a logging level and there is only one instance of it in\n    the hierarchy.\n    \"\"\"\n    def __init__(self, level):\n        \"\"\"\n        Initialize the logger with the name \"root\".\n        \"\"\"\n        Logger.__init__(self, \"root\", level)\n\n_loggerClass = Logger\n\nclass LoggerAdapter(object):\n    \"\"\"\n    An adapter for loggers which makes it easier to specify contextual\n    information in logging output.\n    \"\"\"\n\n    def __init__(self, logger, extra):\n        \"\"\"\n        Initialize the adapter with a logger and a dict-like object which\n        provides contextual information. This constructor signature allows\n        easy stacking of LoggerAdapters, if so desired.\n\n        You can effectively pass keyword arguments as shown in the\n        following example:\n\n        adapter = LoggerAdapter(someLogger, dict(p1=v1, p2=\"v2\"))\n        \"\"\"\n        self.logger = logger\n        self.extra = extra\n\n    def process(self, msg, kwargs):\n        \"\"\"\n        Process the logging message and keyword arguments passed in to\n        a logging call to insert contextual information. You can either\n        manipulate the message itself, the keyword args or both. Return\n        the message and kwargs modified (or not) to suit your needs.\n\n        Normally, you'll only need to override this one method in a\n        LoggerAdapter subclass for your specific needs.\n        \"\"\"\n        kwargs[\"extra\"] = self.extra\n        return msg, kwargs\n\n    def debug(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a debug call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.debug(msg, *args, **kwargs)\n\n    def info(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an info call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.info(msg, *args, **kwargs)\n\n    def warning(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a warning call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.warning(msg, *args, **kwargs)\n\n    def error(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an error call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.error(msg, *args, **kwargs)\n\n    def exception(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate an exception call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        kwargs[\"exc_info\"] = 1\n        self.logger.error(msg, *args, **kwargs)\n\n    def critical(self, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a critical call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.critical(msg, *args, **kwargs)\n\n    def log(self, level, msg, *args, **kwargs):\n        \"\"\"\n        Delegate a log call to the underlying logger, after adding\n        contextual information from this adapter instance.\n        \"\"\"\n        msg, kwargs = self.process(msg, kwargs)\n        self.logger.log(level, msg, *args, **kwargs)\n\n    def isEnabledFor(self, level):\n        \"\"\"\n        See if the underlying logger is enabled for the specified level.\n        \"\"\"\n        return self.logger.isEnabledFor(level)\n\nroot = RootLogger(WARNING)\nLogger.root = root\nLogger.manager = Manager(Logger.root)\n\n#---------------------------------------------------------------------------\n# Configuration classes and functions\n#---------------------------------------------------------------------------\n\nBASIC_FORMAT = \"%(levelname)s:%(name)s:%(message)s\"\n\ndef basicConfig(**kwargs):\n    \"\"\"\n    Do basic configuration for the logging system.\n\n    This function does nothing if the root logger already has handlers\n    configured. It is a convenience method intended for use by simple scripts\n    to do one-shot configuration of the logging package.\n\n    The default behaviour is to create a StreamHandler which writes to\n    sys.stderr, set a formatter using the BASIC_FORMAT format string, and\n    add the handler to the root logger.\n\n    A number of optional keyword arguments may be specified, which can alter\n    the default behaviour.\n\n    filename  Specifies that a FileHandler be created, using the specified\n              filename, rather than a StreamHandler.\n    filemode  Specifies the mode to open the file, if filename is specified\n              (if filemode is unspecified, it defaults to 'a').\n    format    Use the specified format string for the handler.\n    datefmt   Use the specified date/time format.\n    level     Set the root logger level to the specified level.\n    stream    Use the specified stream to initialize the StreamHandler. Note\n              that this argument is incompatible with 'filename' - if both\n              are present, 'stream' is ignored.\n\n    Note that you could specify a stream created using open(filename, mode)\n    rather than passing the filename and mode in. However, it should be\n    remembered that StreamHandler does not close its stream (since it may be\n    using sys.stdout or sys.stderr), whereas FileHandler closes its stream\n    when the handler is closed.\n    \"\"\"\n    # Add thread safety in case someone mistakenly calls\n    # basicConfig() from multiple threads\n    _acquireLock()\n    try:\n        if len(root.handlers) == 0:\n            filename = kwargs.get(\"filename\")\n            if filename:\n                mode = kwargs.get(\"filemode\", 'a')\n                hdlr = FileHandler(filename, mode)\n            else:\n                stream = kwargs.get(\"stream\")\n                hdlr = StreamHandler(stream)\n            fs = kwargs.get(\"format\", BASIC_FORMAT)\n            dfs = kwargs.get(\"datefmt\", None)\n            fmt = Formatter(fs, dfs)\n            hdlr.setFormatter(fmt)\n            root.addHandler(hdlr)\n            level = kwargs.get(\"level\")\n            if level is not None:\n                root.setLevel(level)\n    finally:\n        _releaseLock()\n\n#---------------------------------------------------------------------------\n# Utility functions at module level.\n# Basically delegate everything to the root logger.\n#---------------------------------------------------------------------------\n\ndef getLogger(name=None):\n    \"\"\"\n    Return a logger with the specified name, creating it if necessary.\n\n    If no name is specified, return the root logger.\n    \"\"\"\n    if name:\n        return Logger.manager.getLogger(name)\n    else:\n        return root\n\n#def getRootLogger():\n#    \"\"\"\n#    Return the root logger.\n#\n#    Note that getLogger('') now does the same thing, so this function is\n#    deprecated and may disappear in the future.\n#    \"\"\"\n#    return root\n\ndef critical(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'CRITICAL' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.critical(msg, *args, **kwargs)\n\nfatal = critical\n\ndef error(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.error(msg, *args, **kwargs)\n\ndef exception(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'ERROR' on the root logger,\n    with exception information.\n    \"\"\"\n    kwargs['exc_info'] = 1\n    error(msg, *args, **kwargs)\n\ndef warning(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'WARNING' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.warning(msg, *args, **kwargs)\n\nwarn = warning\n\ndef info(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'INFO' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.info(msg, *args, **kwargs)\n\ndef debug(msg, *args, **kwargs):\n    \"\"\"\n    Log a message with severity 'DEBUG' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.debug(msg, *args, **kwargs)\n\ndef log(level, msg, *args, **kwargs):\n    \"\"\"\n    Log 'msg % args' with the integer severity 'level' on the root logger.\n    \"\"\"\n    if len(root.handlers) == 0:\n        basicConfig()\n    root.log(level, msg, *args, **kwargs)\n\ndef disable(level):\n    \"\"\"\n    Disable all logging calls of severity 'level' and below.\n    \"\"\"\n    root.manager.disable = level\n\ndef shutdown(handlerList=_handlerList):\n    \"\"\"\n    Perform any cleanup actions in the logging system (e.g. flushing\n    buffers).\n\n    Should be called at application exit.\n    \"\"\"\n    for wr in reversed(handlerList[:]):\n        #errors might occur, for example, if files are locked\n        #we just ignore them if raiseExceptions is not set\n        try:\n            h = wr()\n            if h:\n                try:\n                    h.acquire()\n                    h.flush()\n                    h.close()\n                except (IOError, ValueError):\n                    # Ignore errors which might be caused\n                    # because handlers have been closed but\n                    # references to them are still around at\n                    # application exit.\n                    pass\n                finally:\n                    h.release()\n        except:\n            if raiseExceptions:\n                raise\n            #else, swallow\n\n#Let's try and shutdown automatically on application exit...\nimport atexit\natexit.register(shutdown)\n\n# Null handler\n\nclass NullHandler(Handler):\n    \"\"\"\n    This handler does nothing. It's intended to be used to avoid the\n    \"No handlers could be found for logger XXX\" one-off warning. This is\n    important for library code, which may contain code to log events. If a user\n    of the library does not configure logging, the one-off warning might be\n    produced; to avoid this, the library developer simply needs to instantiate\n    a NullHandler and add it to the top-level logger of the library module or\n    package.\n    \"\"\"\n    def handle(self, record):\n        pass\n\n    def emit(self, record):\n        pass\n\n    def createLock(self):\n        self.lock = None\n\n# Warnings integration\n\n_warnings_showwarning = None\n\ndef _showwarning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"\n    Implementation of showwarnings which redirects to logging, which will first\n    check to see if the file parameter is None. If a file is specified, it will\n    delegate to the original warnings implementation of showwarning. Otherwise,\n    it will call warnings.formatwarning and will log the resulting string to a\n    warnings logger named \"py.warnings\" with level logging.WARNING.\n    \"\"\"\n    if file is not None:\n        if _warnings_showwarning is not None:\n            _warnings_showwarning(message, category, filename, lineno, file, line)\n    else:\n        s = warnings.formatwarning(message, category, filename, lineno, line)\n        logger = getLogger(\"py.warnings\")\n        if not logger.handlers:\n            logger.addHandler(NullHandler())\n        logger.warning(\"%s\", s)\n\ndef captureWarnings(capture):\n    \"\"\"\n    If capture is true, redirect all warnings to the logging package.\n    If capture is False, ensure that warnings are not redirected to logging\n    but to their original destinations.\n    \"\"\"\n    global _warnings_showwarning\n    if capture:\n        if _warnings_showwarning is None:\n            _warnings_showwarning = warnings.showwarning\n            warnings.showwarning = _showwarning\n    else:\n        if _warnings_showwarning is not None:\n            warnings.showwarning = _warnings_showwarning\n            _warnings_showwarning = None\n", 
    "marshal": "# Note that PyPy contains also a built-in module 'marshal' which will\n# hide this one if compiled in.\n\nfrom _marshal import __doc__\nfrom _marshal import *\n", 
    "mimetools": "\"\"\"Various tools used by MIME-reading or MIME-writing programs.\"\"\"\n\n\nimport os\nimport sys\nimport tempfile\nfrom warnings import filterwarnings, catch_warnings\nwith catch_warnings():\n    if sys.py3kwarning:\n        filterwarnings(\"ignore\", \".*rfc822 has been removed\", DeprecationWarning)\n    import rfc822\n\nfrom warnings import warnpy3k\nwarnpy3k(\"in 3.x, mimetools has been removed in favor of the email package\",\n         stacklevel=2)\n\n__all__ = [\"Message\",\"choose_boundary\",\"encode\",\"decode\",\"copyliteral\",\n           \"copybinary\"]\n\nclass Message(rfc822.Message):\n    \"\"\"A derived class of rfc822.Message that knows about MIME headers and\n    contains some hooks for decoding encoded and multipart messages.\"\"\"\n\n    def __init__(self, fp, seekable = 1):\n        rfc822.Message.__init__(self, fp, seekable)\n        self.encodingheader = \\\n                self.getheader('content-transfer-encoding')\n        self.typeheader = \\\n                self.getheader('content-type')\n        self.parsetype()\n        self.parseplist()\n\n    def parsetype(self):\n        str = self.typeheader\n        if str is None:\n            str = 'text/plain'\n        if ';' in str:\n            i = str.index(';')\n            self.plisttext = str[i:]\n            str = str[:i]\n        else:\n            self.plisttext = ''\n        fields = str.split('/')\n        for i in range(len(fields)):\n            fields[i] = fields[i].strip().lower()\n        self.type = '/'.join(fields)\n        self.maintype = fields[0]\n        self.subtype = '/'.join(fields[1:])\n\n    def parseplist(self):\n        str = self.plisttext\n        self.plist = []\n        while str[:1] == ';':\n            str = str[1:]\n            if ';' in str:\n                # XXX Should parse quotes!\n                end = str.index(';')\n            else:\n                end = len(str)\n            f = str[:end]\n            if '=' in f:\n                i = f.index('=')\n                f = f[:i].strip().lower() + \\\n                        '=' + f[i+1:].strip()\n            self.plist.append(f.strip())\n            str = str[end:]\n\n    def getplist(self):\n        return self.plist\n\n    def getparam(self, name):\n        name = name.lower() + '='\n        n = len(name)\n        for p in self.plist:\n            if p[:n] == name:\n                return rfc822.unquote(p[n:])\n        return None\n\n    def getparamnames(self):\n        result = []\n        for p in self.plist:\n            i = p.find('=')\n            if i >= 0:\n                result.append(p[:i].lower())\n        return result\n\n    def getencoding(self):\n        if self.encodingheader is None:\n            return '7bit'\n        return self.encodingheader.lower()\n\n    def gettype(self):\n        return self.type\n\n    def getmaintype(self):\n        return self.maintype\n\n    def getsubtype(self):\n        return self.subtype\n\n\n\n\n# Utility functions\n# -----------------\n\ntry:\n    import thread\nexcept ImportError:\n    import dummy_thread as thread\n_counter_lock = thread.allocate_lock()\ndel thread\n\n_counter = 0\ndef _get_next_counter():\n    global _counter\n    _counter_lock.acquire()\n    _counter += 1\n    result = _counter\n    _counter_lock.release()\n    return result\n\n_prefix = None\n\ndef choose_boundary():\n    \"\"\"Return a string usable as a multipart boundary.\n\n    The string chosen is unique within a single program run, and\n    incorporates the user id (if available), process id (if available),\n    and current time.  So it's very unlikely the returned string appears\n    in message text, but there's no guarantee.\n\n    The boundary contains dots so you have to quote it in the header.\"\"\"\n\n    global _prefix\n    import time\n    if _prefix is None:\n        import socket\n        try:\n            hostid = socket.gethostbyname(socket.gethostname())\n        except socket.gaierror:\n            hostid = '127.0.0.1'\n        try:\n            uid = repr(os.getuid())\n        except AttributeError:\n            uid = '1'\n        try:\n            pid = repr(os.getpid())\n        except AttributeError:\n            pid = '1'\n        _prefix = hostid + '.' + uid + '.' + pid\n    return \"%s.%.3f.%d\" % (_prefix, time.time(), _get_next_counter())\n\n\n# Subroutines for decoding some common content-transfer-types\n\ndef decode(input, output, encoding):\n    \"\"\"Decode common content-transfer-encodings (base64, quopri, uuencode).\"\"\"\n    if encoding == 'base64':\n        import base64\n        return base64.decode(input, output)\n    if encoding == 'quoted-printable':\n        import quopri\n        return quopri.decode(input, output)\n    if encoding in ('uuencode', 'x-uuencode', 'uue', 'x-uue'):\n        import uu\n        return uu.decode(input, output)\n    if encoding in ('7bit', '8bit'):\n        return output.write(input.read())\n    if encoding in decodetab:\n        pipethrough(input, decodetab[encoding], output)\n    else:\n        raise ValueError, \\\n              'unknown Content-Transfer-Encoding: %s' % encoding\n\ndef encode(input, output, encoding):\n    \"\"\"Encode common content-transfer-encodings (base64, quopri, uuencode).\"\"\"\n    if encoding == 'base64':\n        import base64\n        return base64.encode(input, output)\n    if encoding == 'quoted-printable':\n        import quopri\n        return quopri.encode(input, output, 0)\n    if encoding in ('uuencode', 'x-uuencode', 'uue', 'x-uue'):\n        import uu\n        return uu.encode(input, output)\n    if encoding in ('7bit', '8bit'):\n        return output.write(input.read())\n    if encoding in encodetab:\n        pipethrough(input, encodetab[encoding], output)\n    else:\n        raise ValueError, \\\n              'unknown Content-Transfer-Encoding: %s' % encoding\n\n# The following is no longer used for standard encodings\n\n# XXX This requires that uudecode and mmencode are in $PATH\n\nuudecode_pipe = '''(\nTEMP=/tmp/@uu.$$\nsed \"s%^begin [0-7][0-7]* .*%begin 600 $TEMP%\" | uudecode\ncat $TEMP\nrm $TEMP\n)'''\n\ndecodetab = {\n        'uuencode':             uudecode_pipe,\n        'x-uuencode':           uudecode_pipe,\n        'uue':                  uudecode_pipe,\n        'x-uue':                uudecode_pipe,\n        'quoted-printable':     'mmencode -u -q',\n        'base64':               'mmencode -u -b',\n}\n\nencodetab = {\n        'x-uuencode':           'uuencode tempfile',\n        'uuencode':             'uuencode tempfile',\n        'x-uue':                'uuencode tempfile',\n        'uue':                  'uuencode tempfile',\n        'quoted-printable':     'mmencode -q',\n        'base64':               'mmencode -b',\n}\n\ndef pipeto(input, command):\n    pipe = os.popen(command, 'w')\n    copyliteral(input, pipe)\n    pipe.close()\n\ndef pipethrough(input, command, output):\n    (fd, tempname) = tempfile.mkstemp()\n    temp = os.fdopen(fd, 'w')\n    copyliteral(input, temp)\n    temp.close()\n    pipe = os.popen(command + ' <' + tempname, 'r')\n    copybinary(pipe, output)\n    pipe.close()\n    os.unlink(tempname)\n\ndef copyliteral(input, output):\n    while 1:\n        line = input.readline()\n        if not line: break\n        output.write(line)\n\ndef copybinary(input, output):\n    BUFSIZE = 8192\n    while 1:\n        line = input.read(BUFSIZE)\n        if not line: break\n        output.write(line)\n", 
    "mimetypes": "\"\"\"Guess the MIME type of a file.\n\nThis module defines two useful functions:\n\nguess_type(url, strict=1) -- guess the MIME type and encoding of a URL.\n\nguess_extension(type, strict=1) -- guess the extension for a given MIME type.\n\nIt also contains the following, for tuning the behavior:\n\nData:\n\nknownfiles -- list of files to parse\ninited -- flag set when init() has been called\nsuffix_map -- dictionary mapping suffixes to suffixes\nencodings_map -- dictionary mapping suffixes to encodings\ntypes_map -- dictionary mapping suffixes to types\n\nFunctions:\n\ninit([files]) -- parse a list of files, default knownfiles (on Windows, the\n  default values are taken from the registry)\nread_mime_types(file) -- parse one file, return a dictionary or None\n\"\"\"\n\nimport os\nimport sys\nimport posixpath\nimport urllib\ntry:\n    import _winreg\nexcept ImportError:\n    _winreg = None\n\n__all__ = [\n    \"guess_type\",\"guess_extension\",\"guess_all_extensions\",\n    \"add_type\",\"read_mime_types\",\"init\"\n]\n\nknownfiles = [\n    \"/etc/mime.types\",\n    \"/etc/httpd/mime.types\",                    # Mac OS X\n    \"/etc/httpd/conf/mime.types\",               # Apache\n    \"/etc/apache/mime.types\",                   # Apache 1\n    \"/etc/apache2/mime.types\",                  # Apache 2\n    \"/usr/local/etc/httpd/conf/mime.types\",\n    \"/usr/local/lib/netscape/mime.types\",\n    \"/usr/local/etc/httpd/conf/mime.types\",     # Apache 1.2\n    \"/usr/local/etc/mime.types\",                # Apache 1.3\n    ]\n\ninited = False\n_db = None\n\n\nclass MimeTypes:\n    \"\"\"MIME-types datastore.\n\n    This datastore can handle information from mime.types-style files\n    and supports basic determination of MIME type from a filename or\n    URL, and can guess a reasonable extension given a MIME type.\n    \"\"\"\n\n    def __init__(self, filenames=(), strict=True):\n        if not inited:\n            init()\n        self.encodings_map = encodings_map.copy()\n        self.suffix_map = suffix_map.copy()\n        self.types_map = ({}, {}) # dict for (non-strict, strict)\n        self.types_map_inv = ({}, {})\n        for (ext, type) in types_map.items():\n            self.add_type(type, ext, True)\n        for (ext, type) in common_types.items():\n            self.add_type(type, ext, False)\n        for name in filenames:\n            self.read(name, strict)\n\n    def add_type(self, type, ext, strict=True):\n        \"\"\"Add a mapping between a type and an extension.\n\n        When the extension is already known, the new\n        type will replace the old one. When the type\n        is already known the extension will be added\n        to the list of known extensions.\n\n        If strict is true, information will be added to\n        list of standard types, else to the list of non-standard\n        types.\n        \"\"\"\n        self.types_map[strict][ext] = type\n        exts = self.types_map_inv[strict].setdefault(type, [])\n        if ext not in exts:\n            exts.append(ext)\n\n    def guess_type(self, url, strict=True):\n        \"\"\"Guess the type of a file based on its URL.\n\n        Return value is a tuple (type, encoding) where type is None if\n        the type can't be guessed (no or unknown suffix) or a string\n        of the form type/subtype, usable for a MIME Content-type\n        header; and encoding is None for no encoding or the name of\n        the program used to encode (e.g. compress or gzip).  The\n        mappings are table driven.  Encoding suffixes are case\n        sensitive; type suffixes are first tried case sensitive, then\n        case insensitive.\n\n        The suffixes .tgz, .taz and .tz (case sensitive!) are all\n        mapped to '.tar.gz'.  (This is table-driven too, using the\n        dictionary suffix_map.)\n\n        Optional `strict' argument when False adds a bunch of commonly found,\n        but non-standard types.\n        \"\"\"\n        scheme, url = urllib.splittype(url)\n        if scheme == 'data':\n            # syntax of data URLs:\n            # dataurl   := \"data:\" [ mediatype ] [ \";base64\" ] \",\" data\n            # mediatype := [ type \"/\" subtype ] *( \";\" parameter )\n            # data      := *urlchar\n            # parameter := attribute \"=\" value\n            # type/subtype defaults to \"text/plain\"\n            comma = url.find(',')\n            if comma < 0:\n                # bad data URL\n                return None, None\n            semi = url.find(';', 0, comma)\n            if semi >= 0:\n                type = url[:semi]\n            else:\n                type = url[:comma]\n            if '=' in type or '/' not in type:\n                type = 'text/plain'\n            return type, None           # never compressed, so encoding is None\n        base, ext = posixpath.splitext(url)\n        while ext in self.suffix_map:\n            base, ext = posixpath.splitext(base + self.suffix_map[ext])\n        if ext in self.encodings_map:\n            encoding = self.encodings_map[ext]\n            base, ext = posixpath.splitext(base)\n        else:\n            encoding = None\n        types_map = self.types_map[True]\n        if ext in types_map:\n            return types_map[ext], encoding\n        elif ext.lower() in types_map:\n            return types_map[ext.lower()], encoding\n        elif strict:\n            return None, encoding\n        types_map = self.types_map[False]\n        if ext in types_map:\n            return types_map[ext], encoding\n        elif ext.lower() in types_map:\n            return types_map[ext.lower()], encoding\n        else:\n            return None, encoding\n\n    def guess_all_extensions(self, type, strict=True):\n        \"\"\"Guess the extensions for a file based on its MIME type.\n\n        Return value is a list of strings giving the possible filename\n        extensions, including the leading dot ('.').  The extension is not\n        guaranteed to have been associated with any particular data stream,\n        but would be mapped to the MIME type `type' by guess_type().\n\n        Optional `strict' argument when false adds a bunch of commonly found,\n        but non-standard types.\n        \"\"\"\n        type = type.lower()\n        extensions = self.types_map_inv[True].get(type, [])\n        if not strict:\n            for ext in self.types_map_inv[False].get(type, []):\n                if ext not in extensions:\n                    extensions.append(ext)\n        return extensions\n\n    def guess_extension(self, type, strict=True):\n        \"\"\"Guess the extension for a file based on its MIME type.\n\n        Return value is a string giving a filename extension,\n        including the leading dot ('.').  The extension is not\n        guaranteed to have been associated with any particular data\n        stream, but would be mapped to the MIME type `type' by\n        guess_type().  If no extension can be guessed for `type', None\n        is returned.\n\n        Optional `strict' argument when false adds a bunch of commonly found,\n        but non-standard types.\n        \"\"\"\n        extensions = self.guess_all_extensions(type, strict)\n        if not extensions:\n            return None\n        return extensions[0]\n\n    def read(self, filename, strict=True):\n        \"\"\"\n        Read a single mime.types-format file, specified by pathname.\n\n        If strict is true, information will be added to\n        list of standard types, else to the list of non-standard\n        types.\n        \"\"\"\n        with open(filename) as fp:\n            self.readfp(fp, strict)\n\n    def readfp(self, fp, strict=True):\n        \"\"\"\n        Read a single mime.types-format file.\n\n        If strict is true, information will be added to\n        list of standard types, else to the list of non-standard\n        types.\n        \"\"\"\n        while 1:\n            line = fp.readline()\n            if not line:\n                break\n            words = line.split()\n            for i in range(len(words)):\n                if words[i][0] == '#':\n                    del words[i:]\n                    break\n            if not words:\n                continue\n            type, suffixes = words[0], words[1:]\n            for suff in suffixes:\n                self.add_type(type, '.' + suff, strict)\n\n    def read_windows_registry(self, strict=True):\n        \"\"\"\n        Load the MIME types database from Windows registry.\n\n        If strict is true, information will be added to\n        list of standard types, else to the list of non-standard\n        types.\n        \"\"\"\n\n        # Windows only\n        if not _winreg:\n            return\n\n        def enum_types(mimedb):\n            i = 0\n            while True:\n                try:\n                    yield _winreg.EnumKey(mimedb, i)\n                except EnvironmentError:\n                    break\n                i += 1\n\n        default_encoding = sys.getdefaultencoding()\n        with _winreg.OpenKey(_winreg.HKEY_CLASSES_ROOT, '') as hkcr:\n            for subkeyname in enum_types(hkcr):\n                try:\n                    with _winreg.OpenKey(hkcr, subkeyname) as subkey:\n                        # Only check file extensions\n                        if not subkeyname.startswith(\".\"):\n                            continue\n                        # raises EnvironmentError if no 'Content Type' value\n                        mimetype, datatype = _winreg.QueryValueEx(\n                            subkey, 'Content Type')\n                        if datatype != _winreg.REG_SZ:\n                            continue\n                        try:\n                            mimetype = mimetype.encode(default_encoding)\n                        except UnicodeEncodeError:\n                            continue\n                        self.add_type(mimetype, subkeyname, strict)\n                except EnvironmentError:\n                    continue\n\ndef guess_type(url, strict=True):\n    \"\"\"Guess the type of a file based on its URL.\n\n    Return value is a tuple (type, encoding) where type is None if the\n    type can't be guessed (no or unknown suffix) or a string of the\n    form type/subtype, usable for a MIME Content-type header; and\n    encoding is None for no encoding or the name of the program used\n    to encode (e.g. compress or gzip).  The mappings are table\n    driven.  Encoding suffixes are case sensitive; type suffixes are\n    first tried case sensitive, then case insensitive.\n\n    The suffixes .tgz, .taz and .tz (case sensitive!) are all mapped\n    to \".tar.gz\".  (This is table-driven too, using the dictionary\n    suffix_map).\n\n    Optional `strict' argument when false adds a bunch of commonly found, but\n    non-standard types.\n    \"\"\"\n    if _db is None:\n        init()\n    return _db.guess_type(url, strict)\n\n\ndef guess_all_extensions(type, strict=True):\n    \"\"\"Guess the extensions for a file based on its MIME type.\n\n    Return value is a list of strings giving the possible filename\n    extensions, including the leading dot ('.').  The extension is not\n    guaranteed to have been associated with any particular data\n    stream, but would be mapped to the MIME type `type' by\n    guess_type().  If no extension can be guessed for `type', None\n    is returned.\n\n    Optional `strict' argument when false adds a bunch of commonly found,\n    but non-standard types.\n    \"\"\"\n    if _db is None:\n        init()\n    return _db.guess_all_extensions(type, strict)\n\ndef guess_extension(type, strict=True):\n    \"\"\"Guess the extension for a file based on its MIME type.\n\n    Return value is a string giving a filename extension, including the\n    leading dot ('.').  The extension is not guaranteed to have been\n    associated with any particular data stream, but would be mapped to the\n    MIME type `type' by guess_type().  If no extension can be guessed for\n    `type', None is returned.\n\n    Optional `strict' argument when false adds a bunch of commonly found,\n    but non-standard types.\n    \"\"\"\n    if _db is None:\n        init()\n    return _db.guess_extension(type, strict)\n\ndef add_type(type, ext, strict=True):\n    \"\"\"Add a mapping between a type and an extension.\n\n    When the extension is already known, the new\n    type will replace the old one. When the type\n    is already known the extension will be added\n    to the list of known extensions.\n\n    If strict is true, information will be added to\n    list of standard types, else to the list of non-standard\n    types.\n    \"\"\"\n    if _db is None:\n        init()\n    return _db.add_type(type, ext, strict)\n\n\ndef init(files=None):\n    global suffix_map, types_map, encodings_map, common_types\n    global inited, _db\n    inited = True    # so that MimeTypes.__init__() doesn't call us again\n    db = MimeTypes()\n    if files is None:\n        if _winreg:\n            db.read_windows_registry()\n        files = knownfiles\n    for file in files:\n        if os.path.isfile(file):\n            db.read(file)\n    encodings_map = db.encodings_map\n    suffix_map = db.suffix_map\n    types_map = db.types_map[True]\n    common_types = db.types_map[False]\n    # Make the DB a global variable now that it is fully initialized\n    _db = db\n\n\ndef read_mime_types(file):\n    try:\n        f = open(file)\n    except IOError:\n        return None\n    with f:\n        db = MimeTypes()\n        db.readfp(f, True)\n        return db.types_map[True]\n\n\ndef _default_mime_types():\n    global suffix_map\n    global encodings_map\n    global types_map\n    global common_types\n\n    suffix_map = {\n        '.tgz': '.tar.gz',\n        '.taz': '.tar.gz',\n        '.tz': '.tar.gz',\n        '.tbz2': '.tar.bz2',\n        '.txz': '.tar.xz',\n        }\n\n    encodings_map = {\n        '.gz': 'gzip',\n        '.Z': 'compress',\n        '.bz2': 'bzip2',\n        '.xz': 'xz',\n        }\n\n    # Before adding new types, make sure they are either registered with IANA,\n    # at http://www.isi.edu/in-notes/iana/assignments/media-types\n    # or extensions, i.e. using the x- prefix\n\n    # If you add to these, please keep them sorted!\n    types_map = {\n        '.a'      : 'application/octet-stream',\n        '.ai'     : 'application/postscript',\n        '.aif'    : 'audio/x-aiff',\n        '.aifc'   : 'audio/x-aiff',\n        '.aiff'   : 'audio/x-aiff',\n        '.au'     : 'audio/basic',\n        '.avi'    : 'video/x-msvideo',\n        '.bat'    : 'text/plain',\n        '.bcpio'  : 'application/x-bcpio',\n        '.bin'    : 'application/octet-stream',\n        '.bmp'    : 'image/x-ms-bmp',\n        '.c'      : 'text/plain',\n        # Duplicates :(\n        '.cdf'    : 'application/x-cdf',\n        '.cdf'    : 'application/x-netcdf',\n        '.cpio'   : 'application/x-cpio',\n        '.csh'    : 'application/x-csh',\n        '.css'    : 'text/css',\n        '.dll'    : 'application/octet-stream',\n        '.doc'    : 'application/msword',\n        '.dot'    : 'application/msword',\n        '.dvi'    : 'application/x-dvi',\n        '.eml'    : 'message/rfc822',\n        '.eps'    : 'application/postscript',\n        '.etx'    : 'text/x-setext',\n        '.exe'    : 'application/octet-stream',\n        '.gif'    : 'image/gif',\n        '.gtar'   : 'application/x-gtar',\n        '.h'      : 'text/plain',\n        '.hdf'    : 'application/x-hdf',\n        '.htm'    : 'text/html',\n        '.html'   : 'text/html',\n        '.ico'    : 'image/vnd.microsoft.icon',\n        '.ief'    : 'image/ief',\n        '.jpe'    : 'image/jpeg',\n        '.jpeg'   : 'image/jpeg',\n        '.jpg'    : 'image/jpeg',\n        '.js'     : 'application/javascript',\n        '.ksh'    : 'text/plain',\n        '.latex'  : 'application/x-latex',\n        '.m1v'    : 'video/mpeg',\n        '.man'    : 'application/x-troff-man',\n        '.me'     : 'application/x-troff-me',\n        '.mht'    : 'message/rfc822',\n        '.mhtml'  : 'message/rfc822',\n        '.mif'    : 'application/x-mif',\n        '.mov'    : 'video/quicktime',\n        '.movie'  : 'video/x-sgi-movie',\n        '.mp2'    : 'audio/mpeg',\n        '.mp3'    : 'audio/mpeg',\n        '.mp4'    : 'video/mp4',\n        '.mpa'    : 'video/mpeg',\n        '.mpe'    : 'video/mpeg',\n        '.mpeg'   : 'video/mpeg',\n        '.mpg'    : 'video/mpeg',\n        '.ms'     : 'application/x-troff-ms',\n        '.nc'     : 'application/x-netcdf',\n        '.nws'    : 'message/rfc822',\n        '.o'      : 'application/octet-stream',\n        '.obj'    : 'application/octet-stream',\n        '.oda'    : 'application/oda',\n        '.p12'    : 'application/x-pkcs12',\n        '.p7c'    : 'application/pkcs7-mime',\n        '.pbm'    : 'image/x-portable-bitmap',\n        '.pdf'    : 'application/pdf',\n        '.pfx'    : 'application/x-pkcs12',\n        '.pgm'    : 'image/x-portable-graymap',\n        '.pl'     : 'text/plain',\n        '.png'    : 'image/png',\n        '.pnm'    : 'image/x-portable-anymap',\n        '.pot'    : 'application/vnd.ms-powerpoint',\n        '.ppa'    : 'application/vnd.ms-powerpoint',\n        '.ppm'    : 'image/x-portable-pixmap',\n        '.pps'    : 'application/vnd.ms-powerpoint',\n        '.ppt'    : 'application/vnd.ms-powerpoint',\n        '.ps'     : 'application/postscript',\n        '.pwz'    : 'application/vnd.ms-powerpoint',\n        '.py'     : 'text/x-python',\n        '.pyc'    : 'application/x-python-code',\n        '.pyo'    : 'application/x-python-code',\n        '.qt'     : 'video/quicktime',\n        '.ra'     : 'audio/x-pn-realaudio',\n        '.ram'    : 'application/x-pn-realaudio',\n        '.ras'    : 'image/x-cmu-raster',\n        '.rdf'    : 'application/xml',\n        '.rgb'    : 'image/x-rgb',\n        '.roff'   : 'application/x-troff',\n        '.rtx'    : 'text/richtext',\n        '.sgm'    : 'text/x-sgml',\n        '.sgml'   : 'text/x-sgml',\n        '.sh'     : 'application/x-sh',\n        '.shar'   : 'application/x-shar',\n        '.snd'    : 'audio/basic',\n        '.so'     : 'application/octet-stream',\n        '.src'    : 'application/x-wais-source',\n        '.sv4cpio': 'application/x-sv4cpio',\n        '.sv4crc' : 'application/x-sv4crc',\n        '.swf'    : 'application/x-shockwave-flash',\n        '.t'      : 'application/x-troff',\n        '.tar'    : 'application/x-tar',\n        '.tcl'    : 'application/x-tcl',\n        '.tex'    : 'application/x-tex',\n        '.texi'   : 'application/x-texinfo',\n        '.texinfo': 'application/x-texinfo',\n        '.tif'    : 'image/tiff',\n        '.tiff'   : 'image/tiff',\n        '.tr'     : 'application/x-troff',\n        '.tsv'    : 'text/tab-separated-values',\n        '.txt'    : 'text/plain',\n        '.ustar'  : 'application/x-ustar',\n        '.vcf'    : 'text/x-vcard',\n        '.wav'    : 'audio/x-wav',\n        '.wiz'    : 'application/msword',\n        '.wsdl'   : 'application/xml',\n        '.xbm'    : 'image/x-xbitmap',\n        '.xlb'    : 'application/vnd.ms-excel',\n        # Duplicates :(\n        '.xls'    : 'application/excel',\n        '.xls'    : 'application/vnd.ms-excel',\n        '.xml'    : 'text/xml',\n        '.xpdl'   : 'application/xml',\n        '.xpm'    : 'image/x-xpixmap',\n        '.xsl'    : 'application/xml',\n        '.xwd'    : 'image/x-xwindowdump',\n        '.zip'    : 'application/zip',\n        }\n\n    # These are non-standard types, commonly found in the wild.  They will\n    # only match if strict=0 flag is given to the API methods.\n\n    # Please sort these too\n    common_types = {\n        '.jpg' : 'image/jpg',\n        '.mid' : 'audio/midi',\n        '.midi': 'audio/midi',\n        '.pct' : 'image/pict',\n        '.pic' : 'image/pict',\n        '.pict': 'image/pict',\n        '.rtf' : 'application/rtf',\n        '.xul' : 'text/xul'\n        }\n\n\n_default_mime_types()\n\n\nif __name__ == '__main__':\n    import getopt\n\n    USAGE = \"\"\"\\\nUsage: mimetypes.py [options] type\n\nOptions:\n    --help / -h       -- print this message and exit\n    --lenient / -l    -- additionally search of some common, but non-standard\n                         types.\n    --extension / -e  -- guess extension instead of type\n\nMore than one type argument may be given.\n\"\"\"\n\n    def usage(code, msg=''):\n        print USAGE\n        if msg: print msg\n        sys.exit(code)\n\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], 'hle',\n                                   ['help', 'lenient', 'extension'])\n    except getopt.error, msg:\n        usage(1, msg)\n\n    strict = 1\n    extension = 0\n    for opt, arg in opts:\n        if opt in ('-h', '--help'):\n            usage(0)\n        elif opt in ('-l', '--lenient'):\n            strict = 0\n        elif opt in ('-e', '--extension'):\n            extension = 1\n    for gtype in args:\n        if extension:\n            guess = guess_extension(gtype, strict)\n            if not guess: print \"I don't know anything about type\", gtype\n            else: print guess\n        else:\n            guess, encoding = guess_type(gtype, strict)\n            if not guess: print \"I don't know anything about type\", gtype\n            else: print 'type:', guess, 'encoding:', encoding\n", 
    "mttest": "import js\nfrom multiprocessing import Queue\nimport os\n\n\njq = js.globals['$']\n\n\n\ndef tst():\n    print \"running test\"\n    try:\n        v=IQ.get(False)\n        print \"got value\",v\n        OQ.put(apply(jq(v[0]).css, v[1]))\n    except Exception as e:\n        print e\n    finally:\n        print js.globals['setTimeout'](tst, 5000)\n\n\ndef helloWorld(iq,oq):\n    global IQ, OQ\n    IQ=iq\n    OQ=oq\n    print js.globals['setTimeout'](tst, 5000)\n", 
    "multiprocessing.__init__": "#\n# Package analogous to 'threading.py' but using processes\n#\n# multiprocessing/__init__.py\n#\n# This package is intended to duplicate the functionality (and much of\n# the API) of threading.py but uses processes instead of threads.  A\n# subpackage 'multiprocessing.dummy' has the same API but is a simple\n# wrapper for 'threading'.\n#\n# Try calling `multiprocessing.doc.main()` to read the html\n# documentation in a webbrowser.\n#\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__version__ = '0.70a1'\n\n__all__ = [\n    'Process', 'current_process', 'active_children', 'freeze_support',\n    'Manager', 'Pipe', 'cpu_count', 'log_to_stderr', 'get_logger',\n    'allow_connection_pickling', 'BufferTooShort', 'TimeoutError',\n    'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Condition',\n    'Event', 'Queue', 'JoinableQueue', 'Pool', 'Value', 'Array',\n    'RawValue', 'RawArray', 'SUBDEBUG', 'SUBWARNING',\n    ]\n\n__author__ = 'R. Oudkerk (r.m.oudkerk@gmail.com)'\n\n#\n# Imports\n#\n\nimport os\nimport sys\n\nfrom multiprocessing.process import Process, current_process, active_children\nfrom multiprocessing.util import SUBDEBUG, SUBWARNING\n\n#\n# Exceptions\n#\n\nclass ProcessError(Exception):\n    pass\n\nclass BufferTooShort(ProcessError):\n    pass\n\nclass TimeoutError(ProcessError):\n    pass\n\nclass AuthenticationError(ProcessError):\n    pass\n\n# This is down here because _multiprocessing uses BufferTooShort\n#import _multiprocessing\n\n#\n# Definitions not depending on native semaphores\n#\n\ndef Manager():\n    '''\n    Returns a manager associated with a running server process\n\n    The managers methods such as `Lock()`, `Condition()` and `Queue()`\n    can be used to create shared objects.\n    '''\n    from multiprocessing.managers import SyncManager\n    m = SyncManager()\n    m.start()\n    return m\n\ndef Pipe(duplex=True, buffer_id = None):\n    '''\n    Returns two connection object connected by a pipe\n    '''\n    from multiprocessing.connection import Pipe\n    return Pipe(duplex, buffer_id)\n\ndef cpu_count():\n    '''\n    Returns the number of CPUs in the system\n    '''\n    if sys.platform == 'win32':\n        try:\n            num = int(os.environ['NUMBER_OF_PROCESSORS'])\n        except (ValueError, KeyError):\n            num = 0\n    elif 'bsd' in sys.platform or sys.platform == 'darwin':\n        comm = '/sbin/sysctl -n hw.ncpu'\n        if sys.platform == 'darwin':\n            comm = '/usr' + comm\n        try:\n            with os.popen(comm) as p:\n                num = int(p.read())\n        except ValueError:\n            num = 0\n    else:\n        try:\n            num = os.sysconf('SC_NPROCESSORS_ONLN')\n        except (ValueError, OSError, AttributeError):\n            num = 0\n\n    if num >= 1:\n        return num\n    else:\n        raise NotImplementedError('cannot determine number of cpus')\n\ndef freeze_support():\n    '''\n    Check whether this is a fake forked process in a frozen executable.\n    If so then run code specified by commandline and exit.\n    '''\n    if sys.platform == 'win32' and getattr(sys, 'frozen', False):\n        from multiprocessing.forking import freeze_support\n        freeze_support()\n\ndef get_logger():\n    '''\n    Return package logger -- if it does not already exist then it is created\n    '''\n    from multiprocessing.util import get_logger\n    return get_logger()\n\ndef log_to_stderr(level=None):\n    '''\n    Turn on logging and add a handler which prints to stderr\n    '''\n    from multiprocessing.util import log_to_stderr\n    return log_to_stderr(level)\n\ndef allow_connection_pickling():\n    '''\n    Install support for sending connections and sockets between processes\n    '''\n    from multiprocessing import reduction\n\n#\n# Definitions depending on native semaphores\n#\n\ndef Lock():\n    '''\n    Returns a non-recursive lock object\n    '''\n    from multiprocessing.synchronize import Lock\n    return Lock()\n\ndef RLock():\n    '''\n    Returns a recursive lock object\n    '''\n    from multiprocessing.synchronize import RLock\n    return RLock()\n\ndef Condition(lock=None):\n    '''\n    Returns a condition object\n    '''\n    from multiprocessing.synchronize import Condition\n    return Condition(lock)\n\ndef Semaphore(value=1):\n    '''\n    Returns a semaphore object\n    '''\n    from multiprocessing.synchronize import Semaphore\n    return Semaphore(value)\n\ndef BoundedSemaphore(value=1):\n    '''\n    Returns a bounded semaphore object\n    '''\n    from multiprocessing.synchronize import BoundedSemaphore\n    return BoundedSemaphore(value)\n\ndef Event():\n    '''\n    Returns an event object\n    '''\n    from multiprocessing.synchronize import Event\n    return Event()\n\ndef Queue(maxsize=0):\n    '''\n    Returns a queue object\n    '''\n    from multiprocessing.queues import Queue\n    return Queue(maxsize)\n\ndef JoinableQueue(maxsize=0):\n    '''\n    Returns a queue object\n    '''\n    from multiprocessing.queues import JoinableQueue\n    return JoinableQueue(maxsize)\n\ndef Pool(processes=None, initializer=None, initargs=(), maxtasksperchild=None):\n    '''\n    Returns a process pool object\n    '''\n    from multiprocessing.pool import Pool\n    return Pool(processes, initializer, initargs, maxtasksperchild)\n\ndef RawValue(typecode_or_type, *args):\n    '''\n    Returns a shared object\n    '''\n    from multiprocessing.sharedctypes import RawValue\n    return RawValue(typecode_or_type, *args)\n\ndef RawArray(typecode_or_type, size_or_initializer):\n    '''\n    Returns a shared array\n    '''\n    from multiprocessing.sharedctypes import RawArray\n    return RawArray(typecode_or_type, size_or_initializer)\n\ndef Value(typecode_or_type, *args, **kwds):\n    '''\n    Returns a synchronized shared object\n    '''\n    from multiprocessing.sharedctypes import Value\n    return Value(typecode_or_type, *args, **kwds)\n\ndef Array(typecode_or_type, size_or_initializer, **kwds):\n    '''\n    Returns a synchronized shared array\n    '''\n    from multiprocessing.sharedctypes import Array\n    return Array(typecode_or_type, size_or_initializer, **kwds)\n\n#\n#\n#\n\nif sys.platform == 'win32':\n\n    def set_executable(executable):\n        '''\n        Sets the path to a python.exe or pythonw.exe binary used to run\n        child processes on Windows instead of sys.executable.\n        Useful for people embedding Python.\n        '''\n        from multiprocessing.forking import set_executable\n        set_executable(executable)\n\n    __all__ += ['set_executable']\n", 
    "multiprocessing.connection": "#\n# A higher level module for using sockets (or Windows named pipes)\n#\n# multiprocessing/connection.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__all__ = [ 'Client', 'Listener', 'Pipe' ]\n\nimport os\nimport sys\nimport socket\nimport errno\nimport time\nimport tempfile\nimport itertools\n\n\nfrom multiprocessing import current_process, AuthenticationError\nfrom multiprocessing.util import get_temp_dir, Finalize, sub_debug, debug\nfrom multiprocessing.forking import duplicate, close\n\nclass ConnectionException(Exception):\n    pass\n#\n#\n#\n\nBUFSIZE = 8192\n# A very generous timeout when it comes to local connections...\nCONNECTION_TIMEOUT = 20.\n\n_mmap_counter = itertools.count()\n\ndefault_family = 'AF_INET'\nfamilies = ['AF_INET']\n\nif hasattr(socket, 'AF_UNIX'):\n    default_family = 'AF_UNIX'\n    families += ['AF_UNIX']\n\n\n\ndef _init_timeout(timeout=CONNECTION_TIMEOUT):\n    return time.time() + timeout\n\ndef _check_timeout(t):\n    return time.time() > t\n\n#\n#\n#\n\ndef arbitrary_address(family):\n    '''\n    Return an arbitrary free address for the given family\n    '''\n    if family == 'AF_INET':\n        return ('localhost', 0)\n    elif family == 'AF_UNIX':\n        return tempfile.mktemp(prefix='listener-', dir=get_temp_dir())\n    elif family == 'AF_PIPE':\n        return tempfile.mktemp(prefix=r'\\\\.\\pipe\\pyc-%d-%d-' %\n                               (os.getpid(), _mmap_counter.next()), dir=\"\")\n    else:\n        raise ValueError('unrecognized family')\n\n\ndef address_type(address):\n    '''\n    Return the types of the address\n\n    This can be 'AF_INET', 'AF_UNIX', or 'AF_PIPE'\n    '''\n    if type(address) == tuple:\n        return 'AF_INET'\n    elif type(address) is str and address.startswith('\\\\\\\\'):\n        return 'AF_PIPE'\n    elif type(address) is str:\n        return 'AF_UNIX'\n    else:\n        raise ValueError('address type of %r unrecognized' % address)\n\n#\n# Public functions\n#\n\nclass Listener(object):\n    '''\n    Returns a listener object.\n\n    This is a wrapper for a bound socket which is 'listening' for\n    connections, or for a Windows named pipe.\n    '''\n    def __init__(self, address=None, family=None, backlog=1, authkey=None):\n        family = family or (address and address_type(address)) \\\n                 or default_family\n        address = address or arbitrary_address(family)\n\n        if family == 'AF_PIPE':\n            self._listener = PipeListener(address, backlog)\n        else:\n            self._listener = SocketListener(address, family, backlog)\n\n        if authkey is not None and not isinstance(authkey, bytes):\n            raise TypeError, 'authkey should be a byte string'\n\n        self._authkey = authkey\n\n    def accept(self):\n        '''\n        Accept a connection on the bound socket or named pipe of `self`.\n\n        Returns a `Connection` object.\n        '''\n        c = self._listener.accept()\n        if self._authkey:\n            deliver_challenge(c, self._authkey)\n            answer_challenge(c, self._authkey)\n        return c\n\n    def close(self):\n        '''\n        Close the bound socket or named pipe of `self`.\n        '''\n        return self._listener.close()\n\n    address = property(lambda self: self._listener._address)\n    last_accepted = property(lambda self: self._listener._last_accepted)\n\n\ndef Client(address, family=None, authkey=None):\n    '''\n    Returns a connection to the address of a `Listener`\n    '''\n    family = family or address_type(address)\n    if family == 'AF_PIPE':\n        c = PipeClient(address)\n    else:\n        c = SocketClient(address)\n\n    if authkey is not None and not isinstance(authkey, bytes):\n        raise TypeError, 'authkey should be a byte string'\n\n    if authkey is not None:\n        answer_challenge(c, authkey)\n        deliver_challenge(c, authkey)\n\n    return c\n\n\n\n\n\nif sys.platform != 'win32':\n\n    def Pipe(duplex=True, buffer_id=None):\n        import _multiprocessing\n        '''\n        Returns pair of connection objects at either end of a pipe\n        '''\n        if duplex:\n            raise ConnectionException('can\\'t do duplex')\n        fd1, fd2 = os.pipe(buffer_id)\n        c1 = _multiprocessing.Connection(fd1, writeable=False)\n        c2 = _multiprocessing.Connection(fd2, readable=False)\n\n        return c1, c2\n\n\n#\n# Definitions for connections based on sockets\n#\n\nclass SocketListener(object):\n    '''\n    Representation of a socket which is bound to an address and listening\n    '''\n    def __init__(self, address, family, backlog=1):\n        self._socket = socket.socket(getattr(socket, family))\n        try:\n            self._socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            self._socket.setblocking(True)\n            self._socket.bind(address)\n            self._socket.listen(backlog)\n            self._address = self._socket.getsockname()\n        except socket.error:\n            self._socket.close()\n            raise\n        self._family = family\n        self._last_accepted = None\n\n        if family == 'AF_UNIX':\n            self._unlink = Finalize(\n                self, os.unlink, args=(address,), exitpriority=0\n                )\n        else:\n            self._unlink = None\n\n    def accept(self):\n        import _multiprocessing\n        while True:\n            try:\n                s, self._last_accepted = self._socket.accept()\n            except socket.error as e:\n                if e.args[0] != errno.EINTR:\n                    raise\n            else:\n                break\n        s.setblocking(True)\n        fd = duplicate(s.fileno())\n        conn = _multiprocessing.Connection(fd)\n        s.close()\n        return conn\n\n    def close(self):\n        self._socket.close()\n        if self._unlink is not None:\n            self._unlink()\n\n\ndef SocketClient(address):\n    '''\n    Return a connection object connected to the socket given by `address`\n    '''\n    import _multiprocessing\n    family = getattr(socket, address_type(address))\n    t = _init_timeout()\n\n    while 1:\n        s = socket.socket(family)\n        s.setblocking(True)\n        try:\n            s.connect(address)\n        except socket.error, e:\n            s.close()\n            if e.args[0] != errno.ECONNREFUSED or _check_timeout(t):\n                debug('failed to connect to address %s', address)\n                raise\n            time.sleep(0.01)\n        else:\n            break\n    else:\n        raise\n\n    fd = duplicate(s.fileno())\n    conn = _multiprocessing.Connection(fd)\n    s.close()\n    return conn\n\n#\n# Definitions for connections based on named pipes\n#\n#\n# Authentication stuff\n#\n\nMESSAGE_LENGTH = 20\n\nCHALLENGE = b'#CHALLENGE#'\nWELCOME = b'#WELCOME#'\nFAILURE = b'#FAILURE#'\n\ndef deliver_challenge(connection, authkey):\n    import hmac\n    assert isinstance(authkey, bytes)\n    message = os.urandom(MESSAGE_LENGTH)\n    connection.send_bytes(CHALLENGE + message)\n    digest = hmac.new(authkey, message).digest()\n    response = connection.recv_bytes(256)        # reject large message\n    if response == digest:\n        connection.send_bytes(WELCOME)\n    else:\n        connection.send_bytes(FAILURE)\n        raise AuthenticationError('digest received was wrong')\n\ndef answer_challenge(connection, authkey):\n    import hmac\n    assert isinstance(authkey, bytes)\n    message = connection.recv_bytes(256)         # reject large message\n    assert message[:len(CHALLENGE)] == CHALLENGE, 'message = %r' % message\n    message = message[len(CHALLENGE):]\n    digest = hmac.new(authkey, message).digest()\n    connection.send_bytes(digest)\n    response = connection.recv_bytes(256)        # reject large message\n    if response != WELCOME:\n        raise AuthenticationError('digest sent was rejected')\n\n#\n# Support for using xmlrpclib for serialization\n#\n\nclass ConnectionWrapper(object):\n    def __init__(self, conn, dumps, loads):\n        self._conn = conn\n        self._dumps = dumps\n        self._loads = loads\n        for attr in ('fileno', 'close', 'poll', 'recv_bytes', 'send_bytes'):\n            obj = getattr(conn, attr)\n            setattr(self, attr, obj)\n    def send(self, obj):\n        s = self._dumps(obj)\n        self._conn.send_bytes(s)\n    def recv(self):\n        s = self._conn.recv_bytes()\n        return self._loads(s)\n\ndef _xml_dumps(obj):\n    return xmlrpclib.dumps((obj,), None, None, None, 1).encode('utf8')\n\ndef _xml_loads(s):\n    (obj,), method = xmlrpclib.loads(s.decode('utf8'))\n    return obj\n\nclass XmlListener(Listener):\n    def accept(self):\n        global xmlrpclib\n        import xmlrpclib\n        obj = Listener.accept(self)\n        return ConnectionWrapper(obj, _xml_dumps, _xml_loads)\n\ndef XmlClient(*args, **kwds):\n    global xmlrpclib\n    import xmlrpclib\n    return ConnectionWrapper(Client(*args, **kwds), _xml_dumps, _xml_loads)\n", 
    "multiprocessing.dummy.__init__": "#\n# Support for the API of the multiprocessing package using threads\n#\n# multiprocessing/dummy/__init__.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__all__ = [\n    'Process', 'current_process', 'active_children', 'freeze_support',\n    'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Condition',\n    'Event', 'Queue', 'Manager', 'Pipe', 'Pool', 'JoinableQueue'\n    ]\n\n#\n# Imports\n#\n\nimport threading\nimport sys\nimport weakref\nimport array\nimport itertools\n\nfrom multiprocessing import TimeoutError, cpu_count\nfrom multiprocessing.dummy.connection import Pipe\nfrom threading import Lock, RLock, Semaphore, BoundedSemaphore\nfrom threading import Event\nfrom Queue import Queue\n\n#\n#\n#\n\nclass DummyProcess(threading.Thread):\n\n    def __init__(self, group=None, target=None, name=None, args=(), kwargs={}):\n        threading.Thread.__init__(self, group, target, name, args, kwargs)\n        self._pid = None\n        self._children = weakref.WeakKeyDictionary()\n        self._start_called = False\n        self._parent = current_process()\n\n    def start(self):\n        assert self._parent is current_process()\n        self._start_called = True\n        if hasattr(self._parent, '_children'):\n            self._parent._children[self] = None\n        threading.Thread.start(self)\n\n    @property\n    def exitcode(self):\n        if self._start_called and not self.is_alive():\n            return 0\n        else:\n            return None\n\n#\n#\n#\n\nclass Condition(threading._Condition):\n    notify_all = threading._Condition.notify_all.im_func\n\n#\n#\n#\n\nProcess = DummyProcess\ncurrent_process = threading.current_thread\ncurrent_process()._children = weakref.WeakKeyDictionary()\n\ndef active_children():\n    children = current_process()._children\n    for p in list(children):\n        if not p.is_alive():\n            children.pop(p, None)\n    return list(children)\n\ndef freeze_support():\n    pass\n\n#\n#\n#\n\nclass Namespace(object):\n    def __init__(self, **kwds):\n        self.__dict__.update(kwds)\n    def __repr__(self):\n        items = self.__dict__.items()\n        temp = []\n        for name, value in items:\n            if not name.startswith('_'):\n                temp.append('%s=%r' % (name, value))\n        temp.sort()\n        return 'Namespace(%s)' % str.join(', ', temp)\n\ndict = dict\nlist = list\n\ndef Array(typecode, sequence, lock=True):\n    return array.array(typecode, sequence)\n\nclass Value(object):\n    def __init__(self, typecode, value, lock=True):\n        self._typecode = typecode\n        self._value = value\n    def _get(self):\n        return self._value\n    def _set(self, value):\n        self._value = value\n    value = property(_get, _set)\n    def __repr__(self):\n        return '<%s(%r, %r)>'%(type(self).__name__,self._typecode,self._value)\n\ndef Manager():\n    return sys.modules[__name__]\n\ndef shutdown():\n    pass\n\ndef Pool(processes=None, initializer=None, initargs=()):\n    from multiprocessing.pool import ThreadPool\n    return ThreadPool(processes, initializer, initargs)\n\nJoinableQueue = Queue\n", 
    "multiprocessing.dummy.connection": "#\n# Analogue of `multiprocessing.connection` which uses queues instead of sockets\n#\n# multiprocessing/dummy/connection.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__all__ = [ 'Client', 'Listener', 'Pipe' ]\n\nfrom Queue import Queue\n\n\nfamilies = [None]\n\n\nclass Listener(object):\n\n    def __init__(self, address=None, family=None, backlog=1):\n        self._backlog_queue = Queue(backlog)\n\n    def accept(self):\n        return Connection(*self._backlog_queue.get())\n\n    def close(self):\n        self._backlog_queue = None\n\n    address = property(lambda self: self._backlog_queue)\n\n\ndef Client(address):\n    _in, _out = Queue(), Queue()\n    address.put((_out, _in))\n    return Connection(_in, _out)\n\n\ndef Pipe(duplex=True):\n    a, b = Queue(), Queue()\n    return Connection(a, b), Connection(b, a)\n\n\nclass Connection(object):\n\n    def __init__(self, _in, _out):\n        self._out = _out\n        self._in = _in\n        self.send = self.send_bytes = _out.put\n        self.recv = self.recv_bytes = _in.get\n\n    def poll(self, timeout=0.0):\n        if self._in.qsize() > 0:\n            return True\n        if timeout <= 0.0:\n            return False\n        self._in.not_empty.acquire()\n        self._in.not_empty.wait(timeout)\n        self._in.not_empty.release()\n        return self._in.qsize() > 0\n\n    def close(self):\n        pass\n", 
    "multiprocessing.forking": "#\n# Module for starting a process object using os.fork() or CreateProcess()\n#\n# multiprocessing/forking.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\nimport os\nimport sys\nimport signal\nimport errno\n\nfrom multiprocessing import util, process\n\n__all__ = ['Popen', 'assert_spawning', 'exit', 'duplicate', 'close', 'ForkingPickler']\n\n#\n# Check that the current thread is spawning a child process\n#\n\ndef assert_spawning(self):\n    #~ if not Popen.thread_is_spawning():\n        #~ raise RuntimeError(\n            #~ '%s objects should only be shared between processes'\n            #~ ' through inheritance' % type(self).__name__\n            #~ )\n    pass\n\n#\n# Try making some callable types picklable\n#\n\nfrom pickle import Pickler\nclass ForkingPickler(Pickler):\n    dispatch = Pickler.dispatch.copy()\n\n    @classmethod\n    def register(cls, type, reduce):\n        def dispatcher(self, obj):\n            rv = reduce(obj)\n            self.save_reduce(obj=obj, *rv)\n        cls.dispatch[type] = dispatcher\n\ndef _reduce_method(m):\n    if m.im_self is None:\n        return getattr, (m.im_class, m.im_func.func_name)\n    else:\n        return getattr, (m.im_self, m.im_func.func_name)\nForkingPickler.register(type(ForkingPickler.save), _reduce_method)\n\nif type(list.append) is not type(ForkingPickler.save):\n    # Some python implementations have unbound methods even for builtin types\n    def _reduce_method_descriptor(m):\n        return getattr, (m.__objclass__, m.__name__)\n    ForkingPickler.register(type(list.append), _reduce_method_descriptor)\n    ForkingPickler.register(type(int.__add__), _reduce_method_descriptor)\n\ntry:\n    from functools import partial\nexcept ImportError:\n    pass\nelse:\n    def _reduce_partial(p):\n        return _rebuild_partial, (p.func, p.args, p.keywords or {})\n    def _rebuild_partial(func, args, keywords):\n        return partial(func, *args, **keywords)\n    ForkingPickler.register(partial, _reduce_partial)\n\n#\n# Unix\n#\n\no=[]\n\nif sys.platform != 'win32':\n    import time\n    import js\n    def fork(obj):\n        import pickle\n        dta = pickle.dumps(obj)\n        return js.globals['fork'](dta)\n    js.fork=fork\n    exit = os._exit\n    duplicate = os.dup\n    close = os.close\n\n    #\n    # We define a Popen class similar to the one from subprocess, but\n    # whose constructor takes a process object as its argument.\n    #\n\n    class Popen(object):\n        isSpawning = False\n        def __init__(self, process_obj, isChild=False):\n            sys.stdout.flush()\n            sys.stderr.flush()\n            self.returncode = None\n            \n            if isChild:\n                if 'random' in sys.modules:\n                    import random\n                    random.seed()\n                code = process_obj._bootstrap()\n                sys.stdout.flush()\n                sys.stderr.flush()\n                os._exit(code)\n            else:\n                Popen.isSpawning = True\n                self.pid = js.fork(process_obj)\n                Popen.isSpawning = False\n        def poll(self, flag=os.WNOHANG):\n            if self.returncode is None:\n                while True:\n                    try:\n                        pid, sts = os.waitpid(self.pid, flag)\n                    except os.error as e:\n                        if e.errno == errno.EINTR:\n                            continue\n                        # Child process not yet created. See #1731717\n                        # e.errno == errno.ECHILD == 10\n                        return None\n                    else:\n                        break\n                if pid == self.pid:\n                    if os.WIFSIGNALED(sts):\n                        self.returncode = -os.WTERMSIG(sts)\n                    else:\n                        assert os.WIFEXITED(sts)\n                        self.returncode = os.WEXITSTATUS(sts)\n            return self.returncode\n\n        def wait(self, timeout=None):\n            if timeout is None:\n                return self.poll(0)\n            deadline = time.time() + timeout\n            delay = 0.0005\n            while 1:\n                res = self.poll()\n                if res is not None:\n                    break\n                remaining = deadline - time.time()\n                if remaining <= 0:\n                    break\n                delay = min(delay * 2, remaining, 0.05)\n                time.sleep(delay)\n            return res\n\n        def terminate(self):\n            if self.returncode is None:\n                try:\n                    os.kill(self.pid, signal.SIGTERM)\n                except OSError, e:\n                    if self.wait(timeout=0.1) is None:\n                        raise\n        @staticmethod\n        def duplicate_for_child(handle):\n            return os.dup(handle)\n        @staticmethod\n        def thread_is_spawning():\n            return Popen.isSpawning\n\n#\n# Prepare current process\n#\n\nold_main_modules = []\n\ndef prepare(data):\n    '''\n    Try to get current process ready to unpickle process object\n    '''\n    old_main_modules.append(sys.modules['__main__'])\n\n    if 'name' in data:\n        process.current_process().name = data['name']\n\n    if 'authkey' in data:\n        process.current_process()._authkey = data['authkey']\n\n    if 'log_to_stderr' in data and data['log_to_stderr']:\n        util.log_to_stderr()\n\n    if 'log_level' in data:\n        util.get_logger().setLevel(data['log_level'])\n\n    if 'sys_path' in data:\n        sys.path = data['sys_path']\n\n    if 'sys_argv' in data:\n        sys.argv = data['sys_argv']\n\n    if 'dir' in data:\n        os.chdir(data['dir'])\n\n    if 'orig_dir' in data:\n        process.ORIGINAL_DIR = data['orig_dir']\n\n    if 'main_path' in data:\n        main_path = data['main_path']\n        main_name = os.path.splitext(os.path.basename(main_path))[0]\n        if main_name == '__init__':\n            main_name = os.path.basename(os.path.dirname(main_path))\n\n        if main_name != 'ipython':\n            import imp\n\n            if main_path is None:\n                dirs = None\n            elif os.path.basename(main_path).startswith('__init__.py'):\n                dirs = [os.path.dirname(os.path.dirname(main_path))]\n            else:\n                dirs = [os.path.dirname(main_path)]\n\n            assert main_name not in sys.modules, main_name\n            file, path_name, etc = imp.find_module(main_name, dirs)\n            try:\n                # We would like to do \"imp.load_module('__main__', ...)\"\n                # here.  However, that would cause 'if __name__ ==\n                # \"__main__\"' clauses to be executed.\n                main_module = imp.load_module(\n                    '__parents_main__', file, path_name, etc\n                    )\n            finally:\n                if file:\n                    file.close()\n\n            sys.modules['__main__'] = main_module\n            main_module.__name__ = '__main__'\n\n            # Try to make the potentially picklable objects in\n            # sys.modules['__main__'] realize they are in the main\n            # module -- somewhat ugly.\n            for obj in main_module.__dict__.values():\n                try:\n                    if obj.__module__ == '__parents_main__':\n                        obj.__module__ = '__main__'\n                except Exception:\n                    pass\n", 
    "multiprocessing.heap": "#\n# Module which supports allocation of memory from an mmap\n#\n# multiprocessing/heap.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\nimport bisect\nimport mmap\nimport tempfile\nimport os\nimport sys\nimport threading\nimport itertools\n\nimport _multiprocessing\nfrom multiprocessing.util import Finalize, info\nfrom multiprocessing.forking import assert_spawning\n\n__all__ = ['BufferWrapper']\n\n#\n# Inheirtable class which wraps an mmap, and from which blocks can be allocated\n#\n\nif sys.platform == 'win32':\n\n    from _multiprocessing import win32\n\n    class Arena(object):\n\n        _counter = itertools.count()\n\n        def __init__(self, size):\n            self.size = size\n            self.name = 'pym-%d-%d' % (os.getpid(), Arena._counter.next())\n            self.buffer = mmap.mmap(-1, self.size, tagname=self.name)\n            assert win32.GetLastError() == 0, 'tagname already in use'\n            self._state = (self.size, self.name)\n\n        def __getstate__(self):\n            assert_spawning(self)\n            return self._state\n\n        def __setstate__(self, state):\n            self.size, self.name = self._state = state\n            self.buffer = mmap.mmap(-1, self.size, tagname=self.name)\n            assert win32.GetLastError() == win32.ERROR_ALREADY_EXISTS\n\nelse:\n\n    class Arena(object):\n\n        def __init__(self, size):\n            self.buffer = mmap.mmap(-1, size)\n            self.size = size\n            self.name = None\n\n#\n# Class allowing allocation of chunks of memory from arenas\n#\n\nclass Heap(object):\n\n    _alignment = 8\n\n    def __init__(self, size=mmap.PAGESIZE):\n        self._lastpid = os.getpid()\n        self._lock = threading.Lock()\n        self._size = size\n        self._lengths = []\n        self._len_to_seq = {}\n        self._start_to_block = {}\n        self._stop_to_block = {}\n        self._allocated_blocks = set()\n        self._arenas = []\n        # list of pending blocks to free - see free() comment below\n        self._pending_free_blocks = []\n\n    @staticmethod\n    def _roundup(n, alignment):\n        # alignment must be a power of 2\n        mask = alignment - 1\n        return (n + mask) & ~mask\n\n    def _malloc(self, size):\n        # returns a large enough block -- it might be much larger\n        i = bisect.bisect_left(self._lengths, size)\n        if i == len(self._lengths):\n            length = self._roundup(max(self._size, size), mmap.PAGESIZE)\n            self._size *= 2\n            info('allocating a new mmap of length %d', length)\n            arena = Arena(length)\n            self._arenas.append(arena)\n            return (arena, 0, length)\n        else:\n            length = self._lengths[i]\n            seq = self._len_to_seq[length]\n            block = seq.pop()\n            if not seq:\n                del self._len_to_seq[length], self._lengths[i]\n\n        (arena, start, stop) = block\n        del self._start_to_block[(arena, start)]\n        del self._stop_to_block[(arena, stop)]\n        return block\n\n    def _free(self, block):\n        # free location and try to merge with neighbours\n        (arena, start, stop) = block\n\n        try:\n            prev_block = self._stop_to_block[(arena, start)]\n        except KeyError:\n            pass\n        else:\n            start, _ = self._absorb(prev_block)\n\n        try:\n            next_block = self._start_to_block[(arena, stop)]\n        except KeyError:\n            pass\n        else:\n            _, stop = self._absorb(next_block)\n\n        block = (arena, start, stop)\n        length = stop - start\n\n        try:\n            self._len_to_seq[length].append(block)\n        except KeyError:\n            self._len_to_seq[length] = [block]\n            bisect.insort(self._lengths, length)\n\n        self._start_to_block[(arena, start)] = block\n        self._stop_to_block[(arena, stop)] = block\n\n    def _absorb(self, block):\n        # deregister this block so it can be merged with a neighbour\n        (arena, start, stop) = block\n        del self._start_to_block[(arena, start)]\n        del self._stop_to_block[(arena, stop)]\n\n        length = stop - start\n        seq = self._len_to_seq[length]\n        seq.remove(block)\n        if not seq:\n            del self._len_to_seq[length]\n            self._lengths.remove(length)\n\n        return start, stop\n\n    def _free_pending_blocks(self):\n        # Free all the blocks in the pending list - called with the lock held.\n        while True:\n            try:\n                block = self._pending_free_blocks.pop()\n            except IndexError:\n                break\n            self._allocated_blocks.remove(block)\n            self._free(block)\n\n    def free(self, block):\n        # free a block returned by malloc()\n        # Since free() can be called asynchronously by the GC, it could happen\n        # that it's called while self._lock is held: in that case,\n        # self._lock.acquire() would deadlock (issue #12352). To avoid that, a\n        # trylock is used instead, and if the lock can't be acquired\n        # immediately, the block is added to a list of blocks to be freed\n        # synchronously sometimes later from malloc() or free(), by calling\n        # _free_pending_blocks() (appending and retrieving from a list is not\n        # strictly thread-safe but under cPython it's atomic thanks to the GIL).\n        assert os.getpid() == self._lastpid\n        if not self._lock.acquire(False):\n            # can't acquire the lock right now, add the block to the list of\n            # pending blocks to free\n            self._pending_free_blocks.append(block)\n        else:\n            # we hold the lock\n            try:\n                self._free_pending_blocks()\n                self._allocated_blocks.remove(block)\n                self._free(block)\n            finally:\n                self._lock.release()\n\n    def malloc(self, size):\n        # return a block of right size (possibly rounded up)\n        assert 0 <= size < sys.maxint\n        if os.getpid() != self._lastpid:\n            self.__init__()                     # reinitialize after fork\n        self._lock.acquire()\n        self._free_pending_blocks()\n        try:\n            size = self._roundup(max(size,1), self._alignment)\n            (arena, start, stop) = self._malloc(size)\n            new_stop = start + size\n            if new_stop < stop:\n                self._free((arena, new_stop, stop))\n            block = (arena, start, new_stop)\n            self._allocated_blocks.add(block)\n            return block\n        finally:\n            self._lock.release()\n\n#\n# Class representing a chunk of an mmap -- can be inherited\n#\n\nclass BufferWrapper(object):\n\n    _heap = Heap()\n\n    def __init__(self, size):\n        assert 0 <= size < sys.maxint\n        block = BufferWrapper._heap.malloc(size)\n        self._state = (block, size)\n        Finalize(self, BufferWrapper._heap.free, args=(block,))\n\n    def get_address(self):\n        (arena, start, stop), size = self._state\n        address, length = _multiprocessing.address_of_buffer(arena.buffer)\n        assert size <= length\n        return address + start\n\n    def get_size(self):\n        return self._state[1]\n", 
    "multiprocessing.managers": "#\n# Module providing the `SyncManager` class for dealing\n# with shared objects\n#\n# multiprocessing/managers.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__all__ = [ 'BaseManager', 'SyncManager', 'BaseProxy', 'Token' ]\n\n#\n# Imports\n#\n\nimport os\nimport sys\nimport weakref\nimport threading\nimport array\nimport Queue\n\nfrom traceback import format_exc\nfrom multiprocessing import Process, current_process, active_children, Pool, util, connection\nfrom multiprocessing.process import AuthenticationString\nfrom multiprocessing.forking import exit, Popen, assert_spawning, ForkingPickler\nfrom multiprocessing.util import Finalize, info\n\ntry:\n    from cPickle import PicklingError\nexcept ImportError:\n    from pickle import PicklingError\n\n#\n# Register some things for pickling\n#\n\ndef reduce_array(a):\n    return array.array, (a.typecode, a.tostring())\nForkingPickler.register(array.array, reduce_array)\n\nview_types = [type(getattr({}, name)()) for name in ('items','keys','values')]\n\n#\n# Type for identifying shared objects\n#\n\nclass Token(object):\n    '''\n    Type to uniquely indentify a shared object\n    '''\n    __slots__ = ('typeid', 'address', 'id')\n\n    def __init__(self, typeid, address, id):\n        (self.typeid, self.address, self.id) = (typeid, address, id)\n\n    def __getstate__(self):\n        return (self.typeid, self.address, self.id)\n\n    def __setstate__(self, state):\n        (self.typeid, self.address, self.id) = state\n\n    def __repr__(self):\n        return 'Token(typeid=%r, address=%r, id=%r)' % \\\n               (self.typeid, self.address, self.id)\n\n#\n# Function for communication with a manager's server process\n#\n\ndef dispatch(c, id, methodname, args=(), kwds={}):\n    '''\n    Send a message to manager using connection `c` and return response\n    '''\n    c.send((id, methodname, args, kwds))\n    kind, result = c.recv()\n    if kind == '#RETURN':\n        return result\n    raise convert_to_error(kind, result)\n\ndef convert_to_error(kind, result):\n    if kind == '#ERROR':\n        return result\n    elif kind == '#TRACEBACK':\n        assert type(result) is str\n        return  RemoteError(result)\n    elif kind == '#UNSERIALIZABLE':\n        assert type(result) is str\n        return RemoteError('Unserializable message: %s\\n' % result)\n    else:\n        return ValueError('Unrecognized message type')\n\nclass RemoteError(Exception):\n    def __str__(self):\n        return ('\\n' + '-'*75 + '\\n' + str(self.args[0]) + '-'*75)\n\n#\n# Functions for finding the method names of an object\n#\n\ndef all_methods(obj):\n    '''\n    Return a list of names of methods of `obj`\n    '''\n    temp = []\n    for name in dir(obj):\n        func = getattr(obj, name)\n        if hasattr(func, '__call__'):\n            temp.append(name)\n    return temp\n\ndef public_methods(obj):\n    '''\n    Return a list of names of methods of `obj` which do not start with '_'\n    '''\n    return [name for name in all_methods(obj) if name[0] != '_']\n\n#\n# Server which is run in a process controlled by a manager\n#\n\nclass Server(object):\n    '''\n    Server class which runs in a process controlled by a manager object\n    '''\n    public = ['shutdown', 'create', 'accept_connection', 'get_methods',\n              'debug_info', 'number_of_objects', 'dummy', 'incref', 'decref']\n\n    def __init__(self, registry, address, authkey, serializer):\n        assert isinstance(authkey, bytes)\n        self.registry = registry\n        self.authkey = AuthenticationString(authkey)\n        Listener, Client = listener_client[serializer]\n\n        # do authentication later\n        self.listener = Listener(address=address, backlog=16)\n        self.address = self.listener.address\n\n        self.id_to_obj = {'0': (None, ())}\n        self.id_to_refcount = {}\n        self.mutex = threading.RLock()\n        self.stop = 0\n\n    def serve_forever(self):\n        '''\n        Run the server forever\n        '''\n        current_process()._manager_server = self\n        try:\n            try:\n                while 1:\n                    try:\n                        c = self.listener.accept()\n                    except (OSError, IOError):\n                        continue\n                    t = threading.Thread(target=self.handle_request, args=(c,))\n                    t.daemon = True\n                    t.start()\n            except (KeyboardInterrupt, SystemExit):\n                pass\n        finally:\n            self.stop = 999\n            self.listener.close()\n\n    def handle_request(self, c):\n        '''\n        Handle a new connection\n        '''\n        funcname = result = request = None\n        try:\n            connection.deliver_challenge(c, self.authkey)\n            connection.answer_challenge(c, self.authkey)\n            request = c.recv()\n            ignore, funcname, args, kwds = request\n            assert funcname in self.public, '%r unrecognized' % funcname\n            func = getattr(self, funcname)\n        except Exception:\n            msg = ('#TRACEBACK', format_exc())\n        else:\n            try:\n                result = func(c, *args, **kwds)\n            except Exception:\n                msg = ('#TRACEBACK', format_exc())\n            else:\n                msg = ('#RETURN', result)\n        try:\n            c.send(msg)\n        except Exception, e:\n            try:\n                c.send(('#TRACEBACK', format_exc()))\n            except Exception:\n                pass\n            util.info('Failure to send message: %r', msg)\n            util.info(' ... request was %r', request)\n            util.info(' ... exception was %r', e)\n\n        c.close()\n\n    def serve_client(self, conn):\n        '''\n        Handle requests from the proxies in a particular process/thread\n        '''\n        util.debug('starting server thread to service %r',\n                   threading.current_thread().name)\n\n        recv = conn.recv\n        send = conn.send\n        id_to_obj = self.id_to_obj\n\n        while not self.stop:\n\n            try:\n                methodname = obj = None\n                request = recv()\n                ident, methodname, args, kwds = request\n                obj, exposed, gettypeid = id_to_obj[ident]\n\n                if methodname not in exposed:\n                    raise AttributeError(\n                        'method %r of %r object is not in exposed=%r' %\n                        (methodname, type(obj), exposed)\n                        )\n\n                function = getattr(obj, methodname)\n\n                try:\n                    res = function(*args, **kwds)\n                except Exception, e:\n                    msg = ('#ERROR', e)\n                else:\n                    typeid = gettypeid and gettypeid.get(methodname, None)\n                    if typeid:\n                        rident, rexposed = self.create(conn, typeid, res)\n                        token = Token(typeid, self.address, rident)\n                        msg = ('#PROXY', (rexposed, token))\n                    else:\n                        msg = ('#RETURN', res)\n\n            except AttributeError:\n                if methodname is None:\n                    msg = ('#TRACEBACK', format_exc())\n                else:\n                    try:\n                        fallback_func = self.fallback_mapping[methodname]\n                        result = fallback_func(\n                            self, conn, ident, obj, *args, **kwds\n                            )\n                        msg = ('#RETURN', result)\n                    except Exception:\n                        msg = ('#TRACEBACK', format_exc())\n\n            except EOFError:\n                util.debug('got EOF -- exiting thread serving %r',\n                           threading.current_thread().name)\n                sys.exit(0)\n\n            except Exception:\n                msg = ('#TRACEBACK', format_exc())\n\n            try:\n                try:\n                    send(msg)\n                except Exception, e:\n                    send(('#UNSERIALIZABLE', repr(msg)))\n            except Exception, e:\n                util.info('exception in thread serving %r',\n                        threading.current_thread().name)\n                util.info(' ... message was %r', msg)\n                util.info(' ... exception was %r', e)\n                conn.close()\n                sys.exit(1)\n\n    def fallback_getvalue(self, conn, ident, obj):\n        return obj\n\n    def fallback_str(self, conn, ident, obj):\n        return str(obj)\n\n    def fallback_repr(self, conn, ident, obj):\n        return repr(obj)\n\n    fallback_mapping = {\n        '__str__':fallback_str,\n        '__repr__':fallback_repr,\n        '#GETVALUE':fallback_getvalue\n        }\n\n    def dummy(self, c):\n        pass\n\n    def debug_info(self, c):\n        '''\n        Return some info --- useful to spot problems with refcounting\n        '''\n        self.mutex.acquire()\n        try:\n            result = []\n            keys = self.id_to_obj.keys()\n            keys.sort()\n            for ident in keys:\n                if ident != '0':\n                    result.append('  %s:       refcount=%s\\n    %s' %\n                                  (ident, self.id_to_refcount[ident],\n                                   str(self.id_to_obj[ident][0])[:75]))\n            return '\\n'.join(result)\n        finally:\n            self.mutex.release()\n\n    def number_of_objects(self, c):\n        '''\n        Number of shared objects\n        '''\n        return len(self.id_to_obj) - 1      # don't count ident='0'\n\n    def shutdown(self, c):\n        '''\n        Shutdown this process\n        '''\n        try:\n            try:\n                util.debug('manager received shutdown message')\n                c.send(('#RETURN', None))\n\n                if sys.stdout != sys.__stdout__:\n                    util.debug('resetting stdout, stderr')\n                    sys.stdout = sys.__stdout__\n                    sys.stderr = sys.__stderr__\n\n                util._run_finalizers(0)\n\n                for p in active_children():\n                    util.debug('terminating a child process of manager')\n                    p.terminate()\n\n                for p in active_children():\n                    util.debug('terminating a child process of manager')\n                    p.join()\n\n                util._run_finalizers()\n                util.info('manager exiting with exitcode 0')\n            except:\n                import traceback\n                traceback.print_exc()\n        finally:\n            exit(0)\n\n    def create(self, c, typeid, *args, **kwds):\n        '''\n        Create a new shared object and return its id\n        '''\n        self.mutex.acquire()\n        try:\n            callable, exposed, method_to_typeid, proxytype = \\\n                      self.registry[typeid]\n\n            if callable is None:\n                assert len(args) == 1 and not kwds\n                obj = args[0]\n            else:\n                obj = callable(*args, **kwds)\n\n            if exposed is None:\n                exposed = public_methods(obj)\n            if method_to_typeid is not None:\n                assert type(method_to_typeid) is dict\n                exposed = list(exposed) + list(method_to_typeid)\n\n            ident = '%x' % id(obj)  # convert to string because xmlrpclib\n                                    # only has 32 bit signed integers\n            util.debug('%r callable returned object with id %r', typeid, ident)\n\n            self.id_to_obj[ident] = (obj, set(exposed), method_to_typeid)\n            if ident not in self.id_to_refcount:\n                self.id_to_refcount[ident] = 0\n            # increment the reference count immediately, to avoid\n            # this object being garbage collected before a Proxy\n            # object for it can be created.  The caller of create()\n            # is responsible for doing a decref once the Proxy object\n            # has been created.\n            self.incref(c, ident)\n            return ident, tuple(exposed)\n        finally:\n            self.mutex.release()\n\n    def get_methods(self, c, token):\n        '''\n        Return the methods of the shared object indicated by token\n        '''\n        return tuple(self.id_to_obj[token.id][1])\n\n    def accept_connection(self, c, name):\n        '''\n        Spawn a new thread to serve this connection\n        '''\n        threading.current_thread().name = name\n        c.send(('#RETURN', None))\n        self.serve_client(c)\n\n    def incref(self, c, ident):\n        self.mutex.acquire()\n        try:\n            self.id_to_refcount[ident] += 1\n        finally:\n            self.mutex.release()\n\n    def decref(self, c, ident):\n        self.mutex.acquire()\n        try:\n            assert self.id_to_refcount[ident] >= 1\n            self.id_to_refcount[ident] -= 1\n            if self.id_to_refcount[ident] == 0:\n                del self.id_to_obj[ident], self.id_to_refcount[ident]\n                util.debug('disposing of obj with id %r', ident)\n        finally:\n            self.mutex.release()\n\n#\n# Class to represent state of a manager\n#\n\nclass State(object):\n    __slots__ = ['value']\n    INITIAL = 0\n    STARTED = 1\n    SHUTDOWN = 2\n\n#\n# Mapping from serializer name to Listener and Client types\n#\n\nlistener_client = {\n    'pickle' : (connection.Listener, connection.Client),\n    'xmlrpclib' : (connection.XmlListener, connection.XmlClient)\n    }\n\n#\n# Definition of BaseManager\n#\n\nclass BaseManager(object):\n    '''\n    Base class for managers\n    '''\n    _registry = {}\n    _Server = Server\n\n    def __init__(self, address=None, authkey=None, serializer='pickle'):\n        if authkey is None:\n            authkey = current_process().authkey\n        self._address = address     # XXX not final address if eg ('', 0)\n        self._authkey = AuthenticationString(authkey)\n        self._state = State()\n        self._state.value = State.INITIAL\n        self._serializer = serializer\n        self._Listener, self._Client = listener_client[serializer]\n\n    def __reduce__(self):\n        return type(self).from_address, \\\n               (self._address, self._authkey, self._serializer)\n\n    def get_server(self):\n        '''\n        Return server object with serve_forever() method and address attribute\n        '''\n        assert self._state.value == State.INITIAL\n        return Server(self._registry, self._address,\n                      self._authkey, self._serializer)\n\n    def connect(self):\n        '''\n        Connect manager object to the server process\n        '''\n        Listener, Client = listener_client[self._serializer]\n        conn = Client(self._address, authkey=self._authkey)\n        dispatch(conn, None, 'dummy')\n        self._state.value = State.STARTED\n\n    def start(self, initializer=None, initargs=()):\n        '''\n        Spawn a server process for this manager object\n        '''\n        assert self._state.value == State.INITIAL\n\n        if initializer is not None and not hasattr(initializer, '__call__'):\n            raise TypeError('initializer must be a callable')\n\n        # pipe over which we will retrieve address of server\n        reader, writer = connection.Pipe(duplex=False)\n\n        # spawn process which runs a server\n        self._process = Process(\n            target=type(self)._run_server,\n            args=(self._registry, self._address, self._authkey,\n                  self._serializer, writer, initializer, initargs),\n            )\n        ident = ':'.join(str(i) for i in self._process._identity)\n        self._process.name = type(self).__name__  + '-' + ident\n        self._process.start()\n\n        # get address of server\n        writer.close()\n        self._address = reader.recv()\n        reader.close()\n\n        # register a finalizer\n        self._state.value = State.STARTED\n        self.shutdown = util.Finalize(\n            self, type(self)._finalize_manager,\n            args=(self._process, self._address, self._authkey,\n                  self._state, self._Client),\n            exitpriority=0\n            )\n\n    @classmethod\n    def _run_server(cls, registry, address, authkey, serializer, writer,\n                    initializer=None, initargs=()):\n        '''\n        Create a server, report its address and run it\n        '''\n        if initializer is not None:\n            initializer(*initargs)\n\n        # create server\n        server = cls._Server(registry, address, authkey, serializer)\n\n        # inform parent process of the server's address\n        writer.send(server.address)\n        writer.close()\n\n        # run the manager\n        util.info('manager serving at %r', server.address)\n        server.serve_forever()\n\n    def _create(self, typeid, *args, **kwds):\n        '''\n        Create a new shared object; return the token and exposed tuple\n        '''\n        assert self._state.value == State.STARTED, 'server not yet started'\n        conn = self._Client(self._address, authkey=self._authkey)\n        try:\n            id, exposed = dispatch(conn, None, 'create', (typeid,)+args, kwds)\n        finally:\n            conn.close()\n        return Token(typeid, self._address, id), exposed\n\n    def join(self, timeout=None):\n        '''\n        Join the manager process (if it has been spawned)\n        '''\n        self._process.join(timeout)\n\n    def _debug_info(self):\n        '''\n        Return some info about the servers shared objects and connections\n        '''\n        conn = self._Client(self._address, authkey=self._authkey)\n        try:\n            return dispatch(conn, None, 'debug_info')\n        finally:\n            conn.close()\n\n    def _number_of_objects(self):\n        '''\n        Return the number of shared objects\n        '''\n        conn = self._Client(self._address, authkey=self._authkey)\n        try:\n            return dispatch(conn, None, 'number_of_objects')\n        finally:\n            conn.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.shutdown()\n\n    @staticmethod\n    def _finalize_manager(process, address, authkey, state, _Client):\n        '''\n        Shutdown the manager process; will be registered as a finalizer\n        '''\n        if process.is_alive():\n            util.info('sending shutdown message to manager')\n            try:\n                conn = _Client(address, authkey=authkey)\n                try:\n                    dispatch(conn, None, 'shutdown')\n                finally:\n                    conn.close()\n            except Exception:\n                pass\n\n            process.join(timeout=0.2)\n            if process.is_alive():\n                util.info('manager still alive')\n                if hasattr(process, 'terminate'):\n                    util.info('trying to `terminate()` manager process')\n                    process.terminate()\n                    process.join(timeout=0.1)\n                    if process.is_alive():\n                        util.info('manager still alive after terminate')\n\n        state.value = State.SHUTDOWN\n        try:\n            del BaseProxy._address_to_local[address]\n        except KeyError:\n            pass\n\n    address = property(lambda self: self._address)\n\n    @classmethod\n    def register(cls, typeid, callable=None, proxytype=None, exposed=None,\n                 method_to_typeid=None, create_method=True):\n        '''\n        Register a typeid with the manager type\n        '''\n        if '_registry' not in cls.__dict__:\n            cls._registry = cls._registry.copy()\n\n        if proxytype is None:\n            proxytype = AutoProxy\n\n        exposed = exposed or getattr(proxytype, '_exposed_', None)\n\n        method_to_typeid = method_to_typeid or \\\n                           getattr(proxytype, '_method_to_typeid_', None)\n\n        if method_to_typeid:\n            for key, value in method_to_typeid.items():\n                assert type(key) is str, '%r is not a string' % key\n                assert type(value) is str, '%r is not a string' % value\n\n        cls._registry[typeid] = (\n            callable, exposed, method_to_typeid, proxytype\n            )\n\n        if create_method:\n            def temp(self, *args, **kwds):\n                util.debug('requesting creation of a shared %r object', typeid)\n                token, exp = self._create(typeid, *args, **kwds)\n                proxy = proxytype(\n                    token, self._serializer, manager=self,\n                    authkey=self._authkey, exposed=exp\n                    )\n                conn = self._Client(token.address, authkey=self._authkey)\n                dispatch(conn, None, 'decref', (token.id,))\n                return proxy\n            temp.__name__ = typeid\n            setattr(cls, typeid, temp)\n\n#\n# Subclass of set which get cleared after a fork\n#\n\nclass ProcessLocalSet(set):\n    def __init__(self):\n        util.register_after_fork(self, lambda obj: obj.clear())\n    def __reduce__(self):\n        return type(self), ()\n\n#\n# Definition of BaseProxy\n#\n\nclass BaseProxy(object):\n    '''\n    A base for proxies of shared objects\n    '''\n    _address_to_local = {}\n    _mutex = util.ForkAwareThreadLock()\n\n    def __init__(self, token, serializer, manager=None,\n                 authkey=None, exposed=None, incref=True):\n        BaseProxy._mutex.acquire()\n        try:\n            tls_idset = BaseProxy._address_to_local.get(token.address, None)\n            if tls_idset is None:\n                tls_idset = util.ForkAwareLocal(), ProcessLocalSet()\n                BaseProxy._address_to_local[token.address] = tls_idset\n        finally:\n            BaseProxy._mutex.release()\n\n        # self._tls is used to record the connection used by this\n        # thread to communicate with the manager at token.address\n        self._tls = tls_idset[0]\n\n        # self._idset is used to record the identities of all shared\n        # objects for which the current process owns references and\n        # which are in the manager at token.address\n        self._idset = tls_idset[1]\n\n        self._token = token\n        self._id = self._token.id\n        self._manager = manager\n        self._serializer = serializer\n        self._Client = listener_client[serializer][1]\n\n        if authkey is not None:\n            self._authkey = AuthenticationString(authkey)\n        elif self._manager is not None:\n            self._authkey = self._manager._authkey\n        else:\n            self._authkey = current_process().authkey\n\n        if incref:\n            self._incref()\n\n        util.register_after_fork(self, BaseProxy._after_fork)\n\n    def _connect(self):\n        util.debug('making connection to manager')\n        name = current_process().name\n        if threading.current_thread().name != 'MainThread':\n            name += '|' + threading.current_thread().name\n        conn = self._Client(self._token.address, authkey=self._authkey)\n        dispatch(conn, None, 'accept_connection', (name,))\n        self._tls.connection = conn\n\n    def _callmethod(self, methodname, args=(), kwds={}):\n        '''\n        Try to call a method of the referrent and return a copy of the result\n        '''\n        try:\n            conn = self._tls.connection\n        except AttributeError:\n            util.debug('thread %r does not own a connection',\n                       threading.current_thread().name)\n            self._connect()\n            conn = self._tls.connection\n\n        conn.send((self._id, methodname, args, kwds))\n        kind, result = conn.recv()\n\n        if kind == '#RETURN':\n            return result\n        elif kind == '#PROXY':\n            exposed, token = result\n            proxytype = self._manager._registry[token.typeid][-1]\n            token.address = self._token.address\n            proxy = proxytype(\n                token, self._serializer, manager=self._manager,\n                authkey=self._authkey, exposed=exposed\n                )\n            conn = self._Client(token.address, authkey=self._authkey)\n            dispatch(conn, None, 'decref', (token.id,))\n            return proxy\n        raise convert_to_error(kind, result)\n\n    def _getvalue(self):\n        '''\n        Get a copy of the value of the referent\n        '''\n        return self._callmethod('#GETVALUE')\n\n    def _incref(self):\n        conn = self._Client(self._token.address, authkey=self._authkey)\n        dispatch(conn, None, 'incref', (self._id,))\n        util.debug('INCREF %r', self._token.id)\n\n        self._idset.add(self._id)\n\n        state = self._manager and self._manager._state\n\n        self._close = util.Finalize(\n            self, BaseProxy._decref,\n            args=(self._token, self._authkey, state,\n                  self._tls, self._idset, self._Client),\n            exitpriority=10\n            )\n\n    @staticmethod\n    def _decref(token, authkey, state, tls, idset, _Client):\n        idset.discard(token.id)\n\n        # check whether manager is still alive\n        if state is None or state.value == State.STARTED:\n            # tell manager this process no longer cares about referent\n            try:\n                util.debug('DECREF %r', token.id)\n                conn = _Client(token.address, authkey=authkey)\n                dispatch(conn, None, 'decref', (token.id,))\n            except Exception, e:\n                util.debug('... decref failed %s', e)\n\n        else:\n            util.debug('DECREF %r -- manager already shutdown', token.id)\n\n        # check whether we can close this thread's connection because\n        # the process owns no more references to objects for this manager\n        if not idset and hasattr(tls, 'connection'):\n            util.debug('thread %r has no more proxies so closing conn',\n                       threading.current_thread().name)\n            tls.connection.close()\n            del tls.connection\n\n    def _after_fork(self):\n        self._manager = None\n        try:\n            self._incref()\n        except Exception, e:\n            # the proxy may just be for a manager which has shutdown\n            util.info('incref failed: %s' % e)\n\n    def __reduce__(self):\n        kwds = {}\n        if Popen.thread_is_spawning():\n            kwds['authkey'] = self._authkey\n\n        if getattr(self, '_isauto', False):\n            kwds['exposed'] = self._exposed_\n            return (RebuildProxy,\n                    (AutoProxy, self._token, self._serializer, kwds))\n        else:\n            return (RebuildProxy,\n                    (type(self), self._token, self._serializer, kwds))\n\n    def __deepcopy__(self, memo):\n        return self._getvalue()\n\n    def __repr__(self):\n        return '<%s object, typeid %r at %s>' % \\\n               (type(self).__name__, self._token.typeid, '0x%x' % id(self))\n\n    def __str__(self):\n        '''\n        Return representation of the referent (or a fall-back if that fails)\n        '''\n        try:\n            return self._callmethod('__repr__')\n        except Exception:\n            return repr(self)[:-1] + \"; '__str__()' failed>\"\n\n#\n# Function used for unpickling\n#\n\ndef RebuildProxy(func, token, serializer, kwds):\n    '''\n    Function used for unpickling proxy objects.\n\n    If possible the shared object is returned, or otherwise a proxy for it.\n    '''\n    server = getattr(current_process(), '_manager_server', None)\n\n    if server and server.address == token.address:\n        return server.id_to_obj[token.id][0]\n    else:\n        incref = (\n            kwds.pop('incref', True) and\n            not getattr(current_process(), '_inheriting', False)\n            )\n        return func(token, serializer, incref=incref, **kwds)\n\n#\n# Functions to create proxies and proxy types\n#\n\ndef MakeProxyType(name, exposed, _cache={}):\n    '''\n    Return an proxy type whose methods are given by `exposed`\n    '''\n    exposed = tuple(exposed)\n    try:\n        return _cache[(name, exposed)]\n    except KeyError:\n        pass\n\n    dic = {}\n\n    for meth in exposed:\n        exec '''def %s(self, *args, **kwds):\n        return self._callmethod(%r, args, kwds)''' % (meth, meth) in dic\n\n    ProxyType = type(name, (BaseProxy,), dic)\n    ProxyType._exposed_ = exposed\n    _cache[(name, exposed)] = ProxyType\n    return ProxyType\n\n\ndef AutoProxy(token, serializer, manager=None, authkey=None,\n              exposed=None, incref=True):\n    '''\n    Return an auto-proxy for `token`\n    '''\n    _Client = listener_client[serializer][1]\n\n    if exposed is None:\n        conn = _Client(token.address, authkey=authkey)\n        try:\n            exposed = dispatch(conn, None, 'get_methods', (token,))\n        finally:\n            conn.close()\n\n    if authkey is None and manager is not None:\n        authkey = manager._authkey\n    if authkey is None:\n        authkey = current_process().authkey\n\n    ProxyType = MakeProxyType('AutoProxy[%s]' % token.typeid, exposed)\n    proxy = ProxyType(token, serializer, manager=manager, authkey=authkey,\n                      incref=incref)\n    proxy._isauto = True\n    return proxy\n\n#\n# Types/callables which we will register with SyncManager\n#\n\nclass Namespace(object):\n    def __init__(self, **kwds):\n        self.__dict__.update(kwds)\n    def __repr__(self):\n        items = self.__dict__.items()\n        temp = []\n        for name, value in items:\n            if not name.startswith('_'):\n                temp.append('%s=%r' % (name, value))\n        temp.sort()\n        return 'Namespace(%s)' % str.join(', ', temp)\n\nclass Value(object):\n    def __init__(self, typecode, value, lock=True):\n        self._typecode = typecode\n        self._value = value\n    def get(self):\n        return self._value\n    def set(self, value):\n        self._value = value\n    def __repr__(self):\n        return '%s(%r, %r)'%(type(self).__name__, self._typecode, self._value)\n    value = property(get, set)\n\ndef Array(typecode, sequence, lock=True):\n    return array.array(typecode, sequence)\n\n#\n# Proxy types used by SyncManager\n#\n\nclass IteratorProxy(BaseProxy):\n    # XXX remove methods for Py3.0 and Py2.6\n    _exposed_ = ('__next__', 'next', 'send', 'throw', 'close')\n    def __iter__(self):\n        return self\n    def __next__(self, *args):\n        return self._callmethod('__next__', args)\n    def next(self, *args):\n        return self._callmethod('next', args)\n    def send(self, *args):\n        return self._callmethod('send', args)\n    def throw(self, *args):\n        return self._callmethod('throw', args)\n    def close(self, *args):\n        return self._callmethod('close', args)\n\n\nclass AcquirerProxy(BaseProxy):\n    _exposed_ = ('acquire', 'release')\n    def acquire(self, blocking=True):\n        return self._callmethod('acquire', (blocking,))\n    def release(self):\n        return self._callmethod('release')\n    def __enter__(self):\n        return self._callmethod('acquire')\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        return self._callmethod('release')\n\n\nclass ConditionProxy(AcquirerProxy):\n    # XXX will Condition.notfyAll() name be available in Py3.0?\n    _exposed_ = ('acquire', 'release', 'wait', 'notify', 'notify_all')\n    def wait(self, timeout=None):\n        return self._callmethod('wait', (timeout,))\n    def notify(self):\n        return self._callmethod('notify')\n    def notify_all(self):\n        return self._callmethod('notify_all')\n\nclass EventProxy(BaseProxy):\n    _exposed_ = ('is_set', 'set', 'clear', 'wait')\n    def is_set(self):\n        return self._callmethod('is_set')\n    def set(self):\n        return self._callmethod('set')\n    def clear(self):\n        return self._callmethod('clear')\n    def wait(self, timeout=None):\n        return self._callmethod('wait', (timeout,))\n\nclass NamespaceProxy(BaseProxy):\n    _exposed_ = ('__getattribute__', '__setattr__', '__delattr__')\n    def __getattr__(self, key):\n        if key[0] == '_':\n            return object.__getattribute__(self, key)\n        callmethod = object.__getattribute__(self, '_callmethod')\n        return callmethod('__getattribute__', (key,))\n    def __setattr__(self, key, value):\n        if key[0] == '_':\n            return object.__setattr__(self, key, value)\n        callmethod = object.__getattribute__(self, '_callmethod')\n        return callmethod('__setattr__', (key, value))\n    def __delattr__(self, key):\n        if key[0] == '_':\n            return object.__delattr__(self, key)\n        callmethod = object.__getattribute__(self, '_callmethod')\n        return callmethod('__delattr__', (key,))\n\n\nclass ValueProxy(BaseProxy):\n    _exposed_ = ('get', 'set')\n    def get(self):\n        return self._callmethod('get')\n    def set(self, value):\n        return self._callmethod('set', (value,))\n    value = property(get, set)\n\n\nBaseListProxy = MakeProxyType('BaseListProxy', (\n    '__add__', '__contains__', '__delitem__', '__delslice__',\n    '__getitem__', '__getslice__', '__len__', '__mul__',\n    '__reversed__', '__rmul__', '__setitem__', '__setslice__',\n    'append', 'count', 'extend', 'index', 'insert', 'pop', 'remove',\n    'reverse', 'sort', '__imul__'\n    ))                  # XXX __getslice__ and __setslice__ unneeded in Py3.0\nclass ListProxy(BaseListProxy):\n    def __iadd__(self, value):\n        self._callmethod('extend', (value,))\n        return self\n    def __imul__(self, value):\n        self._callmethod('__imul__', (value,))\n        return self\n\n\nDictProxy = MakeProxyType('DictProxy', (\n    '__contains__', '__delitem__', '__getitem__', '__len__',\n    '__setitem__', 'clear', 'copy', 'get', 'has_key', 'items',\n    'keys', 'pop', 'popitem', 'setdefault', 'update', 'values'\n    ))\n\n\nArrayProxy = MakeProxyType('ArrayProxy', (\n    '__len__', '__getitem__', '__setitem__', '__getslice__', '__setslice__'\n    ))                  # XXX __getslice__ and __setslice__ unneeded in Py3.0\n\n\nPoolProxy = MakeProxyType('PoolProxy', (\n    'apply', 'apply_async', 'close', 'imap', 'imap_unordered', 'join',\n    'map', 'map_async', 'terminate'\n    ))\nPoolProxy._method_to_typeid_ = {\n    'apply_async': 'AsyncResult',\n    'map_async': 'AsyncResult',\n    'imap': 'Iterator',\n    'imap_unordered': 'Iterator'\n    }\n\n#\n# Definition of SyncManager\n#\n\nclass SyncManager(BaseManager):\n    '''\n    Subclass of `BaseManager` which supports a number of shared object types.\n\n    The types registered are those intended for the synchronization\n    of threads, plus `dict`, `list` and `Namespace`.\n\n    The `multiprocessing.Manager()` function creates started instances of\n    this class.\n    '''\n\nSyncManager.register('Queue', Queue.Queue)\nSyncManager.register('JoinableQueue', Queue.Queue)\nSyncManager.register('Event', threading.Event, EventProxy)\nSyncManager.register('Lock', threading.Lock, AcquirerProxy)\nSyncManager.register('RLock', threading.RLock, AcquirerProxy)\nSyncManager.register('Semaphore', threading.Semaphore, AcquirerProxy)\nSyncManager.register('BoundedSemaphore', threading.BoundedSemaphore,\n                     AcquirerProxy)\nSyncManager.register('Condition', threading.Condition, ConditionProxy)\nSyncManager.register('Pool', Pool, PoolProxy)\nSyncManager.register('list', list, ListProxy)\nSyncManager.register('dict', dict, DictProxy)\nSyncManager.register('Value', Value, ValueProxy)\nSyncManager.register('Array', Array, ArrayProxy)\nSyncManager.register('Namespace', Namespace, NamespaceProxy)\n\n# types returned by methods of PoolProxy\nSyncManager.register('Iterator', proxytype=IteratorProxy, create_method=False)\nSyncManager.register('AsyncResult', create_method=False)\n", 
    "multiprocessing.pool": "#\n# Module providing the `Pool` class for managing a process pool\n#\n# multiprocessing/pool.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__all__ = ['Pool']\n\n#\n# Imports\n#\n\nimport threading\nimport Queue\nimport itertools\nimport collections\nimport time\n\nfrom multiprocessing import Process, cpu_count, TimeoutError\nfrom multiprocessing.util import Finalize, debug\n\n#\n# Constants representing the state of a pool\n#\n\nRUN = 0\nCLOSE = 1\nTERMINATE = 2\n\n#\n# Miscellaneous\n#\n\njob_counter = itertools.count()\n\ndef mapstar(args):\n    return map(*args)\n\n#\n# Code run by worker processes\n#\n\nclass MaybeEncodingError(Exception):\n    \"\"\"Wraps possible unpickleable errors, so they can be\n    safely sent through the socket.\"\"\"\n\n    def __init__(self, exc, value):\n        self.exc = repr(exc)\n        self.value = repr(value)\n        super(MaybeEncodingError, self).__init__(self.exc, self.value)\n\n    def __str__(self):\n        return \"Error sending result: '%s'. Reason: '%s'\" % (self.value,\n                                                             self.exc)\n\n    def __repr__(self):\n        return \"<MaybeEncodingError: %s>\" % str(self)\n\n\ndef worker(inqueue, outqueue, initializer=None, initargs=(), maxtasks=None):\n    assert maxtasks is None or (type(maxtasks) == int and maxtasks > 0)\n    put = outqueue.put\n    get = inqueue.get\n    if hasattr(inqueue, '_writer'):\n        inqueue._writer.close()\n        outqueue._reader.close()\n\n    if initializer is not None:\n        initializer(*initargs)\n\n    completed = 0\n    while maxtasks is None or (maxtasks and completed < maxtasks):\n        try:\n            task = get()\n        except (EOFError, IOError):\n            debug('worker got EOFError or IOError -- exiting')\n            break\n\n        if task is None:\n            debug('worker got sentinel -- exiting')\n            break\n\n        job, i, func, args, kwds = task\n        try:\n            result = (True, func(*args, **kwds))\n        except Exception, e:\n            result = (False, e)\n        try:\n            put((job, i, result))\n        except Exception as e:\n            wrapped = MaybeEncodingError(e, result[1])\n            debug(\"Possible encoding error while sending result: %s\" % (\n                wrapped))\n            put((job, i, (False, wrapped)))\n        completed += 1\n    debug('worker exiting after %d tasks' % completed)\n\n#\n# Class representing a process pool\n#\n\nclass Pool(object):\n    '''\n    Class which supports an async version of the `apply()` builtin\n    '''\n    Process = Process\n\n    def __init__(self, processes=None, initializer=None, initargs=(),\n                 maxtasksperchild=None):\n        self._setup_queues()\n        self._taskqueue = Queue.Queue()\n        self._cache = {}\n        self._state = RUN\n        self._maxtasksperchild = maxtasksperchild\n        self._initializer = initializer\n        self._initargs = initargs\n\n        if processes is None:\n            try:\n                processes = cpu_count()\n            except NotImplementedError:\n                processes = 1\n        if processes < 1:\n            raise ValueError(\"Number of processes must be at least 1\")\n\n        if initializer is not None and not hasattr(initializer, '__call__'):\n            raise TypeError('initializer must be a callable')\n\n        self._processes = processes\n        self._pool = []\n        self._repopulate_pool()\n\n        self._worker_handler = threading.Thread(\n            target=Pool._handle_workers,\n            args=(self, )\n            )\n        self._worker_handler.daemon = True\n        self._worker_handler._state = RUN\n        self._worker_handler.start()\n\n\n        self._task_handler = threading.Thread(\n            target=Pool._handle_tasks,\n            args=(self._taskqueue, self._quick_put, self._outqueue,\n                  self._pool, self._cache)\n            )\n        self._task_handler.daemon = True\n        self._task_handler._state = RUN\n        self._task_handler.start()\n\n        self._result_handler = threading.Thread(\n            target=Pool._handle_results,\n            args=(self._outqueue, self._quick_get, self._cache)\n            )\n        self._result_handler.daemon = True\n        self._result_handler._state = RUN\n        self._result_handler.start()\n\n        self._terminate = Finalize(\n            self, self._terminate_pool,\n            args=(self._taskqueue, self._inqueue, self._outqueue, self._pool,\n                  self._worker_handler, self._task_handler,\n                  self._result_handler, self._cache),\n            exitpriority=15\n            )\n\n    def _join_exited_workers(self):\n        \"\"\"Cleanup after any worker processes which have exited due to reaching\n        their specified lifetime.  Returns True if any workers were cleaned up.\n        \"\"\"\n        cleaned = False\n        for i in reversed(range(len(self._pool))):\n            worker = self._pool[i]\n            if worker.exitcode is not None:\n                # worker exited\n                debug('cleaning up worker %d' % i)\n                worker.join()\n                cleaned = True\n                del self._pool[i]\n        return cleaned\n\n    def _repopulate_pool(self):\n        \"\"\"Bring the number of pool processes up to the specified number,\n        for use after reaping workers which have exited.\n        \"\"\"\n        for i in range(self._processes - len(self._pool)):\n            w = self.Process(target=worker,\n                             args=(self._inqueue, self._outqueue,\n                                   self._initializer,\n                                   self._initargs, self._maxtasksperchild)\n                            )\n            self._pool.append(w)\n            w.name = w.name.replace('Process', 'PoolWorker')\n            w.daemon = True\n            w.start()\n            debug('added worker')\n\n    def _maintain_pool(self):\n        \"\"\"Clean up any exited workers and start replacements for them.\n        \"\"\"\n        if self._join_exited_workers():\n            self._repopulate_pool()\n\n    def _setup_queues(self):\n        from .queues import SimpleQueue\n        self._inqueue = SimpleQueue()\n        self._outqueue = SimpleQueue()\n        self._quick_put = self._inqueue._writer.send\n        self._quick_get = self._outqueue._reader.recv\n\n    def apply(self, func, args=(), kwds={}):\n        '''\n        Equivalent of `apply()` builtin\n        '''\n        assert self._state == RUN\n        return self.apply_async(func, args, kwds).get()\n\n    def map(self, func, iterable, chunksize=None):\n        '''\n        Equivalent of `map()` builtin\n        '''\n        assert self._state == RUN\n        return self.map_async(func, iterable, chunksize).get()\n\n    def imap(self, func, iterable, chunksize=1):\n        '''\n        Equivalent of `itertools.imap()` -- can be MUCH slower than `Pool.map()`\n        '''\n        assert self._state == RUN\n        if chunksize == 1:\n            result = IMapIterator(self._cache)\n            self._taskqueue.put((((result._job, i, func, (x,), {})\n                         for i, x in enumerate(iterable)), result._set_length))\n            return result\n        else:\n            assert chunksize > 1\n            task_batches = Pool._get_tasks(func, iterable, chunksize)\n            result = IMapIterator(self._cache)\n            self._taskqueue.put((((result._job, i, mapstar, (x,), {})\n                     for i, x in enumerate(task_batches)), result._set_length))\n            return (item for chunk in result for item in chunk)\n\n    def imap_unordered(self, func, iterable, chunksize=1):\n        '''\n        Like `imap()` method but ordering of results is arbitrary\n        '''\n        assert self._state == RUN\n        if chunksize == 1:\n            result = IMapUnorderedIterator(self._cache)\n            self._taskqueue.put((((result._job, i, func, (x,), {})\n                         for i, x in enumerate(iterable)), result._set_length))\n            return result\n        else:\n            assert chunksize > 1\n            task_batches = Pool._get_tasks(func, iterable, chunksize)\n            result = IMapUnorderedIterator(self._cache)\n            self._taskqueue.put((((result._job, i, mapstar, (x,), {})\n                     for i, x in enumerate(task_batches)), result._set_length))\n            return (item for chunk in result for item in chunk)\n\n    def apply_async(self, func, args=(), kwds={}, callback=None):\n        '''\n        Asynchronous equivalent of `apply()` builtin\n        '''\n        assert self._state == RUN\n        result = ApplyResult(self._cache, callback)\n        self._taskqueue.put(([(result._job, None, func, args, kwds)], None))\n        return result\n\n    def map_async(self, func, iterable, chunksize=None, callback=None):\n        '''\n        Asynchronous equivalent of `map()` builtin\n        '''\n        assert self._state == RUN\n        if not hasattr(iterable, '__len__'):\n            iterable = list(iterable)\n\n        if chunksize is None:\n            chunksize, extra = divmod(len(iterable), len(self._pool) * 4)\n            if extra:\n                chunksize += 1\n        if len(iterable) == 0:\n            chunksize = 0\n\n        task_batches = Pool._get_tasks(func, iterable, chunksize)\n        result = MapResult(self._cache, chunksize, len(iterable), callback)\n        self._taskqueue.put((((result._job, i, mapstar, (x,), {})\n                              for i, x in enumerate(task_batches)), None))\n        return result\n\n    @staticmethod\n    def _handle_workers(pool):\n        thread = threading.current_thread()\n\n        # Keep maintaining workers until the cache gets drained, unless the pool\n        # is terminated.\n        while thread._state == RUN or (pool._cache and thread._state != TERMINATE):\n            pool._maintain_pool()\n            time.sleep(0.1)\n        # send sentinel to stop workers\n        pool._taskqueue.put(None)\n        debug('worker handler exiting')\n\n    @staticmethod\n    def _handle_tasks(taskqueue, put, outqueue, pool, cache):\n        thread = threading.current_thread()\n\n        for taskseq, set_length in iter(taskqueue.get, None):\n            i = -1\n            for i, task in enumerate(taskseq):\n                if thread._state:\n                    debug('task handler found thread._state != RUN')\n                    break\n                try:\n                    put(task)\n                except Exception as e:\n                    job, ind = task[:2]\n                    try:\n                        cache[job]._set(ind, (False, e))\n                    except KeyError:\n                        pass\n            else:\n                if set_length:\n                    debug('doing set_length()')\n                    set_length(i+1)\n                continue\n            break\n        else:\n            debug('task handler got sentinel')\n\n\n        try:\n            # tell result handler to finish when cache is empty\n            debug('task handler sending sentinel to result handler')\n            outqueue.put(None)\n\n            # tell workers there is no more work\n            debug('task handler sending sentinel to workers')\n            for p in pool:\n                put(None)\n        except IOError:\n            debug('task handler got IOError when sending sentinels')\n\n        debug('task handler exiting')\n\n    @staticmethod\n    def _handle_results(outqueue, get, cache):\n        thread = threading.current_thread()\n\n        while 1:\n            try:\n                task = get()\n            except (IOError, EOFError):\n                debug('result handler got EOFError/IOError -- exiting')\n                return\n\n            if thread._state:\n                assert thread._state == TERMINATE\n                debug('result handler found thread._state=TERMINATE')\n                break\n\n            if task is None:\n                debug('result handler got sentinel')\n                break\n\n            job, i, obj = task\n            try:\n                cache[job]._set(i, obj)\n            except KeyError:\n                pass\n\n        while cache and thread._state != TERMINATE:\n            try:\n                task = get()\n            except (IOError, EOFError):\n                debug('result handler got EOFError/IOError -- exiting')\n                return\n\n            if task is None:\n                debug('result handler ignoring extra sentinel')\n                continue\n            job, i, obj = task\n            try:\n                cache[job]._set(i, obj)\n            except KeyError:\n                pass\n\n        if hasattr(outqueue, '_reader'):\n            debug('ensuring that outqueue is not full')\n            # If we don't make room available in outqueue then\n            # attempts to add the sentinel (None) to outqueue may\n            # block.  There is guaranteed to be no more than 2 sentinels.\n            try:\n                for i in range(10):\n                    if not outqueue._reader.poll():\n                        break\n                    get()\n            except (IOError, EOFError):\n                pass\n\n        debug('result handler exiting: len(cache)=%s, thread._state=%s',\n              len(cache), thread._state)\n\n    @staticmethod\n    def _get_tasks(func, it, size):\n        it = iter(it)\n        while 1:\n            x = tuple(itertools.islice(it, size))\n            if not x:\n                return\n            yield (func, x)\n\n    def __reduce__(self):\n        raise NotImplementedError(\n              'pool objects cannot be passed between processes or pickled'\n              )\n\n    def close(self):\n        debug('closing pool')\n        if self._state == RUN:\n            self._state = CLOSE\n            self._worker_handler._state = CLOSE\n\n    def terminate(self):\n        debug('terminating pool')\n        self._state = TERMINATE\n        self._worker_handler._state = TERMINATE\n        self._terminate()\n\n    def join(self):\n        debug('joining pool')\n        assert self._state in (CLOSE, TERMINATE)\n        self._worker_handler.join()\n        self._task_handler.join()\n        self._result_handler.join()\n        for p in self._pool:\n            p.join()\n\n    @staticmethod\n    def _help_stuff_finish(inqueue, task_handler, size):\n        # task_handler may be blocked trying to put items on inqueue\n        debug('removing tasks from inqueue until task handler finished')\n        inqueue._rlock.acquire()\n        while task_handler.is_alive() and inqueue._reader.poll():\n            inqueue._reader.recv()\n            time.sleep(0)\n\n    @classmethod\n    def _terminate_pool(cls, taskqueue, inqueue, outqueue, pool,\n                        worker_handler, task_handler, result_handler, cache):\n        # this is guaranteed to only be called once\n        debug('finalizing pool')\n\n        worker_handler._state = TERMINATE\n        task_handler._state = TERMINATE\n\n        debug('helping task handler/workers to finish')\n        cls._help_stuff_finish(inqueue, task_handler, len(pool))\n\n        assert result_handler.is_alive() or len(cache) == 0\n\n        result_handler._state = TERMINATE\n        outqueue.put(None)                  # sentinel\n\n        # We must wait for the worker handler to exit before terminating\n        # workers because we don't want workers to be restarted behind our back.\n        debug('joining worker handler')\n        if threading.current_thread() is not worker_handler:\n            worker_handler.join(1e100)\n\n        # Terminate workers which haven't already finished.\n        if pool and hasattr(pool[0], 'terminate'):\n            debug('terminating workers')\n            for p in pool:\n                if p.exitcode is None:\n                    p.terminate()\n\n        debug('joining task handler')\n        if threading.current_thread() is not task_handler:\n            task_handler.join(1e100)\n\n        debug('joining result handler')\n        if threading.current_thread() is not result_handler:\n            result_handler.join(1e100)\n\n        if pool and hasattr(pool[0], 'terminate'):\n            debug('joining pool workers')\n            for p in pool:\n                if p.is_alive():\n                    # worker has not yet exited\n                    debug('cleaning up worker %d' % p.pid)\n                    p.join()\n\n#\n# Class whose instances are returned by `Pool.apply_async()`\n#\n\nclass ApplyResult(object):\n\n    def __init__(self, cache, callback):\n        self._cond = threading.Condition(threading.Lock())\n        self._job = job_counter.next()\n        self._cache = cache\n        self._ready = False\n        self._callback = callback\n        cache[self._job] = self\n\n    def ready(self):\n        return self._ready\n\n    def successful(self):\n        assert self._ready\n        return self._success\n\n    def wait(self, timeout=None):\n        self._cond.acquire()\n        try:\n            if not self._ready:\n                self._cond.wait(timeout)\n        finally:\n            self._cond.release()\n\n    def get(self, timeout=None):\n        self.wait(timeout)\n        if not self._ready:\n            raise TimeoutError\n        if self._success:\n            return self._value\n        else:\n            raise self._value\n\n    def _set(self, i, obj):\n        self._success, self._value = obj\n        if self._callback and self._success:\n            self._callback(self._value)\n        self._cond.acquire()\n        try:\n            self._ready = True\n            self._cond.notify()\n        finally:\n            self._cond.release()\n        del self._cache[self._job]\n\nAsyncResult = ApplyResult       # create alias -- see #17805\n\n#\n# Class whose instances are returned by `Pool.map_async()`\n#\n\nclass MapResult(ApplyResult):\n\n    def __init__(self, cache, chunksize, length, callback):\n        ApplyResult.__init__(self, cache, callback)\n        self._success = True\n        self._value = [None] * length\n        self._chunksize = chunksize\n        if chunksize <= 0:\n            self._number_left = 0\n            self._ready = True\n            del cache[self._job]\n        else:\n            self._number_left = length//chunksize + bool(length % chunksize)\n\n    def _set(self, i, success_result):\n        success, result = success_result\n        if success:\n            self._value[i*self._chunksize:(i+1)*self._chunksize] = result\n            self._number_left -= 1\n            if self._number_left == 0:\n                if self._callback:\n                    self._callback(self._value)\n                del self._cache[self._job]\n                self._cond.acquire()\n                try:\n                    self._ready = True\n                    self._cond.notify()\n                finally:\n                    self._cond.release()\n\n        else:\n            self._success = False\n            self._value = result\n            del self._cache[self._job]\n            self._cond.acquire()\n            try:\n                self._ready = True\n                self._cond.notify()\n            finally:\n                self._cond.release()\n\n#\n# Class whose instances are returned by `Pool.imap()`\n#\n\nclass IMapIterator(object):\n\n    def __init__(self, cache):\n        self._cond = threading.Condition(threading.Lock())\n        self._job = job_counter.next()\n        self._cache = cache\n        self._items = collections.deque()\n        self._index = 0\n        self._length = None\n        self._unsorted = {}\n        cache[self._job] = self\n\n    def __iter__(self):\n        return self\n\n    def next(self, timeout=None):\n        self._cond.acquire()\n        try:\n            try:\n                item = self._items.popleft()\n            except IndexError:\n                if self._index == self._length:\n                    raise StopIteration\n                self._cond.wait(timeout)\n                try:\n                    item = self._items.popleft()\n                except IndexError:\n                    if self._index == self._length:\n                        raise StopIteration\n                    raise TimeoutError\n        finally:\n            self._cond.release()\n\n        success, value = item\n        if success:\n            return value\n        raise value\n\n    __next__ = next                    # XXX\n\n    def _set(self, i, obj):\n        self._cond.acquire()\n        try:\n            if self._index == i:\n                self._items.append(obj)\n                self._index += 1\n                while self._index in self._unsorted:\n                    obj = self._unsorted.pop(self._index)\n                    self._items.append(obj)\n                    self._index += 1\n                self._cond.notify()\n            else:\n                self._unsorted[i] = obj\n\n            if self._index == self._length:\n                del self._cache[self._job]\n        finally:\n            self._cond.release()\n\n    def _set_length(self, length):\n        self._cond.acquire()\n        try:\n            self._length = length\n            if self._index == self._length:\n                self._cond.notify()\n                del self._cache[self._job]\n        finally:\n            self._cond.release()\n\n#\n# Class whose instances are returned by `Pool.imap_unordered()`\n#\n\nclass IMapUnorderedIterator(IMapIterator):\n\n    def _set(self, i, obj):\n        self._cond.acquire()\n        try:\n            self._items.append(obj)\n            self._index += 1\n            self._cond.notify()\n            if self._index == self._length:\n                del self._cache[self._job]\n        finally:\n            self._cond.release()\n\n#\n#\n#\n\nclass ThreadPool(Pool):\n\n    from .dummy import Process\n\n    def __init__(self, processes=None, initializer=None, initargs=()):\n        Pool.__init__(self, processes, initializer, initargs)\n\n    def _setup_queues(self):\n        self._inqueue = Queue.Queue()\n        self._outqueue = Queue.Queue()\n        self._quick_put = self._inqueue.put\n        self._quick_get = self._outqueue.get\n\n    @staticmethod\n    def _help_stuff_finish(inqueue, task_handler, size):\n        # put sentinels at head of inqueue to make workers finish\n        inqueue.not_empty.acquire()\n        try:\n            inqueue.queue.clear()\n            inqueue.queue.extend([None] * size)\n            inqueue.not_empty.notify_all()\n        finally:\n            inqueue.not_empty.release()\n", 
    "multiprocessing.process": "#\n# Module providing the `Process` class which emulates `threading.Thread`\n#\n# multiprocessing/process.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__all__ = ['Process', 'current_process', 'active_children']\n\n#\n# Imports\n#\n\nimport os\nimport sys\nimport signal\nimport itertools\n\n#\n#\n#\n\ntry:\n    ORIGINAL_DIR = os.path.abspath(os.getcwd())\nexcept OSError:\n    ORIGINAL_DIR = None\n\n#\n# Public functions\n#\n\ndef current_process():\n    '''\n    Return process object representing the current process\n    '''\n    return _current_process\n\ndef active_children():\n    '''\n    Return list of process objects corresponding to live child processes\n    '''\n    _cleanup()\n    return list(_current_process._children)\n\n#\n#\n#\n\ndef _cleanup():\n    # check for processes which have finished\n    for p in list(_current_process._children):\n        if p._popen.poll() is not None:\n            _current_process._children.discard(p)\n\n#\n# The `Process` class\n#\n\nclass Process(object):\n    '''\n    Process objects represent activity that is run in a separate process\n\n    The class is analagous to `threading.Thread`\n    '''\n    _Popen = None\n\n    def __init__(self, group=None, target=None, name=None, args=(), kwargs={}):\n        assert group is None, 'group argument must be None for now'\n        count = _current_process._counter.next()\n        self._identity = _current_process._identity + (count,)\n        self._authkey = _current_process._authkey\n        self._daemonic = _current_process._daemonic\n        self._tempdir = _current_process._tempdir\n        self._parent_pid = os.getpid()\n        self._popen = None\n        self._target = target\n        self._args = tuple(args)\n        self._kwargs = dict(kwargs)\n        self._name = name or type(self).__name__ + '-' + \\\n                     ':'.join(str(i) for i in self._identity)\n\n    def run(self):\n        '''\n        Method to be run in sub-process; can be overridden in sub-class\n        '''\n        if self._target:\n            self._target(*self._args, **self._kwargs)\n\n    def start(self):\n        '''\n        Start child process\n        '''\n        assert self._popen is None, 'cannot start a process twice'\n        assert self._parent_pid == os.getpid(), \\\n               'can only start a process object created by current process'\n        assert not _current_process._daemonic, \\\n               'daemonic processes are not allowed to have children'\n        _cleanup()\n        if self._Popen is not None:\n            Popen = self._Popen\n        else:\n            from .forking import Popen\n        self._popen = Popen(self)\n        _current_process._children.add(self)\n\n    def terminate(self):\n        '''\n        Terminate process; sends SIGTERM signal or uses TerminateProcess()\n        '''\n        self._popen.terminate()\n\n    def join(self, timeout=None):\n        '''\n        Wait until child process terminates\n        '''\n        assert self._parent_pid == os.getpid(), 'can only join a child process'\n        assert self._popen is not None, 'can only join a started process'\n        res = self._popen.wait(timeout)\n        if res is not None:\n            _current_process._children.discard(self)\n\n    def is_alive(self):\n        '''\n        Return whether process is alive\n        '''\n        if self is _current_process:\n            return True\n        assert self._parent_pid == os.getpid(), 'can only test a child process'\n        if self._popen is None:\n            return False\n        self._popen.poll()\n        return self._popen.returncode is None\n\n    @property\n    def name(self):\n        return self._name\n\n    @name.setter\n    def name(self, name):\n        assert isinstance(name, basestring), 'name must be a string'\n        self._name = name\n\n    @property\n    def daemon(self):\n        '''\n        Return whether process is a daemon\n        '''\n        return self._daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        '''\n        Set whether process is a daemon\n        '''\n        assert self._popen is None, 'process has already started'\n        self._daemonic = daemonic\n\n    @property\n    def authkey(self):\n        return self._authkey\n\n    @authkey.setter\n    def authkey(self, authkey):\n        '''\n        Set authorization key of process\n        '''\n        self._authkey = AuthenticationString(authkey)\n\n    @property\n    def exitcode(self):\n        '''\n        Return exit code of process or `None` if it has yet to stop\n        '''\n        if self._popen is None:\n            return self._popen\n        return self._popen.poll()\n\n    @property\n    def ident(self):\n        '''\n        Return identifier (PID) of process or `None` if it has yet to start\n        '''\n        if self is _current_process:\n            return os.getpid()\n        else:\n            return self._popen and self._popen.pid\n\n    pid = ident\n\n    def __repr__(self):\n        if self is _current_process:\n            status = 'started'\n        elif self._parent_pid != os.getpid():\n            status = 'unknown'\n        elif self._popen is None:\n            status = 'initial'\n        else:\n            if self._popen.poll() is not None:\n                status = self.exitcode\n            else:\n                status = 'started'\n\n        if type(status) is int:\n            if status == 0:\n                status = 'stopped'\n            else:\n                status = 'stopped[%s]' % _exitcode_to_name.get(status, status)\n\n        return '<%s(%s, %s%s)>' % (type(self).__name__, self._name,\n                                   status, self._daemonic and ' daemon' or '')\n\n    ##\n\n    def _bootstrap(self):\n        from . import util\n        global _current_process\n\n        try:\n            self._children = set()\n            self._counter = itertools.count(1)\n            try:\n                sys.stdin.close()\n                sys.stdin = open(os.devnull)\n            except (OSError, ValueError):\n                pass\n            _current_process = self\n            util._finalizer_registry.clear()\n            util._run_after_forkers()\n            util.info('child process calling self.run()')\n            try:\n                self.run()\n                exitcode = 0\n            finally:\n                util._exit_function()\n        except SystemExit, e:\n            if not e.args:\n                exitcode = 1\n            elif isinstance(e.args[0], int):\n                exitcode = e.args[0]\n            else:\n                sys.stderr.write(str(e.args[0]) + '\\n')\n                sys.stderr.flush()\n                exitcode = 1\n        except:\n            exitcode = 1\n            import traceback\n            sys.stderr.write('Process %s:\\n' % self.name)\n            sys.stderr.flush()\n            traceback.print_exc()\n\n        util.info('process exiting with exitcode %d' % exitcode)\n        return exitcode\n\n#\n# We subclass bytes to avoid accidental transmission of auth keys over network\n#\n\nclass AuthenticationString(bytes):\n    def __reduce__1(self):\n        from .forking import Popen\n        if not Popen.thread_is_spawning():\n            raise TypeError(\n                'Pickling an AuthenticationString object is '\n                'disallowed for security reasons'\n                )\n        return AuthenticationString, (bytes(self),)\n\n#\n# Create object representing the main process\n#\n\nclass _MainProcess(Process):\n\n    def __init__(self):\n        self._identity = ()\n        self._daemonic = False\n        self._name = 'MainProcess'\n        self._parent_pid = None\n        self._popen = None\n        self._counter = itertools.count(1)\n        self._children = set()\n        self._authkey = AuthenticationString(os.urandom(32))\n        self._tempdir = None\n\n_current_process = _MainProcess()\ndel _MainProcess\n\n#\n# Give names to some return codes\n#\n\n_exitcode_to_name = {}\n\n# for name, signum in signal.__dict__.items():\n#     if name[:3]=='SIG' and '_' not in name:\n#         _exitcode_to_name[-signum] = name\n", 
    "multiprocessing.queues": "#\n# Module implementing queues\n#\n# multiprocessing/queues.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__all__ = ['Queue', 'SimpleQueue', 'JoinableQueue']\n\nimport sys\nimport os\nimport threading\nimport collections\nimport time\nimport atexit\nimport weakref\n\nfrom Queue import Empty, Full\nfrom multiprocessing import Pipe\nfrom multiprocessing.synchronize import Lock, BoundedSemaphore, Semaphore, Condition\nfrom multiprocessing.util import debug, info, Finalize, register_after_fork\nfrom multiprocessing.forking import assert_spawning\n\n#\n# Queue type using a pipe, buffer and thread\n#\n\nclass Queue(object):\n\n    def __init__(self, maxsize=0):\n        if maxsize <= 0:\n            maxsize = 2147483647L\n        self._maxsize = maxsize\n        self._reader, self._writer = Pipe(duplex=False)\n        self._rlock = Lock()\n        self._opid = os.getpid()\n        if sys.platform == 'win32':\n            self._wlock = None\n        else:\n            self._wlock = Lock()\n        self._sem = BoundedSemaphore(maxsize)\n\n        self._after_fork()\n\n        if sys.platform != 'win32':\n            register_after_fork(self, Queue._after_fork)\n\n    def __getstate__(self):\n        assert_spawning(self)\n        return (self._maxsize, int(os.fileNumbers[self._writer.fno].buffer.id),\n                self._rlock, self._wlock, self._sem, self._opid)\n\n    def __setstate__(self, state):\n        (self._maxsize, buffer_id, self._rlock, self._wlock, self._sem, self._opid) = state\n        self._reader, self._writer = Pipe(duplex=False, buffer_id=buffer_id)\n        self._after_fork()\n\n    def _after_fork(self):\n        debug('Queue._after_fork()')\n        self._notempty = threading.Condition(threading.Lock())\n        self._buffer = collections.deque()\n        self._thread = None\n        self._jointhread = None\n        self._joincancelled = False\n        self._closed = False\n        self._close = None\n        self._send = self._writer.send\n        self._recv = self._reader.recv\n        self._poll = self._reader.poll\n\n    def put(self, obj, block=True, timeout=None):\n        assert not self._closed\n        if not self._sem.acquire(block, timeout):\n            raise Full\n        self._notempty.acquire()\n        try:\n            if self._thread is None:\n                self._start_thread()\n            self._buffer.append(obj)\n            self._notempty.notify()\n        finally:\n            self._notempty.release()\n        self._thread.next()\n    def get(self, block=True, timeout=None):\n        if block and timeout is None:\n            self._rlock.acquire()\n            try:\n                res = self._recv()\n                self._sem.release()\n                return res\n            finally:\n                self._rlock.release()\n\n        else:\n            if block:\n                deadline = time.time() + timeout\n            if not self._rlock.acquire(block, timeout):\n                raise Empty\n            try:\n                if block:\n                    timeout = deadline - time.time()\n                    if timeout < 0 or not self._poll(timeout):\n                        raise Empty\n                elif not self._poll():\n                    raise Empty\n                res = self._recv()\n                self._sem.release()\n                return res\n            finally:\n                self._rlock.release()\n\n    def qsize(self):\n        # Raises NotImplementedError on Mac OSX because of broken sem_getvalue()\n        return self._maxsize - self._sem._semlock._get_value()\n\n    def empty(self):\n        return not self._poll()\n\n    def full(self):\n        return self._sem._semlock._is_zero()\n\n    def get_nowait(self):\n        return self.get(False)\n\n    def put_nowait(self, obj):\n        return self.put(obj, False)\n\n    def close(self):\n        self._closed = True\n        self._reader.close()\n        if self._close:\n            self._close()\n\n    def join_thread(self):\n        debug('Queue.join_thread()')\n        assert self._closed\n        if self._jointhread:\n            self._jointhread()\n\n    def cancel_join_thread(self):\n        debug('Queue.cancel_join_thread()')\n        self._joincancelled = True\n        try:\n            self._jointhread.cancel()\n        except AttributeError:\n            pass\n\n    def _start_thread(self):\n        debug('Queue._start_thread()')\n\n        # Start thread which transfers data from buffer to pipe\n        self._buffer.clear()\n        self._thread = Queue._feed(self._buffer, self._notempty, self._send, self._wlock, self._writer.close)\n        #self._thread.daemon = True\n        debug('doing self._thread.start()')\n        self._thread.next()\n        debug('... done self._thread.start()')\n\n        # On process exit we will wait for data to be flushed to pipe.\n        if not self._joincancelled:\n            self._jointhread = Finalize(\n                self._thread, Queue._finalize_join,\n                [weakref.ref(self._thread)],\n                exitpriority=-5\n                )\n\n        # Send sentinel to the thread queue object when garbage collected\n        self._close = Finalize(\n            self, Queue._finalize_close,\n            [self._buffer, self._notempty],\n            exitpriority=10\n            )\n\n    @staticmethod\n    def _finalize_join(twr):\n        debug('joining queue thread')\n        thread = twr()\n        if thread is not None:\n            try:\n                thread.next()\n            except StopIteration:\n                pass\n            debug('... queue thread joined')\n        else:\n            debug('... queue thread already dead')\n\n    @staticmethod\n    def _finalize_close(buffer, notempty):\n        debug('telling queue thread to quit')\n        notempty.acquire()\n        try:\n            buffer.append(_sentinel)\n            notempty.notify()\n        finally:\n            notempty.release()\n\n    @staticmethod\n    def _feed(buffer, notempty, send, writelock, close):\n        debug('starting thread to feed data to pipe')\n        from .util import is_exiting\n\n        nacquire = notempty.acquire\n        nrelease = notempty.release\n        nwait = notempty.wait\n        bpopleft = buffer.popleft\n        sentinel = _sentinel\n        wacquire = writelock.acquire\n        wrelease = writelock.release\n\n        try:\n            while 1:\n                try:\n                    while 1:\n                        obj = bpopleft()\n                        if obj is sentinel:\n                            debug('feeder thread got sentinel -- exiting')\n                            close()\n                            return\n\n                        if wacquire is None:\n                            send(obj)\n                        else:\n                            wacquire()\n                            try:\n                                send(obj)\n                            finally:\n                                wrelease()\n                except IndexError:\n                    pass\n                yield \"Done for now\"\n        except Exception as e:\n            # Since this runs in a daemon thread the resources it uses\n            # may be become unusable while the process is cleaning up.\n            # We ignore errors which happen after the process has\n            # started to cleanup.\n            try:\n                if is_exiting():\n                    info('error in queue thread: %s', e)\n                else:\n                    import traceback\n                    traceback.print_exc()\n            except Exception:\n                pass\n\n_sentinel = object()\n\n#\n# A queue type which also supports join() and task_done() methods\n#\n# Note that if you do not call task_done() for each finished task then\n# eventually the counter's semaphore may overflow causing Bad Things\n# to happen.\n#\n\nclass JoinableQueue(Queue):\n\n    def __init__(self, maxsize=0):\n        Queue.__init__(self, maxsize)\n        self._unfinished_tasks = Semaphore(0)\n        self._cond = Condition()\n\n    def __getstate__(self):\n        return Queue.__getstate__(self) + (self._cond, self._unfinished_tasks)\n\n    def __setstate__(self, state):\n        Queue.__setstate__(self, state[:-2])\n        self._cond, self._unfinished_tasks = state[-2:]\n\n    def put(self, obj, block=True, timeout=None):\n        assert not self._closed\n        if not self._sem.acquire(block, timeout):\n            raise Full\n\n        self._notempty.acquire()\n        self._cond.acquire()\n        try:\n            if self._thread is None:\n                self._start_thread()\n            self._buffer.append(obj)\n            self._unfinished_tasks.release()\n            self._notempty.notify()\n        finally:\n            self._cond.release()\n            self._notempty.release()\n\n    def task_done(self):\n        self._cond.acquire()\n        try:\n            if not self._unfinished_tasks.acquire(False):\n                raise ValueError('task_done() called too many times')\n            if self._unfinished_tasks._semlock._is_zero():\n                self._cond.notify_all()\n        finally:\n            self._cond.release()\n\n    def join(self):\n        self._cond.acquire()\n        try:\n            if not self._unfinished_tasks._semlock._is_zero():\n                self._cond.wait()\n        finally:\n            self._cond.release()\n\n#\n# Simplified Queue type -- really just a locked pipe\n#\n\nclass SimpleQueue(object):\n\n    def __init__(self):\n        self._reader, self._writer = Pipe(duplex=False)\n        self._rlock = Lock()\n        if sys.platform == 'win32':\n            self._wlock = None\n        else:\n            self._wlock = Lock()\n        self._make_methods()\n\n    def empty(self):\n        return not self._reader.poll()\n\n    def __getstate__(self):\n        assert_spawning(self)\n        return (int(os.fileNumbers[self._writer.fno].buffer.id), self._rlock, self._wlock)\n\n    def __setstate__(self, state):\n        (buffer_id, self._rlock, self._wlock) = state\n        self._reader, self._writer = Pipe(duplex=False, buffer_id=buffer_id)\n        self._make_methods()\n\n    def _make_methods(self):\n        recv = self._reader.recv\n        racquire, rrelease = self._rlock.acquire, self._rlock.release\n        def get():\n            racquire()\n            try:\n                return recv()\n            finally:\n                rrelease()\n        self.get = get\n\n        if self._wlock is None:\n            # writes to a message oriented win32 pipe are atomic\n            self.put = self._writer.send\n        else:\n            send = self._writer.send\n            wacquire, wrelease = self._wlock.acquire, self._wlock.release\n            def put(obj):\n                wacquire()\n                try:\n                    return send(obj)\n                finally:\n                    wrelease()\n            self.put = put\n", 
    "multiprocessing.reduction": "#\n# Module to allow connection and socket objects to be transferred\n# between processes\n#\n# multiprocessing/reduction.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__all__ = []\n\nimport os\nimport sys\nimport socket\nimport threading\n\nimport _multiprocessing\nfrom multiprocessing import current_process\nfrom multiprocessing.forking import Popen, duplicate, close, ForkingPickler\nfrom multiprocessing.util import register_after_fork, debug, sub_debug\nfrom multiprocessing.connection import Client, Listener\n\n\n#\n#\n#\n\nif not(sys.platform == 'win32' or hasattr(_multiprocessing, 'recvfd')):\n    raise ImportError('pickling of connections not supported')\n\n#\n# Platform specific definitions\n#\n\nif sys.platform == 'win32':\n    import _subprocess\n    from _multiprocessing import win32\n\n    def send_handle(conn, handle, destination_pid):\n        process_handle = win32.OpenProcess(\n            win32.PROCESS_ALL_ACCESS, False, destination_pid\n            )\n        try:\n            new_handle = duplicate(handle, process_handle)\n            conn.send(new_handle)\n        finally:\n            close(process_handle)\n\n    def recv_handle(conn):\n        return conn.recv()\n\nelse:\n    def send_handle(conn, handle, destination_pid):\n        _multiprocessing.sendfd(conn.fileno(), handle)\n\n    def recv_handle(conn):\n        return _multiprocessing.recvfd(conn.fileno())\n\n#\n# Support for a per-process server thread which caches pickled handles\n#\n\n_cache = set()\n\ndef _reset(obj):\n    global _lock, _listener, _cache\n    for h in _cache:\n        close(h)\n    _cache.clear()\n    _lock = threading.Lock()\n    _listener = None\n\n_reset(None)\nregister_after_fork(_reset, _reset)\n\ndef _get_listener():\n    global _listener\n\n    if _listener is None:\n        _lock.acquire()\n        try:\n            if _listener is None:\n                debug('starting listener and thread for sending handles')\n                _listener = Listener(authkey=current_process().authkey)\n                t = threading.Thread(target=_serve)\n                t.daemon = True\n                t.start()\n        finally:\n            _lock.release()\n\n    return _listener\n\ndef _serve():\n    from .util import is_exiting, sub_warning\n\n    while 1:\n        try:\n            conn = _listener.accept()\n            handle_wanted, destination_pid = conn.recv()\n            _cache.remove(handle_wanted)\n            send_handle(conn, handle_wanted, destination_pid)\n            close(handle_wanted)\n            conn.close()\n        except:\n            if not is_exiting():\n                import traceback\n                sub_warning(\n                    'thread for sharing handles raised exception :\\n' +\n                    '-'*79 + '\\n' + traceback.format_exc() + '-'*79\n                    )\n\n#\n# Functions to be used for pickling/unpickling objects with handles\n#\n\ndef reduce_handle(handle):\n    if Popen.thread_is_spawning():\n        return (None, Popen.duplicate_for_child(handle), True)\n    dup_handle = duplicate(handle)\n    _cache.add(dup_handle)\n    sub_debug('reducing handle %d', handle)\n    return (_get_listener().address, dup_handle, False)\n\ndef rebuild_handle(pickled_data):\n    address, handle, inherited = pickled_data\n    if inherited:\n        return handle\n    sub_debug('rebuilding handle %d', handle)\n    conn = Client(address, authkey=current_process().authkey)\n    conn.send((handle, os.getpid()))\n    new_handle = recv_handle(conn)\n    conn.close()\n    return new_handle\n\n#\n# Register `_multiprocessing.Connection` with `ForkingPickler`\n#\n\ndef reduce_connection(conn):\n    rh = reduce_handle(conn.fileno())\n    return rebuild_connection, (rh, conn.readable, conn.writable)\n\ndef rebuild_connection(reduced_handle, readable, writable):\n    handle = rebuild_handle(reduced_handle)\n    return _multiprocessing.Connection(\n        handle, readable=readable, writable=writable\n        )\n\nForkingPickler.register(_multiprocessing.Connection, reduce_connection)\n\n#\n# Register `socket.socket` with `ForkingPickler`\n#\n\ndef fromfd(fd, family, type_, proto=0):\n    s = socket.fromfd(fd, family, type_, proto)\n    if s.__class__ is not socket.socket:\n        s = socket.socket(_sock=s)\n    return s\n\ndef reduce_socket(s):\n    reduced_handle = reduce_handle(s.fileno())\n    return rebuild_socket, (reduced_handle, s.family, s.type, s.proto)\n\ndef rebuild_socket(reduced_handle, family, type_, proto):\n    fd = rebuild_handle(reduced_handle)\n    _sock = fromfd(fd, family, type_, proto)\n    close(fd)\n    return _sock\n\nForkingPickler.register(socket.socket, reduce_socket)\n\n#\n# Register `_multiprocessing.PipeConnection` with `ForkingPickler`\n#\n\nif sys.platform == 'win32':\n\n    def reduce_pipe_connection(conn):\n        rh = reduce_handle(conn.fileno())\n        return rebuild_pipe_connection, (rh, conn.readable, conn.writable)\n\n    def rebuild_pipe_connection(reduced_handle, readable, writable):\n        handle = rebuild_handle(reduced_handle)\n        return _multiprocessing.PipeConnection(\n            handle, readable=readable, writable=writable\n            )\n\n    ForkingPickler.register(_multiprocessing.PipeConnection, reduce_pipe_connection)\n", 
    "multiprocessing.sharedctypes": "#\n# Module which supports allocation of ctypes objects from shared memory\n#\n# multiprocessing/sharedctypes.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\nimport sys\nimport ctypes\nimport weakref\n\nfrom multiprocessing import heap, RLock\nfrom multiprocessing.forking import assert_spawning, ForkingPickler\n\n__all__ = ['RawValue', 'RawArray', 'Value', 'Array', 'copy', 'synchronized']\n\n#\n#\n#\n\ntypecode_to_type = {\n    'c': ctypes.c_char,  'u': ctypes.c_wchar,\n    'b': ctypes.c_byte,  'B': ctypes.c_ubyte,\n    'h': ctypes.c_short, 'H': ctypes.c_ushort,\n    'i': ctypes.c_int,   'I': ctypes.c_uint,\n    'l': ctypes.c_long,  'L': ctypes.c_ulong,\n    'f': ctypes.c_float, 'd': ctypes.c_double\n    }\n\n#\n#\n#\n\ndef _new_value(type_):\n    size = ctypes.sizeof(type_)\n    wrapper = heap.BufferWrapper(size)\n    return rebuild_ctype(type_, wrapper, None)\n\ndef RawValue(typecode_or_type, *args):\n    '''\n    Returns a ctypes object allocated from shared memory\n    '''\n    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)\n    obj = _new_value(type_)\n    ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))\n    obj.__init__(*args)\n    return obj\n\ndef RawArray(typecode_or_type, size_or_initializer):\n    '''\n    Returns a ctypes array allocated from shared memory\n    '''\n    type_ = typecode_to_type.get(typecode_or_type, typecode_or_type)\n    if isinstance(size_or_initializer, (int, long)):\n        type_ = type_ * size_or_initializer\n        obj = _new_value(type_)\n        ctypes.memset(ctypes.addressof(obj), 0, ctypes.sizeof(obj))\n        return obj\n    else:\n        type_ = type_ * len(size_or_initializer)\n        result = _new_value(type_)\n        result.__init__(*size_or_initializer)\n        return result\n\ndef Value(typecode_or_type, *args, **kwds):\n    '''\n    Return a synchronization wrapper for a Value\n    '''\n    lock = kwds.pop('lock', None)\n    if kwds:\n        raise ValueError('unrecognized keyword argument(s): %s' % kwds.keys())\n    obj = RawValue(typecode_or_type, *args)\n    if lock is False:\n        return obj\n    if lock in (True, None):\n        lock = RLock()\n    if not hasattr(lock, 'acquire'):\n        raise AttributeError(\"'%r' has no method 'acquire'\" % lock)\n    return synchronized(obj, lock)\n\ndef Array(typecode_or_type, size_or_initializer, **kwds):\n    '''\n    Return a synchronization wrapper for a RawArray\n    '''\n    lock = kwds.pop('lock', None)\n    if kwds:\n        raise ValueError('unrecognized keyword argument(s): %s' % kwds.keys())\n    obj = RawArray(typecode_or_type, size_or_initializer)\n    if lock is False:\n        return obj\n    if lock in (True, None):\n        lock = RLock()\n    if not hasattr(lock, 'acquire'):\n        raise AttributeError(\"'%r' has no method 'acquire'\" % lock)\n    return synchronized(obj, lock)\n\ndef copy(obj):\n    new_obj = _new_value(type(obj))\n    ctypes.pointer(new_obj)[0] = obj\n    return new_obj\n\ndef synchronized(obj, lock=None):\n    assert not isinstance(obj, SynchronizedBase), 'object already synchronized'\n\n    if isinstance(obj, ctypes._SimpleCData):\n        return Synchronized(obj, lock)\n    elif isinstance(obj, ctypes.Array):\n        if obj._type_ is ctypes.c_char:\n            return SynchronizedString(obj, lock)\n        return SynchronizedArray(obj, lock)\n    else:\n        cls = type(obj)\n        try:\n            scls = class_cache[cls]\n        except KeyError:\n            names = [field[0] for field in cls._fields_]\n            d = dict((name, make_property(name)) for name in names)\n            classname = 'Synchronized' + cls.__name__\n            scls = class_cache[cls] = type(classname, (SynchronizedBase,), d)\n        return scls(obj, lock)\n\n#\n# Functions for pickling/unpickling\n#\n\ndef reduce_ctype(obj):\n    assert_spawning(obj)\n    if isinstance(obj, ctypes.Array):\n        return rebuild_ctype, (obj._type_, obj._wrapper, obj._length_)\n    else:\n        return rebuild_ctype, (type(obj), obj._wrapper, None)\n\ndef rebuild_ctype(type_, wrapper, length):\n    if length is not None:\n        type_ = type_ * length\n    ForkingPickler.register(type_, reduce_ctype)\n    obj = type_.from_address(wrapper.get_address())\n    obj._wrapper = wrapper\n    return obj\n\n#\n# Function to create properties\n#\n\ndef make_property(name):\n    try:\n        return prop_cache[name]\n    except KeyError:\n        d = {}\n        exec template % ((name,)*7) in d\n        prop_cache[name] = d[name]\n        return d[name]\n\ntemplate = '''\ndef get%s(self):\n    self.acquire()\n    try:\n        return self._obj.%s\n    finally:\n        self.release()\ndef set%s(self, value):\n    self.acquire()\n    try:\n        self._obj.%s = value\n    finally:\n        self.release()\n%s = property(get%s, set%s)\n'''\n\nprop_cache = {}\nclass_cache = weakref.WeakKeyDictionary()\n\n#\n# Synchronized wrappers\n#\n\nclass SynchronizedBase(object):\n\n    def __init__(self, obj, lock=None):\n        self._obj = obj\n        self._lock = lock or RLock()\n        self.acquire = self._lock.acquire\n        self.release = self._lock.release\n\n    def __reduce__(self):\n        assert_spawning(self)\n        return synchronized, (self._obj, self._lock)\n\n    def get_obj(self):\n        return self._obj\n\n    def get_lock(self):\n        return self._lock\n\n    def __repr__(self):\n        return '<%s wrapper for %s>' % (type(self).__name__, self._obj)\n\n\nclass Synchronized(SynchronizedBase):\n    value = make_property('value')\n\n\nclass SynchronizedArray(SynchronizedBase):\n\n    def __len__(self):\n        return len(self._obj)\n\n    def __getitem__(self, i):\n        self.acquire()\n        try:\n            return self._obj[i]\n        finally:\n            self.release()\n\n    def __setitem__(self, i, value):\n        self.acquire()\n        try:\n            self._obj[i] = value\n        finally:\n            self.release()\n\n    def __getslice__(self, start, stop):\n        self.acquire()\n        try:\n            return self._obj[start:stop]\n        finally:\n            self.release()\n\n    def __setslice__(self, start, stop, values):\n        self.acquire()\n        try:\n            self._obj[start:stop] = values\n        finally:\n            self.release()\n\n\nclass SynchronizedString(SynchronizedArray):\n    value = make_property('value')\n    raw = make_property('raw')\n", 
    "multiprocessing.synchronize": "#\n# Module implementing synchronization primitives\n#\n# multiprocessing/synchronize.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\n__all__ = [\n    'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Condition', 'Event'\n    ]\n\nimport threading\nimport os\nimport sys\n\nfrom time import time as _time, sleep as _sleep\n\n#~ import _multiprocessing\nfrom multiprocessing.process import current_process\nfrom multiprocessing.util import Finalize, register_after_fork, debug\nfrom multiprocessing.forking import assert_spawning, Popen\n\n# Try to import the mp.synchronize module cleanly, if it fails\n# raise ImportError for platforms lacking a working sem_open implementation.\n# See issue 3770\n#~ try:\n    #~ from _multiprocessing import SemLock\n#~ except (ImportError):\n    #~ raise ImportError(\"This platform lacks a functioning sem_open\" +\n                      #~ \" implementation, therefore, the required\" +\n                      #~ \" synchronization primitives needed will not\" +\n                      #~ \" function, see issue 3770.\")\n\n#\n# Constants\n#\n\nRECURSIVE_MUTEX, SEMAPHORE = range(2)\nSEM_VALUE_MAX = 2147483647L\n#\n# Base class for semaphores and mutexes; wraps `_multiprocessing.SemLock`\n#\nimport js\n\nclass SemLock(object):\n\n    def __init__(self, kind, value, maxvalue):\n        sl = self._semlock = js.eval('new Semaphore(1);')\n        sl.kind=kind\n        sl.value=0\n        sl.maxvalue=maxvalue\n        debug('created semlock with handle %s' % sl.handle)\n        self._make_methods()\n\n        if sys.platform != 'win32':\n            def _after_fork(obj):\n                obj._semlock._after_fork()\n            register_after_fork(self, _after_fork)\n    \n    def _make_methods(self):\n        self.acquire = self._semlock.acquire\n        self.release = self._semlock.release\n\n    def __enter__(self):\n        return self._semlock.__enter__()\n\n    def __exit__(self, *args):\n        return self._semlock.__exit__(*args)\n\n    def __getstate__(self):\n        assert_spawning(self)\n        sl = self._semlock\n        return (Popen.duplicate_for_child(sl.handle), sl.kind, sl.maxvalue)\n\n    def __setstate__(self, state):\n        self._semlock = js.eval('new Semaphore(1);')\n        self._semlock.value=state[1]\n        self._semlock.maxvalue=state[2]\n        debug('recreated blocker with handle %r' % state[0])\n        self._make_methods()\n\n#\n# Semaphore\n#\n\nclass Semaphore(SemLock):\n\n    def __init__(self, value=1):\n        SemLock.__init__(self, SEMAPHORE, value, SEM_VALUE_MAX)\n\n    def get_value(self):\n        return self._semlock._get_value()\n\n    def __repr__(self):\n        try:\n            value = self._semlock._get_value()\n        except Exception:\n            value = 'unknown'\n        return '<Semaphore(value=%s)>' % value\n\n#\n# Bounded semaphore\n#\n\nclass BoundedSemaphore(Semaphore):\n\n    def __init__(self, value=1):\n        SemLock.__init__(self, SEMAPHORE, value, value)\n\n    def __repr__(self):\n        try:\n            value = self._semlock._get_value()\n        except Exception:\n            value = 'unknown'\n        return '<BoundedSemaphore(value=%s, maxvalue=%s)>' % \\\n               (value, self._semlock.maxvalue)\n\n#\n# Non-recursive lock\n#\n\nclass Lock(SemLock):\n\n    def __init__(self):\n        SemLock.__init__(self, SEMAPHORE, 1, 1)\n\n    def __repr__(self):\n        try:\n            if self._semlock._is_mine():\n                name = current_process().name\n                if threading.current_thread().name != 'MainThread':\n                    name += '|' + threading.current_thread().name\n            elif self._semlock._get_value() == 1:\n                name = 'None'\n            elif self._semlock._count() > 0:\n                name = 'SomeOtherThread'\n            else:\n                name = 'SomeOtherProcess'\n        except Exception:\n            name = 'unknown'\n        return '<Lock(owner=%s)>' % name\n\n#\n# Recursive lock\n#\n\nclass RLock(SemLock):\n\n    def __init__(self):\n        SemLock.__init__(self, RECURSIVE_MUTEX, 1, 1)\n\n    def __repr__(self):\n        try:\n            if self._semlock._is_mine():\n                name = current_process().name\n                if threading.current_thread().name != 'MainThread':\n                    name += '|' + threading.current_thread().name\n                count = self._semlock._count()\n            elif self._semlock._get_value() == 1:\n                name, count = 'None', 0\n            elif self._semlock._count() > 0:\n                name, count = 'SomeOtherThread', 'nonzero'\n            else:\n                name, count = 'SomeOtherProcess', 'nonzero'\n        except Exception:\n            name, count = 'unknown', 'unknown'\n        return '<RLock(%s, %s)>' % (name, count)\n\n#\n# Condition variable\n#\n\nclass Condition(object):\n\n    def __init__(self, lock=None):\n        self._lock = lock or RLock()\n        self._sleeping_count = Semaphore(0)\n        self._woken_count = Semaphore(0)\n        self._wait_semaphore = Semaphore(0)\n        self._make_methods()\n\n    def __getstate__(self):\n        assert_spawning(self)\n        return (self._lock, self._sleeping_count,\n                self._woken_count, self._wait_semaphore)\n\n    def __setstate__(self, state):\n        (self._lock, self._sleeping_count,\n         self._woken_count, self._wait_semaphore) = state\n        self._make_methods()\n\n    def __enter__(self):\n        return self._lock.__enter__()\n\n    def __exit__(self, *args):\n        return self._lock.__exit__(*args)\n\n    def _make_methods(self):\n        self.acquire = self._lock.acquire\n        self.release = self._lock.release\n\n    def __repr__(self):\n        try:\n            num_waiters = (self._sleeping_count._semlock._get_value() -\n                           self._woken_count._semlock._get_value())\n        except Exception:\n            num_waiters = 'unknown'\n        return '<Condition(%s, %s)>' % (self._lock, num_waiters)\n\n    def wait(self, timeout=None):\n        assert self._lock._semlock._is_mine(), \\\n               'must acquire() condition before using wait()'\n\n        # indicate that this thread is going to sleep\n        self._sleeping_count.release()\n\n        # release lock\n        count = self._lock._semlock._count()\n        for i in xrange(count):\n            self._lock.release()\n\n        try:\n            # wait for notification or timeout\n            self._wait_semaphore.acquire(True, timeout)\n        finally:\n            # indicate that this thread has woken\n            self._woken_count.release()\n\n            # reacquire lock\n            for i in xrange(count):\n                self._lock.acquire()\n\n    def notify(self):\n        assert self._lock._semlock._is_mine(), 'lock is not owned'\n        assert not self._wait_semaphore.acquire(False)\n\n        # to take account of timeouts since last notify() we subtract\n        # woken_count from sleeping_count and rezero woken_count\n        while self._woken_count.acquire(False):\n            res = self._sleeping_count.acquire(False)\n            assert res\n\n        if self._sleeping_count.acquire(False): # try grabbing a sleeper\n            self._wait_semaphore.release()      # wake up one sleeper\n            self._woken_count.acquire()         # wait for the sleeper to wake\n\n            # rezero _wait_semaphore in case a timeout just happened\n            self._wait_semaphore.acquire(False)\n\n    def notify_all(self):\n        assert self._lock._semlock._is_mine(), 'lock is not owned'\n        assert not self._wait_semaphore.acquire(False)\n\n        # to take account of timeouts since last notify*() we subtract\n        # woken_count from sleeping_count and rezero woken_count\n        while self._woken_count.acquire(False):\n            res = self._sleeping_count.acquire(False)\n            assert res\n\n        sleepers = 0\n        while self._sleeping_count.acquire(False):\n            self._wait_semaphore.release()        # wake up one sleeper\n            sleepers += 1\n\n        if sleepers:\n            for i in xrange(sleepers):\n                self._woken_count.acquire()       # wait for a sleeper to wake\n\n            # rezero wait_semaphore in case some timeouts just happened\n            while self._wait_semaphore.acquire(False):\n                pass\n\n#\n# Event\n#\n\nclass Event(object):\n\n    def __init__(self):\n        self._cond = Condition(Lock())\n        self._flag = Semaphore(0)\n\n    def is_set(self):\n        self._cond.acquire()\n        try:\n            if self._flag.acquire(False):\n                self._flag.release()\n                return True\n            return False\n        finally:\n            self._cond.release()\n\n    def set(self):\n        self._cond.acquire()\n        try:\n            self._flag.acquire(False)\n            self._flag.release()\n            self._cond.notify_all()\n        finally:\n            self._cond.release()\n\n    def clear(self):\n        self._cond.acquire()\n        try:\n            self._flag.acquire(False)\n        finally:\n            self._cond.release()\n\n    def wait(self, timeout=None):\n        self._cond.acquire()\n        try:\n            if self._flag.acquire(False):\n                self._flag.release()\n            else:\n                self._cond.wait(timeout)\n\n            if self._flag.acquire(False):\n                self._flag.release()\n                return True\n            return False\n        finally:\n            self._cond.release()\n", 
    "multiprocessing.util": "#\n# Module providing various facilities to other parts of the package\n#\n# multiprocessing/util.py\n#\n# Copyright (c) 2006-2008, R Oudkerk\n# All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions\n# are met:\n#\n# 1. Redistributions of source code must retain the above copyright\n#    notice, this list of conditions and the following disclaimer.\n# 2. Redistributions in binary form must reproduce the above copyright\n#    notice, this list of conditions and the following disclaimer in the\n#    documentation and/or other materials provided with the distribution.\n# 3. Neither the name of author nor the names of any contributors may be\n#    used to endorse or promote products derived from this software\n#    without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE AUTHOR AND CONTRIBUTORS \"AS IS\" AND\n# ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED.  IN NO EVENT SHALL THE AUTHOR OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS\n# OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\n# HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\n# LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\n# OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\n# SUCH DAMAGE.\n#\n\nimport os\nimport itertools\nimport weakref\nimport atexit\nimport threading        # we want threading to install it's\n                        # cleanup function before multiprocessing does\nfrom subprocess import _args_from_interpreter_flags\n\nfrom multiprocessing.process import current_process, active_children\n\n__all__ = [\n    'sub_debug', 'debug', 'info', 'sub_warning', 'get_logger',\n    'log_to_stderr', 'get_temp_dir', 'register_after_fork',\n    'is_exiting', 'Finalize', 'ForkAwareThreadLock', 'ForkAwareLocal',\n    'SUBDEBUG', 'SUBWARNING',\n    ]\n\n#\n# Logging\n#\n\nNOTSET = 0\nSUBDEBUG = 5\nDEBUG = 10\nINFO = 20\nSUBWARNING = 25\n\nLOGGER_NAME = 'multiprocessing'\nDEFAULT_LOGGING_FORMAT = '[%(levelname)s/%(processName)s] %(message)s'\n\n_logger = None\n_log_to_stderr = False\n\ndef sub_debug(msg, *args):\n    if _logger:\n        _logger.log(SUBDEBUG, msg, *args)\n\ndef debug(msg, *args):\n    if _logger:\n        _logger.log(DEBUG, msg, *args)\n\ndef info(msg, *args):\n    if _logger:\n        _logger.log(INFO, msg, *args)\n\ndef sub_warning(msg, *args):\n    if _logger:\n        _logger.log(SUBWARNING, msg, *args)\n\ndef get_logger():\n    '''\n    Returns logger used by multiprocessing\n    '''\n    global _logger\n    import logging, atexit\n\n    logging._acquireLock()\n    try:\n        if not _logger:\n\n            _logger = logging.getLogger(LOGGER_NAME)\n            _logger.propagate = 0\n            logging.addLevelName(SUBDEBUG, 'SUBDEBUG')\n            logging.addLevelName(SUBWARNING, 'SUBWARNING')\n\n            # XXX multiprocessing should cleanup before logging\n            if hasattr(atexit, 'unregister'):\n                atexit.unregister(_exit_function)\n                atexit.register(_exit_function)\n            else:\n                atexit._exithandlers.remove((_exit_function, (), {}))\n                atexit._exithandlers.append((_exit_function, (), {}))\n\n    finally:\n        logging._releaseLock()\n\n    return _logger\n\ndef log_to_stderr(level=None):\n    '''\n    Turn on logging and add a handler which prints to stderr\n    '''\n    global _log_to_stderr\n    import logging\n\n    logger = get_logger()\n    formatter = logging.Formatter(DEFAULT_LOGGING_FORMAT)\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n\n    if level:\n        logger.setLevel(level)\n    _log_to_stderr = True\n    return _logger\n\n#\n# Function returning a temp directory which will be removed on exit\n#\n\ndef get_temp_dir():\n    # get name of a temp directory which will be automatically cleaned up\n    if current_process()._tempdir is None:\n        import shutil, tempfile\n        tempdir = tempfile.mkdtemp(prefix='pymp-')\n        info('created temp directory %s', tempdir)\n        Finalize(None, shutil.rmtree, args=[tempdir], exitpriority=-100)\n        current_process()._tempdir = tempdir\n    return current_process()._tempdir\n\n#\n# Support for reinitialization of objects when bootstrapping a child process\n#\n\n_afterfork_registry = weakref.WeakValueDictionary()\n_afterfork_counter = itertools.count()\n\ndef _run_after_forkers():\n    items = list(_afterfork_registry.items())\n    items.sort()\n    for (index, ident, func), obj in items:\n        try:\n            func(obj)\n        except Exception, e:\n            info('after forker raised exception %s', e)\n\ndef register_after_fork(obj, func):\n    _afterfork_registry[(_afterfork_counter.next(), id(obj), func)] = obj\n\n#\n# Finalization using weakrefs\n#\n\n_finalizer_registry = {}\n_finalizer_counter = itertools.count()\n\n\nclass Finalize(object):\n    '''\n    Class which supports object finalization using weakrefs\n    '''\n    def __init__(self, obj, callback, args=(), kwargs=None, exitpriority=None):\n        assert exitpriority is None or type(exitpriority) is int\n\n        if obj is not None:\n            self._weakref = weakref.ref(obj, self)\n        else:\n            assert exitpriority is not None\n\n        self._callback = callback\n        self._args = args\n        self._kwargs = kwargs or {}\n        self._key = (exitpriority, _finalizer_counter.next())\n        self._pid = os.getpid()\n\n        _finalizer_registry[self._key] = self\n\n    def __call__(self, wr=None):\n        '''\n        Run the callback unless it has already been called or cancelled\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            sub_debug('finalizer no longer registered')\n        else:\n            if self._pid != os.getpid():\n                sub_debug('finalizer ignored because different process')\n                res = None\n            else:\n                sub_debug('finalizer calling %s with args %s and kwargs %s',\n                          self._callback, self._args, self._kwargs)\n                res = self._callback(*self._args, **self._kwargs)\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n            return res\n\n    def cancel(self):\n        '''\n        Cancel finalization of the object\n        '''\n        try:\n            del _finalizer_registry[self._key]\n        except KeyError:\n            pass\n        else:\n            self._weakref = self._callback = self._args = \\\n                            self._kwargs = self._key = None\n\n    def still_active(self):\n        '''\n        Return whether this finalizer is still waiting to invoke callback\n        '''\n        return self._key in _finalizer_registry\n\n    def __repr__(self):\n        try:\n            obj = self._weakref()\n        except (AttributeError, TypeError):\n            obj = None\n\n        if obj is None:\n            return '<Finalize object, dead>'\n\n        x = '<Finalize object, callback=%s' % \\\n            getattr(self._callback, '__name__', self._callback)\n        if self._args:\n            x += ', args=' + str(self._args)\n        if self._kwargs:\n            x += ', kwargs=' + str(self._kwargs)\n        if self._key[0] is not None:\n            x += ', exitprority=' + str(self._key[0])\n        return x + '>'\n\n\ndef _run_finalizers(minpriority=None):\n    '''\n    Run all finalizers whose exit priority is not None and at least minpriority\n\n    Finalizers with highest priority are called first; finalizers with\n    the same priority will be called in reverse order of creation.\n    '''\n    if _finalizer_registry is None:\n        # This function may be called after this module's globals are\n        # destroyed.  See the _exit_function function in this module for more\n        # notes.\n        return\n\n    if minpriority is None:\n        f = lambda p : p[0][0] is not None\n    else:\n        f = lambda p : p[0][0] is not None and p[0][0] >= minpriority\n\n    items = [x for x in _finalizer_registry.items() if f(x)]\n    items.sort(reverse=True)\n\n    for key, finalizer in items:\n        sub_debug('calling %s', finalizer)\n        try:\n            finalizer()\n        except Exception:\n            import traceback\n            traceback.print_exc()\n\n    if minpriority is None:\n        _finalizer_registry.clear()\n\n#\n# Clean up on exit\n#\n\ndef is_exiting():\n    '''\n    Returns true if the process is shutting down\n    '''\n    return _exiting or _exiting is None\n\n_exiting = False\n\ndef _exit_function(info=info, debug=debug, _run_finalizers=_run_finalizers,\n                   active_children=active_children,\n                   current_process=current_process):\n    # NB: we hold on to references to functions in the arglist due to the\n    # situation described below, where this function is called after this\n    # module's globals are destroyed.\n\n    global _exiting\n\n    info('process shutting down')\n    debug('running all \"atexit\" finalizers with priority >= 0')\n    _run_finalizers(0)\n\n    if current_process() is not None:\n        # NB: we check if the current process is None here because if\n        # it's None, any call to ``active_children()`` will throw an\n        # AttributeError (active_children winds up trying to get\n        # attributes from util._current_process).  This happens in a\n        # variety of shutdown circumstances that are not well-understood\n        # because module-scope variables are not apparently supposed to\n        # be destroyed until after this function is called.  However,\n        # they are indeed destroyed before this function is called.  See\n        # issues 9775 and 15881.  Also related: 4106, 9205, and 9207.\n\n        for p in active_children():\n            if p._daemonic:\n                info('calling terminate() for daemon %s', p.name)\n                p._popen.terminate()\n\n        for p in active_children():\n            info('calling join() for process %s', p.name)\n            p.join()\n\n    debug('running the remaining \"atexit\" finalizers')\n    _run_finalizers()\n\natexit.register(_exit_function)\n\n#\n# Some fork aware types\n#\n\nclass ForkAwareThreadLock(object):\n    def __init__(self):\n        self._reset()\n        register_after_fork(self, ForkAwareThreadLock._reset)\n\n    def _reset(self):\n        self._lock = threading.Lock()\n        self.acquire = self._lock.acquire\n        self.release = self._lock.release\n\nclass ForkAwareLocal(threading.local):\n    def __init__(self):\n        register_after_fork(self, lambda obj : obj.__dict__.clear())\n    def __reduce__(self):\n        return type(self), ()\n", 
    "nturl2path": "\"\"\"Convert a NT pathname to a file URL and vice versa.\"\"\"\n\ndef url2pathname(url):\n    \"\"\"OS-specific conversion from a relative URL of the 'file' scheme\n    to a file system path; not recommended for general use.\"\"\"\n    # e.g.\n    # ///C|/foo/bar/spam.foo\n    # becomes\n    # C:\\foo\\bar\\spam.foo\n    import string, urllib\n    # Windows itself uses \":\" even in URLs.\n    url = url.replace(':', '|')\n    if not '|' in url:\n        # No drive specifier, just convert slashes\n        if url[:4] == '////':\n            # path is something like ////host/path/on/remote/host\n            # convert this to \\\\host\\path\\on\\remote\\host\n            # (notice halving of slashes at the start of the path)\n            url = url[2:]\n        components = url.split('/')\n        # make sure not to convert quoted slashes :-)\n        return urllib.unquote('\\\\'.join(components))\n    comp = url.split('|')\n    if len(comp) != 2 or comp[0][-1] not in string.ascii_letters:\n        error = 'Bad URL: ' + url\n        raise IOError, error\n    drive = comp[0][-1].upper()\n    path = drive + ':'\n    components = comp[1].split('/')\n    for comp in components:\n        if comp:\n            path = path + '\\\\' + urllib.unquote(comp)\n    # Issue #11474: url like '/C|/' should convert into 'C:\\\\'\n    if path.endswith(':') and url.endswith('/'):\n        path += '\\\\'\n    return path\n\ndef pathname2url(p):\n    \"\"\"OS-specific conversion from a file system path to a relative URL\n    of the 'file' scheme; not recommended for general use.\"\"\"\n    # e.g.\n    # C:\\foo\\bar\\spam.foo\n    # becomes\n    # ///C|/foo/bar/spam.foo\n    import urllib\n    if not ':' in p:\n        # No drive specifier, just convert slashes and quote the name\n        if p[:2] == '\\\\\\\\':\n        # path is something like \\\\host\\path\\on\\remote\\host\n        # convert this to ////host/path/on/remote/host\n        # (notice doubling of slashes at the start of the path)\n            p = '\\\\\\\\' + p\n        components = p.split('\\\\')\n        return urllib.quote('/'.join(components))\n    comp = p.split(':')\n    if len(comp) != 2 or len(comp[0]) > 1:\n        error = 'Bad path: ' + p\n        raise IOError, error\n\n    drive = urllib.quote(comp[0].upper())\n    components = comp[1].split('\\\\')\n    path = '///' + drive + ':'\n    for comp in components:\n        if comp:\n            path = path + '/' + urllib.quote(comp)\n    return path\n", 
    "opcode": "\"\"\"\nopcode module - potentially shared between dis and other modules which\noperate on bytecodes (e.g. peephole optimizers).\n\"\"\"\n\n__all__ = [\"cmp_op\", \"hasconst\", \"hasname\", \"hasjrel\", \"hasjabs\",\n           \"haslocal\", \"hascompare\", \"hasfree\", \"opname\", \"opmap\",\n           \"HAVE_ARGUMENT\", \"EXTENDED_ARG\"]\n\ncmp_op = ('<', '<=', '==', '!=', '>', '>=', 'in', 'not in', 'is',\n        'is not', 'exception match', 'BAD')\n\nhasconst = []\nhasname = []\nhasjrel = []\nhasjabs = []\nhaslocal = []\nhascompare = []\nhasfree = []\n\nopmap = {}\nopname = [''] * 256\nfor op in range(256): opname[op] = '<%r>' % (op,)\ndel op\n\ndef def_op(name, op):\n    opname[op] = name\n    opmap[name] = op\n\ndef name_op(name, op):\n    def_op(name, op)\n    hasname.append(op)\n\ndef jrel_op(name, op):\n    def_op(name, op)\n    hasjrel.append(op)\n\ndef jabs_op(name, op):\n    def_op(name, op)\n    hasjabs.append(op)\n\n# Instruction opcodes for compiled code\n# Blank lines correspond to available opcodes\n\ndef_op('STOP_CODE', 0)\ndef_op('POP_TOP', 1)\ndef_op('ROT_TWO', 2)\ndef_op('ROT_THREE', 3)\ndef_op('DUP_TOP', 4)\ndef_op('ROT_FOUR', 5)\n\ndef_op('NOP', 9)\ndef_op('UNARY_POSITIVE', 10)\ndef_op('UNARY_NEGATIVE', 11)\ndef_op('UNARY_NOT', 12)\ndef_op('UNARY_CONVERT', 13)\n\ndef_op('UNARY_INVERT', 15)\n\ndef_op('BINARY_POWER', 19)\ndef_op('BINARY_MULTIPLY', 20)\ndef_op('BINARY_DIVIDE', 21)\ndef_op('BINARY_MODULO', 22)\ndef_op('BINARY_ADD', 23)\ndef_op('BINARY_SUBTRACT', 24)\ndef_op('BINARY_SUBSCR', 25)\ndef_op('BINARY_FLOOR_DIVIDE', 26)\ndef_op('BINARY_TRUE_DIVIDE', 27)\ndef_op('INPLACE_FLOOR_DIVIDE', 28)\ndef_op('INPLACE_TRUE_DIVIDE', 29)\ndef_op('SLICE+0', 30)\ndef_op('SLICE+1', 31)\ndef_op('SLICE+2', 32)\ndef_op('SLICE+3', 33)\n\ndef_op('STORE_SLICE+0', 40)\ndef_op('STORE_SLICE+1', 41)\ndef_op('STORE_SLICE+2', 42)\ndef_op('STORE_SLICE+3', 43)\n\ndef_op('DELETE_SLICE+0', 50)\ndef_op('DELETE_SLICE+1', 51)\ndef_op('DELETE_SLICE+2', 52)\ndef_op('DELETE_SLICE+3', 53)\n\ndef_op('STORE_MAP', 54)\ndef_op('INPLACE_ADD', 55)\ndef_op('INPLACE_SUBTRACT', 56)\ndef_op('INPLACE_MULTIPLY', 57)\ndef_op('INPLACE_DIVIDE', 58)\ndef_op('INPLACE_MODULO', 59)\ndef_op('STORE_SUBSCR', 60)\ndef_op('DELETE_SUBSCR', 61)\ndef_op('BINARY_LSHIFT', 62)\ndef_op('BINARY_RSHIFT', 63)\ndef_op('BINARY_AND', 64)\ndef_op('BINARY_XOR', 65)\ndef_op('BINARY_OR', 66)\ndef_op('INPLACE_POWER', 67)\ndef_op('GET_ITER', 68)\n\ndef_op('PRINT_EXPR', 70)\ndef_op('PRINT_ITEM', 71)\ndef_op('PRINT_NEWLINE', 72)\ndef_op('PRINT_ITEM_TO', 73)\ndef_op('PRINT_NEWLINE_TO', 74)\ndef_op('INPLACE_LSHIFT', 75)\ndef_op('INPLACE_RSHIFT', 76)\ndef_op('INPLACE_AND', 77)\ndef_op('INPLACE_XOR', 78)\ndef_op('INPLACE_OR', 79)\ndef_op('BREAK_LOOP', 80)\ndef_op('WITH_CLEANUP', 81)\ndef_op('LOAD_LOCALS', 82)\ndef_op('RETURN_VALUE', 83)\ndef_op('IMPORT_STAR', 84)\ndef_op('EXEC_STMT', 85)\ndef_op('YIELD_VALUE', 86)\ndef_op('POP_BLOCK', 87)\ndef_op('END_FINALLY', 88)\ndef_op('BUILD_CLASS', 89)\n\nHAVE_ARGUMENT = 90              # Opcodes from here have an argument:\n\nname_op('STORE_NAME', 90)       # Index in name list\nname_op('DELETE_NAME', 91)      # \"\"\ndef_op('UNPACK_SEQUENCE', 92)   # Number of tuple items\njrel_op('FOR_ITER', 93)\ndef_op('LIST_APPEND', 94)\nname_op('STORE_ATTR', 95)       # Index in name list\nname_op('DELETE_ATTR', 96)      # \"\"\nname_op('STORE_GLOBAL', 97)     # \"\"\nname_op('DELETE_GLOBAL', 98)    # \"\"\ndef_op('DUP_TOPX', 99)          # number of items to duplicate\ndef_op('LOAD_CONST', 100)       # Index in const list\nhasconst.append(100)\nname_op('LOAD_NAME', 101)       # Index in name list\ndef_op('BUILD_TUPLE', 102)      # Number of tuple items\ndef_op('BUILD_LIST', 103)       # Number of list items\ndef_op('BUILD_SET', 104)        # Number of set items\ndef_op('BUILD_MAP', 105)        # Number of dict entries (upto 255)\nname_op('LOAD_ATTR', 106)       # Index in name list\ndef_op('COMPARE_OP', 107)       # Comparison operator\nhascompare.append(107)\nname_op('IMPORT_NAME', 108)     # Index in name list\nname_op('IMPORT_FROM', 109)     # Index in name list\njrel_op('JUMP_FORWARD', 110)    # Number of bytes to skip\njabs_op('JUMP_IF_FALSE_OR_POP', 111) # Target byte offset from beginning of code\njabs_op('JUMP_IF_TRUE_OR_POP', 112)  # \"\"\njabs_op('JUMP_ABSOLUTE', 113)        # \"\"\njabs_op('POP_JUMP_IF_FALSE', 114)    # \"\"\njabs_op('POP_JUMP_IF_TRUE', 115)     # \"\"\n\nname_op('LOAD_GLOBAL', 116)     # Index in name list\n\njabs_op('CONTINUE_LOOP', 119)   # Target address\njrel_op('SETUP_LOOP', 120)      # Distance to target address\njrel_op('SETUP_EXCEPT', 121)    # \"\"\njrel_op('SETUP_FINALLY', 122)   # \"\"\n\ndef_op('LOAD_FAST', 124)        # Local variable number\nhaslocal.append(124)\ndef_op('STORE_FAST', 125)       # Local variable number\nhaslocal.append(125)\ndef_op('DELETE_FAST', 126)      # Local variable number\nhaslocal.append(126)\n\ndef_op('RAISE_VARARGS', 130)    # Number of raise arguments (1, 2, or 3)\ndef_op('CALL_FUNCTION', 131)    # #args + (#kwargs << 8)\ndef_op('MAKE_FUNCTION', 132)    # Number of args with default values\ndef_op('BUILD_SLICE', 133)      # Number of items\ndef_op('MAKE_CLOSURE', 134)\ndef_op('LOAD_CLOSURE', 135)\nhasfree.append(135)\ndef_op('LOAD_DEREF', 136)\nhasfree.append(136)\ndef_op('STORE_DEREF', 137)\nhasfree.append(137)\n\ndef_op('CALL_FUNCTION_VAR', 140)     # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_KW', 141)      # #args + (#kwargs << 8)\ndef_op('CALL_FUNCTION_VAR_KW', 142)  # #args + (#kwargs << 8)\n\njrel_op('SETUP_WITH', 143)\n\ndef_op('EXTENDED_ARG', 145)\nEXTENDED_ARG = 145\ndef_op('SET_ADD', 146)\ndef_op('MAP_ADD', 147)\n\n# pypy modification, experimental bytecode\ndef_op('LOOKUP_METHOD', 201)          # Index in name list\nhasname.append(201)\ndef_op('CALL_METHOD', 202)            # #args not including 'self'\ndef_op('BUILD_LIST_FROM_ARG', 203)\njrel_op('JUMP_IF_NOT_DEBUG', 204)     # jump over assert statements\n\ndel def_op, name_op, jrel_op, jabs_op\n", 
    "optparse": "\"\"\"A powerful, extensible, and easy-to-use option parser.\n\nBy Greg Ward <gward@python.net>\n\nOriginally distributed as Optik.\n\nFor support, use the optik-users@lists.sourceforge.net mailing list\n(http://lists.sourceforge.net/lists/listinfo/optik-users).\n\nSimple usage example:\n\n   from optparse import OptionParser\n\n   parser = OptionParser()\n   parser.add_option(\"-f\", \"--file\", dest=\"filename\",\n                     help=\"write report to FILE\", metavar=\"FILE\")\n   parser.add_option(\"-q\", \"--quiet\",\n                     action=\"store_false\", dest=\"verbose\", default=True,\n                     help=\"don't print status messages to stdout\")\n\n   (options, args) = parser.parse_args()\n\"\"\"\n\n__version__ = \"1.5.3\"\n\n__all__ = ['Option',\n           'make_option',\n           'SUPPRESS_HELP',\n           'SUPPRESS_USAGE',\n           'Values',\n           'OptionContainer',\n           'OptionGroup',\n           'OptionParser',\n           'HelpFormatter',\n           'IndentedHelpFormatter',\n           'TitledHelpFormatter',\n           'OptParseError',\n           'OptionError',\n           'OptionConflictError',\n           'OptionValueError',\n           'BadOptionError']\n\n__copyright__ = \"\"\"\nCopyright (c) 2001-2006 Gregory P. Ward.  All rights reserved.\nCopyright (c) 2002-2006 Python Software Foundation.  All rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n  * Redistributions of source code must retain the above copyright\n    notice, this list of conditions and the following disclaimer.\n\n  * Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n\n  * Neither the name of the author nor the names of its\n    contributors may be used to endorse or promote products derived from\n    this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\nIS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\nTO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE AUTHOR OR\nCONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,\nEXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,\nPROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\nPROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\nLIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\nNEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\"\"\"\n\nimport sys, os\nimport types\nimport textwrap\n\ndef _repr(self):\n    return \"<%s at 0x%x: %s>\" % (self.__class__.__name__, id(self), self)\n\n\n# This file was generated from:\n#   Id: option_parser.py 527 2006-07-23 15:21:30Z greg\n#   Id: option.py 522 2006-06-11 16:22:03Z gward\n#   Id: help.py 527 2006-07-23 15:21:30Z greg\n#   Id: errors.py 509 2006-04-20 00:58:24Z gward\n\ntry:\n    from gettext import gettext\nexcept ImportError:\n    def gettext(message):\n        return message\n_ = gettext\n\n\nclass OptParseError (Exception):\n    def __init__(self, msg):\n        self.msg = msg\n\n    def __str__(self):\n        return self.msg\n\n\nclass OptionError (OptParseError):\n    \"\"\"\n    Raised if an Option instance is created with invalid or\n    inconsistent arguments.\n    \"\"\"\n\n    def __init__(self, msg, option):\n        self.msg = msg\n        self.option_id = str(option)\n\n    def __str__(self):\n        if self.option_id:\n            return \"option %s: %s\" % (self.option_id, self.msg)\n        else:\n            return self.msg\n\nclass OptionConflictError (OptionError):\n    \"\"\"\n    Raised if conflicting options are added to an OptionParser.\n    \"\"\"\n\nclass OptionValueError (OptParseError):\n    \"\"\"\n    Raised if an invalid option value is encountered on the command\n    line.\n    \"\"\"\n\nclass BadOptionError (OptParseError):\n    \"\"\"\n    Raised if an invalid option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str):\n        self.opt_str = opt_str\n\n    def __str__(self):\n        return _(\"no such option: %s\") % self.opt_str\n\nclass AmbiguousOptionError (BadOptionError):\n    \"\"\"\n    Raised if an ambiguous option is seen on the command line.\n    \"\"\"\n    def __init__(self, opt_str, possibilities):\n        BadOptionError.__init__(self, opt_str)\n        self.possibilities = possibilities\n\n    def __str__(self):\n        return (_(\"ambiguous option: %s (%s?)\")\n                % (self.opt_str, \", \".join(self.possibilities)))\n\n\nclass HelpFormatter:\n\n    \"\"\"\n    Abstract base class for formatting option help.  OptionParser\n    instances should use one of the HelpFormatter subclasses for\n    formatting help; by default IndentedHelpFormatter is used.\n\n    Instance attributes:\n      parser : OptionParser\n        the controlling OptionParser instance\n      indent_increment : int\n        the number of columns to indent per nesting level\n      max_help_position : int\n        the maximum starting column for option help text\n      help_position : int\n        the calculated starting column for option help text;\n        initially the same as the maximum\n      width : int\n        total number of columns for output (pass None to constructor for\n        this value to be taken from the $COLUMNS environment variable)\n      level : int\n        current indentation level\n      current_indent : int\n        current indentation level (in columns)\n      help_width : int\n        number of columns available for option help text (calculated)\n      default_tag : str\n        text to replace with each option's default value, \"%default\"\n        by default.  Set to false value to disable default value expansion.\n      option_strings : { Option : str }\n        maps Option instances to the snippet of help text explaining\n        the syntax of that option, e.g. \"-h, --help\" or\n        \"-fFILE, --file=FILE\"\n      _short_opt_fmt : str\n        format string controlling how short options with values are\n        printed in help text.  Must be either \"%s%s\" (\"-fFILE\") or\n        \"%s %s\" (\"-f FILE\"), because those are the two syntaxes that\n        Optik supports.\n      _long_opt_fmt : str\n        similar but for long options; must be either \"%s %s\" (\"--file FILE\")\n        or \"%s=%s\" (\"--file=FILE\").\n    \"\"\"\n\n    NO_DEFAULT_VALUE = \"none\"\n\n    def __init__(self,\n                 indent_increment,\n                 max_help_position,\n                 width,\n                 short_first):\n        self.parser = None\n        self.indent_increment = indent_increment\n        if width is None:\n            try:\n                width = int(os.environ['COLUMNS'])\n            except (KeyError, ValueError):\n                width = 80\n            width -= 2\n        self.width = width\n        self.help_position = self.max_help_position = \\\n                min(max_help_position, max(width - 20, indent_increment * 2))\n        self.current_indent = 0\n        self.level = 0\n        self.help_width = None          # computed later\n        self.short_first = short_first\n        self.default_tag = \"%default\"\n        self.option_strings = {}\n        self._short_opt_fmt = \"%s %s\"\n        self._long_opt_fmt = \"%s=%s\"\n\n    def set_parser(self, parser):\n        self.parser = parser\n\n    def set_short_opt_delimiter(self, delim):\n        if delim not in (\"\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for short options: %r\" % delim)\n        self._short_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def set_long_opt_delimiter(self, delim):\n        if delim not in (\"=\", \" \"):\n            raise ValueError(\n                \"invalid metavar delimiter for long options: %r\" % delim)\n        self._long_opt_fmt = \"%s\" + delim + \"%s\"\n\n    def indent(self):\n        self.current_indent += self.indent_increment\n        self.level += 1\n\n    def dedent(self):\n        self.current_indent -= self.indent_increment\n        assert self.current_indent >= 0, \"Indent decreased below 0.\"\n        self.level -= 1\n\n    def format_usage(self, usage):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def format_heading(self, heading):\n        raise NotImplementedError, \"subclasses must implement\"\n\n    def _format_text(self, text):\n        \"\"\"\n        Format a paragraph of free-form text for inclusion in the\n        help output at the current indentation level.\n        \"\"\"\n        text_width = max(self.width - self.current_indent, 11)\n        indent = \" \"*self.current_indent\n        return textwrap.fill(text,\n                             text_width,\n                             initial_indent=indent,\n                             subsequent_indent=indent)\n\n    def format_description(self, description):\n        if description:\n            return self._format_text(description) + \"\\n\"\n        else:\n            return \"\"\n\n    def format_epilog(self, epilog):\n        if epilog:\n            return \"\\n\" + self._format_text(epilog) + \"\\n\"\n        else:\n            return \"\"\n\n\n    def expand_default(self, option):\n        if self.parser is None or not self.default_tag:\n            return option.help\n\n        default_value = self.parser.defaults.get(option.dest)\n        if default_value is NO_DEFAULT or default_value is None:\n            default_value = self.NO_DEFAULT_VALUE\n\n        return option.help.replace(self.default_tag, str(default_value))\n\n    def format_option(self, option):\n        # The help for each option consists of two parts:\n        #   * the opt strings and metavars\n        #     eg. (\"-x\", or \"-fFILENAME, --file=FILENAME\")\n        #   * the user-supplied help string\n        #     eg. (\"turn on expert mode\", \"read data from FILENAME\")\n        #\n        # If possible, we write both of these on the same line:\n        #   -x      turn on expert mode\n        #\n        # But if the opt string list is too long, we put the help\n        # string on a second line, indented to the same column it would\n        # start in if it fit on the first line.\n        #   -fFILENAME, --file=FILENAME\n        #           read data from FILENAME\n        result = []\n        opts = self.option_strings[option]\n        opt_width = self.help_position - self.current_indent - 2\n        if len(opts) > opt_width:\n            opts = \"%*s%s\\n\" % (self.current_indent, \"\", opts)\n            indent_first = self.help_position\n        else:                       # start help on same line as opts\n            opts = \"%*s%-*s  \" % (self.current_indent, \"\", opt_width, opts)\n            indent_first = 0\n        result.append(opts)\n        if option.help:\n            help_text = self.expand_default(option)\n            help_lines = textwrap.wrap(help_text, self.help_width)\n            result.append(\"%*s%s\\n\" % (indent_first, \"\", help_lines[0]))\n            result.extend([\"%*s%s\\n\" % (self.help_position, \"\", line)\n                           for line in help_lines[1:]])\n        elif opts[-1] != \"\\n\":\n            result.append(\"\\n\")\n        return \"\".join(result)\n\n    def store_option_strings(self, parser):\n        self.indent()\n        max_len = 0\n        for opt in parser.option_list:\n            strings = self.format_option_strings(opt)\n            self.option_strings[opt] = strings\n            max_len = max(max_len, len(strings) + self.current_indent)\n        self.indent()\n        for group in parser.option_groups:\n            for opt in group.option_list:\n                strings = self.format_option_strings(opt)\n                self.option_strings[opt] = strings\n                max_len = max(max_len, len(strings) + self.current_indent)\n        self.dedent()\n        self.dedent()\n        self.help_position = min(max_len + 2, self.max_help_position)\n        self.help_width = max(self.width - self.help_position, 11)\n\n    def format_option_strings(self, option):\n        \"\"\"Return a comma-separated list of option strings & metavariables.\"\"\"\n        if option.takes_value():\n            metavar = option.metavar or option.dest.upper()\n            short_opts = [self._short_opt_fmt % (sopt, metavar)\n                          for sopt in option._short_opts]\n            long_opts = [self._long_opt_fmt % (lopt, metavar)\n                         for lopt in option._long_opts]\n        else:\n            short_opts = option._short_opts\n            long_opts = option._long_opts\n\n        if self.short_first:\n            opts = short_opts + long_opts\n        else:\n            opts = long_opts + short_opts\n\n        return \", \".join(opts)\n\nclass IndentedHelpFormatter (HelpFormatter):\n    \"\"\"Format help with indented section bodies.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=2,\n                 max_help_position=24,\n                 width=None,\n                 short_first=1):\n        HelpFormatter.__init__(\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return _(\"Usage: %s\\n\") % usage\n\n    def format_heading(self, heading):\n        return \"%*s%s:\\n\" % (self.current_indent, \"\", heading)\n\n\nclass TitledHelpFormatter (HelpFormatter):\n    \"\"\"Format help with underlined section headers.\n    \"\"\"\n\n    def __init__(self,\n                 indent_increment=0,\n                 max_help_position=24,\n                 width=None,\n                 short_first=0):\n        HelpFormatter.__init__ (\n            self, indent_increment, max_help_position, width, short_first)\n\n    def format_usage(self, usage):\n        return \"%s  %s\\n\" % (self.format_heading(_(\"Usage\")), usage)\n\n    def format_heading(self, heading):\n        return \"%s\\n%s\\n\" % (heading, \"=-\"[self.level] * len(heading))\n\n\ndef _parse_num(val, type):\n    if val[:2].lower() == \"0x\":         # hexadecimal\n        radix = 16\n    elif val[:2].lower() == \"0b\":       # binary\n        radix = 2\n        val = val[2:] or \"0\"            # have to remove \"0b\" prefix\n    elif val[:1] == \"0\":                # octal\n        radix = 8\n    else:                               # decimal\n        radix = 10\n\n    return type(val, radix)\n\ndef _parse_int(val):\n    return _parse_num(val, int)\n\ndef _parse_long(val):\n    return _parse_num(val, long)\n\n_builtin_cvt = { \"int\" : (_parse_int, _(\"integer\")),\n                 \"long\" : (_parse_long, _(\"long integer\")),\n                 \"float\" : (float, _(\"floating-point\")),\n                 \"complex\" : (complex, _(\"complex\")) }\n\ndef check_builtin(option, opt, value):\n    (cvt, what) = _builtin_cvt[option.type]\n    try:\n        return cvt(value)\n    except ValueError:\n        raise OptionValueError(\n            _(\"option %s: invalid %s value: %r\") % (opt, what, value))\n\ndef check_choice(option, opt, value):\n    if value in option.choices:\n        return value\n    else:\n        choices = \", \".join(map(repr, option.choices))\n        raise OptionValueError(\n            _(\"option %s: invalid choice: %r (choose from %s)\")\n            % (opt, value, choices))\n\n# Not supplying a default is different from a default of None,\n# so we need an explicit \"not supplied\" value.\nNO_DEFAULT = (\"NO\", \"DEFAULT\")\n\n\nclass Option:\n    \"\"\"\n    Instance attributes:\n      _short_opts : [string]\n      _long_opts : [string]\n\n      action : string\n      type : string\n      dest : string\n      default : any\n      nargs : int\n      const : any\n      choices : [string]\n      callback : function\n      callback_args : (any*)\n      callback_kwargs : { string : any }\n      help : string\n      metavar : string\n    \"\"\"\n\n    # The list of instance attributes that may be set through\n    # keyword args to the constructor.\n    ATTRS = ['action',\n             'type',\n             'dest',\n             'default',\n             'nargs',\n             'const',\n             'choices',\n             'callback',\n             'callback_args',\n             'callback_kwargs',\n             'help',\n             'metavar']\n\n    # The set of actions allowed by option parsers.  Explicitly listed\n    # here so the constructor can validate its arguments.\n    ACTIONS = (\"store\",\n               \"store_const\",\n               \"store_true\",\n               \"store_false\",\n               \"append\",\n               \"append_const\",\n               \"count\",\n               \"callback\",\n               \"help\",\n               \"version\")\n\n    # The set of actions that involve storing a value somewhere;\n    # also listed just for constructor argument validation.  (If\n    # the action is one of these, there must be a destination.)\n    STORE_ACTIONS = (\"store\",\n                     \"store_const\",\n                     \"store_true\",\n                     \"store_false\",\n                     \"append\",\n                     \"append_const\",\n                     \"count\")\n\n    # The set of actions for which it makes sense to supply a value\n    # type, ie. which may consume an argument from the command line.\n    TYPED_ACTIONS = (\"store\",\n                     \"append\",\n                     \"callback\")\n\n    # The set of actions which *require* a value type, ie. that\n    # always consume an argument from the command line.\n    ALWAYS_TYPED_ACTIONS = (\"store\",\n                            \"append\")\n\n    # The set of actions which take a 'const' attribute.\n    CONST_ACTIONS = (\"store_const\",\n                     \"append_const\")\n\n    # The set of known types for option parsers.  Again, listed here for\n    # constructor argument validation.\n    TYPES = (\"string\", \"int\", \"long\", \"float\", \"complex\", \"choice\")\n\n    # Dictionary of argument checking functions, which convert and\n    # validate option arguments according to the option type.\n    #\n    # Signature of checking functions is:\n    #   check(option : Option, opt : string, value : string) -> any\n    # where\n    #   option is the Option instance calling the checker\n    #   opt is the actual option seen on the command-line\n    #     (eg. \"-a\", \"--file\")\n    #   value is the option argument seen on the command-line\n    #\n    # The return value should be in the appropriate Python type\n    # for option.type -- eg. an integer if option.type == \"int\".\n    #\n    # If no checker is defined for a type, arguments will be\n    # unchecked and remain strings.\n    TYPE_CHECKER = { \"int\"    : check_builtin,\n                     \"long\"   : check_builtin,\n                     \"float\"  : check_builtin,\n                     \"complex\": check_builtin,\n                     \"choice\" : check_choice,\n                   }\n\n\n    # CHECK_METHODS is a list of unbound method objects; they are called\n    # by the constructor, in order, after all attributes are\n    # initialized.  The list is created and filled in later, after all\n    # the methods are actually defined.  (I just put it here because I\n    # like to define and document all class attributes in the same\n    # place.)  Subclasses that add another _check_*() method should\n    # define their own CHECK_METHODS list that adds their check method\n    # to those from this class.\n    CHECK_METHODS = None\n\n\n    # -- Constructor/initialization methods ----------------------------\n\n    def __init__(self, *opts, **attrs):\n        # Set _short_opts, _long_opts attrs from 'opts' tuple.\n        # Have to be set now, in case no option strings are supplied.\n        self._short_opts = []\n        self._long_opts = []\n        opts = self._check_opt_strings(opts)\n        self._set_opt_strings(opts)\n\n        # Set all other attrs (action, type, etc.) from 'attrs' dict\n        self._set_attrs(attrs)\n\n        # Check all the attributes we just set.  There are lots of\n        # complicated interdependencies, but luckily they can be farmed\n        # out to the _check_*() methods listed in CHECK_METHODS -- which\n        # could be handy for subclasses!  The one thing these all share\n        # is that they raise OptionError if they discover a problem.\n        for checker in self.CHECK_METHODS:\n            checker(self)\n\n    def _check_opt_strings(self, opts):\n        # Filter out None because early versions of Optik had exactly\n        # one short option and one long option, either of which\n        # could be None.\n        opts = filter(None, opts)\n        if not opts:\n            raise TypeError(\"at least one option string must be supplied\")\n        return opts\n\n    def _set_opt_strings(self, opts):\n        for opt in opts:\n            if len(opt) < 2:\n                raise OptionError(\n                    \"invalid option string %r: \"\n                    \"must be at least two characters long\" % opt, self)\n            elif len(opt) == 2:\n                if not (opt[0] == \"-\" and opt[1] != \"-\"):\n                    raise OptionError(\n                        \"invalid short option string %r: \"\n                        \"must be of the form -x, (x any non-dash char)\" % opt,\n                        self)\n                self._short_opts.append(opt)\n            else:\n                if not (opt[0:2] == \"--\" and opt[2] != \"-\"):\n                    raise OptionError(\n                        \"invalid long option string %r: \"\n                        \"must start with --, followed by non-dash\" % opt,\n                        self)\n                self._long_opts.append(opt)\n\n    def _set_attrs(self, attrs):\n        for attr in self.ATTRS:\n            if attr in attrs:\n                setattr(self, attr, attrs[attr])\n                del attrs[attr]\n            else:\n                if attr == 'default':\n                    setattr(self, attr, NO_DEFAULT)\n                else:\n                    setattr(self, attr, None)\n        if attrs:\n            attrs = attrs.keys()\n            attrs.sort()\n            raise OptionError(\n                \"invalid keyword arguments: %s\" % \", \".join(attrs),\n                self)\n\n\n    # -- Constructor validation methods --------------------------------\n\n    def _check_action(self):\n        if self.action is None:\n            self.action = \"store\"\n        elif self.action not in self.ACTIONS:\n            raise OptionError(\"invalid action: %r\" % self.action, self)\n\n    def _check_type(self):\n        if self.type is None:\n            if self.action in self.ALWAYS_TYPED_ACTIONS:\n                if self.choices is not None:\n                    # The \"choices\" attribute implies \"choice\" type.\n                    self.type = \"choice\"\n                else:\n                    # No type given?  \"string\" is the most sensible default.\n                    self.type = \"string\"\n        else:\n            # Allow type objects or builtin type conversion functions\n            # (int, str, etc.) as an alternative to their names.  (The\n            # complicated check of __builtin__ is only necessary for\n            # Python 2.1 and earlier, and is short-circuited by the\n            # first check on modern Pythons.)\n            import __builtin__\n            if ( type(self.type) is types.TypeType or\n                 (hasattr(self.type, \"__name__\") and\n                  getattr(__builtin__, self.type.__name__, None) is self.type) ):\n                self.type = self.type.__name__\n\n            if self.type == \"str\":\n                self.type = \"string\"\n\n            if self.type not in self.TYPES:\n                raise OptionError(\"invalid option type: %r\" % self.type, self)\n            if self.action not in self.TYPED_ACTIONS:\n                raise OptionError(\n                    \"must not supply a type for action %r\" % self.action, self)\n\n    def _check_choice(self):\n        if self.type == \"choice\":\n            if self.choices is None:\n                raise OptionError(\n                    \"must supply a list of choices for type 'choice'\", self)\n            elif type(self.choices) not in (types.TupleType, types.ListType):\n                raise OptionError(\n                    \"choices must be a list of strings ('%s' supplied)\"\n                    % str(type(self.choices)).split(\"'\")[1], self)\n        elif self.choices is not None:\n            raise OptionError(\n                \"must not supply choices for type %r\" % self.type, self)\n\n    def _check_dest(self):\n        # No destination given, and we need one for this action.  The\n        # self.type check is for callbacks that take a value.\n        takes_value = (self.action in self.STORE_ACTIONS or\n                       self.type is not None)\n        if self.dest is None and takes_value:\n\n            # Glean a destination from the first long option string,\n            # or from the first short option string if no long options.\n            if self._long_opts:\n                # eg. \"--foo-bar\" -> \"foo_bar\"\n                self.dest = self._long_opts[0][2:].replace('-', '_')\n            else:\n                self.dest = self._short_opts[0][1]\n\n    def _check_const(self):\n        if self.action not in self.CONST_ACTIONS and self.const is not None:\n            raise OptionError(\n                \"'const' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_nargs(self):\n        if self.action in self.TYPED_ACTIONS:\n            if self.nargs is None:\n                self.nargs = 1\n        elif self.nargs is not None:\n            raise OptionError(\n                \"'nargs' must not be supplied for action %r\" % self.action,\n                self)\n\n    def _check_callback(self):\n        if self.action == \"callback\":\n            if not hasattr(self.callback, '__call__'):\n                raise OptionError(\n                    \"callback not callable: %r\" % self.callback, self)\n            if (self.callback_args is not None and\n                type(self.callback_args) is not types.TupleType):\n                raise OptionError(\n                    \"callback_args, if supplied, must be a tuple: not %r\"\n                    % self.callback_args, self)\n            if (self.callback_kwargs is not None and\n                type(self.callback_kwargs) is not types.DictType):\n                raise OptionError(\n                    \"callback_kwargs, if supplied, must be a dict: not %r\"\n                    % self.callback_kwargs, self)\n        else:\n            if self.callback is not None:\n                raise OptionError(\n                    \"callback supplied (%r) for non-callback option\"\n                    % self.callback, self)\n            if self.callback_args is not None:\n                raise OptionError(\n                    \"callback_args supplied for non-callback option\", self)\n            if self.callback_kwargs is not None:\n                raise OptionError(\n                    \"callback_kwargs supplied for non-callback option\", self)\n\n\n    CHECK_METHODS = [_check_action,\n                     _check_type,\n                     _check_choice,\n                     _check_dest,\n                     _check_const,\n                     _check_nargs,\n                     _check_callback]\n\n\n    # -- Miscellaneous methods -----------------------------------------\n\n    def __str__(self):\n        return \"/\".join(self._short_opts + self._long_opts)\n\n    __repr__ = _repr\n\n    def takes_value(self):\n        return self.type is not None\n\n    def get_opt_string(self):\n        if self._long_opts:\n            return self._long_opts[0]\n        else:\n            return self._short_opts[0]\n\n\n    # -- Processing methods --------------------------------------------\n\n    def check_value(self, opt, value):\n        checker = self.TYPE_CHECKER.get(self.type)\n        if checker is None:\n            return value\n        else:\n            return checker(self, opt, value)\n\n    def convert_value(self, opt, value):\n        if value is not None:\n            if self.nargs == 1:\n                return self.check_value(opt, value)\n            else:\n                return tuple([self.check_value(opt, v) for v in value])\n\n    def process(self, opt, value, values, parser):\n\n        # First, convert the value(s) to the right type.  Howl if any\n        # value(s) are bogus.\n        value = self.convert_value(opt, value)\n\n        # And then take whatever action is expected of us.\n        # This is a separate method to make life easier for\n        # subclasses to add new actions.\n        return self.take_action(\n            self.action, self.dest, opt, value, values, parser)\n\n    def take_action(self, action, dest, opt, value, values, parser):\n        if action == \"store\":\n            setattr(values, dest, value)\n        elif action == \"store_const\":\n            setattr(values, dest, self.const)\n        elif action == \"store_true\":\n            setattr(values, dest, True)\n        elif action == \"store_false\":\n            setattr(values, dest, False)\n        elif action == \"append\":\n            values.ensure_value(dest, []).append(value)\n        elif action == \"append_const\":\n            values.ensure_value(dest, []).append(self.const)\n        elif action == \"count\":\n            setattr(values, dest, values.ensure_value(dest, 0) + 1)\n        elif action == \"callback\":\n            args = self.callback_args or ()\n            kwargs = self.callback_kwargs or {}\n            self.callback(self, opt, value, parser, *args, **kwargs)\n        elif action == \"help\":\n            parser.print_help()\n            parser.exit()\n        elif action == \"version\":\n            parser.print_version()\n            parser.exit()\n        else:\n            raise ValueError(\"unknown action %r\" % self.action)\n\n        return 1\n\n# class Option\n\n\nSUPPRESS_HELP = \"SUPPRESS\"+\"HELP\"\nSUPPRESS_USAGE = \"SUPPRESS\"+\"USAGE\"\n\ntry:\n    basestring\nexcept NameError:\n    def isbasestring(x):\n        return isinstance(x, (types.StringType, types.UnicodeType))\nelse:\n    def isbasestring(x):\n        return isinstance(x, basestring)\n\nclass Values:\n\n    def __init__(self, defaults=None):\n        if defaults:\n            for (attr, val) in defaults.items():\n                setattr(self, attr, val)\n\n    def __str__(self):\n        return str(self.__dict__)\n\n    __repr__ = _repr\n\n    def __cmp__(self, other):\n        if isinstance(other, Values):\n            return cmp(self.__dict__, other.__dict__)\n        elif isinstance(other, types.DictType):\n            return cmp(self.__dict__, other)\n        else:\n            return -1\n\n    def _update_careful(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary, but only\n        use keys from dict that already have a corresponding attribute\n        in self.  Any keys in dict without a corresponding attribute\n        are silently ignored.\n        \"\"\"\n        for attr in dir(self):\n            if attr in dict:\n                dval = dict[attr]\n                if dval is not None:\n                    setattr(self, attr, dval)\n\n    def _update_loose(self, dict):\n        \"\"\"\n        Update the option values from an arbitrary dictionary,\n        using all keys from the dictionary regardless of whether\n        they have a corresponding attribute in self or not.\n        \"\"\"\n        self.__dict__.update(dict)\n\n    def _update(self, dict, mode):\n        if mode == \"careful\":\n            self._update_careful(dict)\n        elif mode == \"loose\":\n            self._update_loose(dict)\n        else:\n            raise ValueError, \"invalid update mode: %r\" % mode\n\n    def read_module(self, modname, mode=\"careful\"):\n        __import__(modname)\n        mod = sys.modules[modname]\n        self._update(vars(mod), mode)\n\n    def read_file(self, filename, mode=\"careful\"):\n        vars = {}\n        execfile(filename, vars)\n        self._update(vars, mode)\n\n    def ensure_value(self, attr, value):\n        if not hasattr(self, attr) or getattr(self, attr) is None:\n            setattr(self, attr, value)\n        return getattr(self, attr)\n\n\nclass OptionContainer:\n\n    \"\"\"\n    Abstract base class.\n\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      option_list : [Option]\n        the list of Option objects contained by this OptionContainer\n      _short_opt : { string : Option }\n        dictionary mapping short option strings, eg. \"-f\" or \"-X\",\n        to the Option instances that implement them.  If an Option\n        has multiple short option strings, it will appears in this\n        dictionary multiple times. [1]\n      _long_opt : { string : Option }\n        dictionary mapping long option strings, eg. \"--file\" or\n        \"--exclude\", to the Option instances that implement them.\n        Again, a given Option can occur multiple times in this\n        dictionary. [1]\n      defaults : { string : any }\n        dictionary mapping option destination names to default\n        values for each destination [1]\n\n    [1] These mappings are common to (shared by) all components of the\n        controlling OptionParser, where they are initially created.\n\n    \"\"\"\n\n    def __init__(self, option_class, conflict_handler, description):\n        # Initialize the option list and related data structures.\n        # This method must be provided by subclasses, and it must\n        # initialize at least the following instance attributes:\n        # option_list, _short_opt, _long_opt, defaults.\n        self._create_option_list()\n\n        self.option_class = option_class\n        self.set_conflict_handler(conflict_handler)\n        self.set_description(description)\n\n    def _create_option_mappings(self):\n        # For use by OptionParser constructor -- create the master\n        # option mappings used by this OptionParser and all\n        # OptionGroups that it owns.\n        self._short_opt = {}            # single letter -> Option instance\n        self._long_opt = {}             # long option -> Option instance\n        self.defaults = {}              # maps option dest -> default value\n\n\n    def _share_option_mappings(self, parser):\n        # For use by OptionGroup constructor -- use shared option\n        # mappings from the OptionParser that owns this OptionGroup.\n        self._short_opt = parser._short_opt\n        self._long_opt = parser._long_opt\n        self.defaults = parser.defaults\n\n    def set_conflict_handler(self, handler):\n        if handler not in (\"error\", \"resolve\"):\n            raise ValueError, \"invalid conflict_resolution value %r\" % handler\n        self.conflict_handler = handler\n\n    def set_description(self, description):\n        self.description = description\n\n    def get_description(self):\n        return self.description\n\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        del self._short_opt\n        del self._long_opt\n        del self.defaults\n\n\n    # -- Option-adding methods -----------------------------------------\n\n    def _check_conflict(self, option):\n        conflict_opts = []\n        for opt in option._short_opts:\n            if opt in self._short_opt:\n                conflict_opts.append((opt, self._short_opt[opt]))\n        for opt in option._long_opts:\n            if opt in self._long_opt:\n                conflict_opts.append((opt, self._long_opt[opt]))\n\n        if conflict_opts:\n            handler = self.conflict_handler\n            if handler == \"error\":\n                raise OptionConflictError(\n                    \"conflicting option string(s): %s\"\n                    % \", \".join([co[0] for co in conflict_opts]),\n                    option)\n            elif handler == \"resolve\":\n                for (opt, c_option) in conflict_opts:\n                    if opt.startswith(\"--\"):\n                        c_option._long_opts.remove(opt)\n                        del self._long_opt[opt]\n                    else:\n                        c_option._short_opts.remove(opt)\n                        del self._short_opt[opt]\n                    if not (c_option._short_opts or c_option._long_opts):\n                        c_option.container.option_list.remove(c_option)\n\n    def add_option(self, *args, **kwargs):\n        \"\"\"add_option(Option)\n           add_option(opt_str, ..., kwarg=val, ...)\n        \"\"\"\n        if type(args[0]) in types.StringTypes:\n            option = self.option_class(*args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            option = args[0]\n            if not isinstance(option, Option):\n                raise TypeError, \"not an Option instance: %r\" % option\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self._check_conflict(option)\n\n        self.option_list.append(option)\n        option.container = self\n        for opt in option._short_opts:\n            self._short_opt[opt] = option\n        for opt in option._long_opts:\n            self._long_opt[opt] = option\n\n        if option.dest is not None:     # option has a dest, we need a default\n            if option.default is not NO_DEFAULT:\n                self.defaults[option.dest] = option.default\n            elif option.dest not in self.defaults:\n                self.defaults[option.dest] = None\n\n        return option\n\n    def add_options(self, option_list):\n        for option in option_list:\n            self.add_option(option)\n\n    # -- Option query/removal methods ----------------------------------\n\n    def get_option(self, opt_str):\n        return (self._short_opt.get(opt_str) or\n                self._long_opt.get(opt_str))\n\n    def has_option(self, opt_str):\n        return (opt_str in self._short_opt or\n                opt_str in self._long_opt)\n\n    def remove_option(self, opt_str):\n        option = self._short_opt.get(opt_str)\n        if option is None:\n            option = self._long_opt.get(opt_str)\n        if option is None:\n            raise ValueError(\"no such option %r\" % opt_str)\n\n        for opt in option._short_opts:\n            del self._short_opt[opt]\n        for opt in option._long_opts:\n            del self._long_opt[opt]\n        option.container.option_list.remove(option)\n\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_option_help(self, formatter):\n        if not self.option_list:\n            return \"\"\n        result = []\n        for option in self.option_list:\n            if not option.help is SUPPRESS_HELP:\n                result.append(formatter.format_option(option))\n        return \"\".join(result)\n\n    def format_description(self, formatter):\n        return formatter.format_description(self.get_description())\n\n    def format_help(self, formatter):\n        result = []\n        if self.description:\n            result.append(self.format_description(formatter))\n        if self.option_list:\n            result.append(self.format_option_help(formatter))\n        return \"\\n\".join(result)\n\n\nclass OptionGroup (OptionContainer):\n\n    def __init__(self, parser, title, description=None):\n        self.parser = parser\n        OptionContainer.__init__(\n            self, parser.option_class, parser.conflict_handler, description)\n        self.title = title\n\n    def _create_option_list(self):\n        self.option_list = []\n        self._share_option_mappings(self.parser)\n\n    def set_title(self, title):\n        self.title = title\n\n    def destroy(self):\n        \"\"\"see OptionParser.destroy().\"\"\"\n        OptionContainer.destroy(self)\n        del self.option_list\n\n    # -- Help-formatting methods ---------------------------------------\n\n    def format_help(self, formatter):\n        result = formatter.format_heading(self.title)\n        formatter.indent()\n        result += OptionContainer.format_help(self, formatter)\n        formatter.dedent()\n        return result\n\n\nclass OptionParser (OptionContainer):\n\n    \"\"\"\n    Class attributes:\n      standard_option_list : [Option]\n        list of standard options that will be accepted by all instances\n        of this parser class (intended to be overridden by subclasses).\n\n    Instance attributes:\n      usage : string\n        a usage string for your program.  Before it is displayed\n        to the user, \"%prog\" will be expanded to the name of\n        your program (self.prog or os.path.basename(sys.argv[0])).\n      prog : string\n        the name of the current program (to override\n        os.path.basename(sys.argv[0])).\n      description : string\n        A paragraph of text giving a brief overview of your program.\n        optparse reformats this paragraph to fit the current terminal\n        width and prints it when the user requests help (after usage,\n        but before the list of options).\n      epilog : string\n        paragraph of help text to print after option help\n\n      option_groups : [OptionGroup]\n        list of option groups in this parser (option groups are\n        irrelevant for parsing the command-line, but very useful\n        for generating help)\n\n      allow_interspersed_args : bool = true\n        if true, positional arguments may be interspersed with options.\n        Assuming -a and -b each take a single argument, the command-line\n          -ablah foo bar -bboo baz\n        will be interpreted the same as\n          -ablah -bboo -- foo bar baz\n        If this flag were false, that command line would be interpreted as\n          -ablah -- foo bar -bboo baz\n        -- ie. we stop processing options as soon as we see the first\n        non-option argument.  (This is the tradition followed by\n        Python's getopt module, Perl's Getopt::Std, and other argument-\n        parsing libraries, but it is generally annoying to users.)\n\n      process_default_values : bool = true\n        if true, option default values are processed similarly to option\n        values from the command line: that is, they are passed to the\n        type-checking function for the option's type (as long as the\n        default value is a string).  (This really only matters if you\n        have defined custom types; see SF bug #955889.)  Set it to false\n        to restore the behaviour of Optik 1.4.1 and earlier.\n\n      rargs : [string]\n        the argument list currently being parsed.  Only set when\n        parse_args() is active, and continually trimmed down as\n        we consume arguments.  Mainly there for the benefit of\n        callback options.\n      largs : [string]\n        the list of leftover arguments that we have skipped while\n        parsing options.  If allow_interspersed_args is false, this\n        list is always empty.\n      values : Values\n        the set of option values currently being accumulated.  Only\n        set when parse_args() is active.  Also mainly for callbacks.\n\n    Because of the 'rargs', 'largs', and 'values' attributes,\n    OptionParser is not thread-safe.  If, for some perverse reason, you\n    need to parse command-line arguments simultaneously in different\n    threads, use different OptionParser instances.\n\n    \"\"\"\n\n    standard_option_list = []\n\n    def __init__(self,\n                 usage=None,\n                 option_list=None,\n                 option_class=Option,\n                 version=None,\n                 conflict_handler=\"error\",\n                 description=None,\n                 formatter=None,\n                 add_help_option=True,\n                 prog=None,\n                 epilog=None):\n        OptionContainer.__init__(\n            self, option_class, conflict_handler, description)\n        self.set_usage(usage)\n        self.prog = prog\n        self.version = version\n        self.allow_interspersed_args = True\n        self.process_default_values = True\n        if formatter is None:\n            formatter = IndentedHelpFormatter()\n        self.formatter = formatter\n        self.formatter.set_parser(self)\n        self.epilog = epilog\n\n        # Populate the option list; initial sources are the\n        # standard_option_list class attribute, the 'option_list'\n        # argument, and (if applicable) the _add_version_option() and\n        # _add_help_option() methods.\n        self._populate_option_list(option_list,\n                                   add_help=add_help_option)\n\n        self._init_parsing_state()\n\n\n    def destroy(self):\n        \"\"\"\n        Declare that you are done with this OptionParser.  This cleans up\n        reference cycles so the OptionParser (and all objects referenced by\n        it) can be garbage-collected promptly.  After calling destroy(), the\n        OptionParser is unusable.\n        \"\"\"\n        OptionContainer.destroy(self)\n        for group in self.option_groups:\n            group.destroy()\n        del self.option_list\n        del self.option_groups\n        del self.formatter\n\n\n    # -- Private methods -----------------------------------------------\n    # (used by our or OptionContainer's constructor)\n\n    def _create_option_list(self):\n        self.option_list = []\n        self.option_groups = []\n        self._create_option_mappings()\n\n    def _add_help_option(self):\n        self.add_option(\"-h\", \"--help\",\n                        action=\"help\",\n                        help=_(\"show this help message and exit\"))\n\n    def _add_version_option(self):\n        self.add_option(\"--version\",\n                        action=\"version\",\n                        help=_(\"show program's version number and exit\"))\n\n    def _populate_option_list(self, option_list, add_help=True):\n        if self.standard_option_list:\n            self.add_options(self.standard_option_list)\n        if option_list:\n            self.add_options(option_list)\n        if self.version:\n            self._add_version_option()\n        if add_help:\n            self._add_help_option()\n\n    def _init_parsing_state(self):\n        # These are set in parse_args() for the convenience of callbacks.\n        self.rargs = None\n        self.largs = None\n        self.values = None\n\n\n    # -- Simple modifier methods ---------------------------------------\n\n    def set_usage(self, usage):\n        if usage is None:\n            self.usage = _(\"%prog [options]\")\n        elif usage is SUPPRESS_USAGE:\n            self.usage = None\n        # For backwards compatibility with Optik 1.3 and earlier.\n        elif usage.lower().startswith(\"usage: \"):\n            self.usage = usage[7:]\n        else:\n            self.usage = usage\n\n    def enable_interspersed_args(self):\n        \"\"\"Set parsing to not stop on the first non-option, allowing\n        interspersing switches with command arguments. This is the\n        default behavior. See also disable_interspersed_args() and the\n        class documentation description of the attribute\n        allow_interspersed_args.\"\"\"\n        self.allow_interspersed_args = True\n\n    def disable_interspersed_args(self):\n        \"\"\"Set parsing to stop on the first non-option. Use this if\n        you have a command processor which runs another command that\n        has options of its own and you want to make sure these options\n        don't get confused.\n        \"\"\"\n        self.allow_interspersed_args = False\n\n    def set_process_default_values(self, process):\n        self.process_default_values = process\n\n    def set_default(self, dest, value):\n        self.defaults[dest] = value\n\n    def set_defaults(self, **kwargs):\n        self.defaults.update(kwargs)\n\n    def _get_all_options(self):\n        options = self.option_list[:]\n        for group in self.option_groups:\n            options.extend(group.option_list)\n        return options\n\n    def get_default_values(self):\n        if not self.process_default_values:\n            # Old, pre-Optik 1.5 behaviour.\n            return Values(self.defaults)\n\n        defaults = self.defaults.copy()\n        for option in self._get_all_options():\n            default = defaults.get(option.dest)\n            if isbasestring(default):\n                opt_str = option.get_opt_string()\n                defaults[option.dest] = option.check_value(opt_str, default)\n\n        return Values(defaults)\n\n\n    # -- OptionGroup methods -------------------------------------------\n\n    def add_option_group(self, *args, **kwargs):\n        # XXX lots of overlap with OptionContainer.add_option()\n        if type(args[0]) is types.StringType:\n            group = OptionGroup(self, *args, **kwargs)\n        elif len(args) == 1 and not kwargs:\n            group = args[0]\n            if not isinstance(group, OptionGroup):\n                raise TypeError, \"not an OptionGroup instance: %r\" % group\n            if group.parser is not self:\n                raise ValueError, \"invalid OptionGroup (wrong parser)\"\n        else:\n            raise TypeError, \"invalid arguments\"\n\n        self.option_groups.append(group)\n        return group\n\n    def get_option_group(self, opt_str):\n        option = (self._short_opt.get(opt_str) or\n                  self._long_opt.get(opt_str))\n        if option and option.container is not self:\n            return option.container\n        return None\n\n\n    # -- Option-parsing methods ----------------------------------------\n\n    def _get_args(self, args):\n        if args is None:\n            return sys.argv[1:]\n        else:\n            return args[:]              # don't modify caller's list\n\n    def parse_args(self, args=None, values=None):\n        \"\"\"\n        parse_args(args : [string] = sys.argv[1:],\n                   values : Values = None)\n        -> (values : Values, args : [string])\n\n        Parse the command-line options found in 'args' (default:\n        sys.argv[1:]).  Any errors result in a call to 'error()', which\n        by default prints the usage message to stderr and calls\n        sys.exit() with an error message.  On success returns a pair\n        (values, args) where 'values' is an Values instance (with all\n        your option values) and 'args' is the list of arguments left\n        over after parsing options.\n        \"\"\"\n        rargs = self._get_args(args)\n        if values is None:\n            values = self.get_default_values()\n\n        # Store the halves of the argument list as attributes for the\n        # convenience of callbacks:\n        #   rargs\n        #     the rest of the command-line (the \"r\" stands for\n        #     \"remaining\" or \"right-hand\")\n        #   largs\n        #     the leftover arguments -- ie. what's left after removing\n        #     options and their arguments (the \"l\" stands for \"leftover\"\n        #     or \"left-hand\")\n        self.rargs = rargs\n        self.largs = largs = []\n        self.values = values\n\n        try:\n            stop = self._process_args(largs, rargs, values)\n        except (BadOptionError, OptionValueError), err:\n            self.error(str(err))\n\n        args = largs + rargs\n        return self.check_values(values, args)\n\n    def check_values(self, values, args):\n        \"\"\"\n        check_values(values : Values, args : [string])\n        -> (values : Values, args : [string])\n\n        Check that the supplied option values and leftover arguments are\n        valid.  Returns the option values and leftover arguments\n        (possibly adjusted, possibly completely new -- whatever you\n        like).  Default implementation just returns the passed-in\n        values; subclasses may override as desired.\n        \"\"\"\n        return (values, args)\n\n    def _process_args(self, largs, rargs, values):\n        \"\"\"_process_args(largs : [string],\n                         rargs : [string],\n                         values : Values)\n\n        Process command-line arguments and populate 'values', consuming\n        options and arguments from 'rargs'.  If 'allow_interspersed_args' is\n        false, stop at the first non-option argument.  If true, accumulate any\n        interspersed non-option arguments in 'largs'.\n        \"\"\"\n        while rargs:\n            arg = rargs[0]\n            # We handle bare \"--\" explicitly, and bare \"-\" is handled by the\n            # standard arg handler since the short arg case ensures that the\n            # len of the opt string is greater than 1.\n            if arg == \"--\":\n                del rargs[0]\n                return\n            elif arg[0:2] == \"--\":\n                # process a single long option (possibly with value(s))\n                self._process_long_opt(rargs, values)\n            elif arg[:1] == \"-\" and len(arg) > 1:\n                # process a cluster of short options (possibly with\n                # value(s) for the last one only)\n                self._process_short_opts(rargs, values)\n            elif self.allow_interspersed_args:\n                largs.append(arg)\n                del rargs[0]\n            else:\n                return                  # stop now, leave this arg in rargs\n\n        # Say this is the original argument list:\n        # [arg0, arg1, ..., arg(i-1), arg(i), arg(i+1), ..., arg(N-1)]\n        #                            ^\n        # (we are about to process arg(i)).\n        #\n        # Then rargs is [arg(i), ..., arg(N-1)] and largs is a *subset* of\n        # [arg0, ..., arg(i-1)] (any options and their arguments will have\n        # been removed from largs).\n        #\n        # The while loop will usually consume 1 or more arguments per pass.\n        # If it consumes 1 (eg. arg is an option that takes no arguments),\n        # then after _process_arg() is done the situation is:\n        #\n        #   largs = subset of [arg0, ..., arg(i)]\n        #   rargs = [arg(i+1), ..., arg(N-1)]\n        #\n        # If allow_interspersed_args is false, largs will always be\n        # *empty* -- still a subset of [arg0, ..., arg(i-1)], but\n        # not a very interesting subset!\n\n    def _match_long_opt(self, opt):\n        \"\"\"_match_long_opt(opt : string) -> string\n\n        Determine which long option string 'opt' matches, ie. which one\n        it is an unambiguous abbreviation for.  Raises BadOptionError if\n        'opt' doesn't unambiguously match any long option string.\n        \"\"\"\n        return _match_abbrev(opt, self._long_opt)\n\n    def _process_long_opt(self, rargs, values):\n        arg = rargs.pop(0)\n\n        # Value explicitly attached to arg?  Pretend it's the next\n        # argument.\n        if \"=\" in arg:\n            (opt, next_arg) = arg.split(\"=\", 1)\n            rargs.insert(0, next_arg)\n            had_explicit_value = True\n        else:\n            opt = arg\n            had_explicit_value = False\n\n        opt = self._match_long_opt(opt)\n        option = self._long_opt[opt]\n        if option.takes_value():\n            nargs = option.nargs\n            if len(rargs) < nargs:\n                if nargs == 1:\n                    self.error(_(\"%s option requires an argument\") % opt)\n                else:\n                    self.error(_(\"%s option requires %d arguments\")\n                               % (opt, nargs))\n            elif nargs == 1:\n                value = rargs.pop(0)\n            else:\n                value = tuple(rargs[0:nargs])\n                del rargs[0:nargs]\n\n        elif had_explicit_value:\n            self.error(_(\"%s option does not take a value\") % opt)\n\n        else:\n            value = None\n\n        option.process(opt, value, values, self)\n\n    def _process_short_opts(self, rargs, values):\n        arg = rargs.pop(0)\n        stop = False\n        i = 1\n        for ch in arg[1:]:\n            opt = \"-\" + ch\n            option = self._short_opt.get(opt)\n            i += 1                      # we have consumed a character\n\n            if not option:\n                raise BadOptionError(opt)\n            if option.takes_value():\n                # Any characters left in arg?  Pretend they're the\n                # next arg, and stop consuming characters of arg.\n                if i < len(arg):\n                    rargs.insert(0, arg[i:])\n                    stop = True\n\n                nargs = option.nargs\n                if len(rargs) < nargs:\n                    if nargs == 1:\n                        self.error(_(\"%s option requires an argument\") % opt)\n                    else:\n                        self.error(_(\"%s option requires %d arguments\")\n                                   % (opt, nargs))\n                elif nargs == 1:\n                    value = rargs.pop(0)\n                else:\n                    value = tuple(rargs[0:nargs])\n                    del rargs[0:nargs]\n\n            else:                       # option doesn't take a value\n                value = None\n\n            option.process(opt, value, values, self)\n\n            if stop:\n                break\n\n\n    # -- Feedback methods ----------------------------------------------\n\n    def get_prog_name(self):\n        if self.prog is None:\n            return os.path.basename(sys.argv[0])\n        else:\n            return self.prog\n\n    def expand_prog_name(self, s):\n        return s.replace(\"%prog\", self.get_prog_name())\n\n    def get_description(self):\n        return self.expand_prog_name(self.description)\n\n    def exit(self, status=0, msg=None):\n        if msg:\n            sys.stderr.write(msg)\n        sys.exit(status)\n\n    def error(self, msg):\n        \"\"\"error(msg : string)\n\n        Print a usage message incorporating 'msg' to stderr and exit.\n        If you override this in a subclass, it should not return -- it\n        should either exit or raise an exception.\n        \"\"\"\n        self.print_usage(sys.stderr)\n        self.exit(2, \"%s: error: %s\\n\" % (self.get_prog_name(), msg))\n\n    def get_usage(self):\n        if self.usage:\n            return self.formatter.format_usage(\n                self.expand_prog_name(self.usage))\n        else:\n            return \"\"\n\n    def print_usage(self, file=None):\n        \"\"\"print_usage(file : file = stdout)\n\n        Print the usage message for the current program (self.usage) to\n        'file' (default stdout).  Any occurrence of the string \"%prog\" in\n        self.usage is replaced with the name of the current program\n        (basename of sys.argv[0]).  Does nothing if self.usage is empty\n        or not defined.\n        \"\"\"\n        if self.usage:\n            print >>file, self.get_usage()\n\n    def get_version(self):\n        if self.version:\n            return self.expand_prog_name(self.version)\n        else:\n            return \"\"\n\n    def print_version(self, file=None):\n        \"\"\"print_version(file : file = stdout)\n\n        Print the version message for this program (self.version) to\n        'file' (default stdout).  As with print_usage(), any occurrence\n        of \"%prog\" in self.version is replaced by the current program's\n        name.  Does nothing if self.version is empty or undefined.\n        \"\"\"\n        if self.version:\n            print >>file, self.get_version()\n\n    def format_option_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        formatter.store_option_strings(self)\n        result = []\n        result.append(formatter.format_heading(_(\"Options\")))\n        formatter.indent()\n        if self.option_list:\n            result.append(OptionContainer.format_option_help(self, formatter))\n            result.append(\"\\n\")\n        for group in self.option_groups:\n            result.append(group.format_help(formatter))\n            result.append(\"\\n\")\n        formatter.dedent()\n        # Drop the last \"\\n\", or the header if no options or option groups:\n        return \"\".join(result[:-1])\n\n    def format_epilog(self, formatter):\n        return formatter.format_epilog(self.epilog)\n\n    def format_help(self, formatter=None):\n        if formatter is None:\n            formatter = self.formatter\n        result = []\n        if self.usage:\n            result.append(self.get_usage() + \"\\n\")\n        if self.description:\n            result.append(self.format_description(formatter) + \"\\n\")\n        result.append(self.format_option_help(formatter))\n        result.append(self.format_epilog(formatter))\n        return \"\".join(result)\n\n    # used by test suite\n    def _get_encoding(self, file):\n        encoding = getattr(file, \"encoding\", None)\n        if not encoding:\n            encoding = sys.getdefaultencoding()\n        return encoding\n\n    def print_help(self, file=None):\n        \"\"\"print_help(file : file = stdout)\n\n        Print an extended help message, listing all options and any\n        help text provided with them, to 'file' (default stdout).\n        \"\"\"\n        if file is None:\n            file = sys.stdout\n        encoding = self._get_encoding(file)\n        file.write(self.format_help().encode(encoding, \"replace\"))\n\n# class OptionParser\n\n\ndef _match_abbrev(s, wordmap):\n    \"\"\"_match_abbrev(s : string, wordmap : {string : Option}) -> string\n\n    Return the string key in 'wordmap' for which 's' is an unambiguous\n    abbreviation.  If 's' is found to be ambiguous or doesn't match any of\n    'words', raise BadOptionError.\n    \"\"\"\n    # Is there an exact match?\n    if s in wordmap:\n        return s\n    else:\n        # Isolate all words with s as a prefix.\n        possibilities = [word for word in wordmap.keys()\n                         if word.startswith(s)]\n        # No exact match, so there had better be just one possibility.\n        if len(possibilities) == 1:\n            return possibilities[0]\n        elif not possibilities:\n            raise BadOptionError(s)\n        else:\n            # More than one possible completion: ambiguous prefix.\n            possibilities.sort()\n            raise AmbiguousOptionError(s, possibilities)\n\n\n# Some day, there might be many Option classes.  As of Optik 1.3, the\n# preferred way to instantiate Options is indirectly, via make_option(),\n# which will become a factory function when there are many Option\n# classes.\nmake_option = Option\n", 
    "os": "r\"\"\"OS routines for NT or Posix depending on what system we're on.\n\nThis exports:\n  - all functions from posix, nt, os2, or ce, e.g. unlink, stat, etc.\n  - os.path is one of the modules posixpath, or ntpath\n  - os.name is 'posix', 'nt', 'os2', 'ce' or 'riscos'\n  - os.curdir is a string representing the current directory ('.' or ':')\n  - os.pardir is a string representing the parent directory ('..' or '::')\n  - os.sep is the (or a most common) pathname separator ('/' or ':' or '\\\\')\n  - os.extsep is the extension separator ('.' or '/')\n  - os.altsep is the alternate pathname separator (None or '/')\n  - os.pathsep is the component separator used in $PATH etc\n  - os.linesep is the line separator in text files ('\\r' or '\\n' or '\\r\\n')\n  - os.defpath is the default search path for executables\n  - os.devnull is the file path of the null device ('/dev/null', etc.)\n\nPrograms that import and use 'os' stand a better chance of being\nportable between different platforms.  Of course, they must then\nonly use functions that are defined by all platforms (e.g., unlink\nand opendir), and leave all pathname manipulation to os.path\n(e.g., split and join).\n\"\"\"\n\n#'\n\nimport sys, errno\n\n_names = sys.builtin_module_names\n\n# Note:  more names are added to __all__ later.\n__all__ = [\"altsep\", \"curdir\", \"pardir\", \"sep\", \"extsep\", \"pathsep\", \"linesep\",\n           \"defpath\", \"name\", \"path\", \"devnull\",\n           \"SEEK_SET\", \"SEEK_CUR\", \"SEEK_END\"]\n\ndef _get_exports_list(module):\n    try:\n        return list(module.__all__)\n    except AttributeError:\n        return [n for n in dir(module) if n[0] != '_']\n\nif 'posix' in _names:\n    name = 'posix'\n    linesep = '\\n'\n    from posix import *\n    try:\n        from posix import _exit\n    except ImportError:\n        pass\n    import posixpath as path\n\n    import posix\n    __all__.extend(_get_exports_list(posix))\n    del posix\n\nelif 'nt' in _names:\n    name = 'nt'\n    linesep = '\\r\\n'\n    from nt import *\n    try:\n        from nt import _exit\n    except ImportError:\n        pass\n    import ntpath as path\n\n    import nt\n    __all__.extend(_get_exports_list(nt))\n    del nt\n\nelif 'os2' in _names:\n    name = 'os2'\n    linesep = '\\r\\n'\n    from os2 import *\n    try:\n        from os2 import _exit\n    except ImportError:\n        pass\n    if sys.version.find('EMX GCC') == -1:\n        import ntpath as path\n    else:\n        import os2emxpath as path\n        from _emx_link import link\n\n    import os2\n    __all__.extend(_get_exports_list(os2))\n    del os2\n\nelif 'ce' in _names:\n    name = 'ce'\n    linesep = '\\r\\n'\n    from ce import *\n    try:\n        from ce import _exit\n    except ImportError:\n        pass\n    # We can use the standard Windows path.\n    import ntpath as path\n\n    import ce\n    __all__.extend(_get_exports_list(ce))\n    del ce\n\nelif 'riscos' in _names:\n    name = 'riscos'\n    linesep = '\\n'\n    from riscos import *\n    try:\n        from riscos import _exit\n    except ImportError:\n        pass\n    import riscospath as path\n\n    import riscos\n    __all__.extend(_get_exports_list(riscos))\n    del riscos\n\nelse:\n    raise ImportError, 'no os specific module found'\n\nsys.modules['os.path'] = path\nfrom os.path import (curdir, pardir, sep, pathsep, defpath, extsep, altsep,\n    devnull)\n\ndel _names\n\n# Python uses fixed values for the SEEK_ constants; they are mapped\n# to native constants if necessary in posixmodule.c\nSEEK_SET = 0\nSEEK_CUR = 1\nSEEK_END = 2\n\n#'\n\n# Super directory utilities.\n# (Inspired by Eric Raymond; the doc strings are mostly his)\n\ndef makedirs(name, mode=0777):\n    \"\"\"makedirs(path [, mode=0777])\n\n    Super-mkdir; create a leaf directory and all intermediate ones.\n    Works like mkdir, except that any intermediate path segment (not\n    just the rightmost) will be created if it does not exist.  This is\n    recursive.\n\n    \"\"\"\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    if head and tail and not path.exists(head):\n        try:\n            makedirs(head, mode)\n        except OSError, e:\n            # be happy if someone already created the path\n            if e.errno != errno.EEXIST:\n                raise\n        if tail == curdir:           # xxx/newdir/. exists if xxx/newdir exists\n            return\n    mkdir(name, mode)\n\ndef removedirs(name):\n    \"\"\"removedirs(path)\n\n    Super-rmdir; remove a leaf directory and all empty intermediate\n    ones.  Works like rmdir except that, if the leaf directory is\n    successfully removed, directories corresponding to rightmost path\n    segments will be pruned away until either the whole path is\n    consumed or an error occurs.  Errors during this latter phase are\n    ignored -- they generally mean that a directory was not empty.\n\n    \"\"\"\n    rmdir(name)\n    head, tail = path.split(name)\n    if not tail:\n        head, tail = path.split(head)\n    while head and tail:\n        try:\n            rmdir(head)\n        except error:\n            break\n        head, tail = path.split(head)\n\ndef renames(old, new):\n    \"\"\"renames(old, new)\n\n    Super-rename; create directories as necessary and delete any left\n    empty.  Works like rename, except creation of any intermediate\n    directories needed to make the new pathname good is attempted\n    first.  After the rename, directories corresponding to rightmost\n    path segments of the old name will be pruned way until either the\n    whole path is consumed or a nonempty directory is found.\n\n    Note: this function can fail with the new directory structure made\n    if you lack permissions needed to unlink the leaf directory or\n    file.\n\n    \"\"\"\n    head, tail = path.split(new)\n    if head and tail and not path.exists(head):\n        makedirs(head)\n    rename(old, new)\n    head, tail = path.split(old)\n    if head and tail:\n        try:\n            removedirs(head)\n        except error:\n            pass\n\n__all__.extend([\"makedirs\", \"removedirs\", \"renames\"])\n\ndef walk(top, topdown=True, onerror=None, followlinks=False):\n    \"\"\"Directory tree generator.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), yields a 3-tuple\n\n        dirpath, dirnames, filenames\n\n    dirpath is a string, the path to the directory.  dirnames is a list of\n    the names of the subdirectories in dirpath (excluding '.' and '..').\n    filenames is a list of the names of the non-directory files in dirpath.\n    Note that the names in the lists are just names, with no path components.\n    To get a full path (which begins with top) to a file or directory in\n    dirpath, do os.path.join(dirpath, name).\n\n    If optional arg 'topdown' is true or not specified, the triple for a\n    directory is generated before the triples for any of its subdirectories\n    (directories are generated top down).  If topdown is false, the triple\n    for a directory is generated after the triples for all of its\n    subdirectories (directories are generated bottom up).\n\n    When topdown is true, the caller can modify the dirnames list in-place\n    (e.g., via del or slice assignment), and walk will only recurse into the\n    subdirectories whose names remain in dirnames; this can be used to prune the\n    search, or to impose a specific order of visiting.  Modifying dirnames when\n    topdown is false is ineffective, since the directories in dirnames have\n    already been generated by the time dirnames itself is generated. No matter\n    the value of topdown, the list of subdirectories is retrieved before the\n    tuples for the directory and its subdirectories are generated.\n\n    By default errors from the os.listdir() call are ignored.  If\n    optional arg 'onerror' is specified, it should be a function; it\n    will be called with one argument, an os.error instance.  It can\n    report the error to continue with the walk, or raise the exception\n    to abort the walk.  Note that the filename is available as the\n    filename attribute of the exception object.\n\n    By default, os.walk does not follow symbolic links to subdirectories on\n    systems that support them.  In order to get this functionality, set the\n    optional argument 'followlinks' to true.\n\n    Caution:  if you pass a relative pathname for top, don't change the\n    current working directory between resumptions of walk.  walk never\n    changes the current directory, and assumes that the client doesn't\n    either.\n\n    Example:\n\n    import os\n    from os.path import join, getsize\n    for root, dirs, files in os.walk('python/Lib/email'):\n        print root, \"consumes\",\n        print sum([getsize(join(root, name)) for name in files]),\n        print \"bytes in\", len(files), \"non-directory files\"\n        if 'CVS' in dirs:\n            dirs.remove('CVS')  # don't visit CVS directories\n\n    \"\"\"\n\n    islink, join, isdir = path.islink, path.join, path.isdir\n\n    # We may not have read permission for top, in which case we can't\n    # get a list of the files the directory contains.  os.path.walk\n    # always suppressed the exception then, rather than blow up for a\n    # minor reason when (say) a thousand readable directories are still\n    # left to visit.  That logic is copied here.\n    try:\n        # Note that listdir and error are globals in this module due\n        # to earlier import-*.\n        names = listdir(top)\n    except error, err:\n        if onerror is not None:\n            onerror(err)\n        return\n\n    dirs, nondirs = [], []\n    for name in names:\n        if isdir(join(top, name)):\n            dirs.append(name)\n        else:\n            nondirs.append(name)\n\n    if topdown:\n        yield top, dirs, nondirs\n    for name in dirs:\n        new_path = join(top, name)\n        if followlinks or not islink(new_path):\n            for x in walk(new_path, topdown, onerror, followlinks):\n                yield x\n    if not topdown:\n        yield top, dirs, nondirs\n\n__all__.append(\"walk\")\n\n# Make sure os.environ exists, at least\ntry:\n    environ\nexcept NameError:\n    environ = {}\n\ndef execl(file, *args):\n    \"\"\"execl(file, *args)\n\n    Execute the executable file with argument list args, replacing the\n    current process. \"\"\"\n    execv(file, args)\n\ndef execle(file, *args):\n    \"\"\"execle(file, *args, env)\n\n    Execute the executable file with argument list args and\n    environment env, replacing the current process. \"\"\"\n    env = args[-1]\n    execve(file, args[:-1], env)\n\ndef execlp(file, *args):\n    \"\"\"execlp(file, *args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process. \"\"\"\n    execvp(file, args)\n\ndef execlpe(file, *args):\n    \"\"\"execlpe(file, *args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env, replacing the current\n    process. \"\"\"\n    env = args[-1]\n    execvpe(file, args[:-1], env)\n\ndef execvp(file, args):\n    \"\"\"execvp(file, args)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args, replacing the current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args)\n\ndef execvpe(file, args, env):\n    \"\"\"execvpe(file, args, env)\n\n    Execute the executable file (which is searched for along $PATH)\n    with argument list args and environment env , replacing the\n    current process.\n    args may be a list or tuple of strings. \"\"\"\n    _execvpe(file, args, env)\n\n__all__.extend([\"execl\",\"execle\",\"execlp\",\"execlpe\",\"execvp\",\"execvpe\"])\n\ndef _execvpe(file, args, env=None):\n    if env is not None:\n        func = execve\n        argrest = (args, env)\n    else:\n        func = execv\n        argrest = (args,)\n        env = environ\n\n    head, tail = path.split(file)\n    if head:\n        func(file, *argrest)\n        return\n    if 'PATH' in env:\n        envpath = env['PATH']\n    else:\n        envpath = defpath\n    PATH = envpath.split(pathsep)\n    saved_exc = None\n    saved_tb = None\n    for dir in PATH:\n        fullname = path.join(dir, file)\n        try:\n            func(fullname, *argrest)\n        except error, e:\n            tb = sys.exc_info()[2]\n            if (e.errno != errno.ENOENT and e.errno != errno.ENOTDIR\n                and saved_exc is None):\n                saved_exc = e\n                saved_tb = tb\n    if saved_exc:\n        raise error, saved_exc, saved_tb\n    raise error, e, tb\n\n# Change environ to automatically call putenv() if it exists\ntry:\n    # This will fail if there's no putenv\n    putenv\nexcept NameError:\n    pass\nelse:\n    import UserDict\n\n    # Fake unsetenv() for Windows\n    # not sure about os2 here but\n    # I'm guessing they are the same.\n\n    if name in ('os2', 'nt'):\n        def unsetenv(key):\n            putenv(key, \"\")\n\n    if name == \"riscos\":\n        # On RISC OS, all env access goes through getenv and putenv\n        from riscosenviron import _Environ\n    elif name in ('os2', 'nt'):  # Where Env Var Names Must Be UPPERCASE\n        # But we store them as upper case\n        class _Environ(UserDict.IterableUserDict):\n            def __init__(self, environ):\n                UserDict.UserDict.__init__(self)\n                data = self.data\n                for k, v in environ.items():\n                    data[k.upper()] = v\n            def __setitem__(self, key, item):\n                putenv(key, item)\n                self.data[key.upper()] = item\n            def __getitem__(self, key):\n                return self.data[key.upper()]\n            try:\n                unsetenv\n            except NameError:\n                def __delitem__(self, key):\n                    del self.data[key.upper()]\n            else:\n                def __delitem__(self, key):\n                    unsetenv(key)\n                    del self.data[key.upper()]\n                def clear(self):\n                    for key in self.data.keys():\n                        unsetenv(key)\n                        del self.data[key]\n                def pop(self, key, *args):\n                    unsetenv(key)\n                    return self.data.pop(key.upper(), *args)\n            def has_key(self, key):\n                return key.upper() in self.data\n            def __contains__(self, key):\n                return key.upper() in self.data\n            def get(self, key, failobj=None):\n                return self.data.get(key.upper(), failobj)\n            def update(self, dict=None, **kwargs):\n                if dict:\n                    try:\n                        keys = dict.keys()\n                    except AttributeError:\n                        # List of (key, value)\n                        for k, v in dict:\n                            self[k] = v\n                    else:\n                        # got keys\n                        # cannot use items(), since mappings\n                        # may not have them.\n                        for k in keys:\n                            self[k] = dict[k]\n                if kwargs:\n                    self.update(kwargs)\n            def copy(self):\n                return dict(self)\n\n    else:  # Where Env Var Names Can Be Mixed Case\n        class _Environ(UserDict.IterableUserDict):\n            def __init__(self, environ):\n                UserDict.UserDict.__init__(self)\n                self.data = environ\n            def __setitem__(self, key, item):\n                putenv(key, item)\n                self.data[key] = item\n            def update(self,  dict=None, **kwargs):\n                if dict:\n                    try:\n                        keys = dict.keys()\n                    except AttributeError:\n                        # List of (key, value)\n                        for k, v in dict:\n                            self[k] = v\n                    else:\n                        # got keys\n                        # cannot use items(), since mappings\n                        # may not have them.\n                        for k in keys:\n                            self[k] = dict[k]\n                if kwargs:\n                    self.update(kwargs)\n            try:\n                unsetenv\n            except NameError:\n                pass\n            else:\n                def __delitem__(self, key):\n                    unsetenv(key)\n                    del self.data[key]\n                def clear(self):\n                    for key in self.data.keys():\n                        unsetenv(key)\n                        del self.data[key]\n                def pop(self, key, *args):\n                    unsetenv(key)\n                    return self.data.pop(key, *args)\n            def copy(self):\n                return dict(self)\n\n\n    environ = _Environ(environ)\n\ndef getenv(key, default=None):\n    \"\"\"Get an environment variable, return None if it doesn't exist.\n    The optional second argument can specify an alternate default.\"\"\"\n    return environ.get(key, default)\n__all__.append(\"getenv\")\n\ndef _exists(name):\n    return name in globals()\n\n# Supply spawn*() (probably only for Unix)\nif _exists(\"fork\") and not _exists(\"spawnv\") and _exists(\"execv\"):\n\n    P_WAIT = 0\n    P_NOWAIT = P_NOWAITO = 1\n\n    # XXX Should we support P_DETACH?  I suppose it could fork()**2\n    # and close the std I/O streams.  Also, P_OVERLAY is the same\n    # as execv*()?\n\n    def _spawnvef(mode, file, args, env, func):\n        # Internal helper; func is the exec*() function to use\n        pid = fork()\n        if not pid:\n            # Child\n            try:\n                if env is None:\n                    func(file, args)\n                else:\n                    func(file, args, env)\n            except:\n                _exit(127)\n        else:\n            # Parent\n            if mode == P_NOWAIT:\n                return pid # Caller is responsible for waiting!\n            while 1:\n                wpid, sts = waitpid(pid, 0)\n                if WIFSTOPPED(sts):\n                    continue\n                elif WIFSIGNALED(sts):\n                    return -WTERMSIG(sts)\n                elif WIFEXITED(sts):\n                    return WEXITSTATUS(sts)\n                else:\n                    raise error, \"Not stopped, signaled or exited???\"\n\n    def spawnv(mode, file, args):\n        \"\"\"spawnv(mode, file, args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execv)\n\n    def spawnve(mode, file, args, env):\n        \"\"\"spawnve(mode, file, args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nspecified environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execve)\n\n    # Note: spawnvp[e] is't currently supported on Windows\n\n    def spawnvp(mode, file, args):\n        \"\"\"spawnvp(mode, file, args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, None, execvp)\n\n    def spawnvpe(mode, file, args, env):\n        \"\"\"spawnvpe(mode, file, args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return _spawnvef(mode, file, args, env, execvpe)\n\nif _exists(\"spawnv\"):\n    # These aren't supplied by the basic Windows code\n    # but can be easily implemented in Python\n\n    def spawnl(mode, file, *args):\n        \"\"\"spawnl(mode, file, *args) -> integer\n\nExecute file with arguments from args in a subprocess.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnv(mode, file, args)\n\n    def spawnle(mode, file, *args):\n        \"\"\"spawnle(mode, file, *args, env) -> integer\n\nExecute file with arguments from args in a subprocess with the\nsupplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnve(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnv\", \"spawnve\", \"spawnl\", \"spawnle\",])\n\n\nif _exists(\"spawnvp\"):\n    # At the moment, Windows doesn't implement spawnvp[e],\n    # so it won't have spawnlp[e] either.\n    def spawnlp(mode, file, *args):\n        \"\"\"spawnlp(mode, file, *args) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        return spawnvp(mode, file, args)\n\n    def spawnlpe(mode, file, *args):\n        \"\"\"spawnlpe(mode, file, *args, env) -> integer\n\nExecute file (which is looked for along $PATH) with arguments from\nargs in a subprocess with the supplied environment.\nIf mode == P_NOWAIT return the pid of the process.\nIf mode == P_WAIT return the process's exit code if it exits normally;\notherwise return -SIG, where SIG is the signal that killed it. \"\"\"\n        env = args[-1]\n        return spawnvpe(mode, file, args[:-1], env)\n\n\n    __all__.extend([\"spawnvp\", \"spawnvpe\", \"spawnlp\", \"spawnlpe\",])\n\n\n# Supply popen2 etc. (for Unix)\nif _exists(\"fork\"):\n    if not _exists(\"popen2\"):\n        def popen2(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen2 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 close_fds=True)\n            return p.stdin, p.stdout\n        __all__.append(\"popen2\")\n\n    if not _exists(\"popen3\"):\n        def popen3(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout, child_stderr) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen3 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 stderr=PIPE, close_fds=True)\n            return p.stdin, p.stdout, p.stderr\n        __all__.append(\"popen3\")\n\n    if not _exists(\"popen4\"):\n        def popen4(cmd, mode=\"t\", bufsize=-1):\n            \"\"\"Execute the shell command 'cmd' in a sub-process.  On UNIX, 'cmd'\n            may be a sequence, in which case arguments will be passed directly to\n            the program without shell intervention (as with os.spawnv()).  If 'cmd'\n            is a string it will be passed to the shell (as with os.system()). If\n            'bufsize' is specified, it sets the buffer size for the I/O pipes.  The\n            file objects (child_stdin, child_stdout_stderr) are returned.\"\"\"\n            import warnings\n            msg = \"os.popen4 is deprecated.  Use the subprocess module.\"\n            warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n            import subprocess\n            PIPE = subprocess.PIPE\n            p = subprocess.Popen(cmd, shell=isinstance(cmd, basestring),\n                                 bufsize=bufsize, stdin=PIPE, stdout=PIPE,\n                                 stderr=subprocess.STDOUT, close_fds=True)\n            return p.stdin, p.stdout\n        __all__.append(\"popen4\")\n\nimport copy_reg as _copy_reg\n\ndef _make_stat_result(tup, dict):\n    return stat_result(tup, dict)\n\ndef _pickle_stat_result(sr):\n    (type, args) = sr.__reduce__()\n    return (_make_stat_result, args)\n\ntry:\n    _copy_reg.pickle(stat_result, _pickle_stat_result, _make_stat_result)\nexcept NameError: # stat_result may not exist\n    pass\n\ndef _make_statvfs_result(tup, dict):\n    return statvfs_result(tup, dict)\n\ndef _pickle_statvfs_result(sr):\n    (type, args) = sr.__reduce__()\n    return (_make_statvfs_result, args)\n\ntry:\n    _copy_reg.pickle(statvfs_result, _pickle_statvfs_result,\n                     _make_statvfs_result)\nexcept NameError: # statvfs_result may not exist\n    pass\n\n\n\n\nbase_read=read\nbase_write=write\nimport sys, js\nclass FileNumbers(list):\n    def __init__(self, default):\n        super(FileNumbers, self).extend(default)\n    def extend(self, other):\n        for i in other:\n            self.append(i)\n    def append(self, p_object):\n        tf = tmpfile()\n        while tf.fileno() < len(self):\n            tf=tmpfile()\n        while not tf.fileno() <= len(self):\n            super(FileNumbers, self).append(None)\n        super(FileNumbers, self).append(p_object)\n    def __getitem__(self, item):\n        try:\n            return super(FileNumbers, self).__getitem__(item)\n        except Exception as e:\n            return None\n\n\nfileNumbers = FileNumbers([sys.stdin,sys.stdout,sys.stderr])\n\n\n\n\ndef read(fd, buffersize):\n    if fileNumbers[fd] is not None:\n        return str(fileNumbers[fd].read(buffersize))\n    return base_read(fd, buffersize)\n\ndef write(fd, string):\n    if fileNumbers[fd] is not None:\n        return fileNumbers[fd].write(string)\n    return base_write(fd, string)\n\n\ndef pipe(buffer_id=None):\n    import js\n    if buffer_id is None:\n        po = js.eval('new pipe()')\n    else:\n        po = js.eval('new pipe(%i)'%buffer_id)\n    fileNumbers.append(0)\n    fileNumbers.append(1)\n    re,we=fileNumbers.index(0), fileNumbers.index(1)\n    po.read_end.fno=re\n    po.write_end.fno=we\n    fileNumbers[re],fileNumbers[we]=po.read_end, po.write_end\n    return re,we\n\nbase_dup=dup\n\ndef dup(n):\n    if n==128:\n        return 128\n    if fileNumbers[n] is not None:\n        fo = fileNumbers[n]\n        fileNumbers.append(fo)\n        return len(fileNumbers)-1\n    new_n = base_dup(n)\n    while new_n in fileNumbers:\n        new_n = base_dup(n)\n    return new_n\n", 
    "pdb": "#! /usr/bin/env python\n\n\"\"\"A Python debugger.\"\"\"\n\n# (See pdb.doc for documentation.)\n\nimport sys\nimport linecache\nimport cmd\nimport bdb\nfrom repr import Repr\nimport os\nimport re\nimport pprint\nimport traceback\n\n\nclass Restart(Exception):\n    \"\"\"Causes a debugger to be restarted for the debugged python program.\"\"\"\n    pass\n\n# Create a custom safe Repr instance and increase its maxstring.\n# The default of 30 truncates error messages too easily.\n_repr = Repr()\n_repr.maxstring = 200\n_saferepr = _repr.repr\n\n__all__ = [\"run\", \"pm\", \"Pdb\", \"runeval\", \"runctx\", \"runcall\", \"set_trace\",\n           \"post_mortem\", \"help\"]\n\ndef find_function(funcname, filename):\n    cre = re.compile(r'def\\s+%s\\s*[(]' % re.escape(funcname))\n    try:\n        fp = open(filename)\n    except IOError:\n        return None\n    # consumer of this info expects the first line to be 1\n    lineno = 1\n    answer = None\n    while 1:\n        line = fp.readline()\n        if line == '':\n            break\n        if cre.match(line):\n            answer = funcname, filename, lineno\n            break\n        lineno = lineno + 1\n    fp.close()\n    return answer\n\n\n# Interaction prompt line will separate file and call info from code\n# text using value of line_prefix string.  A newline and arrow may\n# be to your liking.  You can set it once pdb is imported using the\n# command \"pdb.line_prefix = '\\n% '\".\n# line_prefix = ': '    # Use this to get the old situation back\nline_prefix = '\\n-> '   # Probably a better default\n\nclass Pdb(bdb.Bdb, cmd.Cmd):\n\n    def __init__(self, completekey='tab', stdin=None, stdout=None, skip=None):\n        bdb.Bdb.__init__(self, skip=skip)\n        cmd.Cmd.__init__(self, completekey, stdin, stdout)\n        if stdout:\n            self.use_rawinput = 0\n        self.prompt = '(Pdb) '\n        self.aliases = {}\n        self.mainpyfile = ''\n        self._wait_for_mainpyfile = 0\n        # Try to load readline if it exists\n        try:\n            import readline\n        except ImportError:\n            pass\n\n        # Read $HOME/.pdbrc and ./.pdbrc\n        self.rcLines = []\n        if 'HOME' in os.environ:\n            envHome = os.environ['HOME']\n            try:\n                rcFile = open(os.path.join(envHome, \".pdbrc\"))\n            except IOError:\n                pass\n            else:\n                for line in rcFile.readlines():\n                    self.rcLines.append(line)\n                rcFile.close()\n        try:\n            rcFile = open(\".pdbrc\")\n        except IOError:\n            pass\n        else:\n            for line in rcFile.readlines():\n                self.rcLines.append(line)\n            rcFile.close()\n\n        self.commands = {} # associates a command list to breakpoint numbers\n        self.commands_doprompt = {} # for each bp num, tells if the prompt\n                                    # must be disp. after execing the cmd list\n        self.commands_silent = {} # for each bp num, tells if the stack trace\n                                  # must be disp. after execing the cmd list\n        self.commands_defining = False # True while in the process of defining\n                                       # a command list\n        self.commands_bnum = None # The breakpoint number for which we are\n                                  # defining a list\n\n    def reset(self):\n        bdb.Bdb.reset(self)\n        self.forget()\n\n    def forget(self):\n        self.lineno = None\n        self.stack = []\n        self.curindex = 0\n        self.curframe = None\n\n    def setup(self, f, t):\n        self.forget()\n        self.stack, self.curindex = self.get_stack(f, t)\n        self.curframe = self.stack[self.curindex][0]\n        # The f_locals dictionary is updated from the actual frame\n        # locals whenever the .f_locals accessor is called, so we\n        # cache it here to ensure that modifications are not overwritten.\n        self.curframe_locals = self.curframe.f_locals\n        self.execRcLines()\n\n    # Can be executed earlier than 'setup' if desired\n    def execRcLines(self):\n        if self.rcLines:\n            # Make local copy because of recursion\n            rcLines = self.rcLines\n            # executed only once\n            self.rcLines = []\n            for line in rcLines:\n                line = line[:-1]\n                if len(line) > 0 and line[0] != '#':\n                    self.onecmd(line)\n\n    # Override Bdb methods\n\n    def user_call(self, frame, argument_list):\n        \"\"\"This method is called when there is the remote possibility\n        that we ever need to stop in this function.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        if self.stop_here(frame):\n            print >>self.stdout, '--Call--'\n            self.interaction(frame, None)\n\n    def user_line(self, frame):\n        \"\"\"This function is called when we stop or break at this line.\"\"\"\n        if self._wait_for_mainpyfile:\n            if (self.mainpyfile != self.canonic(frame.f_code.co_filename)\n                or frame.f_lineno<= 0):\n                return\n            self._wait_for_mainpyfile = 0\n        if self.bp_commands(frame):\n            self.interaction(frame, None)\n\n    def bp_commands(self,frame):\n        \"\"\"Call every command that was set for the current active breakpoint\n        (if there is one).\n\n        Returns True if the normal interaction function must be called,\n        False otherwise.\"\"\"\n        # self.currentbp is set in bdb in Bdb.break_here if a breakpoint was hit\n        if getattr(self, \"currentbp\", False) and \\\n               self.currentbp in self.commands:\n            currentbp = self.currentbp\n            self.currentbp = 0\n            lastcmd_back = self.lastcmd\n            self.setup(frame, None)\n            for line in self.commands[currentbp]:\n                self.onecmd(line)\n            self.lastcmd = lastcmd_back\n            if not self.commands_silent[currentbp]:\n                self.print_stack_entry(self.stack[self.curindex])\n            if self.commands_doprompt[currentbp]:\n                self.cmdloop()\n            self.forget()\n            return\n        return 1\n\n    def user_return(self, frame, return_value):\n        \"\"\"This function is called when a return trap is set here.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        frame.f_locals['__return__'] = return_value\n        print >>self.stdout, '--Return--'\n        self.interaction(frame, None)\n\n    def user_exception(self, frame, exc_info):\n        \"\"\"This function is called if an exception occurs,\n        but only if we are to stop at or just below this level.\"\"\"\n        if self._wait_for_mainpyfile:\n            return\n        exc_type, exc_value, exc_traceback = exc_info\n        frame.f_locals['__exception__'] = exc_type, exc_value\n        if type(exc_type) == type(''):\n            exc_type_name = exc_type\n        else: exc_type_name = exc_type.__name__\n        print >>self.stdout, exc_type_name + ':', _saferepr(exc_value)\n        self.interaction(frame, exc_traceback)\n\n    # General interaction function\n\n    def interaction(self, frame, traceback):\n        self.setup(frame, traceback)\n        self.print_stack_entry(self.stack[self.curindex])\n        self.cmdloop()\n        self.forget()\n\n    def displayhook(self, obj):\n        \"\"\"Custom displayhook for the exec in default(), which prevents\n        assignment of the _ variable in the builtins.\n        \"\"\"\n        # reproduce the behavior of the standard displayhook, not printing None\n        if obj is not None:\n            print repr(obj)\n\n    def default(self, line):\n        if line[:1] == '!': line = line[1:]\n        locals = self.curframe_locals\n        globals = self.curframe.f_globals\n        try:\n            code = compile(line + '\\n', '<stdin>', 'single')\n            save_stdout = sys.stdout\n            save_stdin = sys.stdin\n            save_displayhook = sys.displayhook\n            try:\n                sys.stdin = self.stdin\n                sys.stdout = self.stdout\n                sys.displayhook = self.displayhook\n                exec code in globals, locals\n            finally:\n                sys.stdout = save_stdout\n                sys.stdin = save_stdin\n                sys.displayhook = save_displayhook\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', v\n\n    def precmd(self, line):\n        \"\"\"Handle alias expansion and ';;' separator.\"\"\"\n        if not line.strip():\n            return line\n        args = line.split()\n        while args[0] in self.aliases:\n            line = self.aliases[args[0]]\n            ii = 1\n            for tmpArg in args[1:]:\n                line = line.replace(\"%\" + str(ii),\n                                      tmpArg)\n                ii = ii + 1\n            line = line.replace(\"%*\", ' '.join(args[1:]))\n            args = line.split()\n        # split into ';;' separated commands\n        # unless it's an alias command\n        if args[0] != 'alias':\n            marker = line.find(';;')\n            if marker >= 0:\n                # queue up everything after marker\n                next = line[marker+2:].lstrip()\n                self.cmdqueue.append(next)\n                line = line[:marker].rstrip()\n        return line\n\n    def onecmd(self, line):\n        \"\"\"Interpret the argument as though it had been typed in response\n        to the prompt.\n\n        Checks whether this line is typed at the normal prompt or in\n        a breakpoint command list definition.\n        \"\"\"\n        if not self.commands_defining:\n            return cmd.Cmd.onecmd(self, line)\n        else:\n            return self.handle_command_def(line)\n\n    def handle_command_def(self,line):\n        \"\"\"Handles one command line during command list definition.\"\"\"\n        cmd, arg, line = self.parseline(line)\n        if not cmd:\n            return\n        if cmd == 'silent':\n            self.commands_silent[self.commands_bnum] = True\n            return # continue to handle other cmd def in the cmd list\n        elif cmd == 'end':\n            self.cmdqueue = []\n            return 1 # end of cmd list\n        cmdlist = self.commands[self.commands_bnum]\n        if arg:\n            cmdlist.append(cmd+' '+arg)\n        else:\n            cmdlist.append(cmd)\n        # Determine if we must stop\n        try:\n            func = getattr(self, 'do_' + cmd)\n        except AttributeError:\n            func = self.default\n        # one of the resuming commands\n        if func.func_name in self.commands_resuming:\n            self.commands_doprompt[self.commands_bnum] = False\n            self.cmdqueue = []\n            return 1\n        return\n\n    # Command definitions, called by cmdloop()\n    # The argument is the remaining string on the command line\n    # Return true to exit from the command loop\n\n    do_h = cmd.Cmd.do_help\n\n    def do_commands(self, arg):\n        \"\"\"Defines a list of commands associated to a breakpoint.\n\n        Those commands will be executed whenever the breakpoint causes\n        the program to stop execution.\"\"\"\n        if not arg:\n            bnum = len(bdb.Breakpoint.bpbynumber)-1\n        else:\n            try:\n                bnum = int(arg)\n            except:\n                print >>self.stdout, \"Usage : commands [bnum]\\n        ...\" \\\n                                     \"\\n        end\"\n                return\n        self.commands_bnum = bnum\n        self.commands[bnum] = []\n        self.commands_doprompt[bnum] = True\n        self.commands_silent[bnum] = False\n        prompt_back = self.prompt\n        self.prompt = '(com) '\n        self.commands_defining = True\n        try:\n            self.cmdloop()\n        finally:\n            self.commands_defining = False\n            self.prompt = prompt_back\n\n    def do_break(self, arg, temporary = 0):\n        # break [ ([filename:]lineno | function) [, \"condition\"] ]\n        if not arg:\n            if self.breaks:  # There's at least one\n                print >>self.stdout, \"Num Type         Disp Enb   Where\"\n                for bp in bdb.Breakpoint.bpbynumber:\n                    if bp:\n                        bp.bpprint(self.stdout)\n            return\n        # parse arguments; comma has lowest precedence\n        # and cannot occur in filename\n        filename = None\n        lineno = None\n        cond = None\n        comma = arg.find(',')\n        if comma > 0:\n            # parse stuff after comma: \"condition\"\n            cond = arg[comma+1:].lstrip()\n            arg = arg[:comma].rstrip()\n        # parse stuff before comma: [filename:]lineno | function\n        colon = arg.rfind(':')\n        funcname = None\n        if colon >= 0:\n            filename = arg[:colon].rstrip()\n            f = self.lookupmodule(filename)\n            if not f:\n                print >>self.stdout, '*** ', repr(filename),\n                print >>self.stdout, 'not found from sys.path'\n                return\n            else:\n                filename = f\n            arg = arg[colon+1:].lstrip()\n            try:\n                lineno = int(arg)\n            except ValueError, msg:\n                print >>self.stdout, '*** Bad lineno:', arg\n                return\n        else:\n            # no colon; can be lineno or function\n            try:\n                lineno = int(arg)\n            except ValueError:\n                try:\n                    func = eval(arg,\n                                self.curframe.f_globals,\n                                self.curframe_locals)\n                except:\n                    func = arg\n                try:\n                    if hasattr(func, 'im_func'):\n                        func = func.im_func\n                    code = func.func_code\n                    #use co_name to identify the bkpt (function names\n                    #could be aliased, but co_name is invariant)\n                    funcname = code.co_name\n                    lineno = code.co_firstlineno\n                    filename = code.co_filename\n                except:\n                    # last thing to try\n                    (ok, filename, ln) = self.lineinfo(arg)\n                    if not ok:\n                        print >>self.stdout, '*** The specified object',\n                        print >>self.stdout, repr(arg),\n                        print >>self.stdout, 'is not a function'\n                        print >>self.stdout, 'or was not found along sys.path.'\n                        return\n                    funcname = ok # ok contains a function name\n                    lineno = int(ln)\n        if not filename:\n            filename = self.defaultFile()\n        # Check for reasonable breakpoint\n        line = self.checkline(filename, lineno)\n        if line:\n            # now set the break point\n            err = self.set_break(filename, line, temporary, cond, funcname)\n            if err: print >>self.stdout, '***', err\n            else:\n                bp = self.get_breaks(filename, line)[-1]\n                print >>self.stdout, \"Breakpoint %d at %s:%d\" % (bp.number,\n                                                                 bp.file,\n                                                                 bp.line)\n\n    # To be overridden in derived debuggers\n    def defaultFile(self):\n        \"\"\"Produce a reasonable default.\"\"\"\n        filename = self.curframe.f_code.co_filename\n        if filename == '<string>' and self.mainpyfile:\n            filename = self.mainpyfile\n        return filename\n\n    do_b = do_break\n\n    def do_tbreak(self, arg):\n        self.do_break(arg, 1)\n\n    def lineinfo(self, identifier):\n        failed = (None, None, None)\n        # Input is identifier, may be in single quotes\n        idstring = identifier.split(\"'\")\n        if len(idstring) == 1:\n            # not in single quotes\n            id = idstring[0].strip()\n        elif len(idstring) == 3:\n            # quoted\n            id = idstring[1].strip()\n        else:\n            return failed\n        if id == '': return failed\n        parts = id.split('.')\n        # Protection for derived debuggers\n        if parts[0] == 'self':\n            del parts[0]\n            if len(parts) == 0:\n                return failed\n        # Best first guess at file to look at\n        fname = self.defaultFile()\n        if len(parts) == 1:\n            item = parts[0]\n        else:\n            # More than one part.\n            # First is module, second is method/class\n            f = self.lookupmodule(parts[0])\n            if f:\n                fname = f\n            item = parts[1]\n        answer = find_function(item, fname)\n        return answer or failed\n\n    def checkline(self, filename, lineno):\n        \"\"\"Check whether specified line seems to be executable.\n\n        Return `lineno` if it is, 0 if not (e.g. a docstring, comment, blank\n        line or EOF). Warning: testing is not comprehensive.\n        \"\"\"\n        # this method should be callable before starting debugging, so default\n        # to \"no globals\" if there is no current frame\n        globs = self.curframe.f_globals if hasattr(self, 'curframe') else None\n        line = linecache.getline(filename, lineno, globs)\n        if not line:\n            print >>self.stdout, 'End of file'\n            return 0\n        line = line.strip()\n        # Don't allow setting breakpoint at a blank line\n        if (not line or (line[0] == '#') or\n             (line[:3] == '\"\"\"') or line[:3] == \"'''\"):\n            print >>self.stdout, '*** Blank or comment'\n            return 0\n        return lineno\n\n    def do_enable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.enable()\n\n    def do_disable(self, arg):\n        args = arg.split()\n        for i in args:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n\n            bp = bdb.Breakpoint.bpbynumber[i]\n            if bp:\n                bp.disable()\n\n    def do_condition(self, arg):\n        # arg is breakpoint number and condition\n        args = arg.split(' ', 1)\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            cond = args[1]\n        except:\n            cond = None\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.cond = cond\n            if not cond:\n                print >>self.stdout, 'Breakpoint', bpnum,\n                print >>self.stdout, 'is now unconditional.'\n\n    def do_ignore(self,arg):\n        \"\"\"arg is bp number followed by ignore count.\"\"\"\n        args = arg.split()\n        try:\n            bpnum = int(args[0].strip())\n        except ValueError:\n            # something went wrong\n            print >>self.stdout, \\\n                'Breakpoint index %r is not a number' % args[0]\n            return\n        try:\n            count = int(args[1].strip())\n        except:\n            count = 0\n        try:\n            bp = bdb.Breakpoint.bpbynumber[bpnum]\n        except IndexError:\n            print >>self.stdout, 'Breakpoint index %r is not valid' % args[0]\n            return\n        if bp:\n            bp.ignore = count\n            if count > 0:\n                reply = 'Will ignore next '\n                if count > 1:\n                    reply = reply + '%d crossings' % count\n                else:\n                    reply = reply + '1 crossing'\n                print >>self.stdout, reply + ' of breakpoint %d.' % bpnum\n            else:\n                print >>self.stdout, 'Will stop next time breakpoint',\n                print >>self.stdout, bpnum, 'is reached.'\n\n    def do_clear(self, arg):\n        \"\"\"Three possibilities, tried in this order:\n        clear -> clear all breaks, ask for confirmation\n        clear file:lineno -> clear all breaks at file:lineno\n        clear bpno bpno ... -> clear breakpoints by number\"\"\"\n        if not arg:\n            try:\n                reply = raw_input('Clear all breaks? ')\n            except EOFError:\n                reply = 'no'\n            reply = reply.strip().lower()\n            if reply in ('y', 'yes'):\n                self.clear_all_breaks()\n            return\n        if ':' in arg:\n            # Make sure it works for \"clear C:\\foo\\bar.py:12\"\n            i = arg.rfind(':')\n            filename = arg[:i]\n            arg = arg[i+1:]\n            try:\n                lineno = int(arg)\n            except ValueError:\n                err = \"Invalid line number (%s)\" % arg\n            else:\n                err = self.clear_break(filename, lineno)\n            if err: print >>self.stdout, '***', err\n            return\n        numberlist = arg.split()\n        for i in numberlist:\n            try:\n                i = int(i)\n            except ValueError:\n                print >>self.stdout, 'Breakpoint index %r is not a number' % i\n                continue\n\n            if not (0 <= i < len(bdb.Breakpoint.bpbynumber)):\n                print >>self.stdout, 'No breakpoint numbered', i\n                continue\n            err = self.clear_bpbynumber(i)\n            if err:\n                print >>self.stdout, '***', err\n            else:\n                print >>self.stdout, 'Deleted breakpoint', i\n    do_cl = do_clear # 'c' is already an abbreviation for 'continue'\n\n    def do_where(self, arg):\n        self.print_stack_trace()\n    do_w = do_where\n    do_bt = do_where\n\n    def do_up(self, arg):\n        if self.curindex == 0:\n            print >>self.stdout, '*** Oldest frame'\n        else:\n            self.curindex = self.curindex - 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_u = do_up\n\n    def do_down(self, arg):\n        if self.curindex + 1 == len(self.stack):\n            print >>self.stdout, '*** Newest frame'\n        else:\n            self.curindex = self.curindex + 1\n            self.curframe = self.stack[self.curindex][0]\n            self.curframe_locals = self.curframe.f_locals\n            self.print_stack_entry(self.stack[self.curindex])\n            self.lineno = None\n    do_d = do_down\n\n    def do_until(self, arg):\n        self.set_until(self.curframe)\n        return 1\n    do_unt = do_until\n\n    def do_step(self, arg):\n        self.set_step()\n        return 1\n    do_s = do_step\n\n    def do_next(self, arg):\n        self.set_next(self.curframe)\n        return 1\n    do_n = do_next\n\n    def do_run(self, arg):\n        \"\"\"Restart program by raising an exception to be caught in the main\n        debugger loop.  If arguments were given, set them in sys.argv.\"\"\"\n        if arg:\n            import shlex\n            argv0 = sys.argv[0:1]\n            sys.argv = shlex.split(arg)\n            sys.argv[:0] = argv0\n        raise Restart\n\n    do_restart = do_run\n\n    def do_return(self, arg):\n        self.set_return(self.curframe)\n        return 1\n    do_r = do_return\n\n    def do_continue(self, arg):\n        self.set_continue()\n        return 1\n    do_c = do_cont = do_continue\n\n    def do_jump(self, arg):\n        if self.curindex + 1 != len(self.stack):\n            print >>self.stdout, \"*** You can only jump within the bottom frame\"\n            return\n        try:\n            arg = int(arg)\n        except ValueError:\n            print >>self.stdout, \"*** The 'jump' command requires a line number.\"\n        else:\n            try:\n                # Do the jump, fix up our copy of the stack, and display the\n                # new position\n                self.curframe.f_lineno = arg\n                self.stack[self.curindex] = self.stack[self.curindex][0], arg\n                self.print_stack_entry(self.stack[self.curindex])\n            except ValueError, e:\n                print >>self.stdout, '*** Jump failed:', e\n    do_j = do_jump\n\n    def do_debug(self, arg):\n        sys.settrace(None)\n        globals = self.curframe.f_globals\n        locals = self.curframe_locals\n        p = Pdb(self.completekey, self.stdin, self.stdout)\n        p.prompt = \"(%s) \" % self.prompt.strip()\n        print >>self.stdout, \"ENTERING RECURSIVE DEBUGGER\"\n        sys.call_tracing(p.run, (arg, globals, locals))\n        print >>self.stdout, \"LEAVING RECURSIVE DEBUGGER\"\n        sys.settrace(self.trace_dispatch)\n        self.lastcmd = p.lastcmd\n\n    def do_quit(self, arg):\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    do_q = do_quit\n    do_exit = do_quit\n\n    def do_EOF(self, arg):\n        print >>self.stdout\n        self._user_requested_quit = 1\n        self.set_quit()\n        return 1\n\n    def do_args(self, arg):\n        co = self.curframe.f_code\n        dict = self.curframe_locals\n        n = co.co_argcount\n        if co.co_flags & 4: n = n+1\n        if co.co_flags & 8: n = n+1\n        for i in range(n):\n            name = co.co_varnames[i]\n            print >>self.stdout, name, '=',\n            if name in dict: print >>self.stdout, dict[name]\n            else: print >>self.stdout, \"*** undefined ***\"\n    do_a = do_args\n\n    def do_retval(self, arg):\n        if '__return__' in self.curframe_locals:\n            print >>self.stdout, self.curframe_locals['__return__']\n        else:\n            print >>self.stdout, '*** Not yet returned!'\n    do_rv = do_retval\n\n    def _getval(self, arg):\n        try:\n            return eval(arg, self.curframe.f_globals,\n                        self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if isinstance(t, str):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            raise\n\n    def do_p(self, arg):\n        try:\n            print >>self.stdout, repr(self._getval(arg))\n        except:\n            pass\n\n    def do_pp(self, arg):\n        try:\n            pprint.pprint(self._getval(arg), self.stdout)\n        except:\n            pass\n\n    def do_list(self, arg):\n        self.lastcmd = 'list'\n        last = None\n        if arg:\n            try:\n                x = eval(arg, {}, {})\n                if type(x) == type(()):\n                    first, last = x\n                    first = int(first)\n                    last = int(last)\n                    if last < first:\n                        # Assume it's a count\n                        last = first + last\n                else:\n                    first = max(1, int(x) - 5)\n            except:\n                print >>self.stdout, '*** Error in argument:', repr(arg)\n                return\n        elif self.lineno is None:\n            first = max(1, self.curframe.f_lineno - 5)\n        else:\n            first = self.lineno + 1\n        if last is None:\n            last = first + 10\n        filename = self.curframe.f_code.co_filename\n        breaklist = self.get_file_breaks(filename)\n        try:\n            for lineno in range(first, last+1):\n                line = linecache.getline(filename, lineno,\n                                         self.curframe.f_globals)\n                if not line:\n                    print >>self.stdout, '[EOF]'\n                    break\n                else:\n                    s = repr(lineno).rjust(3)\n                    if len(s) < 4: s = s + ' '\n                    if lineno in breaklist: s = s + 'B'\n                    else: s = s + ' '\n                    if lineno == self.curframe.f_lineno:\n                        s = s + '->'\n                    print >>self.stdout, s + '\\t' + line,\n                    self.lineno = lineno\n        except KeyboardInterrupt:\n            pass\n    do_l = do_list\n\n    def do_whatis(self, arg):\n        try:\n            value = eval(arg, self.curframe.f_globals,\n                            self.curframe_locals)\n        except:\n            t, v = sys.exc_info()[:2]\n            if type(t) == type(''):\n                exc_type_name = t\n            else: exc_type_name = t.__name__\n            print >>self.stdout, '***', exc_type_name + ':', repr(v)\n            return\n        code = None\n        # Is it a function?\n        try: code = value.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Function', code.co_name\n            return\n        # Is it an instance method?\n        try: code = value.im_func.func_code\n        except: pass\n        if code:\n            print >>self.stdout, 'Method', code.co_name\n            return\n        # None of the above...\n        print >>self.stdout, type(value)\n\n    def do_alias(self, arg):\n        args = arg.split()\n        if len(args) == 0:\n            keys = self.aliases.keys()\n            keys.sort()\n            for alias in keys:\n                print >>self.stdout, \"%s = %s\" % (alias, self.aliases[alias])\n            return\n        if args[0] in self.aliases and len(args) == 1:\n            print >>self.stdout, \"%s = %s\" % (args[0], self.aliases[args[0]])\n        else:\n            self.aliases[args[0]] = ' '.join(args[1:])\n\n    def do_unalias(self, arg):\n        args = arg.split()\n        if len(args) == 0: return\n        if args[0] in self.aliases:\n            del self.aliases[args[0]]\n\n    #list of all the commands making the program resume execution.\n    commands_resuming = ['do_continue', 'do_step', 'do_next', 'do_return',\n                         'do_quit', 'do_jump']\n\n    # Print a traceback starting at the top stack frame.\n    # The most recently entered frame is printed last;\n    # this is different from dbx and gdb, but consistent with\n    # the Python interpreter's stack trace.\n    # It is also consistent with the up/down commands (which are\n    # compatible with dbx and gdb: up moves towards 'main()'\n    # and down moves towards the most recent stack frame).\n\n    def print_stack_trace(self):\n        try:\n            for frame_lineno in self.stack:\n                self.print_stack_entry(frame_lineno)\n        except KeyboardInterrupt:\n            pass\n\n    def print_stack_entry(self, frame_lineno, prompt_prefix=line_prefix):\n        frame, lineno = frame_lineno\n        if frame is self.curframe:\n            print >>self.stdout, '>',\n        else:\n            print >>self.stdout, ' ',\n        print >>self.stdout, self.format_stack_entry(frame_lineno,\n                                                     prompt_prefix)\n\n\n    # Help methods (derived from pdb.doc)\n\n    def help_help(self):\n        self.help_h()\n\n    def help_h(self):\n        print >>self.stdout, \"\"\"h(elp)\nWithout argument, print the list of available commands.\nWith a command name as argument, print help about that command\n\"help pdb\" pipes the full documentation file to the $PAGER\n\"help exec\" gives help on the ! command\"\"\"\n\n    def help_where(self):\n        self.help_w()\n\n    def help_w(self):\n        print >>self.stdout, \"\"\"w(here)\nPrint a stack trace, with the most recent frame at the bottom.\nAn arrow indicates the \"current frame\", which determines the\ncontext of most commands.  'bt' is an alias for this command.\"\"\"\n\n    help_bt = help_w\n\n    def help_down(self):\n        self.help_d()\n\n    def help_d(self):\n        print >>self.stdout, \"\"\"d(own)\nMove the current frame one level down in the stack trace\n(to a newer frame).\"\"\"\n\n    def help_up(self):\n        self.help_u()\n\n    def help_u(self):\n        print >>self.stdout, \"\"\"u(p)\nMove the current frame one level up in the stack trace\n(to an older frame).\"\"\"\n\n    def help_break(self):\n        self.help_b()\n\n    def help_b(self):\n        print >>self.stdout, \"\"\"b(reak) ([file:]lineno | function) [, condition]\nWith a line number argument, set a break there in the current\nfile.  With a function name, set a break at first executable line\nof that function.  Without argument, list all breaks.  If a second\nargument is present, it is a string specifying an expression\nwhich must evaluate to true before the breakpoint is honored.\n\nThe line number may be prefixed with a filename and a colon,\nto specify a breakpoint in another file (probably one that\nhasn't been loaded yet).  The file is searched for on sys.path;\nthe .py suffix may be omitted.\"\"\"\n\n    def help_clear(self):\n        self.help_cl()\n\n    def help_cl(self):\n        print >>self.stdout, \"cl(ear) filename:lineno\"\n        print >>self.stdout, \"\"\"cl(ear) [bpnumber [bpnumber...]]\nWith a space separated list of breakpoint numbers, clear\nthose breakpoints.  Without argument, clear all breaks (but\nfirst ask confirmation).  With a filename:lineno argument,\nclear all breaks at that line in that file.\n\nNote that the argument is different from previous versions of\nthe debugger (in python distributions 1.5.1 and before) where\na linenumber was used instead of either filename:lineno or\nbreakpoint numbers.\"\"\"\n\n    def help_tbreak(self):\n        print >>self.stdout, \"\"\"tbreak  same arguments as break, but breakpoint\nis removed when first hit.\"\"\"\n\n    def help_enable(self):\n        print >>self.stdout, \"\"\"enable bpnumber [bpnumber ...]\nEnables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_disable(self):\n        print >>self.stdout, \"\"\"disable bpnumber [bpnumber ...]\nDisables the breakpoints given as a space separated list of\nbp numbers.\"\"\"\n\n    def help_ignore(self):\n        print >>self.stdout, \"\"\"ignore bpnumber count\nSets the ignore count for the given breakpoint number.  A breakpoint\nbecomes active when the ignore count is zero.  When non-zero, the\ncount is decremented each time the breakpoint is reached and the\nbreakpoint is not disabled and any associated condition evaluates\nto true.\"\"\"\n\n    def help_condition(self):\n        print >>self.stdout, \"\"\"condition bpnumber str_condition\nstr_condition is a string specifying an expression which\nmust evaluate to true before the breakpoint is honored.\nIf str_condition is absent, any existing condition is removed;\ni.e., the breakpoint is made unconditional.\"\"\"\n\n    def help_step(self):\n        self.help_s()\n\n    def help_s(self):\n        print >>self.stdout, \"\"\"s(tep)\nExecute the current line, stop at the first possible occasion\n(either in a function that is called or in the current function).\"\"\"\n\n    def help_until(self):\n        self.help_unt()\n\n    def help_unt(self):\n        print \"\"\"unt(il)\nContinue execution until the line with a number greater than the current\none is reached or until the current frame returns\"\"\"\n\n    def help_next(self):\n        self.help_n()\n\n    def help_n(self):\n        print >>self.stdout, \"\"\"n(ext)\nContinue execution until the next line in the current function\nis reached or it returns.\"\"\"\n\n    def help_return(self):\n        self.help_r()\n\n    def help_r(self):\n        print >>self.stdout, \"\"\"r(eturn)\nContinue execution until the current function returns.\"\"\"\n\n    def help_continue(self):\n        self.help_c()\n\n    def help_cont(self):\n        self.help_c()\n\n    def help_c(self):\n        print >>self.stdout, \"\"\"c(ont(inue))\nContinue execution, only stop when a breakpoint is encountered.\"\"\"\n\n    def help_jump(self):\n        self.help_j()\n\n    def help_j(self):\n        print >>self.stdout, \"\"\"j(ump) lineno\nSet the next line that will be executed.\"\"\"\n\n    def help_debug(self):\n        print >>self.stdout, \"\"\"debug code\nEnter a recursive debugger that steps through the code argument\n(which is an arbitrary expression or statement to be executed\nin the current environment).\"\"\"\n\n    def help_list(self):\n        self.help_l()\n\n    def help_l(self):\n        print >>self.stdout, \"\"\"l(ist) [first [,last]]\nList source code for the current file.\nWithout arguments, list 11 lines around the current line\nor continue the previous listing.\nWith one argument, list 11 lines starting at that line.\nWith two arguments, list the given range;\nif the second argument is less than the first, it is a count.\"\"\"\n\n    def help_args(self):\n        self.help_a()\n\n    def help_a(self):\n        print >>self.stdout, \"\"\"a(rgs)\nPrint the arguments of the current function.\"\"\"\n\n    def help_p(self):\n        print >>self.stdout, \"\"\"p expression\nPrint the value of the expression.\"\"\"\n\n    def help_pp(self):\n        print >>self.stdout, \"\"\"pp expression\nPretty-print the value of the expression.\"\"\"\n\n    def help_exec(self):\n        print >>self.stdout, \"\"\"(!) statement\nExecute the (one-line) statement in the context of\nthe current stack frame.\nThe exclamation point can be omitted unless the first word\nof the statement resembles a debugger command.\nTo assign to a global variable you must always prefix the\ncommand with a 'global' command, e.g.:\n(Pdb) global list_options; list_options = ['-l']\n(Pdb)\"\"\"\n\n    def help_run(self):\n        print \"\"\"run [args...]\nRestart the debugged python program. If a string is supplied, it is\nsplit with \"shlex\" and the result is used as the new sys.argv.\nHistory, breakpoints, actions and debugger options are preserved.\n\"restart\" is an alias for \"run\".\"\"\"\n\n    help_restart = help_run\n\n    def help_quit(self):\n        self.help_q()\n\n    def help_q(self):\n        print >>self.stdout, \"\"\"q(uit) or exit - Quit from the debugger.\nThe program being executed is aborted.\"\"\"\n\n    help_exit = help_q\n\n    def help_whatis(self):\n        print >>self.stdout, \"\"\"whatis arg\nPrints the type of the argument.\"\"\"\n\n    def help_EOF(self):\n        print >>self.stdout, \"\"\"EOF\nHandles the receipt of EOF as a command.\"\"\"\n\n    def help_alias(self):\n        print >>self.stdout, \"\"\"alias [name [command [parameter parameter ...]]]\nCreates an alias called 'name' the executes 'command'.  The command\nmust *not* be enclosed in quotes.  Replaceable parameters are\nindicated by %1, %2, and so on, while %* is replaced by all the\nparameters.  If no command is given, the current alias for name\nis shown. If no name is given, all aliases are listed.\n\nAliases may be nested and can contain anything that can be\nlegally typed at the pdb prompt.  Note!  You *can* override\ninternal pdb commands with aliases!  Those internal commands\nare then hidden until the alias is removed.  Aliasing is recursively\napplied to the first word of the command line; all other words\nin the line are left alone.\n\nSome useful aliases (especially when placed in the .pdbrc file) are:\n\n#Print instance variables (usage \"pi classInst\")\nalias pi for k in %1.__dict__.keys(): print \"%1.\",k,\"=\",%1.__dict__[k]\n\n#Print instance variables in self\nalias ps pi self\n\"\"\"\n\n    def help_unalias(self):\n        print >>self.stdout, \"\"\"unalias name\nDeletes the specified alias.\"\"\"\n\n    def help_commands(self):\n        print >>self.stdout, \"\"\"commands [bpnumber]\n(com) ...\n(com) end\n(Pdb)\n\nSpecify a list of commands for breakpoint number bpnumber.  The\ncommands themselves appear on the following lines.  Type a line\ncontaining just 'end' to terminate the commands.\n\nTo remove all commands from a breakpoint, type commands and\nfollow it immediately with  end; that is, give no commands.\n\nWith no bpnumber argument, commands refers to the last\nbreakpoint set.\n\nYou can use breakpoint commands to start your program up again.\nSimply use the continue command, or step, or any other\ncommand that resumes execution.\n\nSpecifying any command resuming execution (currently continue,\nstep, next, return, jump, quit and their abbreviations) terminates\nthe command list (as if that command was immediately followed by end).\nThis is because any time you resume execution\n(even with a simple next or step), you may encounter\nanother breakpoint--which could have its own command list, leading to\nambiguities about which list to execute.\n\n   If you use the 'silent' command in the command list, the\nusual message about stopping at a breakpoint is not printed.  This may\nbe desirable for breakpoints that are to print a specific message and\nthen continue.  If none of the other commands print anything, you\nsee no sign that the breakpoint was reached.\n\"\"\"\n\n    def help_pdb(self):\n        help()\n\n    def lookupmodule(self, filename):\n        \"\"\"Helper function for break/clear parsing -- may be overridden.\n\n        lookupmodule() translates (possibly incomplete) file or module name\n        into an absolute file name.\n        \"\"\"\n        if os.path.isabs(filename) and  os.path.exists(filename):\n            return filename\n        f = os.path.join(sys.path[0], filename)\n        if  os.path.exists(f) and self.canonic(f) == self.mainpyfile:\n            return f\n        root, ext = os.path.splitext(filename)\n        if ext == '':\n            filename = filename + '.py'\n        if os.path.isabs(filename):\n            return filename\n        for dirname in sys.path:\n            while os.path.islink(dirname):\n                dirname = os.readlink(dirname)\n            fullname = os.path.join(dirname, filename)\n            if os.path.exists(fullname):\n                return fullname\n        return None\n\n    def _runscript(self, filename):\n        # The script has to run in __main__ namespace (or imports from\n        # __main__ will break).\n        #\n        # So we clear up the __main__ and set several special variables\n        # (this gets rid of pdb's globals and cleans old variables on restarts).\n        import __main__\n        __main__.__dict__.clear()\n        __main__.__dict__.update({\"__name__\"    : \"__main__\",\n                                  \"__file__\"    : filename,\n                                  \"__builtins__\": __builtins__,\n                                 })\n\n        # When bdb sets tracing, a number of call and line events happens\n        # BEFORE debugger even reaches user's code (and the exact sequence of\n        # events depends on python version). So we take special measures to\n        # avoid stopping before we reach the main script (see user_line and\n        # user_call for details).\n        self._wait_for_mainpyfile = 1\n        self.mainpyfile = self.canonic(filename)\n        self._user_requested_quit = 0\n        statement = 'execfile(%r)' % filename\n        self.run(statement)\n\n# Simplified interface\n\ndef run(statement, globals=None, locals=None):\n    Pdb().run(statement, globals, locals)\n\ndef runeval(expression, globals=None, locals=None):\n    return Pdb().runeval(expression, globals, locals)\n\ndef runctx(statement, globals, locals):\n    # B/W compatibility\n    run(statement, globals, locals)\n\ndef runcall(*args, **kwds):\n    return Pdb().runcall(*args, **kwds)\n\ndef set_trace():\n    Pdb().set_trace(sys._getframe().f_back)\n\n# Post-Mortem interface\n\ndef post_mortem(t=None):\n    # handling the default\n    if t is None:\n        # sys.exc_info() returns (type, value, traceback) if an exception is\n        # being handled, otherwise it returns None\n        t = sys.exc_info()[2]\n        if t is None:\n            raise ValueError(\"A valid traceback must be passed if no \"\n                                               \"exception is being handled\")\n\n    p = Pdb()\n    p.reset()\n    p.interaction(None, t)\n\ndef pm():\n    post_mortem(sys.last_traceback)\n\n\n# Main program for testing\n\nTESTCMD = 'import x; x.main()'\n\ndef test():\n    run(TESTCMD)\n\n# print help\ndef help():\n    for dirname in sys.path:\n        fullname = os.path.join(dirname, 'pdb.doc')\n        if os.path.exists(fullname):\n            sts = os.system('${PAGER-more} '+fullname)\n            if sts: print '*** Pager exit status:', sts\n            break\n    else:\n        print 'Sorry, can\\'t find the help file \"pdb.doc\"',\n        print 'along the Python search path'\n\ndef main():\n    if not sys.argv[1:] or sys.argv[1] in (\"--help\", \"-h\"):\n        print \"usage: pdb.py scriptfile [arg] ...\"\n        sys.exit(2)\n\n    mainpyfile =  sys.argv[1]     # Get script filename\n    if not os.path.exists(mainpyfile):\n        print 'Error:', mainpyfile, 'does not exist'\n        sys.exit(1)\n\n    del sys.argv[0]         # Hide \"pdb.py\" from argument list\n\n    # Replace pdb's dir with script's dir in front of module search path.\n    sys.path[0] = os.path.dirname(mainpyfile)\n\n    # Note on saving/restoring sys.argv: it's a good idea when sys.argv was\n    # modified by the script being debugged. It's a bad idea when it was\n    # changed by the user from the command line. There is a \"restart\" command\n    # which allows explicit specification of command line arguments.\n    pdb = Pdb()\n    while True:\n        try:\n            pdb._runscript(mainpyfile)\n            if pdb._user_requested_quit:\n                break\n            print \"The program finished and will be restarted\"\n        except Restart:\n            print \"Restarting\", mainpyfile, \"with arguments:\"\n            print \"\\t\" + \" \".join(sys.argv[1:])\n        except SystemExit:\n            # In most cases SystemExit does not warrant a post-mortem session.\n            print \"The program exited via sys.exit(). Exit status: \",\n            print sys.exc_info()[1]\n        except:\n            traceback.print_exc()\n            print \"Uncaught exception. Entering post mortem debugging\"\n            print \"Running 'cont' or 'step' will restart the program\"\n            t = sys.exc_info()[2]\n            pdb.interaction(None, t)\n            print \"Post mortem debugger finished. The \" + mainpyfile + \\\n                  \" will be restarted\"\n\n\n# When invoked as main program, invoke the debugger on a script\nif __name__ == '__main__':\n    import pdb\n    pdb.main()\n", 
    "pickle": "\"\"\"Create portable serialized representations of Python objects.\n\nSee module cPickle for a (much) faster implementation.\nSee module copy_reg for a mechanism for registering custom picklers.\nSee module pickletools source for extensive comments.\n\nClasses:\n\n    Pickler\n    Unpickler\n\nFunctions:\n\n    dump(object, file)\n    dumps(object) -> string\n    load(file) -> object\n    loads(string) -> object\n\nMisc variables:\n\n    __version__\n    format_version\n    compatible_formats\n\n\"\"\"\n\n__version__ = \"$Revision: 72223 $\"       # Code version\n\nfrom types import *\nfrom copy_reg import dispatch_table\nfrom copy_reg import _extension_registry, _inverted_registry, _extension_cache\nimport marshal\nimport sys\nimport struct\nimport re\n\n__all__ = [\"PickleError\", \"PicklingError\", \"UnpicklingError\", \"Pickler\",\n           \"Unpickler\", \"dump\", \"dumps\", \"load\", \"loads\"]\n\n# These are purely informational; no code uses these.\nformat_version = \"2.0\"                  # File format version we write\ncompatible_formats = [\"1.0\",            # Original protocol 0\n                      \"1.1\",            # Protocol 0 with INST added\n                      \"1.2\",            # Original protocol 1\n                      \"1.3\",            # Protocol 1 with BINFLOAT added\n                      \"2.0\",            # Protocol 2\n                      ]                 # Old format versions we can read\n\n# Keep in synch with cPickle.  This is the highest protocol number we\n# know how to read.\nHIGHEST_PROTOCOL = 2\n\n# Why use struct.pack() for pickling but marshal.loads() for\n# unpickling?  struct.pack() is 40% faster than marshal.dumps(), but\n# marshal.loads() is twice as fast as struct.unpack()!\nmloads = marshal.loads\n\nclass PickleError(Exception):\n    \"\"\"A common base class for the other pickling exceptions.\"\"\"\n    pass\n\nclass PicklingError(PickleError):\n    \"\"\"This exception is raised when an unpicklable object is passed to the\n    dump() method.\n\n    \"\"\"\n    pass\n\nclass UnpicklingError(PickleError):\n    \"\"\"This exception is raised when there is a problem unpickling an object,\n    such as a security violation.\n\n    Note that other exceptions may also be raised during unpickling, including\n    (but not necessarily limited to) AttributeError, EOFError, ImportError,\n    and IndexError.\n\n    \"\"\"\n    pass\n\n# An instance of _Stop is raised by Unpickler.load_stop() in response to\n# the STOP opcode, passing the object that is the result of unpickling.\nclass _Stop(Exception):\n    def __init__(self, value):\n        self.value = value\n\n# Jython has PyStringMap; it's a dict subclass with string keys\ntry:\n    from org.python.core import PyStringMap\nexcept ImportError:\n    PyStringMap = None\n\n# UnicodeType may or may not be exported (normally imported from types)\ntry:\n    UnicodeType\nexcept NameError:\n    UnicodeType = None\n\n# Pickle opcodes.  See pickletools.py for extensive docs.  The listing\n# here is in kind-of alphabetical order of 1-character pickle code.\n# pickletools groups them by purpose.\n\nMARK            = '('   # push special markobject on stack\nSTOP            = '.'   # every pickle ends with STOP\nPOP             = '0'   # discard topmost stack item\nPOP_MARK        = '1'   # discard stack top through topmost markobject\nDUP             = '2'   # duplicate top stack item\nFLOAT           = 'F'   # push float object; decimal string argument\nINT             = 'I'   # push integer or bool; decimal string argument\nBININT          = 'J'   # push four-byte signed int\nBININT1         = 'K'   # push 1-byte unsigned int\nLONG            = 'L'   # push long; decimal string argument\nBININT2         = 'M'   # push 2-byte unsigned int\nNONE            = 'N'   # push None\nPERSID          = 'P'   # push persistent object; id is taken from string arg\nBINPERSID       = 'Q'   #  \"       \"         \"  ;  \"  \"   \"     \"  stack\nREDUCE          = 'R'   # apply callable to argtuple, both on stack\nSTRING          = 'S'   # push string; NL-terminated string argument\nBINSTRING       = 'T'   # push string; counted binary string argument\nSHORT_BINSTRING = 'U'   #  \"     \"   ;    \"      \"       \"      \" < 256 bytes\nUNICODE         = 'V'   # push Unicode string; raw-unicode-escaped'd argument\nBINUNICODE      = 'X'   #   \"     \"       \"  ; counted UTF-8 string argument\nAPPEND          = 'a'   # append stack top to list below it\nBUILD           = 'b'   # call __setstate__ or __dict__.update()\nGLOBAL          = 'c'   # push self.find_class(modname, name); 2 string args\nDICT            = 'd'   # build a dict from stack items\nEMPTY_DICT      = '}'   # push empty dict\nAPPENDS         = 'e'   # extend list on stack by topmost stack slice\nGET             = 'g'   # push item from memo on stack; index is string arg\nBINGET          = 'h'   #   \"    \"    \"    \"   \"   \"  ;   \"    \" 1-byte arg\nINST            = 'i'   # build & push class instance\nLONG_BINGET     = 'j'   # push item from memo on stack; index is 4-byte arg\nLIST            = 'l'   # build list from topmost stack items\nEMPTY_LIST      = ']'   # push empty list\nOBJ             = 'o'   # build & push class instance\nPUT             = 'p'   # store stack top in memo; index is string arg\nBINPUT          = 'q'   #   \"     \"    \"   \"   \" ;   \"    \" 1-byte arg\nLONG_BINPUT     = 'r'   #   \"     \"    \"   \"   \" ;   \"    \" 4-byte arg\nSETITEM         = 's'   # add key+value pair to dict\nTUPLE           = 't'   # build tuple from topmost stack items\nEMPTY_TUPLE     = ')'   # push empty tuple\nSETITEMS        = 'u'   # modify dict by adding topmost key+value pairs\nBINFLOAT        = 'G'   # push float; arg is 8-byte float encoding\n\nTRUE            = 'I01\\n'  # not an opcode; see INT docs in pickletools.py\nFALSE           = 'I00\\n'  # not an opcode; see INT docs in pickletools.py\n\n# Protocol 2\n\nPROTO           = '\\x80'  # identify pickle protocol\nNEWOBJ          = '\\x81'  # build object by applying cls.__new__ to argtuple\nEXT1            = '\\x82'  # push object from extension registry; 1-byte index\nEXT2            = '\\x83'  # ditto, but 2-byte index\nEXT4            = '\\x84'  # ditto, but 4-byte index\nTUPLE1          = '\\x85'  # build 1-tuple from stack top\nTUPLE2          = '\\x86'  # build 2-tuple from two topmost stack items\nTUPLE3          = '\\x87'  # build 3-tuple from three topmost stack items\nNEWTRUE         = '\\x88'  # push True\nNEWFALSE        = '\\x89'  # push False\nLONG1           = '\\x8a'  # push long from < 256 bytes\nLONG4           = '\\x8b'  # push really big long\n\n_tuplesize2code = [EMPTY_TUPLE, TUPLE1, TUPLE2, TUPLE3]\n\n\n__all__.extend([x for x in dir() if re.match(\"[A-Z][A-Z0-9_]+$\",x)])\ndel x\n\n\n# Pickling machinery\n\nclass Pickler(object):\n\n    def __init__(self, file, protocol=None):\n        \"\"\"This takes a file-like object for writing a pickle data stream.\n\n        The optional protocol argument tells the pickler to use the\n        given protocol; supported protocols are 0, 1, 2.  The default\n        protocol is 0, to be backwards compatible.  (Protocol 0 is the\n        only protocol that can be written to a file opened in text\n        mode and read back successfully.  When using a protocol higher\n        than 0, make sure the file is opened in binary mode, both when\n        pickling and unpickling.)\n\n        Protocol 1 is more efficient than protocol 0; protocol 2 is\n        more efficient than protocol 1.\n\n        Specifying a negative protocol version selects the highest\n        protocol version supported.  The higher the protocol used, the\n        more recent the version of Python needed to read the pickle\n        produced.\n\n        The file parameter must have a write() method that accepts a single\n        string argument.  It can thus be an open file object, a StringIO\n        object, or any other custom object that meets this interface.\n\n        \"\"\"\n        if protocol is None:\n            protocol = 0\n        if protocol < 0:\n            protocol = HIGHEST_PROTOCOL\n        elif not 0 <= protocol <= HIGHEST_PROTOCOL:\n            raise ValueError(\"pickle protocol must be <= %d\" % HIGHEST_PROTOCOL)\n        self.write = file.write\n        self.memo = {}\n        self.proto = int(protocol)\n        self.bin = protocol >= 1\n        self.fast = 0\n\n    def clear_memo(self):\n        \"\"\"Clears the pickler's \"memo\".\n\n        The memo is the data structure that remembers which objects the\n        pickler has already seen, so that shared or recursive objects are\n        pickled by reference and not by value.  This method is useful when\n        re-using picklers.\n\n        \"\"\"\n        self.memo.clear()\n\n    def dump(self, obj):\n        \"\"\"Write a pickled representation of obj to the open file.\"\"\"\n        if self.proto >= 2:\n            self.write(PROTO + chr(self.proto))\n        self.save(obj)\n        self.write(STOP)\n\n    def memoize(self, obj):\n        \"\"\"Store an object in the memo.\"\"\"\n\n        # The Pickler memo is a dictionary mapping object ids to 2-tuples\n        # that contain the Unpickler memo key and the object being memoized.\n        # The memo key is written to the pickle and will become\n        # the key in the Unpickler's memo.  The object is stored in the\n        # Pickler memo so that transient objects are kept alive during\n        # pickling.\n\n        # The use of the Unpickler memo length as the memo key is just a\n        # convention.  The only requirement is that the memo values be unique.\n        # But there appears no advantage to any other scheme, and this\n        # scheme allows the Unpickler memo to be implemented as a plain (but\n        # growable) array, indexed by memo key.\n        if self.fast:\n            return\n        assert id(obj) not in self.memo\n        memo_len = len(self.memo)\n        self.write(self.put(memo_len))\n        self.memo[id(obj)] = memo_len, obj\n\n    # Return a PUT (BINPUT, LONG_BINPUT) opcode string, with argument i.\n    def put(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINPUT + chr(i)\n            else:\n                return LONG_BINPUT + pack(\"<i\", i)\n\n        return PUT + repr(i) + '\\n'\n\n    # Return a GET (BINGET, LONG_BINGET) opcode string, with argument i.\n    def get(self, i, pack=struct.pack):\n        if self.bin:\n            if i < 256:\n                return BINGET + chr(i)\n            else:\n                return LONG_BINGET + pack(\"<i\", i)\n\n        return GET + repr(i) + '\\n'\n\n    def save(self, obj):\n        # Check for persistent id (defined by a subclass)\n        pid = self.persistent_id(obj)\n        if pid is not None:\n            self.save_pers(pid)\n            return\n\n        # Check the memo\n        x = self.memo.get(id(obj))\n        if x:\n            self.write(self.get(x[0]))\n            return\n\n        # Check the type dispatch table\n        t = type(obj)\n        f = self.dispatch.get(t)\n        if f:\n            f(self, obj) # Call unbound method with explicit self\n            return\n\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(t)\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a class with a custom metaclass; treat as regular class\n            try:\n                issc = issubclass(t, TypeType)\n            except TypeError: # t is not a class (old Boost; see SF #502085)\n                issc = 0\n            if issc:\n                self.save_global(obj)\n                return\n\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise PicklingError(\"Can't pickle %r object: %r\" %\n                                        (t.__name__, obj))\n\n        # Check for string returned by reduce(), meaning \"save as global\"\n        if type(rv) is StringType:\n            self.save_global(obj, rv)\n            return\n\n        # Assert that reduce() returned a tuple\n        if type(rv) is not TupleType:\n            raise PicklingError(\"%s must return string or tuple\" % reduce)\n\n        # Assert that it returned an appropriately sized tuple\n        l = len(rv)\n        if not (2 <= l <= 5):\n            raise PicklingError(\"Tuple returned by %s must have \"\n                                \"two to five elements\" % reduce)\n\n        # Save the reduce() output and finally memoize the object\n        self.save_reduce(obj=obj, *rv)\n\n    def persistent_id(self, obj):\n        # This exists so a subclass can override it\n        return None\n\n    def save_pers(self, pid):\n        # Save a persistent id reference\n        if self.bin:\n            self.save(pid)\n            self.write(BINPERSID)\n        else:\n            self.write(PERSID + str(pid) + '\\n')\n\n    def save_reduce(self, func, args, state=None,\n                    listitems=None, dictitems=None, obj=None):\n        # This API is called by some subclasses\n\n        # Assert that args is a tuple or None\n        if not isinstance(args, TupleType):\n            raise PicklingError(\"args from reduce() should be a tuple\")\n\n        # Assert that func is callable\n        if not hasattr(func, '__call__'):\n            raise PicklingError(\"func from reduce should be callable\")\n\n        save = self.save\n        write = self.write\n\n        # Protocol 2 special case: if func's name is __newobj__, use NEWOBJ\n        if self.proto >= 2 and getattr(func, \"__name__\", \"\") == \"__newobj__\":\n            # A __reduce__ implementation can direct protocol 2 to\n            # use the more efficient NEWOBJ opcode, while still\n            # allowing protocol 0 and 1 to work normally.  For this to\n            # work, the function returned by __reduce__ should be\n            # called __newobj__, and its first argument should be a\n            # new-style class.  The implementation for __newobj__\n            # should be as follows, although pickle has no way to\n            # verify this:\n            #\n            # def __newobj__(cls, *args):\n            #     return cls.__new__(cls, *args)\n            #\n            # Protocols 0 and 1 will pickle a reference to __newobj__,\n            # while protocol 2 (and above) will pickle a reference to\n            # cls, the remaining args tuple, and the NEWOBJ code,\n            # which calls cls.__new__(cls, *args) at unpickling time\n            # (see load_newobj below).  If __reduce__ returns a\n            # three-tuple, the state from the third tuple item will be\n            # pickled regardless of the protocol, calling __setstate__\n            # at unpickling time (see load_build below).\n            #\n            # Note that no standard __newobj__ implementation exists;\n            # you have to provide your own.  This is to enforce\n            # compatibility with Python 2.2 (pickles written using\n            # protocol 0 or 1 in Python 2.3 should be unpicklable by\n            # Python 2.2).\n            cls = args[0]\n            if not hasattr(cls, \"__new__\"):\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has no __new__\")\n            if obj is not None and cls is not obj.__class__:\n                raise PicklingError(\n                    \"args[0] from __newobj__ args has the wrong class\")\n            args = args[1:]\n            save(cls)\n            save(args)\n            write(NEWOBJ)\n        else:\n            save(func)\n            save(args)\n            write(REDUCE)\n\n        if obj is not None:\n            self.memoize(obj)\n\n        # More new special cases (that work with older protocols as\n        # well): when __reduce__ returns a tuple with 4 or 5 items,\n        # the 4th and 5th item should be iterators that provide list\n        # items and dict items (as (key, value) tuples), or None.\n\n        if listitems is not None:\n            self._batch_appends(listitems)\n\n        if dictitems is not None:\n            self._batch_setitems(dictitems)\n\n        if state is not None:\n            save(state)\n            write(BUILD)\n\n    # Methods below this point are dispatched through the dispatch table\n\n    dispatch = {}\n\n    def save_none(self, obj):\n        self.write(NONE)\n    dispatch[NoneType] = save_none\n\n    def save_bool(self, obj):\n        if self.proto >= 2:\n            self.write(obj and NEWTRUE or NEWFALSE)\n        else:\n            self.write(obj and TRUE or FALSE)\n    dispatch[bool] = save_bool\n\n    def save_int(self, obj, pack=struct.pack):\n        if self.bin:\n            # If the int is small enough to fit in a signed 4-byte 2's-comp\n            # format, we can store it more efficiently than the general\n            # case.\n            # First one- and two-byte unsigned ints:\n            if obj >= 0:\n                if obj <= 0xff:\n                    self.write(BININT1 + chr(obj))\n                    return\n                if obj <= 0xffff:\n                    self.write(\"%c%c%c\" % (BININT2, obj&0xff, obj>>8))\n                    return\n            # Next check for 4-byte signed ints:\n            high_bits = obj >> 31  # note that Python shift sign-extends\n            if high_bits == 0 or high_bits == -1:\n                # All high bits are copies of bit 2**31, so the value\n                # fits in a 4-byte signed int.\n                self.write(BININT + pack(\"<i\", obj))\n                return\n        # Text pickle, or int too big to fit in signed 4-byte format.\n        self.write(INT + repr(obj) + '\\n')\n    dispatch[IntType] = save_int\n\n    def save_long(self, obj, pack=struct.pack):\n        if self.proto >= 2:\n            bytes = encode_long(obj)\n            n = len(bytes)\n            if n < 256:\n                self.write(LONG1 + chr(n) + bytes)\n            else:\n                self.write(LONG4 + pack(\"<i\", n) + bytes)\n            return\n        self.write(LONG + repr(obj) + '\\n')\n    dispatch[LongType] = save_long\n\n    def save_float(self, obj, pack=struct.pack):\n        if self.bin:\n            self.write(BINFLOAT + pack('>d', obj))\n        else:\n            self.write(FLOAT + repr(obj) + '\\n')\n    dispatch[FloatType] = save_float\n\n    def save_string(self, obj, pack=struct.pack):\n        if self.bin:\n            n = len(obj)\n            if n < 256:\n                self.write(SHORT_BINSTRING + chr(n) + obj)\n            else:\n                self.write(BINSTRING + pack(\"<i\", n) + obj)\n        else:\n            self.write(STRING + repr(obj) + '\\n')\n        self.memoize(obj)\n    dispatch[StringType] = save_string\n\n    def save_unicode(self, obj, pack=struct.pack):\n        if self.bin:\n            encoding = obj.encode('utf-8')\n            n = len(encoding)\n            self.write(BINUNICODE + pack(\"<i\", n) + encoding)\n        else:\n            obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n            obj = obj.replace(\"\\n\", \"\\\\u000a\")\n            self.write(UNICODE + obj.encode('raw-unicode-escape') + '\\n')\n        self.memoize(obj)\n    dispatch[UnicodeType] = save_unicode\n\n    if StringType is UnicodeType:\n        # This is true for Jython\n        def save_string(self, obj, pack=struct.pack):\n            unicode = obj.isunicode()\n\n            if self.bin:\n                if unicode:\n                    obj = obj.encode(\"utf-8\")\n                l = len(obj)\n                if l < 256 and not unicode:\n                    self.write(SHORT_BINSTRING + chr(l) + obj)\n                else:\n                    s = pack(\"<i\", l)\n                    if unicode:\n                        self.write(BINUNICODE + s + obj)\n                    else:\n                        self.write(BINSTRING + s + obj)\n            else:\n                if unicode:\n                    obj = obj.replace(\"\\\\\", \"\\\\u005c\")\n                    obj = obj.replace(\"\\n\", \"\\\\u000a\")\n                    obj = obj.encode('raw-unicode-escape')\n                    self.write(UNICODE + obj + '\\n')\n                else:\n                    self.write(STRING + repr(obj) + '\\n')\n            self.memoize(obj)\n        dispatch[StringType] = save_string\n\n    def save_tuple(self, obj):\n        write = self.write\n        proto = self.proto\n\n        n = len(obj)\n        if n == 0:\n            if proto:\n                write(EMPTY_TUPLE)\n            else:\n                write(MARK + TUPLE)\n            return\n\n        save = self.save\n        memo = self.memo\n        if n <= 3 and proto >= 2:\n            for element in obj:\n                save(element)\n            # Subtle.  Same as in the big comment below.\n            if id(obj) in memo:\n                get = self.get(memo[id(obj)][0])\n                write(POP * n + get)\n            else:\n                write(_tuplesize2code[n])\n                self.memoize(obj)\n            return\n\n        # proto 0 or proto 1 and tuple isn't empty, or proto > 1 and tuple\n        # has more than 3 elements.\n        write(MARK)\n        for element in obj:\n            save(element)\n\n        if id(obj) in memo:\n            # Subtle.  d was not in memo when we entered save_tuple(), so\n            # the process of saving the tuple's elements must have saved\n            # the tuple itself:  the tuple is recursive.  The proper action\n            # now is to throw away everything we put on the stack, and\n            # simply GET the tuple (it's already constructed).  This check\n            # could have been done in the \"for element\" loop instead, but\n            # recursive tuples are a rare thing.\n            get = self.get(memo[id(obj)][0])\n            if proto:\n                write(POP_MARK + get)\n            else:   # proto 0 -- POP_MARK not available\n                write(POP * (n+1) + get)\n            return\n\n        # No recursion.\n        self.write(TUPLE)\n        self.memoize(obj)\n\n    dispatch[TupleType] = save_tuple\n\n    # save_empty_tuple() isn't used by anything in Python 2.3.  However, I\n    # found a Pickler subclass in Zope3 that calls it, so it's not harmless\n    # to remove it.\n    def save_empty_tuple(self, obj):\n        self.write(EMPTY_TUPLE)\n\n    def save_list(self, obj):\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_LIST)\n        else:   # proto 0 -- can't use EMPTY_LIST\n            write(MARK + LIST)\n\n        self.memoize(obj)\n        self._batch_appends(iter(obj))\n\n    dispatch[ListType] = save_list\n\n    # Keep in synch with cPickle's BATCHSIZE.  Nothing will break if it gets\n    # out of synch, though.\n    _BATCHSIZE = 1000\n\n    def _batch_appends(self, items):\n        # Helper to batch up APPENDS sequences\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for x in items:\n                save(x)\n                write(APPEND)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    x = items.next()\n                    tmp.append(x)\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for x in tmp:\n                    save(x)\n                write(APPENDS)\n            elif n:\n                save(tmp[0])\n                write(APPEND)\n            # else tmp is empty, and we're done\n\n    def save_dict(self, obj):\n        modict_saver = self._pickle_maybe_moduledict(obj)\n        if modict_saver is not None:\n            return self.save_reduce(*modict_saver)\n\n        write = self.write\n\n        if self.bin:\n            write(EMPTY_DICT)\n        else:   # proto 0 -- can't use EMPTY_DICT\n            write(MARK + DICT)\n\n        self.memoize(obj)\n        self._batch_setitems(obj.iteritems())\n\n    dispatch[DictionaryType] = save_dict\n    if not PyStringMap is None:\n        dispatch[PyStringMap] = save_dict\n\n    def _batch_setitems(self, items):\n        # Helper to batch up SETITEMS sequences; proto >= 1 only\n        save = self.save\n        write = self.write\n\n        if not self.bin:\n            for k, v in items:\n                save(k)\n                save(v)\n                write(SETITEM)\n            return\n\n        r = xrange(self._BATCHSIZE)\n        while items is not None:\n            tmp = []\n            for i in r:\n                try:\n                    tmp.append(items.next())\n                except StopIteration:\n                    items = None\n                    break\n            n = len(tmp)\n            if n > 1:\n                write(MARK)\n                for k, v in tmp:\n                    save(k)\n                    save(v)\n                write(SETITEMS)\n            elif n:\n                k, v = tmp[0]\n                save(k)\n                save(v)\n                write(SETITEM)\n            # else tmp is empty, and we're done\n\n    def _pickle_maybe_moduledict(self, obj):\n        # save module dictionary as \"getattr(module, '__dict__')\"\n        try:\n            name = obj['__name__']\n            if type(name) is not str:\n                return None\n            themodule = sys.modules[name]\n            if type(themodule) is not ModuleType:\n                return None\n            if themodule.__dict__ is not obj:\n                return None\n        except (AttributeError, KeyError, TypeError):\n            return None\n\n        return getattr, (themodule, '__dict__')\n\n\n    def save_inst(self, obj):\n        cls = obj.__class__\n\n        memo  = self.memo\n        write = self.write\n        save  = self.save\n\n        if hasattr(obj, '__getinitargs__'):\n            args = obj.__getinitargs__()\n            len(args) # XXX Assert it's a sequence\n            _keep_alive(args, memo)\n        else:\n            args = ()\n\n        write(MARK)\n\n        if self.bin:\n            save(cls)\n            for arg in args:\n                save(arg)\n            write(OBJ)\n        else:\n            for arg in args:\n                save(arg)\n            write(INST + cls.__module__ + '\\n' + cls.__name__ + '\\n')\n\n        self.memoize(obj)\n\n        try:\n            getstate = obj.__getstate__\n        except AttributeError:\n            stuff = obj.__dict__\n        else:\n            stuff = getstate()\n            _keep_alive(stuff, memo)\n        save(stuff)\n        write(BUILD)\n\n    dispatch[InstanceType] = save_inst\n\n    def save_function(self, obj):\n        try:\n            return self.save_global(obj)\n        except PicklingError, e:\n            pass\n        # Check copy_reg.dispatch_table\n        reduce = dispatch_table.get(type(obj))\n        if reduce:\n            rv = reduce(obj)\n        else:\n            # Check for a __reduce_ex__ method, fall back to __reduce__\n            reduce = getattr(obj, \"__reduce_ex__\", None)\n            if reduce:\n                rv = reduce(self.proto)\n            else:\n                reduce = getattr(obj, \"__reduce__\", None)\n                if reduce:\n                    rv = reduce()\n                else:\n                    raise e\n        return self.save_reduce(obj=obj, *rv)\n    dispatch[FunctionType] = save_function\n\n    def save_global(self, obj, name=None, pack=struct.pack):\n        write = self.write\n        memo = self.memo\n\n        if name is None:\n            name = obj.__name__\n\n        module = getattr(obj, \"__module__\", None)\n        if module is None:\n            module = whichmodule(obj, name)\n\n        try:\n            __import__(module)\n            mod = sys.modules[module]\n            klass = getattr(mod, name)\n        except (ImportError, KeyError, AttributeError):\n            raise PicklingError(\n                \"Can't pickle %r: it's not found as %s.%s\" %\n                (obj, module, name))\n        else:\n            if klass is not obj:\n                raise PicklingError(\n                    \"Can't pickle %r: it's not the same object as %s.%s\" %\n                    (obj, module, name))\n\n        if self.proto >= 2:\n            code = _extension_registry.get((module, name))\n            if code:\n                assert code > 0\n                if code <= 0xff:\n                    write(EXT1 + chr(code))\n                elif code <= 0xffff:\n                    write(\"%c%c%c\" % (EXT2, code&0xff, code>>8))\n                else:\n                    write(EXT4 + pack(\"<i\", code))\n                return\n\n        write(GLOBAL + module + '\\n' + name + '\\n')\n        self.memoize(obj)\n\n    dispatch[ClassType] = save_global\n    dispatch[BuiltinFunctionType] = save_global\n    dispatch[TypeType] = save_global\n\n# Pickling helpers\n\ndef _keep_alive(x, memo):\n    \"\"\"Keeps a reference to the object x in the memo.\n\n    Because we remember objects by their id, we have\n    to assure that possibly temporary objects are kept\n    alive by referencing them.\n    We store a reference at the id of the memo, which should\n    normally not be used unless someone tries to deepcopy\n    the memo itself...\n    \"\"\"\n    try:\n        memo[id(memo)].append(x)\n    except KeyError:\n        # aha, this is the first one :-)\n        memo[id(memo)]=[x]\n\n\n# A cache for whichmodule(), mapping a function object to the name of\n# the module in which the function was found.\n\nclassmap = {} # called classmap for backwards compatibility\n\ndef whichmodule(func, funcname):\n    \"\"\"Figure out the module in which a function occurs.\n\n    Search sys.modules for the module.\n    Cache in classmap.\n    Return a module name.\n    If the function cannot be found, return \"__main__\".\n    \"\"\"\n    # Python functions should always get an __module__ from their globals.\n    mod = getattr(func, \"__module__\", None)\n    if mod is not None:\n        return mod\n    if func in classmap:\n        return classmap[func]\n\n    for name, module in sys.modules.items():\n        if module is None:\n            continue # skip dummy package entries\n        if name != '__main__' and getattr(module, funcname, None) is func:\n            break\n    else:\n        name = '__main__'\n    classmap[func] = name\n    return name\n\n\n# Unpickling machinery\n\nclass Unpickler(object):\n\n    def __init__(self, file):\n        \"\"\"This takes a file-like object for reading a pickle data stream.\n\n        The protocol version of the pickle is detected automatically, so no\n        proto argument is needed.\n\n        The file-like object must have two methods, a read() method that\n        takes an integer argument, and a readline() method that requires no\n        arguments.  Both methods should return a string.  Thus file-like\n        object can be a file object opened for reading, a StringIO object,\n        or any other custom object that meets this interface.\n        \"\"\"\n        self.readline = file.readline\n        self.read = file.read\n        self.memo = {}\n\n    def load(self):\n        \"\"\"Read a pickled object representation from the open file.\n\n        Return the reconstituted object hierarchy specified in the file.\n        \"\"\"\n        self.mark = object() # any new unique object\n        self.stack = []\n        self.append = self.stack.append\n        read = self.read\n        dispatch = self.dispatch\n        try:\n            while 1:\n                key = read(1)\n                dispatch[key](self)\n        except _Stop, stopinst:\n            return stopinst.value\n\n    # Return largest index k such that self.stack[k] is self.mark.\n    # If the stack doesn't contain a mark, eventually raises IndexError.\n    # This could be sped by maintaining another stack, of indices at which\n    # the mark appears.  For that matter, the latter stack would suffice,\n    # and we wouldn't need to push mark objects on self.stack at all.\n    # Doing so is probably a good thing, though, since if the pickle is\n    # corrupt (or hostile) we may get a clue from finding self.mark embedded\n    # in unpickled objects.\n    def marker(self):\n        stack = self.stack\n        mark = self.mark\n        k = len(stack)-1\n        while stack[k] is not mark: k = k-1\n        return k\n\n    dispatch = {}\n\n    def load_eof(self):\n        raise EOFError\n    dispatch[''] = load_eof\n\n    def load_proto(self):\n        proto = ord(self.read(1))\n        if not 0 <= proto <= 2:\n            raise ValueError, \"unsupported pickle protocol: %d\" % proto\n    dispatch[PROTO] = load_proto\n\n    def load_persid(self):\n        pid = self.readline()[:-1]\n        self.append(self.persistent_load(pid))\n    dispatch[PERSID] = load_persid\n\n    def load_binpersid(self):\n        pid = self.stack.pop()\n        self.append(self.persistent_load(pid))\n    dispatch[BINPERSID] = load_binpersid\n\n    def load_none(self):\n        self.append(None)\n    dispatch[NONE] = load_none\n\n    def load_false(self):\n        self.append(False)\n    dispatch[NEWFALSE] = load_false\n\n    def load_true(self):\n        self.append(True)\n    dispatch[NEWTRUE] = load_true\n\n    def load_int(self):\n        data = self.readline()\n        if data == FALSE[1:]:\n            val = False\n        elif data == TRUE[1:]:\n            val = True\n        else:\n            try:\n                val = int(data)\n            except ValueError:\n                val = long(data)\n        self.append(val)\n    dispatch[INT] = load_int\n\n    def load_binint(self):\n        self.append(mloads('i' + self.read(4)))\n    dispatch[BININT] = load_binint\n\n    def load_binint1(self):\n        self.append(ord(self.read(1)))\n    dispatch[BININT1] = load_binint1\n\n    def load_binint2(self):\n        self.append(mloads('i' + self.read(2) + '\\000\\000'))\n    dispatch[BININT2] = load_binint2\n\n    def load_long(self):\n        self.append(long(self.readline()[:-1], 0))\n    dispatch[LONG] = load_long\n\n    def load_long1(self):\n        n = ord(self.read(1))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG1] = load_long1\n\n    def load_long4(self):\n        n = mloads('i' + self.read(4))\n        bytes = self.read(n)\n        self.append(decode_long(bytes))\n    dispatch[LONG4] = load_long4\n\n    def load_float(self):\n        self.append(float(self.readline()[:-1]))\n    dispatch[FLOAT] = load_float\n\n    def load_binfloat(self, unpack=struct.unpack):\n        self.append(unpack('>d', self.read(8))[0])\n    dispatch[BINFLOAT] = load_binfloat\n\n    def load_string(self):\n        rep = self.readline()[:-1]\n        for q in \"\\\"'\": # double or single quote\n            if rep.startswith(q):\n                if len(rep) < 2 or not rep.endswith(q):\n                    raise ValueError, \"insecure string pickle\"\n                rep = rep[len(q):-len(q)]\n                break\n        else:\n            raise ValueError, \"insecure string pickle\"\n        self.append(rep.decode(\"string-escape\"))\n    dispatch[STRING] = load_string\n\n    def load_binstring(self):\n        len = mloads('i' + self.read(4))\n        self.append(self.read(len))\n    dispatch[BINSTRING] = load_binstring\n\n    def load_unicode(self):\n        self.append(unicode(self.readline()[:-1],'raw-unicode-escape'))\n    dispatch[UNICODE] = load_unicode\n\n    def load_binunicode(self):\n        len = mloads('i' + self.read(4))\n        self.append(unicode(self.read(len),'utf-8'))\n    dispatch[BINUNICODE] = load_binunicode\n\n    def load_short_binstring(self):\n        len = ord(self.read(1))\n        self.append(self.read(len))\n    dispatch[SHORT_BINSTRING] = load_short_binstring\n\n    def load_tuple(self):\n        k = self.marker()\n        self.stack[k:] = [tuple(self.stack[k+1:])]\n    dispatch[TUPLE] = load_tuple\n\n    def load_empty_tuple(self):\n        self.stack.append(())\n    dispatch[EMPTY_TUPLE] = load_empty_tuple\n\n    def load_tuple1(self):\n        self.stack[-1] = (self.stack[-1],)\n    dispatch[TUPLE1] = load_tuple1\n\n    def load_tuple2(self):\n        self.stack[-2:] = [(self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE2] = load_tuple2\n\n    def load_tuple3(self):\n        self.stack[-3:] = [(self.stack[-3], self.stack[-2], self.stack[-1])]\n    dispatch[TUPLE3] = load_tuple3\n\n    def load_empty_list(self):\n        self.stack.append([])\n    dispatch[EMPTY_LIST] = load_empty_list\n\n    def load_empty_dictionary(self):\n        self.stack.append({})\n    dispatch[EMPTY_DICT] = load_empty_dictionary\n\n    def load_list(self):\n        k = self.marker()\n        self.stack[k:] = [self.stack[k+1:]]\n    dispatch[LIST] = load_list\n\n    def load_dict(self):\n        k = self.marker()\n        d = {}\n        items = self.stack[k+1:]\n        for i in range(0, len(items), 2):\n            key = items[i]\n            value = items[i+1]\n            d[key] = value\n        self.stack[k:] = [d]\n    dispatch[DICT] = load_dict\n\n    # INST and OBJ differ only in how they get a class object.  It's not\n    # only sensible to do the rest in a common routine, the two routines\n    # previously diverged and grew different bugs.\n    # klass is the class to instantiate, and k points to the topmost mark\n    # object, following which are the arguments for klass.__init__.\n    def _instantiate(self, klass, k):\n        args = tuple(self.stack[k+1:])\n        del self.stack[k:]\n        instantiated = 0\n        if (not args and\n                type(klass) is ClassType and\n                not hasattr(klass, \"__getinitargs__\")):\n            try:\n                value = _EmptyClass()\n                value.__class__ = klass\n                instantiated = 1\n            except RuntimeError:\n                # In restricted execution, assignment to inst.__class__ is\n                # prohibited\n                pass\n        if not instantiated:\n            try:\n                value = klass(*args)\n            except TypeError, err:\n                raise TypeError, \"in constructor for %s: %s\" % (\n                    klass.__name__, str(err)), sys.exc_info()[2]\n        self.append(value)\n\n    def load_inst(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self._instantiate(klass, self.marker())\n    dispatch[INST] = load_inst\n\n    def load_obj(self):\n        # Stack is ... markobject classobject arg1 arg2 ...\n        k = self.marker()\n        klass = self.stack.pop(k+1)\n        self._instantiate(klass, k)\n    dispatch[OBJ] = load_obj\n\n    def load_newobj(self):\n        args = self.stack.pop()\n        cls = self.stack[-1]\n        obj = cls.__new__(cls, *args)\n        self.stack[-1] = obj\n    dispatch[NEWOBJ] = load_newobj\n\n    def load_global(self):\n        module = self.readline()[:-1]\n        name = self.readline()[:-1]\n        klass = self.find_class(module, name)\n        self.append(klass)\n    dispatch[GLOBAL] = load_global\n\n    def load_ext1(self):\n        code = ord(self.read(1))\n        self.get_extension(code)\n    dispatch[EXT1] = load_ext1\n\n    def load_ext2(self):\n        code = mloads('i' + self.read(2) + '\\000\\000')\n        self.get_extension(code)\n    dispatch[EXT2] = load_ext2\n\n    def load_ext4(self):\n        code = mloads('i' + self.read(4))\n        self.get_extension(code)\n    dispatch[EXT4] = load_ext4\n\n    def get_extension(self, code):\n        nil = []\n        obj = _extension_cache.get(code, nil)\n        if obj is not nil:\n            self.append(obj)\n            return\n        key = _inverted_registry.get(code)\n        if not key:\n            raise ValueError(\"unregistered extension code %d\" % code)\n        obj = self.find_class(*key)\n        _extension_cache[code] = obj\n        self.append(obj)\n\n    def find_class(self, module, name):\n        # Subclasses may override this\n        __import__(module)\n        mod = sys.modules[module]\n        klass = getattr(mod, name)\n        return klass\n\n    def load_reduce(self):\n        stack = self.stack\n        args = stack.pop()\n        func = stack[-1]\n        value = func(*args)\n        stack[-1] = value\n    dispatch[REDUCE] = load_reduce\n\n    def load_pop(self):\n        del self.stack[-1]\n    dispatch[POP] = load_pop\n\n    def load_pop_mark(self):\n        k = self.marker()\n        del self.stack[k:]\n    dispatch[POP_MARK] = load_pop_mark\n\n    def load_dup(self):\n        self.append(self.stack[-1])\n    dispatch[DUP] = load_dup\n\n    def load_get(self):\n        self.append(self.memo[self.readline()[:-1]])\n    dispatch[GET] = load_get\n\n    def load_binget(self):\n        i = ord(self.read(1))\n        self.append(self.memo[repr(i)])\n    dispatch[BINGET] = load_binget\n\n    def load_long_binget(self):\n        i = mloads('i' + self.read(4))\n        self.append(self.memo[repr(i)])\n    dispatch[LONG_BINGET] = load_long_binget\n\n    def load_put(self):\n        self.memo[self.readline()[:-1]] = self.stack[-1]\n    dispatch[PUT] = load_put\n\n    def load_binput(self):\n        i = ord(self.read(1))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[BINPUT] = load_binput\n\n    def load_long_binput(self):\n        i = mloads('i' + self.read(4))\n        self.memo[repr(i)] = self.stack[-1]\n    dispatch[LONG_BINPUT] = load_long_binput\n\n    def load_append(self):\n        stack = self.stack\n        value = stack.pop()\n        list = stack[-1]\n        list.append(value)\n    dispatch[APPEND] = load_append\n\n    def load_appends(self):\n        stack = self.stack\n        mark = self.marker()\n        list = stack[mark - 1]\n        list.extend(stack[mark + 1:])\n        del stack[mark:]\n    dispatch[APPENDS] = load_appends\n\n    def load_setitem(self):\n        stack = self.stack\n        value = stack.pop()\n        key = stack.pop()\n        dict = stack[-1]\n        dict[key] = value\n    dispatch[SETITEM] = load_setitem\n\n    def load_setitems(self):\n        stack = self.stack\n        mark = self.marker()\n        dict = stack[mark - 1]\n        for i in range(mark + 1, len(stack), 2):\n            dict[stack[i]] = stack[i + 1]\n\n        del stack[mark:]\n    dispatch[SETITEMS] = load_setitems\n\n    def load_build(self):\n        stack = self.stack\n        state = stack.pop()\n        inst = stack[-1]\n        setstate = getattr(inst, \"__setstate__\", None)\n        if setstate:\n            setstate(state)\n            return\n        slotstate = None\n        if isinstance(state, tuple) and len(state) == 2:\n            state, slotstate = state\n        if state:\n            try:\n                d = inst.__dict__\n                try:\n                    for k, v in state.iteritems():\n                        d[intern(k)] = v\n                # keys in state don't have to be strings\n                # don't blow up, but don't go out of our way\n                except TypeError:\n                    d.update(state)\n\n            except RuntimeError:\n                # XXX In restricted execution, the instance's __dict__\n                # is not accessible.  Use the old way of unpickling\n                # the instance variables.  This is a semantic\n                # difference when unpickling in restricted\n                # vs. unrestricted modes.\n                # Note, however, that cPickle has never tried to do the\n                # .update() business, and always uses\n                #     PyObject_SetItem(inst.__dict__, key, value) in a\n                # loop over state.items().\n                for k, v in state.items():\n                    setattr(inst, k, v)\n        if slotstate:\n            for k, v in slotstate.items():\n                setattr(inst, k, v)\n    dispatch[BUILD] = load_build\n\n    def load_mark(self):\n        self.append(self.mark)\n    dispatch[MARK] = load_mark\n\n    def load_stop(self):\n        value = self.stack.pop()\n        raise _Stop(value)\n    dispatch[STOP] = load_stop\n\n# Helper class for load_inst/load_obj\n\nclass _EmptyClass:\n    pass\n\n# Encode/decode longs in linear time.\n\nimport binascii as _binascii\n\ndef encode_long(x):\n    r\"\"\"Encode a long to a two's complement little-endian binary string.\n    Note that 0L is a special case, returning an empty string, to save a\n    byte in the LONG1 pickling context.\n\n    >>> encode_long(0L)\n    ''\n    >>> encode_long(255L)\n    '\\xff\\x00'\n    >>> encode_long(32767L)\n    '\\xff\\x7f'\n    >>> encode_long(-256L)\n    '\\x00\\xff'\n    >>> encode_long(-32768L)\n    '\\x00\\x80'\n    >>> encode_long(-128L)\n    '\\x80'\n    >>> encode_long(127L)\n    '\\x7f'\n    >>>\n    \"\"\"\n\n    if x == 0:\n        return ''\n    if x > 0:\n        ashex = hex(x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # need an even # of nibbles for unhexlify\n            ashex = \"0x0\" + ashex[2:]\n        elif int(ashex[2], 16) >= 8:\n            # \"looks negative\", so need a byte of sign bits\n            ashex = \"0x00\" + ashex[2:]\n    else:\n        # Build the 256's-complement:  (1L << nbytes) + x.  The trick is\n        # to find the number of bytes in linear time (although that should\n        # really be a constant-time task).\n        ashex = hex(-x)\n        assert ashex.startswith(\"0x\")\n        njunkchars = 2 + ashex.endswith('L')\n        nibbles = len(ashex) - njunkchars\n        if nibbles & 1:\n            # Extend to a full byte.\n            nibbles += 1\n        nbits = nibbles * 4\n        x += 1L << nbits\n        assert x > 0\n        ashex = hex(x)\n        njunkchars = 2 + ashex.endswith('L')\n        newnibbles = len(ashex) - njunkchars\n        if newnibbles < nibbles:\n            ashex = \"0x\" + \"0\" * (nibbles - newnibbles) + ashex[2:]\n        if int(ashex[2], 16) < 8:\n            # \"looks positive\", so need a byte of sign bits\n            ashex = \"0xff\" + ashex[2:]\n\n    if ashex.endswith('L'):\n        ashex = ashex[2:-1]\n    else:\n        ashex = ashex[2:]\n    assert len(ashex) & 1 == 0, (x, ashex)\n    binary = _binascii.unhexlify(ashex)\n    return binary[::-1]\n\ndef decode_long(data):\n    r\"\"\"Decode a long from a two's complement little-endian binary string.\n\n    >>> decode_long('')\n    0L\n    >>> decode_long(\"\\xff\\x00\")\n    255L\n    >>> decode_long(\"\\xff\\x7f\")\n    32767L\n    >>> decode_long(\"\\x00\\xff\")\n    -256L\n    >>> decode_long(\"\\x00\\x80\")\n    -32768L\n    >>> decode_long(\"\\x80\")\n    -128L\n    >>> decode_long(\"\\x7f\")\n    127L\n    \"\"\"\n\n    nbytes = len(data)\n    if nbytes == 0:\n        return 0L\n    ashex = _binascii.hexlify(data[::-1])\n    n = long(ashex, 16) # quadratic time before Python 2.3; linear now\n    if data[-1] >= '\\x80':\n        n -= 1L << (nbytes * 8)\n    return n\n\nimport js\n\njdir = js.eval('''function jdir(obj){\n    var L = [];\n    for (var i in obj) {\n        L.push(i);\n    }\n    return L;\n};jdir\n''')\n\njlist = lambda val: [val[i] for i in xrange(val.length)]\n\nPickler.dispatch[js.Number]=lambda pickler, val: pickler.save(float(val))\nPickler.dispatch[js.String]=lambda pickler, val: pickler.save(unicode(val))\nPickler.dispatch[js.Boolean]=lambda pickler, val: pickler.save(bool(val))\nPickler.dispatch[js.Object]=lambda pickler, val: pickler.save({k:val[k] for k in jlist(jdir(val))})\nPickler.dispatch[js.Array]=lambda pickler, val: pickler.save(jlist(val))\n\n# Shorthands\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\ndef dump(obj, file, protocol=None):\n    Pickler(file, protocol).dump(obj)\n\ndef dumps(obj, protocol=None):\n    file = StringIO()\n    Pickler(file, protocol).dump(obj)\n    return file.getvalue()\n\ndef load(file):\n    return Unpickler(file).load()\n\ndef loads(str):\n    file = StringIO(str)\n    return Unpickler(file).load()\n\n# Doctest\n\ndef _test():\n    import doctest\n    return doctest.testmod()\n\nif __name__ == \"__main__\":\n    _test()\n", 
    "posixpath": "\"\"\"Common operations on Posix pathnames.\n\nInstead of importing this module directly, import os and refer to\nthis module as os.path.  The \"os.path\" name is an alias for this\nmodule on Posix systems; on other systems (e.g. Mac, Windows),\nos.path provides the same operations in a manner specific to that\nplatform, and is an alias to another module (e.g. macpath, ntpath).\n\nSome of this can actually be useful on non-Posix systems too, e.g.\nfor manipulation of the pathname component of URLs.\n\"\"\"\n\nimport os\nimport sys\nimport stat\nimport genericpath\nimport warnings\nfrom genericpath import *\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n__all__ = [\"normcase\",\"isabs\",\"join\",\"splitdrive\",\"split\",\"splitext\",\n           \"basename\",\"dirname\",\"commonprefix\",\"getsize\",\"getmtime\",\n           \"getatime\",\"getctime\",\"islink\",\"exists\",\"lexists\",\"isdir\",\"isfile\",\n           \"ismount\",\"walk\",\"expanduser\",\"expandvars\",\"normpath\",\"abspath\",\n           \"samefile\",\"sameopenfile\",\"samestat\",\n           \"curdir\",\"pardir\",\"sep\",\"pathsep\",\"defpath\",\"altsep\",\"extsep\",\n           \"devnull\",\"realpath\",\"supports_unicode_filenames\",\"relpath\"]\n\n# strings representing various path-related bits and pieces\ncurdir = '.'\npardir = '..'\nextsep = '.'\nsep = '/'\npathsep = ':'\ndefpath = ':/bin:/usr/bin'\naltsep = None\ndevnull = '/dev/null'\n\n# Normalize the case of a pathname.  Trivial in Posix, string.lower on Mac.\n# On MS-DOS this may also turn slashes into backslashes; however, other\n# normalizations (such as optimizing '../' away) are not allowed\n# (another function should be defined to do that).\n\ndef normcase(s):\n    \"\"\"Normalize case of pathname.  Has no effect under Posix\"\"\"\n    return s\n\n\n# Return whether a path is absolute.\n# Trivial in Posix, harder on the Mac or MS-DOS.\n\ndef isabs(s):\n    \"\"\"Test whether a path is absolute\"\"\"\n    return s.startswith('/')\n\n\n# Join pathnames.\n# Ignore the previous parts if a part is absolute.\n# Insert a '/' unless the first part is empty or already ends in '/'.\n\ndef join(a, *p):\n    \"\"\"Join two or more pathname components, inserting '/' as needed.\n    If any component is an absolute path, all previous path components\n    will be discarded.  An empty last part will result in a path that\n    ends with a separator.\"\"\"\n    path = a\n    for b in p:\n        if b.startswith('/'):\n            path = b\n        elif path == '' or path.endswith('/'):\n            path +=  b\n        else:\n            path += '/' + b\n    return path\n\n\n# Split a path in head (everything up to the last '/') and tail (the\n# rest).  If the path ends in '/', tail will be empty.  If there is no\n# '/' in the path, head  will be empty.\n# Trailing '/'es are stripped from head unless it is the root.\n\ndef split(p):\n    \"\"\"Split a pathname.  Returns tuple \"(head, tail)\" where \"tail\" is\n    everything after the final slash.  Either part may be empty.\"\"\"\n    i = p.rfind('/') + 1\n    head, tail = p[:i], p[i:]\n    if head and head != '/'*len(head):\n        head = head.rstrip('/')\n    return head, tail\n\n\n# Split a path in root and extension.\n# The extension is everything starting at the last dot in the last\n# pathname component; the root is everything before that.\n# It is always true that root + ext == p.\n\ndef splitext(p):\n    return genericpath._splitext(p, sep, altsep, extsep)\nsplitext.__doc__ = genericpath._splitext.__doc__\n\n# Split a pathname into a drive specification and the rest of the\n# path.  Useful on DOS/Windows/NT; on Unix, the drive is always empty.\n\ndef splitdrive(p):\n    \"\"\"Split a pathname into drive and path. On Posix, drive is always\n    empty.\"\"\"\n    return '', p\n\n\n# Return the tail (basename) part of a path, same as split(path)[1].\n\ndef basename(p):\n    \"\"\"Returns the final component of a pathname\"\"\"\n    i = p.rfind('/') + 1\n    return p[i:]\n\n\n# Return the head (dirname) part of a path, same as split(path)[0].\n\ndef dirname(p):\n    \"\"\"Returns the directory component of a pathname\"\"\"\n    i = p.rfind('/') + 1\n    head = p[:i]\n    if head and head != '/'*len(head):\n        head = head.rstrip('/')\n    return head\n\n\n# Is a path a symbolic link?\n# This will always return false on systems where os.lstat doesn't exist.\n\ndef islink(path):\n    \"\"\"Test whether a path is a symbolic link\"\"\"\n    try:\n        st = os.lstat(path)\n    except (os.error, AttributeError):\n        return False\n    return stat.S_ISLNK(st.st_mode)\n\n# Being true for dangling symbolic links is also useful.\n\ndef lexists(path):\n    \"\"\"Test whether a path exists.  Returns True for broken symbolic links\"\"\"\n    try:\n        os.lstat(path)\n    except os.error:\n        return False\n    return True\n\n\n# Are two filenames really pointing to the same file?\n\ndef samefile(f1, f2):\n    \"\"\"Test whether two pathnames reference the same actual file\"\"\"\n    s1 = os.stat(f1)\n    s2 = os.stat(f2)\n    return samestat(s1, s2)\n\n\n# Are two open files really referencing the same file?\n# (Not necessarily the same file descriptor!)\n\ndef sameopenfile(fp1, fp2):\n    \"\"\"Test whether two open file objects reference the same file\"\"\"\n    s1 = os.fstat(fp1)\n    s2 = os.fstat(fp2)\n    return samestat(s1, s2)\n\n\n# Are two stat buffers (obtained from stat, fstat or lstat)\n# describing the same file?\n\ndef samestat(s1, s2):\n    \"\"\"Test whether two stat buffers reference the same file\"\"\"\n    return s1.st_ino == s2.st_ino and \\\n           s1.st_dev == s2.st_dev\n\n\n# Is a path a mount point?\n# (Does this work for all UNIXes?  Is it even guaranteed to work by Posix?)\n\ndef ismount(path):\n    \"\"\"Test whether a path is a mount point\"\"\"\n    if islink(path):\n        # A symlink can never be a mount point\n        return False\n    try:\n        s1 = os.lstat(path)\n        s2 = os.lstat(join(path, '..'))\n    except os.error:\n        return False # It doesn't exist -- so not a mount point :-)\n    dev1 = s1.st_dev\n    dev2 = s2.st_dev\n    if dev1 != dev2:\n        return True     # path/.. on a different device as path\n    ino1 = s1.st_ino\n    ino2 = s2.st_ino\n    if ino1 == ino2:\n        return True     # path/.. is the same i-node as path\n    return False\n\n\n# Directory tree walk.\n# For each directory under top (including top itself, but excluding\n# '.' and '..'), func(arg, dirname, filenames) is called, where\n# dirname is the name of the directory and filenames is the list\n# of files (and subdirectories etc.) in the directory.\n# The func may modify the filenames list, to implement a filter,\n# or to impose a different order of visiting.\n\ndef walk(top, func, arg):\n    \"\"\"Directory tree walk with callback function.\n\n    For each directory in the directory tree rooted at top (including top\n    itself, but excluding '.' and '..'), call func(arg, dirname, fnames).\n    dirname is the name of the directory, and fnames a list of the names of\n    the files and subdirectories in dirname (excluding '.' and '..').  func\n    may modify the fnames list in-place (e.g. via del or slice assignment),\n    and walk will only recurse into the subdirectories whose names remain in\n    fnames; this can be used to implement a filter, or to impose a specific\n    order of visiting.  No semantics are defined for, or required of, arg,\n    beyond that arg is always passed to func.  It can be used, e.g., to pass\n    a filename pattern, or a mutable object designed to accumulate\n    statistics.  Passing None for arg is common.\"\"\"\n    warnings.warnpy3k(\"In 3.x, os.path.walk is removed in favor of os.walk.\",\n                      stacklevel=2)\n    try:\n        names = os.listdir(top)\n    except os.error:\n        return\n    func(arg, top, names)\n    for name in names:\n        name = join(top, name)\n        try:\n            st = os.lstat(name)\n        except os.error:\n            continue\n        if stat.S_ISDIR(st.st_mode):\n            walk(name, func, arg)\n\n\n# Expand paths beginning with '~' or '~user'.\n# '~' means $HOME; '~user' means that user's home directory.\n# If the path doesn't begin with '~', or if the user or $HOME is unknown,\n# the path is returned unchanged (leaving error reporting to whatever\n# function is called with the expanded path as argument).\n# See also module 'glob' for expansion of *, ? and [...] in pathnames.\n# (A function should also be defined to do full *sh-style environment\n# variable expansion.)\n\ndef expanduser(path):\n    \"\"\"Expand ~ and ~user constructions.  If user or $HOME is unknown,\n    do nothing.\"\"\"\n    if not path.startswith('~'):\n        return path\n    i = path.find('/', 1)\n    if i < 0:\n        i = len(path)\n    if i == 1:\n        if 'HOME' not in os.environ:\n            import pwd\n            userhome = pwd.getpwuid(os.getuid()).pw_dir\n        else:\n            userhome = os.environ['HOME']\n    else:\n        import pwd\n        try:\n            pwent = pwd.getpwnam(path[1:i])\n        except KeyError:\n            return path\n        userhome = pwent.pw_dir\n    userhome = userhome.rstrip('/')\n    return (userhome + path[i:]) or '/'\n\n\n# Expand paths containing shell variable substitutions.\n# This expands the forms $variable and ${variable} only.\n# Non-existent variables are left unchanged.\n\n_varprog = None\n_uvarprog = None\n\ndef expandvars(path):\n    \"\"\"Expand shell variables of form $var and ${var}.  Unknown variables\n    are left unchanged.\"\"\"\n    global _varprog, _uvarprog\n    if '$' not in path:\n        return path\n    if isinstance(path, _unicode):\n        if not _varprog:\n            import re\n            _varprog = re.compile(r'\\$(\\w+|\\{[^}]*\\})')\n        varprog = _varprog\n        encoding = sys.getfilesystemencoding()\n    else:\n        if not _uvarprog:\n            import re\n            _uvarprog = re.compile(_unicode(r'\\$(\\w+|\\{[^}]*\\})'), re.UNICODE)\n        varprog = _uvarprog\n        encoding = None\n    i = 0\n    while True:\n        m = varprog.search(path, i)\n        if not m:\n            break\n        i, j = m.span(0)\n        name = m.group(1)\n        if name.startswith('{') and name.endswith('}'):\n            name = name[1:-1]\n        if encoding:\n            name = name.encode(encoding)\n        if name in os.environ:\n            tail = path[j:]\n            value = os.environ[name]\n            if encoding:\n                value = value.decode(encoding)\n            path = path[:i] + value\n            i = len(path)\n            path += tail\n        else:\n            i = j\n    return path\n\n\n# Normalize a path, e.g. A//B, A/./B and A/foo/../B all become A/B.\n# It should be understood that this may change the meaning of the path\n# if it contains symbolic links!\n\ndef normpath(path):\n    \"\"\"Normalize path, eliminating double slashes, etc.\"\"\"\n    # Preserve unicode (if path is unicode)\n    slash, dot = (u'/', u'.') if isinstance(path, _unicode) else ('/', '.')\n    if path == '':\n        return dot\n    initial_slashes = path.startswith('/')\n    # POSIX allows one or two initial slashes, but treats three or more\n    # as single slash.\n    if (initial_slashes and\n        path.startswith('//') and not path.startswith('///')):\n        initial_slashes = 2\n    comps = path.split('/')\n    new_comps = []\n    for comp in comps:\n        if comp in ('', '.'):\n            continue\n        if (comp != '..' or (not initial_slashes and not new_comps) or\n             (new_comps and new_comps[-1] == '..')):\n            new_comps.append(comp)\n        elif new_comps:\n            new_comps.pop()\n    comps = new_comps\n    path = slash.join(comps)\n    if initial_slashes:\n        path = slash*initial_slashes + path\n    return path or dot\n\n\ndef abspath(path):\n    \"\"\"Return an absolute path.\"\"\"\n    if not isabs(path):\n        if isinstance(path, _unicode):\n            cwd = os.getcwdu()\n        else:\n            cwd = os.getcwd()\n        path = join(cwd, path)\n    return normpath(path)\n\n\n# Return a canonical path (i.e. the absolute location of a file on the\n# filesystem).\n\ndef realpath(filename):\n    \"\"\"Return the canonical path of the specified filename, eliminating any\nsymbolic links encountered in the path.\"\"\"\n    path, ok = _joinrealpath('', filename, {})\n    return abspath(path)\n\n# Join two paths, normalizing ang eliminating any symbolic links\n# encountered in the second path.\ndef _joinrealpath(path, rest, seen):\n    if isabs(rest):\n        rest = rest[1:]\n        path = sep\n\n    while rest:\n        name, _, rest = rest.partition(sep)\n        if not name or name == curdir:\n            # current dir\n            continue\n        if name == pardir:\n            # parent dir\n            if path:\n                path, name = split(path)\n                if name == pardir:\n                    path = join(path, pardir, pardir)\n            else:\n                path = pardir\n            continue\n        newpath = join(path, name)\n        if not islink(newpath):\n            path = newpath\n            continue\n        # Resolve the symbolic link\n        if newpath in seen:\n            # Already seen this path\n            path = seen[newpath]\n            if path is not None:\n                # use cached value\n                continue\n            # The symlink is not resolved, so we must have a symlink loop.\n            # Return already resolved part + rest of the path unchanged.\n            return join(newpath, rest), False\n        seen[newpath] = None # not resolved symlink\n        path, ok = _joinrealpath(path, os.readlink(newpath), seen)\n        if not ok:\n            return join(path, rest), False\n        seen[newpath] = path # resolved symlink\n\n    return path, True\n\n\nsupports_unicode_filenames = (sys.platform == 'darwin')\n\ndef relpath(path, start=curdir):\n    \"\"\"Return a relative version of a path\"\"\"\n\n    if not path:\n        raise ValueError(\"no path specified\")\n\n    start_list = [x for x in abspath(start).split(sep) if x]\n    path_list = [x for x in abspath(path).split(sep) if x]\n\n    # Work out how much of the filepath is shared by start and path.\n    i = len(commonprefix([start_list, path_list]))\n\n    rel_list = [pardir] * (len(start_list)-i) + path_list[i:]\n    if not rel_list:\n        return curdir\n    return join(*rel_list)\n", 
    "pprint": "#  Author:      Fred L. Drake, Jr.\n#               fdrake@acm.org\n#\n#  This is a simple little module I wrote to make life easier.  I didn't\n#  see anything quite like it in the library, though I may have overlooked\n#  something.  I wrote this when I was trying to read some heavily nested\n#  tuples with fairly non-descriptive content.  This is modeled very much\n#  after Lisp/Scheme - style pretty-printing of lists.  If you find it\n#  useful, thank small children who sleep at night.\n\n\"\"\"Support to pretty-print lists, tuples, & dictionaries recursively.\n\nVery simple, but useful, especially in debugging data structures.\n\nClasses\n-------\n\nPrettyPrinter()\n    Handle pretty-printing operations onto a stream using a configured\n    set of formatting parameters.\n\nFunctions\n---------\n\npformat()\n    Format a Python object into a pretty-printed representation.\n\npprint()\n    Pretty-print a Python object to a stream [default is sys.stdout].\n\nsaferepr()\n    Generate a 'standard' repr()-like value, but protect against recursive\n    data structures.\n\n\"\"\"\n\nimport sys as _sys\nimport warnings\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\n__all__ = [\"pprint\",\"pformat\",\"isreadable\",\"isrecursive\",\"saferepr\",\n           \"PrettyPrinter\"]\n\n# cache these for faster access:\n_commajoin = \", \".join\n_id = id\n_len = len\n_type = type\n\n\ndef pprint(object, stream=None, indent=1, width=80, depth=None):\n    \"\"\"Pretty-print a Python object to a stream [default is sys.stdout].\"\"\"\n    printer = PrettyPrinter(\n        stream=stream, indent=indent, width=width, depth=depth)\n    printer.pprint(object)\n\ndef pformat(object, indent=1, width=80, depth=None):\n    \"\"\"Format a Python object into a pretty-printed representation.\"\"\"\n    return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(object)\n\ndef saferepr(object):\n    \"\"\"Version of repr() which can handle recursive data structures.\"\"\"\n    return _safe_repr(object, {}, None, 0)[0]\n\ndef isreadable(object):\n    \"\"\"Determine if saferepr(object) is readable by eval().\"\"\"\n    return _safe_repr(object, {}, None, 0)[1]\n\ndef isrecursive(object):\n    \"\"\"Determine if object requires a recursive representation.\"\"\"\n    return _safe_repr(object, {}, None, 0)[2]\n\ndef _sorted(iterable):\n    with warnings.catch_warnings():\n        if _sys.py3kwarning:\n            warnings.filterwarnings(\"ignore\", \"comparing unequal types \"\n                                    \"not supported\", DeprecationWarning)\n        return sorted(iterable)\n\nclass PrettyPrinter:\n    def __init__(self, indent=1, width=80, depth=None, stream=None):\n        \"\"\"Handle pretty printing operations onto a stream using a set of\n        configured parameters.\n\n        indent\n            Number of spaces to indent for each level of nesting.\n\n        width\n            Attempted maximum number of columns in the output.\n\n        depth\n            The maximum depth to print out nested structures.\n\n        stream\n            The desired output stream.  If omitted (or false), the standard\n            output stream available at construction will be used.\n\n        \"\"\"\n        indent = int(indent)\n        width = int(width)\n        assert indent >= 0, \"indent must be >= 0\"\n        assert depth is None or depth > 0, \"depth must be > 0\"\n        assert width, \"width must be != 0\"\n        self._depth = depth\n        self._indent_per_level = indent\n        self._width = width\n        if stream is not None:\n            self._stream = stream\n        else:\n            self._stream = _sys.stdout\n\n    def pprint(self, object):\n        self._format(object, self._stream, 0, 0, {}, 0)\n        self._stream.write(\"\\n\")\n\n    def pformat(self, object):\n        sio = _StringIO()\n        self._format(object, sio, 0, 0, {}, 0)\n        return sio.getvalue()\n\n    def isrecursive(self, object):\n        return self.format(object, {}, 0, 0)[2]\n\n    def isreadable(self, object):\n        s, readable, recursive = self.format(object, {}, 0, 0)\n        return readable and not recursive\n\n    def _format(self, object, stream, indent, allowance, context, level):\n        level = level + 1\n        objid = _id(object)\n        if objid in context:\n            stream.write(_recursion(object))\n            self._recursive = True\n            self._readable = False\n            return\n        rep = self._repr(object, context, level - 1)\n        typ = _type(object)\n        sepLines = _len(rep) > (self._width - 1 - indent - allowance)\n        write = stream.write\n\n        if self._depth and level > self._depth:\n            write(rep)\n            return\n\n        r = getattr(typ, \"__repr__\", None)\n        if issubclass(typ, dict) and r == dict.__repr__:\n            write('{')\n            if self._indent_per_level > 1:\n                write((self._indent_per_level - 1) * ' ')\n            length = _len(object)\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                items = _sorted(object.items())\n                key, ent = items[0]\n                rep = self._repr(key, context, level)\n                write(rep)\n                write(': ')\n                self._format(ent, stream, indent + _len(rep) + 2,\n                              allowance + 1, context, level)\n                if length > 1:\n                    for key, ent in items[1:]:\n                        rep = self._repr(key, context, level)\n                        if sepLines:\n                            write(',\\n%s%s: ' % (' '*indent, rep))\n                        else:\n                            write(', %s: ' % rep)\n                        self._format(ent, stream, indent + _len(rep) + 2,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            write('}')\n            return\n\n        if ((issubclass(typ, list) and r == list.__repr__) or\n            (issubclass(typ, tuple) and r == tuple.__repr__) or\n            (issubclass(typ, set) and r == set.__repr__) or\n            (issubclass(typ, frozenset) and r == frozenset.__repr__)\n           ):\n            length = _len(object)\n            if issubclass(typ, list):\n                write('[')\n                endchar = ']'\n            elif issubclass(typ, tuple):\n                write('(')\n                endchar = ')'\n            else:\n                if not length:\n                    write(rep)\n                    return\n                write(typ.__name__)\n                write('([')\n                endchar = '])'\n                indent += len(typ.__name__) + 1\n                object = _sorted(object)\n            if self._indent_per_level > 1 and sepLines:\n                write((self._indent_per_level - 1) * ' ')\n            if length:\n                context[objid] = 1\n                indent = indent + self._indent_per_level\n                self._format(object[0], stream, indent, allowance + 1,\n                             context, level)\n                if length > 1:\n                    for ent in object[1:]:\n                        if sepLines:\n                            write(',\\n' + ' '*indent)\n                        else:\n                            write(', ')\n                        self._format(ent, stream, indent,\n                                      allowance + 1, context, level)\n                indent = indent - self._indent_per_level\n                del context[objid]\n            if issubclass(typ, tuple) and length == 1:\n                write(',')\n            write(endchar)\n            return\n\n        write(rep)\n\n    def _repr(self, object, context, level):\n        repr, readable, recursive = self.format(object, context.copy(),\n                                                self._depth, level)\n        if not readable:\n            self._readable = False\n        if recursive:\n            self._recursive = True\n        return repr\n\n    def format(self, object, context, maxlevels, level):\n        \"\"\"Format object for a specific context, returning a string\n        and flags indicating whether the representation is 'readable'\n        and whether the object represents a recursive construct.\n        \"\"\"\n        return _safe_repr(object, context, maxlevels, level)\n\n\n# Return triple (repr_string, isreadable, isrecursive).\n\ndef _safe_repr(object, context, maxlevels, level):\n    typ = _type(object)\n    if typ is str:\n        if 'locale' not in _sys.modules:\n            return repr(object), True, False\n        if \"'\" in object and '\"' not in object:\n            closure = '\"'\n            quotes = {'\"': '\\\\\"'}\n        else:\n            closure = \"'\"\n            quotes = {\"'\": \"\\\\'\"}\n        qget = quotes.get\n        sio = _StringIO()\n        write = sio.write\n        for char in object:\n            if char.isalpha():\n                write(char)\n            else:\n                write(qget(char, repr(char)[1:-1]))\n        return (\"%s%s%s\" % (closure, sio.getvalue(), closure)), True, False\n\n    r = getattr(typ, \"__repr__\", None)\n    if issubclass(typ, dict) and r == dict.__repr__:\n        if not object:\n            return \"{}\", True, False\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return \"{...}\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        saferepr = _safe_repr\n        for k, v in _sorted(object.items()):\n            krepr, kreadable, krecur = saferepr(k, context, maxlevels, level)\n            vrepr, vreadable, vrecur = saferepr(v, context, maxlevels, level)\n            append(\"%s: %s\" % (krepr, vrepr))\n            readable = readable and kreadable and vreadable\n            if krecur or vrecur:\n                recursive = True\n        del context[objid]\n        return \"{%s}\" % _commajoin(components), readable, recursive\n\n    if (issubclass(typ, list) and r == list.__repr__) or \\\n       (issubclass(typ, tuple) and r == tuple.__repr__):\n        if issubclass(typ, list):\n            if not object:\n                return \"[]\", True, False\n            format = \"[%s]\"\n        elif _len(object) == 1:\n            format = \"(%s,)\"\n        else:\n            if not object:\n                return \"()\", True, False\n            format = \"(%s)\"\n        objid = _id(object)\n        if maxlevels and level >= maxlevels:\n            return format % \"...\", False, objid in context\n        if objid in context:\n            return _recursion(object), False, True\n        context[objid] = 1\n        readable = True\n        recursive = False\n        components = []\n        append = components.append\n        level += 1\n        for o in object:\n            orepr, oreadable, orecur = _safe_repr(o, context, maxlevels, level)\n            append(orepr)\n            if not oreadable:\n                readable = False\n            if orecur:\n                recursive = True\n        del context[objid]\n        return format % _commajoin(components), readable, recursive\n\n    rep = repr(object)\n    return rep, (rep and not rep.startswith('<')), False\n\n\ndef _recursion(object):\n    return (\"<Recursion on %s with id=%s>\"\n            % (_type(object).__name__, _id(object)))\n\n\ndef _perfcheck(object=None):\n    import time\n    if object is None:\n        object = [(\"string\", (1, 2), [3, 4], {5: 6, 7: 8})] * 100000\n    p = PrettyPrinter()\n    t1 = time.time()\n    _safe_repr(object, {}, None, 0)\n    t2 = time.time()\n    p.pformat(object)\n    t3 = time.time()\n    print \"_safe_repr:\", t2 - t1\n    print \"pformat:\", t3 - t2\n\nif __name__ == \"__main__\":\n    _perfcheck()\n", 
    "pwd": "# ctypes implementation: Victor Stinner, 2008-05-08\n\"\"\"\nThis module provides access to the Unix password database.\nIt is available on all Unix versions.\n\nPassword database entries are reported as 7-tuples containing the following\nitems from the password database (see `<pwd.h>'), in order:\npw_name, pw_passwd, pw_uid, pw_gid, pw_gecos, pw_dir, pw_shell.\nThe uid and gid items are integers, all others are strings. An\nexception is raised if the entry asked for cannot be found.\n\"\"\"\n\nimport _structseq\n\ntry: from __pypy__ import builtinify\nexcept ImportError: builtinify = lambda f: f\n\n\nclass struct_passwd:\n    \"\"\"\n    pwd.struct_passwd: Results from getpw*() routines.\n\n    This object may be accessed either as a tuple of\n      (pw_name,pw_passwd,pw_uid,pw_gid,pw_gecos,pw_dir,pw_shell)\n    or via the object attributes as named in the above tuple.\n    \"\"\"\n    name = \"pwd.struct_passwd\"\n\n    pw_name = \"pypyjs\"\n    pw_uid = 1000\n    pw_gid = 1000\n    pw_dir = '/'\n    pw_shell = '/lib/pypyjs/pypyjs.js'\n\n\n@builtinify\ndef getpwuid(uid):\n    \"\"\"\n    getpwuid(uid) -> (pw_name,pw_passwd,pw_uid,\n                      pw_gid,pw_gecos,pw_dir,pw_shell)\n    Return the password database entry for the given numeric user ID.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    return struct_passwd()\n\n@builtinify\ndef getpwnam(name):\n    \"\"\"\n    getpwnam(name) -> (pw_name,pw_passwd,pw_uid,\n                        pw_gid,pw_gecos,pw_dir,pw_shell)\n    Return the password database entry for the given user name.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    if not isinstance(name, basestring):\n        raise TypeError(\"expected string\")\n    name = str(name)\n    raise KeyError(\"getpwname(): name not found: %s\" % name)\n\n@builtinify\ndef getpwall():\n    \"\"\"\n    getpwall() -> list_of_entries\n    Return a list of all available password database entries, in arbitrary order.\n    See pwd.__doc__ for more on password database entries.\n    \"\"\"\n    users = []\n    return users\n\n__all__ = ('struct_passwd', 'getpwuid', 'getpwnam', 'getpwall')\n\nif __name__ == \"__main__\":\n# Uncomment next line to test CPython implementation\n#    from pwd import getpwuid, getpwnam, getpwall\n    from os import getuid\n    uid = getuid()\n    pw = getpwuid(uid)\n    print(\"uid %s: %s\" % (pw.pw_uid, pw))\n    name = pw.pw_name\n    print(\"name %r: %s\" % (name, getpwnam(name)))\n    print(\"All:\")\n    for pw in getpwall():\n        print(pw)\n", 
    "py_compile": "\"\"\"Routine to \"compile\" a .py file to a .pyc (or .pyo) file.\n\nThis module has intimate knowledge of the format of .pyc files.\n\"\"\"\n\nimport __builtin__\nimport imp\nimport marshal\nimport os\nimport sys\nimport traceback\n\nMAGIC = imp.get_magic()\n\n__all__ = [\"compile\", \"main\", \"PyCompileError\"]\n\n\nclass PyCompileError(Exception):\n    \"\"\"Exception raised when an error occurs while attempting to\n    compile the file.\n\n    To raise this exception, use\n\n        raise PyCompileError(exc_type,exc_value,file[,msg])\n\n    where\n\n        exc_type:   exception type to be used in error message\n                    type name can be accesses as class variable\n                    'exc_type_name'\n\n        exc_value:  exception value to be used in error message\n                    can be accesses as class variable 'exc_value'\n\n        file:       name of file being compiled to be used in error message\n                    can be accesses as class variable 'file'\n\n        msg:        string message to be written as error message\n                    If no value is given, a default exception message will be given,\n                    consistent with 'standard' py_compile output.\n                    message (or default) can be accesses as class variable 'msg'\n\n    \"\"\"\n\n    def __init__(self, exc_type, exc_value, file, msg=''):\n        exc_type_name = exc_type.__name__\n        if exc_type is SyntaxError:\n            tbtext = ''.join(traceback.format_exception_only(exc_type, exc_value))\n            errmsg = tbtext.replace('File \"<string>\"', 'File \"%s\"' % file)\n        else:\n            errmsg = \"Sorry: %s: %s\" % (exc_type_name,exc_value)\n\n        Exception.__init__(self,msg or errmsg,exc_type_name,exc_value,file)\n\n        self.exc_type_name = exc_type_name\n        self.exc_value = exc_value\n        self.file = file\n        self.msg = msg or errmsg\n\n    def __str__(self):\n        return self.msg\n\n\ndef wr_long(f, x):\n    \"\"\"Internal; write a 32-bit int to a file in little-endian order.\"\"\"\n    f.write(chr( x        & 0xff))\n    f.write(chr((x >> 8)  & 0xff))\n    f.write(chr((x >> 16) & 0xff))\n    f.write(chr((x >> 24) & 0xff))\n\ndef compile(file, cfile=None, dfile=None, doraise=False):\n    \"\"\"Byte-compile one Python source file to Python bytecode.\n\n    Arguments:\n\n    file:    source filename\n    cfile:   target filename; defaults to source with 'c' or 'o' appended\n             ('c' normally, 'o' in optimizing mode, giving .pyc or .pyo)\n    dfile:   purported filename; defaults to source (this is the filename\n             that will show up in error messages)\n    doraise: flag indicating whether or not an exception should be\n             raised when a compile error is found. If an exception\n             occurs and this flag is set to False, a string\n             indicating the nature of the exception will be printed,\n             and the function will return to the caller. If an\n             exception occurs and this flag is set to True, a\n             PyCompileError exception will be raised.\n\n    Note that it isn't necessary to byte-compile Python modules for\n    execution efficiency -- Python itself byte-compiles a module when\n    it is loaded, and if it can, writes out the bytecode to the\n    corresponding .pyc (or .pyo) file.\n\n    However, if a Python installation is shared between users, it is a\n    good idea to byte-compile all modules upon installation, since\n    other users may not be able to write in the source directories,\n    and thus they won't be able to write the .pyc/.pyo file, and then\n    they would be byte-compiling every module each time it is loaded.\n    This can slow down program start-up considerably.\n\n    See compileall.py for a script/module that uses this module to\n    byte-compile all installed files (or all files in selected\n    directories).\n\n    \"\"\"\n    with open(file, 'U') as f:\n        try:\n            timestamp = long(os.fstat(f.fileno()).st_mtime)\n        except AttributeError:\n            timestamp = long(os.stat(file).st_mtime)\n        codestring = f.read()\n    try:\n        codeobject = __builtin__.compile(codestring, dfile or file,'exec')\n    except Exception,err:\n        py_exc = PyCompileError(err.__class__, err, dfile or file)\n        if doraise:\n            raise py_exc\n        else:\n            sys.stderr.write(py_exc.msg + '\\n')\n            return\n    if cfile is None:\n        cfile = file + (__debug__ and 'c' or 'o')\n    with open(cfile, 'wb') as fc:\n        fc.write('\\0\\0\\0\\0')\n        wr_long(fc, timestamp)\n        marshal.dump(codeobject, fc)\n        fc.flush()\n        fc.seek(0, 0)\n        fc.write(MAGIC)\n\ndef main(args=None):\n    \"\"\"Compile several source files.\n\n    The files named in 'args' (or on the command line, if 'args' is\n    not specified) are compiled and the resulting bytecode is cached\n    in the normal manner.  This function does not search a directory\n    structure to locate source files; it only compiles files named\n    explicitly.  If '-' is the only parameter in args, the list of\n    files is taken from standard input.\n\n    \"\"\"\n    if args is None:\n        args = sys.argv[1:]\n    rv = 0\n    if args == ['-']:\n        while True:\n            filename = sys.stdin.readline()\n            if not filename:\n                break\n            filename = filename.rstrip('\\n')\n            try:\n                compile(filename, doraise=True)\n            except PyCompileError as error:\n                rv = 1\n                sys.stderr.write(\"%s\\n\" % error.msg)\n            except IOError as error:\n                rv = 1\n                sys.stderr.write(\"%s\\n\" % error)\n    else:\n        for filename in args:\n            try:\n                compile(filename, doraise=True)\n            except PyCompileError as error:\n                # return value to indicate at least one failure\n                rv = 1\n                sys.stderr.write(error.msg)\n    return rv\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", 
    "pyexcel.__init__": "\"\"\"\n    pyexcel\n    ~~~~~~~~~~~~~~~~~~~\n\n    **pyexcel** is a wrapper library to read, manipulate and\n    write data in different excel formats: csv, ods, xls, xlsx\n    and xlsm. It does not support formulas, styles and charts.\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom pyexcel_io import get_io\nfrom .book import Book\nfrom .writers import Writer, BookWriter\nfrom .sheets import (\n    Sheet,\n    transpose)\nfrom .utils import (\n    to_dict,\n    to_array,\n    to_records,\n    dict_to_array,\n    from_records)\nfrom .formatters import (\n    ColumnFormatter,\n    RowFormatter,\n    SheetFormatter,\n    NamedColumnFormatter,\n    NamedRowFormatter)\nfrom .filters import (\n    ColumnIndexFilter,\n    ColumnFilter,\n    RowFilter,\n    EvenColumnFilter,\n    OddColumnFilter,\n    EvenRowFilter,\n    OddRowFilter,\n    RowIndexFilter,\n    SingleColumnFilter,\n    RowValueFilter,\n    NamedRowValueFilter,\n    ColumnValueFilter,\n    NamedColumnValueFilter,\n    SingleRowFilter)\nfrom .cookbook import (\n    merge_csv_to_a_book,\n    merge_all_to_a_book,\n    split_a_book,\n    extract_a_sheet_from_a_book)\nfrom .sources import (\n    get_sheet,\n    get_book,\n    save_as,\n    save_book_as)\nfrom .deprecated import (\n    load_book,\n    load_book_from_memory,\n    load_book_from_sql,\n    load,\n    load_from_memory,\n    load_from_dict,\n    load_from_sql,\n    load_from_records,\n    Reader,\n    SeriesReader,\n    ColumnSeriesReader,\n    BookReader)\n\n\ndef get_array(**keywords):\n    \"\"\"Obtain an array from an excel source\n\n    :param keywords: see :meth:`~pyexcel.get_sheet`\n    \"\"\"\n    sheet = get_sheet(**keywords)\n    if sheet:\n        return sheet.to_array()\n    else:\n        return None\n\n\ndef get_dict(name_columns_by_row=0, **keywords):\n    \"\"\"Obtain a dictionary from an excel source\n\n    :param name_columns_by_row: specify a row to be a dictionary key.\n                                It is default to 0 or first row. \n    :param keywords: see :meth:`~pyexcel.get_sheet`\n\n    If you would use a column index 0 instead, you should do::\n\n        get_dict(name_columns_by_row=-1, name_rows_by_column=0)\n\n    \"\"\"\n    sheet = get_sheet(name_columns_by_row=name_columns_by_row,\n                      **keywords)\n    if sheet:\n        return sheet.to_dict()\n    else:\n        return None\n\n\ndef get_records(name_columns_by_row=0, **keywords):\n    \"\"\"Obtain a list of records from an excel source\n\n    :param name_columns_by_row: specify a row to be a dictionary key.\n                                It is default to 0 or first row.\n    :param keywords: see :meth:`~pyexcel.get_sheet`\n\n    If you would use a column index 0 instead, you should do::\n\n        get_records(name_columns_by_row=-1, name_rows_by_column=0)\n\n    \"\"\"\n    sheet = get_sheet(name_columns_by_row=name_columns_by_row,\n                      **keywords)\n    if sheet:\n        return sheet.to_records()\n    else:\n        return None\n\n\ndef get_book_dict(**keywords):\n    \"\"\"Obtain a dictionary of two dimensional arrays\n\n    :param keywords: see :meth:`~pyexcel.get_book`\n    \"\"\"\n    book = get_book(**keywords)\n    if book:\n        return book.to_dict()\n    else:\n        return None\n\n\n__VERSION__ = '0.1.8'\n", 
    "pyexcel._compact": "\"\"\"\n    pyexcel._compact\n    ~~~~~~~~~~~~~~~~~~~\n\n    Compatibles\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nimport sys\n\n\nif sys.version_info[0] == 2 and sys.version_info[1] < 7:\n    from ordereddict import OrderedDict\nelse:\n    from collections import OrderedDict\n\nPY2 = sys.version_info[0] == 2\n    \nif PY2:\n    from StringIO import StringIO\n    from StringIO import StringIO as BytesIO\n    text_type = unicode\n    exec('def reraise(tp, value, tb=None):\\n raise tp, value, tb')\n    class Iterator(object):\n        def next(self):\n            return type(self).__next__(self)\n    import urllib2 as request\nelse:\n    from io import StringIO, BytesIO\n    text_type = str\n    def reraise(tp, value, tb=None):\n        if value.__traceback__ is not tb:\n            raise value.with_traceback(tb)\n        raise value\n    Iterator = object\n    import urllib.request as request\n\ndef is_array_type(an_array, atype):\n    tmp = [i for i in an_array if not isinstance(i, atype)]\n    return len(tmp) == 0\n\n\ndef is_string(atype):\n    \"\"\"find out if a type is str or not\"\"\"\n    if atype == str:\n            return True\n    elif PY2:\n        if atype == unicode:\n            return True\n        elif atype == str:\n            return True\n    return False\n", 
    "pyexcel.book": "\"\"\"\n    pyexcel.readers\n    ~~~~~~~~~~~~~~~~~~~\n\n    Uniform interface for describing a excel book\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom .iterators import SheetIterator\nfrom .sheets import Sheet\nfrom .utils import to_dict, local_uuid\nfrom ._compact import OrderedDict\nfrom .presentation import outsource\n\n\nclass Book(object):\n    \"\"\"Read an excel book that has one or more sheets\n\n    For csv file, there will be just one sheet\n    \"\"\"\n    def __init__(self, sheets={}, filename=\"memory\", path=None):\n        \"\"\"Book constructor\n\n        Selecting a specific book according to filename extension\n        :param OrderedDict/dict sheets: a dictionary of data\n        :param str filename: the physical file\n        :param str path: the relative path or abosolute path\n        :param set keywords: additional parameters to be passed on\n        \"\"\"\n        self.path = path\n        self.filename = filename\n        self.name_array = []\n        self.load_from_sheets(sheets)\n\n    def load_from_sheets(self, sheets):\n        \"\"\"Load content from existing sheets\n\n        :param dict sheets: a dictionary of sheets. Each sheet is\n        a list of lists\n        \"\"\"\n        self.sheets = OrderedDict()\n        keys = sheets.keys()\n        if not isinstance(sheets, OrderedDict):\n            # if the end user does not care about the order\n            # we put alphatical order\n            keys = sorted(keys)\n        for name in keys:\n            sheet = self.get_sheet(sheets[name], name)\n            # this sheets keep sheet order\n            self.sheets.update({name: sheet})\n            # this provide the convenience of access the sheet\n            self.__dict__[name] = sheet\n        self.name_array = list(self.sheets.keys())\n\n    def get_sheet(self, array, name):\n        \"\"\"Create a sheet from a list of lists\"\"\"\n        return Sheet(array, name)\n\n    def __iter__(self):\n        return SheetIterator(self)\n\n    def number_of_sheets(self):\n        \"\"\"Return the number of sheets\"\"\"\n        return len(self.name_array)\n\n    def sheet_names(self):\n        \"\"\"Return all sheet names\"\"\"\n        return self.name_array\n\n    def sheet_by_name(self, name):\n        \"\"\"Get the sheet with the specified name\"\"\"\n        return self.sheets[name]\n\n    def sheet_by_index(self, index):\n        \"\"\"Get the sheet with the specified index\"\"\"\n        if index < len(self.name_array):\n            sheet_name = self.name_array[index]\n            return self.sheets[sheet_name]\n\n    def remove_sheet(self, sheet):\n        \"\"\"Remove a sheet\"\"\"\n        if isinstance(sheet, int):\n            if sheet < len(self.name_array):\n                sheet_name = self.name_array[sheet]\n                del self.sheets[sheet_name]\n                self.name_array = list(self.sheets.keys())\n            else:\n                raise IndexError\n        elif isinstance(sheet, str):\n            if sheet in self.name_array:\n                del self.sheets[sheet]\n                self.name_array = list(self.sheets.keys())\n            else:\n                raise KeyError\n        else:\n            raise TypeError\n\n    def __getitem__(self, key):\n        \"\"\"Override operator[]\"\"\"\n        if type(key) == int:\n            return self.sheet_by_index(key)\n        else:\n            return self.sheet_by_name(key)\n\n    def __delitem__(self, other):\n        \"\"\"Override del book[index]\"\"\"\n        self.remove_sheet(other)\n        return self\n\n    def __add__(self, other):\n        \"\"\"Override operator +\n\n        example::\n\n            book3 = book1 + book2\n            book3 = book1 + book2[\"Sheet 1\"]\n\n        \"\"\"\n        content = {}\n        a = to_dict(self)\n        for k in a.keys():\n            new_key = k\n            if len(a.keys()) == 1:\n                new_key = \"%s_%s\" % (self.filename, k)\n            content[new_key] = a[k]\n        if isinstance(other, Book):\n            b = to_dict(other)\n            for l in b.keys():\n                new_key = l\n                if len(b.keys()) == 1:\n                    new_key = other.filename\n                if new_key in content:\n                    uid = local_uuid()\n                    new_key = \"%s_%s\" % (l, uid)\n                content[new_key] = b[l]\n        elif isinstance(other, Sheet):\n            new_key = other.name\n            if new_key in content:\n                uid = local_uuid()\n                new_key = \"%s_%s\" % (other.name, uid)\n            content[new_key] = other.to_array()\n        else:\n            raise TypeError\n        c = Book()\n        c.load_from_sheets(content)\n        return c\n\n    def __iadd__(self, other):\n        \"\"\"Operator overloading +=\n\n        example::\n\n            book += book2\n            book += book2[\"Sheet1\"]\n\n        \"\"\"\n        if isinstance(other, Book):\n            names = other.sheet_names()\n            for name in names:\n                new_key = name\n                if len(names) == 1:\n                    new_key = other.filename\n                if new_key in self.name_array:\n                    uid = local_uuid()\n                    new_key = \"%s_%s\" % (name, uid)\n                self.sheets[new_key] = self.get_sheet(other[name].to_array(),\n                                                      new_key)\n        elif isinstance(other, Sheet):\n            new_key = other.name\n            if new_key in self.name_array:\n                uid = local_uuid()\n                new_key = \"%s_%s\" % (other.name, uid)\n            self.sheets[new_key] = self.get_sheet(other.to_array(), new_key)\n        else:\n            raise TypeError\n        self.name_array = list(self.sheets.keys())\n        return self\n\n    def save_to(self, source):\n        \"\"\"Save to a writeable data source\"\"\"\n        source.write_data(self)\n        \n    def save_as(self, filename):\n        \"\"\"Save the content to a new file\n\n        :param str filename: a file path\n        \"\"\"\n        from .sources import BookSource\n        out_source = BookSource(file_name=filename)\n        self.save_to(out_source)\n\n    def save_to_memory(self, file_type, stream, **keywords):\n        \"\"\"Save the content to a memory stream\n\n        :param file_type: what format the stream is in\n        :param stream: a memory stream.  Note in Python 3, for csv and tsv\n                       format, please pass an instance of StringIO. For xls,\n                       xlsx, and ods, an instance of BytesIO.\n        \"\"\"\n        self.save_as((file_type, stream), **keywords)\n\n    def save_to_django_models(self, models,\n                              initializers=None, mapdicts=None, batch_size=None):\n        \"\"\"Save to database table through django model\n        \n        :param models: a list of database models, that is accepted by\n                       :meth:`Sheet.save_to_django_model`. The sequence of tables\n                       matters when there is dependencies in between the tables.\n                       For example, **Car** is made by **Car Maker**. **Car Maker**\n                       table should be specified before **Car** table.\n        :param initializers: a list of intialization functions for your talbes and\n                             the sequence should match tables,\n        :param mapdicts: custom map dictionary for your data columns and the sequence should\n                   match tables        \n        \"\"\"\n        from .sources import BookDjangoSource\n        out_source = BookDjangoSource(\n            models=models,\n            initializers=initializers,\n            mapdicts=mapdicts,\n            batch_size=batch_size\n        )\n        self.save_to(out_source)\n\n    def save_to_database(self, session, tables, initializers=None, mapdicts=None,\n                         auto_commit=True):\n        \"\"\"Save data in sheets to database tables\n\n        :param session: database session\n        :param tables: a list of database tables, that is accepted by\n                       :meth:`Sheet.save_to_database`. The sequence of tables matters\n                       when there is dependencies in between the tables. For example,\n                       **Car** is made by **Car Maker**. **Car Maker** table should\n                       be specified before **Car** table.\n        :param initializers: a list of intialization functions for your tables and\n                             the sequence should match tables,\n        :param mapdicts: custom map dictionary for your data columns and the sequence should\n                   match tables\n        :param auto_commit: by default, data is committed.\n        \n        \"\"\"\n        from .sources import BookSQLSource\n        out_source = BookSQLSource(\n            session=session,\n            tables=tables,\n            initializers=initializers,\n            mapdicts=mapdicts,\n            auto_commit=auto_commit\n        )\n        self.save_to(out_source)\n\n    def to_dict(self):\n        \"\"\"Convert the book to a dictionary\"\"\"\n        from .utils import to_dict\n        return to_dict(self)\n\n    def __repr__(self):\n        return self.__str__()\n\n    @outsource\n    def __str__(self):\n        ret = \"\"\n        for sheet in self.sheets:\n            ret += str(self.sheets[sheet])\n            ret += \"\\n\"\n        return ret.strip('\\n')\n", 
    "pyexcel.constants": "\"\"\"\n    pyexcel.constants\n    ~~~~~~~~~~~~~~~~~~~\n\n    Constants appeared in pyexcel\n\n    :copyright: (c) 2015 by Onni Software Ltd.\n    :license: New BSD License\n\"\"\"\nDEFAULT_NAME = 'pyexcel'\nDEFAULT_SHEET_NAME = 'pyexcel_sheet1'\n\nMESSAGE_WARNING = \"We do not overwrite files\"\nMESSAGE_WRITE_ERROR = \"Cannot write sheet\"\nMESSAGE_ERROR_02 = \"No valid parameters found!\"\nMESSAGE_DATA_ERROR_NO_SERIES = \"No column names or row names found\"\nMESSAGE_DATA_ERROR_EMPTY_COLUMN_LIST = \"Column list is empty. Do not waste resource\"\nMESSAGE_DATA_ERROR_COLUMN_LIST_INTEGER_TYPE = \"Column list should be a list of integers\"\nMESSAGE_DATA_ERROR_COLUMN_LIST_STRING_TYPE = \"Column list should be a list of integers\"\nMESSAGE_INDEX_OUT_OF_RANGE = \"Index out of range\"\nMESSAGE_DATA_ERROR_EMPTY_CONTENT = \"Nothing to be pasted!\"\nMESSAGE_DATA_ERROR_DATA_TYPE_MISMATCH = \"Data type mismatch\"\nMESSAGE_DATA_ERROR_ORDEREDDICT_IS_EXPECTED = \"Please give a ordered list\"\n\nMESSAGE_DEPRECATED_ROW_COLUMN = \"Deprecated usage. Please use [row, column]\"\nMESSAGE_DEPRECATED_OUT_FILE = \"Depreciated usage of 'out_file'. please use dest_file_name\"\nMESSAGE_DEPRECATED_CONTENT = \"Depreciated usage of 'content'. please use file_content\"\n\nMESSAGE_NOT_IMPLEMENTED_01 = \"Please use attribute row or column to extend sheet\"\nMESSAGE_NOT_IMPLEMENTED_02 = \"Confused! What do you want to put as column names\"\nMESSAGE_READONLY = \"This attribute is readonly\"\nMESSAGE_ERROR_NO_HANDLER = \"No suitable plugins imported or installed\"\n\n# Used by sources\nKEYWORD_MEMORY = 'memory'\nKEYWORD_SOURCE = 'source'\nKEYWORD_FILE_TYPE = 'file_type'\nKEYWORD_FILE_NAME = 'file_name'\nKEYWORD_FILE_STREAM = 'file_stream'\nKEYWORD_SESSION = 'session'\nKEYWORD_TABLE = 'table'\nKEYWORD_MODEL = 'model'\nKEYWORD_TABLES = 'tables'\nKEYWORD_MODELS = 'models'\nDEPRECATED_KEYWORD_CONTENT = 'content'\nKEYWORD_FILE_CONTENT = 'file_content'\nKEYWORD_ADICT = 'adict'\nKEYWORD_RECORDS = 'records'\nKEYWORD_ARRAY = 'array'\nKEYWORD_COLUMN_NAMES = 'column_names'\nKEYWORD_QUERY_SETS = 'query_sets'\nDEPRECATED_KEYWORD_OUT_FILE = 'out_file'\nKEYWORD_BOOKDICT = 'bookdict'\nKEYWORD_MAPDICT = 'mapdict'\nKEYWORD_MAPDICTS = 'mapdicts'\nKEYWORD_INITIALIZER = 'initializer'\nKEYWORD_INITIALIZERS = 'initializers'\nKEYWORD_BATCH_SIZE = 'batch_size'\nKEYWORD_URL = 'url'\nKEYWORD_STARTS_WITH_DEST = '^dest_(.*)'\n", 
    "pyexcel.cookbook": "\"\"\"\n    pyexcel.cookbook\n    ~~~~~~~~~~~~~~~~~~~\n\n    Cookbook for pyexcel\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nimport os\nfrom .book import Book\nfrom .sources import get_book, get_sheet\nfrom .utils import to_dict, to_array\nfrom .writers import Writer, BookWriter\nfrom ._compact import OrderedDict\nfrom .constants import MESSAGE_WARNING\n\n\nDEFAULT_OUT_FILE = 'pyexcel_merged.csv'\nDEFAULT_OUT_XLS_FILE = 'merged.xls'\nOUT_FILE_FORMATTER = 'pyexcel_%s'\n\ndef update_columns(infilename, column_dicts, outfilename=None):\n    \"\"\"Update one or more columns of a data file with series\n\n    The data structure of column_dicts should be:\n    key should be first row of the column\n    the rest of the value should an array\n    :param str infilename: an accessible file name\n    :param dict column_dicts: dictionaries of columns\n    :param str outfilename: save the sheet as\n\n\n    \"\"\"\n    default_out_file = OUT_FILE_FORMATTER % infilename\n    if outfilename:\n        default_out_file = outfilename\n    if os.path.exists(default_out_file):\n        raise NotImplementedError(MESSAGE_WARNING)\n    r = get_sheet(file_name=infilename, name_columns_by_row=0)\n    series = r.colnames\n    for k in column_dicts.keys():\n        index = series.index(str(k))\n        r.set_column_at(index, column_dicts[k])\n    w = Writer(default_out_file)\n    w.write_reader(r)\n    w.close()\n\n\ndef update_rows(infilename, row_dicts, outfilename=None):\n    \"\"\"Update one or more rows of a data file with series\n\n    datastucture: key should an integer of the row to be updated\n    value should be an array of the data\n    :param str infilename: an accessible file name\n    :param dict row_dicts: dictionaries of rows\n    :param str outfilename: save the sheet as\n    \"\"\"\n    default_out_file = OUT_FILE_FORMATTER % infilename\n    if outfilename:\n        default_out_file = outfilename\n    if os.path.exists(default_out_file):\n        raise NotImplementedError(MESSAGE_WARNING)\n    r = get_sheet(file_name=infilename, name_rows_by_column=0)\n    series = r.rownames\n    for k in row_dicts.keys():\n        index = series.index(str(k))\n        r.set_row_at(index, row_dicts[k])\n    r.save_as(default_out_file)\n\n\ndef merge_files(file_array, outfilename=DEFAULT_OUT_FILE):\n    \"\"\"merge many files horizontally column after column\n    :param str outfilename: save the sheet as\n    \"\"\"\n    if os.path.exists(outfilename):\n        raise NotImplementedError(MESSAGE_WARNING)\n    content = []\n    for f in file_array:\n        r = get_sheet(file_name=f)\n        content.extend(to_array(r.columns()))\n    w = Writer(outfilename)\n    w.write_columns(content)\n    w.close()\n    return outfilename\n\n\ndef merge_two_files(file1, file2, outfilename=DEFAULT_OUT_FILE):\n    \"\"\"merge two files\n    \n    :param str file1: an accessible file name\n    :param str file2: an accessible file name\n    :param str outfilename: save the sheet as\n    \"\"\"\n    if os.path.exists(outfilename):\n        raise NotImplementedError(MESSAGE_WARNING)\n    files = [file1, file2]\n    merge_files(files, outfilename)\n\n\ndef merge_readers(reader_array, outfilename=DEFAULT_OUT_FILE):\n    \"\"\"merge many readers\n\n    With FilterableReader and SeriesReader, you can do custom filtering\n    :param str outfilename: save the sheet as\n    \"\"\"\n    if os.path.exists(outfilename):\n        raise NotImplementedError(MESSAGE_WARNING)\n    content = OrderedDict()\n    for r in reader_array:\n        content.update(to_dict(r))\n    w = Writer(outfilename)\n    w.write_dict(content)\n    w.close()\n\n\ndef merge_two_readers(reader1, reader2, outfilename=DEFAULT_OUT_FILE):\n    \"\"\"merge two readers\n\n    :param str outfilename: save the sheet as\n\n    \"\"\"\n    if os.path.exists(outfilename):\n        raise NotImplementedError(MESSAGE_WARNING)\n    reader_array = [reader1, reader2]\n    merge_readers(reader_array, outfilename)\n\n\ndef merge_csv_to_a_book(filelist, outfilename=DEFAULT_OUT_XLS_FILE):\n    \"\"\"merge a list of csv files into a excel book\n\n    :param list filelist: a list of accessible file path\n    :param str outfilename: save the sheet as\n    \"\"\"\n    w = BookWriter(outfilename)\n    for file in filelist:\n        r = get_sheet(file_name=file)\n        head, tail = os.path.split(file)\n        sheet = w.create_sheet(tail)\n        sheet.write_reader(r)\n        sheet.close()\n    w.close()\n\n\ndef merge_all_to_a_book(filelist, outfilename=DEFAULT_OUT_XLS_FILE):\n    \"\"\"merge a list of excel files into a excel book\n\n    :param list filelist: a list of accessible file path\n    :param str outfilename: save the sheet as\n    \"\"\"\n    merged = Book()\n    for file in filelist:\n        merged += get_book(file_name=file)\n    w = BookWriter(outfilename)\n    w.write_book_reader(merged)\n    w.close()\n\n\ndef split_a_book(file, outfilename=None):\n    \"\"\"Split a file into separate sheets\n    \n    :param str file: an accessible file name\n    :param str outfilename: save the sheets with file suffix\n    \"\"\"\n    r = get_book(file_name=file)\n    if outfilename:\n        saveas = outfilename\n    else:\n        saveas = file\n    for sheet in r:\n        w = Writer(\"%s_%s\" % (sheet.name, saveas))\n        w.write_reader(sheet)\n        w.close()\n\n\ndef extract_a_sheet_from_a_book(file, sheetname, outfilename=None):\n    \"\"\"Extract a sheet from a excel book\n\n    :param str file: an accessible file\n    :param str sheetname: a valid sheet name\n    :param str outfilename: save the sheet as\n    \"\"\"\n    r = get_book(file_name=file)\n    if outfilename:\n        saveas = outfilename\n    else:\n        saveas = file\n    sheet = r[sheetname]\n    w = Writer(\"%s_%s\" % (sheetname, saveas))\n    w.write_reader(sheet)\n    w.close()\n", 
    "pyexcel.deprecated": "\"\"\"\n    pyexcel.deprecated\n    ~~~~~~~~~~~~~~~~~~~\n\n    List of apis that become deprecated but was kept for backward compatibility\n\n    :copyright: (c) 2015 by Onni Software Ltd.\n    :license: New BSD License\n\"\"\"\nfrom .sources import get_sheet, get_book\nfrom functools import partial\n\n\ndef deprecated(func, message=\"Deprecated!\"):\n    def inner(*arg, **keywords):\n        print(message)\n        return func(*arg, **keywords)\n    return inner\n\n\ndeprecated_loader = partial(\n    deprecated,\n    message=\"Deprecated since v0.1.5! Please use get_sheet instead.\")\ndeprecated_book_loader = partial(\n    deprecated,\n    message=\"Deprecated since v0.1.5! Please use get_book instead.\")\n\n\n@deprecated_book_loader\ndef load_book(file, **keywords):\n    \"\"\"Load content from physical file\n\n    :param str file: the file name\n    :param any keywords: additional parameters\n    \"\"\"\n    return get_book(file_name=file, **keywords)\n\n\n@deprecated_book_loader\ndef load_book_from_memory(file_type, file_content, **keywords):\n    \"\"\"Load content from memory content\n\n    :param tuple the_tuple: first element should be file extension,\n    second element should be file content\n    :param any keywords: additional parameters\n    \"\"\"\n    return get_book(file_type=file_type, file_content=file_content, **keywords)\n\n\n@deprecated_book_loader\ndef load_book_from_sql(session, tables):\n    \"\"\"Get an instance of :class:`Book` from a list of tables\n\n    :param session: sqlalchemy session\n    :param tables: a list of database tables\n    \"\"\"\n    return get_book(session=session, tables=tables)\n\n\n@deprecated_book_loader\ndef load_book_from_django_models(models):\n    \"\"\"Get an instance of :class:`Book` from a list of tables\n\n    :param session: sqlalchemy session\n    :param tables: a list of database tables\n    \"\"\"\n    return get_book(models=models)\n\n\n@deprecated_loader\ndef load(file, sheetname=None, **keywords):\n    \"\"\"Constructs an instance :class:`Sheet` from a sheet of an excel file\n\n    except csv, most excel files has more than one sheet.\n    Hence sheetname is required here to indicate from which sheet the instance\n    should be constructed. If this parameter is omitted, the first sheet, which\n    is indexed at 0, is used. For csv, sheetname is always omitted because csv\n    file contains always one sheet.\n    :param str sheetname: which sheet to be used for construction\n    :param int name_colmns_by_row: which row to give column names\n    :param int name_rows_by_column: which column to give row names\n    :param dict keywords: other parameters\n    \"\"\"\n    if isinstance(file, tuple):\n        sheet = get_sheet(file_type=file[0],\n                          file_content=file[1],\n                          sheet_name=sheetname,\n                          **keywords)\n    else:\n        sheet = get_sheet(file_name=file, sheet_name=sheetname, **keywords)\n    return sheet\n\n\n@deprecated_loader\ndef load_from_memory(file_type,\n                     file_content,\n                     sheetname=None,\n                     **keywords):\n    \"\"\"Constructs an instance :class:`Sheet` from memory\n\n    :param str file_type: one value of these: 'csv', 'tsv', 'csvz',\n    'tsvz', 'xls', 'xlsm', 'xslm', 'ods'\n    :param iostream file_content: file content\n    :param str sheetname: which sheet to be used for construction\n    :param dict keywords: any other parameters\n    \"\"\"\n    return get_sheet(file_type=file_type,\n                     file_content=file_content,\n                     sheet_name=sheetname,\n                     **keywords)\n\n\n@deprecated_loader\ndef load_from_query_sets(column_names, query_sets, **keywords):\n    \"\"\"Constructs an instance :class:`Sheet` from a database query sets\n    :param column_names: the field names\n    :param query_sets: the values\n    :returns: :class:`Sheet`\n    \"\"\"\n    return get_sheet(column_names=column_names, query_sets=query_sets)\n\n\n@deprecated_loader\ndef load_from_sql(session, table, **keywords):\n    \"\"\"Constructs an instance :class:`Sheet` from database table\n\n    :param session: SQLAlchemy session object\n    :param table: SQLAlchemy database table\n    :returns: :class:`Sheet`\n    \"\"\"\n    return get_sheet(session=session, table=table, **keywords)\n\n\n@deprecated_loader\ndef load_from_django_model(model, **keywords):\n    \"\"\"Constructs an instance :class:`Sheet` from a django model\n\n    :param model: Django model\n    :returns: :class:`Sheet`\n    \"\"\"\n    return get_sheet(model=model, **keywords)\n\n\n@deprecated_loader\ndef load_from_dict(the_dict, with_keys=True, **keywords):\n    \"\"\"Return a sheet from a dictionary of one dimensional arrays\n\n    :param dict the_dict: its value should be one dimensional array\n    :param bool with_keys: indicate if dictionary keys should be\n                           appended or not\n    \"\"\"\n    return get_sheet(adict=the_dict, with_keys=with_keys, **keywords)\n\n\n@deprecated_loader\ndef load_from_records(records, **keywords):\n    \"\"\"Return a sheet from a list of records\n\n    Sheet.to_records() would produce a list of dictionaries. All dictionaries\n    share the same keys.\n    :params list records: records are likely to be produced by\n                          Sheet.to_records() method.\n    \"\"\"\n    return get_sheet(records=records, **keywords)\n\n\n@partial(deprecated,\n         message=\"Deprecated since v0.0.7! Please use class Sheet instead\")\ndef Reader(file=None, sheetname=None, **keywords):\n    \"\"\"\n    A single sheet excel file reader\n\n    Default is the sheet at index 0. Or you specify one using sheet index\n    or sheet name. The short coming of this reader is: column filter is\n    applied first then row filter is applied next\n\n    use as class would fail though\n    changed since 0.0.7\n    \"\"\"\n    if isinstance(file, tuple):\n        return get_sheet(file_type=file[0],\n                         file_content=file[1],\n                         sheet_name=sheetname,\n                         **keywords)\n    else:\n        return get_sheet(file_name=file, sheet_name=sheetname, **keywords)\n\n\n@partial(deprecated,\n         message=\"Deprecated since v0.0.7! Please use class Sheet(..., name_columns_by_row=0,..) instead\")\ndef SeriesReader(file=None, sheetname=None, series=0, **keywords):\n    \"\"\"A single sheet excel file reader and it has column headers in a selected row\n\n    use as class would fail\n    changed since 0.0.7\n    \"\"\"\n    if isinstance(file, tuple):\n        return get_sheet(file_type=file[0],\n                         file_content=file[1],\n                         name_columns_by_row=series,\n                         **keywords)\n    else:\n        return load(file,\n                    sheetname=sheetname,\n                    name_columns_by_row=series,\n                    **keywords)\n\n\n@partial(deprecated,\n         message=\"Please use class Sheet(..., name_rows_by_column=0..) instead\")\ndef ColumnSeriesReader(file=None, sheetname=None, series=0, **keywords):\n    \"\"\"A single sheet excel file reader and it has row headers in a selected column\n\n    use as class would fail\n    changed since 0.0.7\n    \"\"\"\n    if isinstance(file, tuple):\n        return get_sheet(file_type=file[0],\n                         file_content=file[1],\n                         name_rows_by_column=series,\n                         **keywords)\n    else:\n        return load(file,\n                    sheetname=sheetname,\n                    name_rows_by_column=series,\n                    **keywords)\n\n\n@partial(deprecated,\n         message=\"Deprecated since v0.0.7! Please use class Book instead\")\ndef BookReader(file, **keywords):\n    \"\"\"For backward compatibility\n    \"\"\"\n    return load_book(file, **keywords)\n", 
    "pyexcel.filters": "\"\"\"\n    pyexcel.filters\n    ~~~~~~~~~~~~~~~\n\n    Filtering functions for pyexcel readers\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\n    Design note for filter algorithm::\n\n        #1 2 3 4 5 6 7  <- original index\n        #  x     x\n        #1   3 4   6 7  <- filtered index\n        #1   2 3   4 5  <- actual index after filtering\n\n    Design note for multiple filter algorithm::\n\n        #    1 2 3 4 5 6 7 8 9\n        f1     x       x\n             1   2 3 4   5 6 7\n        f2       x   x     x\n             1     2     3   4\n        f3         x\n             1           2   3\n\"\"\"\nfrom ._compact import PY2\n\n\nclass IndexFilter:\n    \"\"\"A generic index filter\"\"\"\n    def __init__(self, func):\n        \"\"\"Constructor\n        :param Function func: a evaluation function\n        \"\"\"\n        self.eval_func = func\n        self.shallow_eval_func = None\n        # indices to be filtered out\n        self.indices = None\n\n    def invert(self):\n        if self.eval_func:\n            if self.shallow_eval_func is None:\n                self.shallow_eval_func = self.eval_func\n                self.eval_func = lambda value: not self.shallow_eval_func(value)\n            else:\n                self.eval_func = self.shallow_eval_func\n                self.shallow_eval_func = None\n        return self\n        \n    def rows(self):\n        \"\"\"Rows that were filtered out\n        \"\"\"\n        return 0\n\n    def columns(self):\n        \"\"\"Columns that were filtered out\"\"\"\n        return 0\n\n    def validate_filter(self, reader):\n        \"\"\"\n        Find out which column index to be filtered\n\n        :param Matrix reader: a Matrix instance\n\n        \"\"\"\n        pass\n\n    def translate(self, row, column):\n        \"\"\"Map the row, column after filtering to the\n        original ones before filtering\"\"\"\n        pass\n\n\nclass RegionFilter(IndexFilter):\n    \"\"\"Filter on both row index and column index\"\"\"\n\n    def __init__(self, row_slice, column_slice):\n        \"\"\"Constructor\n\n        :param slice row_slice: row index range\n        :param slice column_slice: column index range\n        \"\"\"\n        self.row_indices = range(row_slice.start,\n                                 row_slice.stop,\n                                 row_slice.step)\n        self.column_indices = range(column_slice.start,\n                                    column_slice.stop,\n                                    column_slice.step)\n        if not PY2:\n            self.row_indices = list(self.row_indices)\n            self.column_indices = list(self.column_indices)\n\n    def columns(self):\n        \"\"\"Columns that were filtered out\"\"\"\n        return len(self.column_indices)\n\n    def rows(self):\n        \"\"\"Rows that were filtered out\"\"\"\n        return len(self.row_indices)\n\n    def validate_filter(self, reader):\n        self.row_indices = [i for i in reader.row_range()\n                            if i not in self.row_indices]\n        self.column_indices = [i for i in reader.column_range()\n                               if i not in self.column_indices]\n\n    def translate(self, row, column):\n        \"\"\"Map the row, column after filtering to the\n        original ones before filtering\n\n        :param int row: row index after filtering\n        :param int column: column index after filtering\n        :returns: set of (row, new_column)\n        \"\"\"\n        new_column = column\n        if self.column_indices:\n            for i in self.column_indices:\n                if i <= new_column:\n                    new_column += 1\n        new_row = row\n        if self.row_indices:\n            for i in self.row_indices:\n                if i <= new_row:\n                    new_row += 1\n        return new_row, new_column\n\n\nclass ColumnIndexFilter(IndexFilter):\n    \"\"\"A column filter that operates on column indices\"\"\"\n    def columns(self):\n        \"\"\"Columns that were filtered out\"\"\"\n        return len(self.indices)\n\n    def validate_filter(self, reader):\n        \"\"\"\n        Find out which column index to be filtered\n\n        :param Matrix reader: a Matrix instance\n        \"\"\"\n        self.indices = [i for i in reader.column_range() if self.eval_func(i)]\n\n    def translate(self, row, column):\n        \"\"\"Map the row, column after filtering to the\n        original ones before filtering\n\n        :param int row: row index after filtering\n        :param int column: column index after filtering\n        :returns: set of (row, new_column)\n        \"\"\"\n        if self.indices:\n            new_column = column\n            for i in self.indices:\n                if i <= new_column:\n                    new_column += 1\n            return row, new_column\n        else:\n            return row, column\n\n\nclass ColumnFilter(ColumnIndexFilter):\n    \"\"\"Filters out a list of columns\"\"\"\n    def __init__(self, indices):\n        \"\"\"Constructor\n\n        :param list indices: a list of column indices to be filtered out\n        \"\"\"\n        eval_func = lambda x: x in indices\n        ColumnIndexFilter.__init__(self, eval_func)\n\n\nclass SingleColumnFilter(ColumnIndexFilter):\n    \"\"\"Filters out a single column index\"\"\"\n    def __init__(self, index):\n        \"\"\"Constructor\n\n        :param list indices: a list of column indices to be filtered out\n        \"\"\"\n        eval_func = lambda x: x == index\n        ColumnIndexFilter.__init__(self, eval_func)\n\n\nclass OddColumnFilter(ColumnIndexFilter):\n    \"\"\"Filters out odd indexed columns\n\n    * column 0 is regarded as the first column.\n    * column 1 is regarded as the seocond column -> this will be filtered out\n    \"\"\"\n    def __init__(self):\n        eval_func = lambda x: (x+1) % 2 == 1\n        ColumnIndexFilter.__init__(self, eval_func)\n\n\nclass EvenColumnFilter(ColumnIndexFilter):\n    \"\"\"Filters out even indexed columns\n\n    * column 0 is regarded as the first column. -> this will be filtered out\n    * column 1 is regarded as the seocond column\n    \"\"\"\n    def __init__(self):\n        eval_func = lambda x: (x+1) % 2 == 0\n        ColumnIndexFilter.__init__(self, eval_func)\n\n\nclass RowIndexFilter(IndexFilter):\n    \"\"\"Filter out rows by its row index \"\"\"\n    def rows(self):\n        \"\"\"number of rows to be filtered out\"\"\"\n        if self.indices:\n            return len(self.indices)\n        else:\n            return 0\n\n    def validate_filter(self, reader):\n        \"\"\"\n        Find out which column index to be filtered\n\n        :param Matrix reader: a Matrix instance\n        \"\"\"\n        self.indices = [i for i in reader.row_range() if self.eval_func(i)]\n\n    def translate(self, row, column):\n        \"\"\"Map the row, column after filtering to the\n        original ones before filtering\n\n        :param int row: row index after filtering\n        :param int column: column index after filtering\n        :returns: set of (row, new_column)\n        \"\"\"\n        if self.indices:\n            new_row = row\n            for i in self.indices:\n                if i <= new_row:\n                    new_row += 1\n            return new_row, column\n        else:\n            return row, column\n\n\nclass RowFilter(RowIndexFilter):\n    \"\"\"Filters a list of rows\"\"\"\n    def __init__(self, indices):\n        \"\"\"Constructor\n\n        :param list indices: a list of column indices to be filtered out\n        \"\"\"\n        eval_func = lambda x: x in indices\n        RowIndexFilter.__init__(self, eval_func)\n\n\nclass SingleRowFilter(RowIndexFilter):\n    \"\"\"Filters out a single row\"\"\"\n    def __init__(self, index):\n        \"\"\"Constructor\n\n        :param list indices: a list of column indices to be filtered out\n        \"\"\"\n        eval_func = lambda x: x == index\n        RowIndexFilter.__init__(self, eval_func)\n\n\nclass OddRowFilter(RowIndexFilter):\n    \"\"\"Filters out odd indexed rows\n\n    row 0 is seen as the first row\n    \"\"\"\n    def __init__(self):\n        eval_func = lambda x: (x+1) % 2 == 1\n        RowIndexFilter.__init__(self, eval_func)\n\n\nclass EvenRowFilter(RowIndexFilter):\n    \"\"\"Filters out even indexed rows\n\n    row 0 is seen as the first row\n    \"\"\"\n    def __init__(self):\n        eval_func = lambda x: (x+1) % 2 == 0\n        RowIndexFilter.__init__(self, eval_func)\n\n\nclass RowValueFilter(RowIndexFilter):\n    \"\"\"Filters out rows based on its row values\n\n    .. note:: it takes time as it needs to go through all values\n    \"\"\"\n    def validate_filter(self, reader):\n        \"\"\"\n        Filter out the row indices\n\n        This is what it does::\n\n            new_indices = []\n            index = 0\n            for r in reader.rows():\n                if not self.eval_func(r):\n                    new_indices.append(index)\n                index += 1\n\n        :param Matrix reader: a Matrix instance\n        \"\"\"\n        self.indices = [row[0]\n                        for row in enumerate(reader.rows())\n                        if self.eval_func(row[1])]\n\n\nclass NamedRowValueFilter(RowIndexFilter):\n    \"\"\"Filter out rows that satisfy a condition\n\n    .. note:: it takes time as it needs to go through all values\n    \"\"\"\n    def validate_filter(self, reader):\n        \"\"\"\n        Filter out the row indices\n\n        This is what it does::\n\n            new_indices = []\n            index = 0\n            for r in reader.rows():\n                if not self.eval_func(r):\n                    new_indices.append(index)\n                index += 1\n\n        :param Matrix reader: a Matrix instance\n        \"\"\"\n        series = reader.colnames\n        self.indices = [row[0]\n                        for row in enumerate(reader.rows())\n                        if self.eval_func(dict(zip(series, row[1])))]\n\n\nclass SeriesRowValueFilter(NamedRowValueFilter):\n    \"\"\"Backword compactibility\"\"\"\n    pass\n\n\nclass ColumnValueFilter(ColumnIndexFilter):\n    \"\"\"Filters out rows based on its row values\n\n    .. note:: it takes time as it needs to go through all values\n    \"\"\"\n    def validate_filter(self, reader):\n        \"\"\"\n        Filter out the row indices\n\n        This is what it does::\n\n            new_indices = []\n            index = 0\n            for r in reader.rows():\n                if not self.eval_func(r):\n                    new_indices.append(index)\n                index += 1\n\n        :param Matrix reader: a Matrix instance\n        \"\"\"\n        self.indices = [column[0]\n                        for column in enumerate(reader.columns())\n                        if self.eval_func(column[1])]\n\n\nclass NamedColumnValueFilter(ColumnIndexFilter):\n    \"\"\"Filter out rows that satisfy a condition\n\n    .. note:: it takes time as it needs to go through all values\n    \"\"\"\n    def validate_filter(self, reader):\n        \"\"\"\n        Filter out the row indices\n\n        This is what it does::\n\n            new_indices = []\n            index = 0\n            for r in reader.rows():\n                if not self.eval_func(r):\n                    new_indices.append(index)\n                index += 1\n\n        :param Matrix reader: a Matrix instance\n        \"\"\"\n        series = reader.rownames\n        self.indices = [column[0]\n                        for column in enumerate(reader.columns())\n                        if self.eval_func(dict(zip(series, column[1])))]\n\n\nclass RowInFileFilter(RowValueFilter):\n    \"\"\"Filter out rows that has a row from another reader\"\"\"\n    def __init__(self, reader):\n        \"\"\"\n        Constructor\n\n        :param Matrix reader: a Matrix instance\n        \"\"\"\n        func = lambda row_a: not reader.contains((lambda row_b: row_a == row_b))\n        RowValueFilter.__init__(self, func)\n", 
    "pyexcel.formatters": "\"\"\"\n    pyexcel.formatters\n    ~~~~~~~~~~~~~~~~~~~\n\n    These utilities help format the content\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nimport types\nimport datetime\nfrom ._compact import is_array_type, PY2\nfrom .constants import (\n    MESSAGE_DATA_ERROR_EMPTY_COLUMN_LIST,\n    MESSAGE_DATA_ERROR_COLUMN_LIST_INTEGER_TYPE,\n    MESSAGE_DATA_ERROR_COLUMN_LIST_STRING_TYPE\n)\n\ndef string_to_format(value, FORMAT):\n    \"\"\"Convert string to specified format\"\"\"\n    if FORMAT == float:\n        try:\n            ret = float(value)\n        except ValueError:\n            ret = value\n    elif FORMAT == int:\n        try:\n            ret = float(value)\n            ret = int(ret)\n        except ValueError:\n            ret = value\n    else:\n        ret = value\n\n    return ret\n\n\ndef float_to_format(value, FORMAT):\n    \"\"\"Convert float to specified format\"\"\"\n    if FORMAT == int:\n        ret = int(value)\n    elif FORMAT == str:\n        ret = str(value)\n    else:\n        ret = value\n\n    return ret\n\n\ndef int_to_format(value, FORMAT):\n    \"\"\"Convert int to specified format\"\"\"\n    if FORMAT == float:\n        ret = float(value)\n    elif FORMAT == str:\n        ret = str(value)\n    else:\n        ret = value\n    return ret\n\n\ndef date_to_format(value, FORMAT):\n    \"\"\"Convert date to specified format\"\"\"\n    if FORMAT == str:\n        if isinstance(value, datetime.date):\n            ret = value.strftime(\"%d/%m/%y\")\n        elif isinstance(value, datetime.datetime):\n            ret = value.strftime(\"%d/%m/%y\")\n        elif isinstance(value, datetime.time):\n            ret = value.strftime(\"%H:%M:%S\")\n    else:\n        ret = value\n    return ret\n\n\ndef boolean_to_format(value, FORMAT):\n    \"\"\"Convert bool to specified format\"\"\"\n    if FORMAT == float:\n        ret = float(value)\n    elif FORMAT == str:\n        if value == 1:\n            ret = \"true\"\n        else:\n            ret = \"false\"\n    else:\n        ret = value\n    return ret\n\n\ndef empty_to_format(value, FORMAT):\n    \"\"\"Convert empty value to specified format\"\"\"\n    if FORMAT == float:\n        ret = 0.0\n    elif FORMAT == int:\n        ret = 0\n    else:\n        ret = \"\"\n    return ret\n\n\nCONVERSION_FUNCTIONS = {\n    str: string_to_format,\n    float: float_to_format,\n    int: int_to_format,\n    datetime.datetime: date_to_format,\n    datetime.time: date_to_format,\n    datetime.date: date_to_format,\n    bool: boolean_to_format,\n    None: empty_to_format,\n}\n\nif PY2:\n    CONVERSION_FUNCTIONS[unicode] = string_to_format\n\n\ndef to_format(to_type, value):\n    \"\"\"Wrapper utility function for format different formats\n\n    :param type from_type: a python type\n    :param type to_type: a python type\n    :param value value: a python value\n    \"\"\"\n    if value is not None:\n        if value == \"\":\n            from_type = None\n        else:\n            from_type = type(value)\n    else:\n        from_type = None\n    func = CONVERSION_FUNCTIONS[from_type]\n    return func(value, to_type)\n\n\nclass Formatter:\n    \"\"\"Generic formatter\n\n    Formatter starts when the quanlifying functions returns true\n    cell's row, column and value are fed to the quanlifying functions\n    \"\"\"\n    def __init__(self, quanlify_func, formatter, custom_converter=None):\n        self.quanlify_func = quanlify_func\n        self.formatter = formatter\n        self.converter = custom_converter\n\n    def is_my_business(self, row, column, value):\n        \"\"\"Check should this formatter be active for cell at (row, column) with value\n\n        :param int row: the row index of current cell\n        :param int column: the column index of current cell\n        :param any value: the value of current cell\n        :returns: True or False\n            * True if the cell qualitifies\n            * False if the cell does not\n        \n        \"\"\"\n        return self.quanlify_func(row, column, value)\n\n    def do_format(self, value):\n        new_value = value\n        if value == \"\":\n            new_value = None\n        if isinstance(self.formatter, types.FunctionType):\n            return self.formatter(new_value)\n        else:\n            return to_format(self.formatter, new_value)\n\n\nclass ColumnFormatter(Formatter):\n    \"\"\"Apply formatting on columns\"\"\"\n    def __init__(self, column_index, formatter):\n        \"\"\"Constructor\n        \n        :param int or list column_index: to which column or what columns to apply the formatter\n        :param type FORMAT: the target format\n        :param func custom_converter: the custom functional formatter\n        \n        \"\"\"\n        self.indices = column_index\n        if isinstance(column_index, int):\n            func = lambda r, c, v: c == column_index\n        elif isinstance(column_index, list):\n            if len(column_index) == 0:\n                raise IndexError(MESSAGE_DATA_ERROR_EMPTY_COLUMN_LIST)\n            if is_array_type(column_index, int):\n                func = lambda r, c, v: c in column_index\n            else:\n                raise IndexError(MESSAGE_DATA_ERROR_COLUMN_LIST_INTEGER_TYPE)\n        else:\n            raise NotImplementedError(\"%s is not supported\" % type(column_index))\n        Formatter.__init__(self, func, formatter)\n\n\nclass NamedColumnFormatter(ColumnFormatter):\n    \"\"\"Apply formatting using named columns\"\"\"\n    def __init__(self, column_index, formatter):\n        \"\"\"Constructor\n        \n        :param int or list column_index: to which column or what columns to apply the formatter\n        :param type FORMAT: the target format\n        :param func custom_converter: the custom functional formatter\n        \n        \"\"\"\n        self.indices = column_index\n        if isinstance(column_index, str):\n            func = lambda r, c, v: c == column_index\n        elif isinstance(column_index, list):\n            if len(column_index) == 0:\n                raise IndexError(MESSAGE_DATA_ERROR_EMPTY_COLUMN_LIST)\n            if is_array_type(column_index, str):\n                func = lambda r, c, v: c in column_index\n            else:\n                raise IndexError(MESSAGE_DATA_ERROR_COLUMN_LIST_STRING_TYPE)\n        else:\n            raise NotImplementedError(\"%s is not supported\" % type(column_index))\n        Formatter.__init__(self, func, formatter)\n\n    def update_index(self, new_indices):\n        self.indices = new_indices\n        if isinstance(new_indices, int):\n            self.quanlify_func = lambda r, c, v: c == new_indices\n        elif isinstance(new_indices, list):\n            self.quanlify_func = lambda r, c, v: c in new_indices\n        else:\n            raise NotImplementedError(\"%s is not supported\" % type(new_indices))\n\nclass RowFormatter(Formatter):\n    \"\"\"Row Formatter\"\"\"    \n    def __init__(self, row_index, formatter):\n        \"\"\"Constructor\n\n        :param int or list row_index: to which row or what rows to apply the formatter\n        :param type FORMAT: the target format\n        :param func custom_converter: the custom functional formatter\n        \n        \"\"\"\n        self.indices = row_index\n        if isinstance(row_index, int):\n            func = lambda r, c, v: r == row_index\n        elif isinstance(row_index, list):\n            if len(row_index) == 0:\n                raise IndexError(MESSAGE_DATA_ERROR_EMPTY_COLUMN_LIST)\n            if is_array_type(row_index, int):\n                func = lambda r, c, v: r in row_index\n            else:\n                raise IndexError(MESSAGE_DATA_ERROR_COLUMN_LIST_INTEGER_TYPE)\n        else:\n            raise NotImplementedError(\"%s is not supported\" % type(row_index))\n        Formatter.__init__(self, func, formatter)\n\n\nclass NamedRowFormatter(RowFormatter):\n    \"\"\"Formatting rows using named rows\"\"\"    \n    def __init__(self, row_index, formatter):\n        \"\"\"Constructor\n\n        :param int or list row_index: to which row or what rows to apply the formatter\n        :param type FORMAT: the target format\n        :param func custom_converter: the custom functional formatter\n        \"\"\"\n        self.indices = row_index\n        if isinstance(row_index, str):\n            func = lambda r, c, v: r == row_index\n        elif isinstance(row_index, list):\n            if len(row_index) == 0:\n                raise IndexError(MESSAGE_DATA_ERROR_EMPTY_COLUMN_LIST)\n            if is_array_type(row_index, str):\n                func = lambda r, c, v: r in row_index\n            else:\n                raise IndexError(MESSAGE_DATA_ERROR_COLUMN_LIST_STRING_TYPE)\n        else:\n            raise NotImplementedError(\"%s is not supported\" % type(row_index))\n        Formatter.__init__(self, func, formatter)\n            \n    def update_index(self, new_indices):\n        if isinstance(new_indices, int):\n            self.quanlify_func = lambda r, c, v: r == new_indices\n        elif isinstance(new_indices, list):\n            self.quanlify_func = lambda r, c, v: r in new_indices\n        else:\n            raise NotImplementedError(\"%s is not supported\" % type(new_indices))\n            \n\nclass SheetFormatter(Formatter):\n    \"\"\"Apply the formatter to all cells in the sheet\n    \"\"\"\n    def __init__(self, formatter):\n        Formatter.__init__(self, None, formatter)\n\n    def is_my_business(self, row, column, value):\n        return True\n", 
    "pyexcel.iterators": "\"\"\"\n    pyexcel.iterators\n    ~~~~~~~~~~~~~~~~~~~\n\n    Iterate through the two dimensional arrays\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\n\n\nclass PyexcelIterator:\n    \"\"\"\n    A parent class is used to distiguish pyexcel iterators in pyexcel utilities\n    \"\"\"\n    def __next__(self):\n        return self.next()\n\nclass HTLBRIterator(PyexcelIterator):\n    \"\"\"\n    Horizontal Top Left to Bottom Right Iterator\n\n    Iterate horizontally from top left to bottom right.\n    see :func:`Matrix.enumerate` for more details\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.current = 0\n        self.columns = reader.number_of_columns()\n        self.rows = reader.number_of_rows()\n        self.total = self.columns * self.rows\n\n    def __iter__(self):\n        return self\n\n    def next_cell_position(self):\n        \"\"\"\n        Determine next cell position\n        \"\"\"\n        return (int(self.current / self.columns),\n                int(self.current % self.columns))\n\n    def move_cursor(self):\n        \"\"\"\n        move internal cursor\n        \"\"\"\n        self.current += 1\n\n    def get_next_value(self):\n        \"\"\"\n        Get next value\n        \"\"\"\n        row, column = self.next_cell_position()\n        self.move_cursor()\n        return self.reader_ref.cell_value(row, column)\n\n    def exit_condition(self):\n        \"\"\"\n        Determine if all data have been iterated\n        \"\"\"\n        return self.current >= self.total\n\n    def next(self):\n        \"\"\"\n        determine next value\n\n        this function is further divided into small functions\n        so that other kind of iterators can easily change\n        its behavior\n        \"\"\"\n        if self.exit_condition():\n            raise StopIteration\n        else:\n            return self.get_next_value()\n\n\nclass VTLBRIterator(HTLBRIterator):\n    \"\"\"\n    Vertical Top Left to Bottom Right Iterator\n\n    Iterate vertically from top left to bottom right\n    see :func:`Matrix.vertical` for more details\n    \"\"\"\n    def next_cell_position(self):\n        \"\"\"\n        this function controls the iterator's path\n        \"\"\"\n        return (int(self.current % self.rows),\n                int(self.current / self.rows))\n\n\nclass HBRTLIterator(HTLBRIterator):\n    \"\"\"\n    Horizontal Bottom Right to Top Left Iterator\n\n    Iterate horizontally from bottom right to top left\n    see :func:`Matrix.reverse` for more details\n    \"\"\"\n\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.columns = reader.number_of_columns()\n        self.rows = reader.number_of_rows()\n        self.current = self.rows * self.columns\n\n    def exit_condition(self):\n        return self.current <= 0\n\n    def move_cursor(self):\n        self.current -= 1\n\n    def get_next_value(self):\n        self.move_cursor()\n        row, column = self.next_cell_position()\n        return self.reader_ref.cell_value(row, column)\n\n\nclass VBRTLIterator(HBRTLIterator):\n    \"\"\"\n    Vertical Bottom Right to Top Left Iterator\n\n    Iterate vertically from bottom right to top left\n    see :func:`Matrix.rvertical` for more details\n    \"\"\"\n    def next_cell_position(self):\n        return (int(self.current % self.rows),\n                int(self.current / self.rows))\n\n\nclass HTRBLIterator(PyexcelIterator):\n    \"\"\"\n    Horizontal Top Right to Bottom Left Iterator\n\n    Iterate horizontally from top right to bottom left::\n\n        <<S\n        <<<\n        E<<\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.columns = reader.number_of_columns()\n        self.rows = reader.number_of_rows()\n        self.total = self.rows * self.columns\n        self.row = 0\n        self.column = self.columns\n\n    def __iter__(self):\n        return self\n\n    def get_next_value(self):\n        self.column -= 1\n        if self.column == -1:\n            self.column = self.columns - 1\n            self.row += 1\n        return self.reader_ref.cell_value(self.row, self.column)\n\n    def exit_condition(self):\n        return self.column == 0 and self.row == (self.rows - 1)\n\n    def next(self):\n        if self.exit_condition():\n            raise StopIteration\n        else:\n            return self.get_next_value()\n\n\nclass VTRBLIterator(HTRBLIterator):\n    \"\"\"\n    Vertical Top Right to Bottom Left Iterator\n\n    Iterate horizontally from top left to bottom right::\n\n        ||S\n        |||\n        E||\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.columns = reader.number_of_columns()\n        self.rows = reader.number_of_rows()\n        self.total = self.rows * self.columns\n        self.row = -1\n        self.column = self.columns - 1\n\n    def get_next_value(self):\n        self.row += 1\n        if self.row >= self.rows:\n            self.column -= 1\n            self.row = 0\n        return self.reader_ref.cell_value(self.row, self.column)\n\n\nclass VBLTRIterator(HTRBLIterator):\n    \"\"\"\n    Vertical Bottom Left to Top Right Iterator\n\n    Iterate vertically from bottom left to top right::\n\n        ^^E\n        ^^^\n        S^^\n        ->\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.columns = reader.number_of_columns()\n        self.rows = reader.number_of_rows()\n        self.total = self.rows * self.columns\n        self.row = self.rows\n        self.column = 0\n\n    def __iter__(self):\n        return self\n\n    def get_next_value(self):\n        self.row -= 1\n        if self.row == -1:\n            self.row = self.rows - 1\n            self.column += 1\n        return self.reader_ref.cell_value(self.row, self.column)\n\n    def exit_condition(self):\n        return self.row == 0 and self.column == (self.columns - 1)\n\n\nclass HBLTRIterator(VBLTRIterator):\n    \"\"\"\n    Horizontal Bottom Left to Top Right Iterator\n\n    Iterate horizontally from bottom left to top right\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.columns = reader.number_of_columns()\n        self.rows = reader.number_of_rows()\n        self.total = self.rows * self.columns\n        self.row = self.rows - 1\n        self.column = -1\n\n    def __iter__(self):\n        return self\n\n    def get_next_value(self):\n        self.column += 1\n        if self.column >= self.columns:\n            self.row -= 1\n            self.column = 0\n        return self.reader_ref.cell_value(self.row, self.column)\n\n\nclass RowIterator(PyexcelIterator):\n    \"\"\"\n    Iterate data row by row from top to bottom\n\n    default iterator for :class:`Matrix`.\n    See :func:`Matrix.rows` for more details\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.current = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        if self.current in self.reader_ref.row_range():\n            row = self.current\n            self.current += 1\n            return self.reader_ref.row_at(row)\n        else:\n            raise StopIteration\n\n\nclass NamedRowIterator(RowIterator):\n    \"\"\"\n    Iterate data dictionary row by row from top to bottom\n\n    default iterator for :class:`Matrix`.\n    See :func:`Matrix.rows` for more details\n    \"\"\"\n    def next(self):\n        if self.current in self.reader_ref.row_range():\n            row = self.current\n            self.current += 1\n            row_array = self.reader_ref.row_at(row)\n            row_dict = dict(zip(self.reader_ref.colnames, row_array))\n            return row_dict\n        else:\n            raise StopIteration\n\n\nclass RowReverseIterator(PyexcelIterator):\n    \"\"\"\n    Iterate data row by row from bottom to top\n\n    see :func:`Matrix.rrows` for more details\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.current = reader.number_of_rows() - 1\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        if self.current in self.reader_ref.row_range():\n            self.current -= 1\n            return self.reader_ref.row_at(self.current+1)\n        else:\n            raise StopIteration\n\n\nclass ColumnIterator(PyexcelIterator):\n    \"\"\"\n    Column Iterator from left to right\n\n    see :func:`Matrix.columns` for more details\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.current = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        if self.current in self.reader_ref.column_range():\n            self.current += 1\n            return self.reader_ref.column_at(self.current-1)\n        else:\n            raise StopIteration\n\n\nclass NamedColumnIterator(ColumnIterator):\n    \"\"\"\n    Column Iterator from left to right\n\n    see :func:`Matrix.columns` for more details\n    \"\"\"\n    def next(self):\n        if self.current in self.reader_ref.column_range():\n            self.current += 1\n            column_array = self.reader_ref.column_at(self.current-1)\n            column_dict = dict(zip(self.reader_ref.rownames, column_array))\n            return column_dict\n        else:\n            raise StopIteration\n\n\nclass ColumnReverseIterator(PyexcelIterator):\n    \"\"\"\n    Column Reverse Iterator from right to left\n\n    see :func:`Matrix.rcolumns` for more details\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.current = reader.number_of_columns() - 1\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        if self.current in self.reader_ref.column_range():\n            self.current -= 1\n            return self.reader_ref.column_at(self.current+1)\n        else:\n            raise StopIteration\n\n\nclass ColumnIndexIterator(PyexcelIterator):\n    \"\"\"\n    Column Iterator\n\n    Default iterator for :class:`Sheet` when it becomes Series\n    See :func:`Sheet.__iter__` for more details\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.current = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        if self.current in self.reader_ref.column_range():\n            index = self.current\n            self.current += 1\n            column_header = self.reader_ref.colnames[index]\n            return {\n                column_header: self.reader_ref.named_column_at(column_header)}\n        else:\n            raise StopIteration\n\n\nclass RowIndexIterator(PyexcelIterator):\n    \"\"\"\n    Row Iterator\n\n    Default iterator for :class:`Sheet` when it becomes Series\n    See :func:`Sheet.__iter__` for more details\n    \"\"\"\n    def __init__(self, reader):\n        self.reader_ref = reader\n        self.current = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        if self.current in self.reader_ref.row_range():\n            index = self.current\n            self.current += 1\n            column_header = self.reader_ref.rownames[index]\n            return {\n                column_header: self.reader_ref.named_row_at(column_header)}\n        else:\n            raise StopIteration\n\n\nclass SheetIterator(PyexcelIterator):\n    \"\"\"\n    Sheet Iterator\n    \"\"\"\n    def __init__(self, bookreader):\n        self.book_reader_ref = bookreader\n        self.current = 0\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        if self.current < self.book_reader_ref.number_of_sheets():\n            self.current += 1\n            return self.book_reader_ref[self.current-1]\n        else:\n            raise StopIteration\n", 
    "pyexcel.presentation": "\"\"\"\n    pyexcel.sheets.presentation\n    ~~~~~~~~~~~~~~~~~~~\n\n    Provide readable string prestation\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\n\"\"\"External presentation plugin register\"\"\"\nSTRINGIFICATION = {}\n\n\ndef outsource(func):\n    \"\"\"Presentation injector\"\"\"\n    def inner(self):\n        plugin = STRINGIFICATION.get(str(self.__class__), None)\n        if plugin:\n            return plugin(self)\n        else:\n            return func(self)\n    return inner\n", 
    "pyexcel.sheets.__init__": "\"\"\"\n    pyexcel.sheets\n    ~~~~~~~~~~~~~~~~~~~\n\n    Core functionality of pyexcel, data model\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom .sheet import (\n    Sheet,\n    NominableSheet)\nfrom .formattablesheet import FormattableSheet\nfrom .filterablesheet import FilterableSheet\nfrom .nominablesheet import NamedRow, NamedColumn, VALID_SHEET_PARAMETERS\nfrom .matrix import Matrix, transpose, Row, Column\n", 
    "pyexcel.sheets.filterablesheet": "\"\"\"\n    pyexcel.sheets.filterablesheet\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Building on top of formattablesheet, adding filtering feature\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nimport copy\nfrom .matrix import Matrix, uniform\nfrom .formattablesheet import FormattableSheet\nfrom ..filters import ColumnIndexFilter, RowIndexFilter, RegionFilter\n\n\nclass FilterableSheet(FormattableSheet):\n    \"\"\"\n    A represetation of Matrix that can be filtered\n    by as many filters as it is applied\n    \"\"\"\n    def __init__(self, sheet):\n        FormattableSheet.__init__(self, sheet)\n        self._filters = []\n\n    def _number_of_rows(self):\n        return len(self.array)\n\n    def _number_of_columns(self):\n        if self._number_of_rows() > 0:\n            return len(self.array[0])\n        else:\n            return 0\n\n    def number_of_rows(self):\n        \"\"\"\n        Number of rows in the data sheet\n        \"\"\"\n        number_of_rows = self._number_of_rows()\n        if len(self._filters) != 0:\n            new_rows = number_of_rows\n            for f in self._filters:\n                new_rows = new_rows - f.rows()\n            return new_rows\n        else:\n            return number_of_rows\n\n    def number_of_columns(self):\n        \"\"\"\n        Number of columns in the data sheet\n        \"\"\"\n        number_of_columns = self._number_of_columns()\n        if len(self._filters) != 0:\n            new_cols = number_of_columns\n            for f in self._filters:\n                new_cols = new_cols - f.columns()\n            return new_cols\n        else:\n            return number_of_columns\n\n    def cell_value(self, row, column, new_value=None):\n        \"\"\"\n        Random access to the data cells\n        \"\"\"\n        if row in self.row_range() and column in self.column_range():\n            if len(self._filters) != 0:\n                new_row = row\n                new_column = column\n                number_of_filters = len(self._filters)\n                for i in range(number_of_filters-1, -1, -1):\n                    new_row, new_column = self._filters[i].translate(\n                        new_row,\n                        new_column)\n                return self._cell_value(new_row, new_column, new_value)\n            else:\n                return self._cell_value(row, column, new_value)\n        else:\n            return None\n\n    def add_filter(self, afilter):\n        \"\"\"Apply a filter\n\n        :param Filter afilter: a custom filter\n        \"\"\"\n        afilter.validate_filter(self)\n        self._filters.append(afilter)\n\n    def remove_filter(self, afilter):\n        \"\"\"Remove a named filter\n\n        have to remove all filters in order to re-validate the\n        rest of the filters\n        \"\"\"\n        self._filters.remove(afilter)\n        local_filters = self._filters\n        self._filters = []\n        for f in local_filters:\n            f.validate_filter(self)\n            self._filters.append(f)\n\n    def clear_filters(self):\n        \"\"\"Clears all filters\"\"\"\n        self._filters = []\n\n    def filter(self, afilter):\n        \"\"\"Apply the filter with immediate effect\"\"\"\n        if isinstance(afilter, ColumnIndexFilter):\n            self._apply_column_filters(afilter)\n        elif isinstance(afilter, RowIndexFilter):\n            self._apply_row_filters(afilter)\n        elif isinstance(afilter, RegionFilter):\n            afilter.validate_filter(self)\n            decending_list = sorted(afilter.row_indices, reverse=True)\n            for i in decending_list:\n                del self.row[i]\n            decending_list = sorted(afilter.column_indices, reverse=True)\n            for i in decending_list:\n                del self.column[i]\n        else:\n            raise NotImplementedError(\"Invalid Filter!\")\n\n    def _apply_row_filters(self, afilter):\n        afilter.validate_filter(self)\n        decending_list = sorted(afilter.indices, reverse=True)\n        for i in decending_list:\n            del self.row[i]\n\n    def _apply_column_filters(self, afilter):\n        \"\"\"Private method to apply column filter\"\"\"\n        afilter.validate_filter(self)\n        decending_list = sorted(afilter.indices, reverse=True)\n        for i in decending_list:\n            del self.column[i]\n\n    def validate_filters(self):\n        \"\"\"Re-apply filters\n\n        It is called when some data is updated\n        \"\"\"\n        local_filters = self._filters\n        self._filters = []\n        for filter in local_filters:\n            filter.validate_filter(self)\n            self._filters.append(filter)\n\n    def freeze_filters(self):\n        \"\"\"Apply all filters and delete them\"\"\"\n        local_filters = self._filters\n        self._filters = []\n        for f in local_filters:\n            self.filter(f)\n\n    def _lift_filters(func):\n        \"\"\"\n        disable filters, do something and enable fitlers\n        \"\"\"\n        def wrapper(self, *args):\n            local_filters = []\n            # if filter exist\n            if len(self._filters) > 0:\n                local_filters = self._filters\n                self._filters = []\n            func(self, *args)\n            # if filter exist\n            if len(local_filters) > 0:\n                self._filters = local_filters\n                self.validate_filters()\n        return wrapper\n\n    @_lift_filters\n    def extend_rows(self, rows):\n        \"\"\"expected the rows to be off the same length\n\n        :param list rows: a list of arrays\n        \"\"\"\n        Matrix.extend_rows(self, rows)\n\n    @_lift_filters\n    def delete_rows(self, row_indices):\n        \"\"\"delete rows\n\n        :param list row_indices: a list of row indices to be removed\n        \"\"\"\n        Matrix.delete_rows(self, row_indices)\n\n    @_lift_filters\n    def extend_columns(self, columns):\n        \"\"\"expected the rows to be of the same length\n\n        :param list columns: a list of arrays\n        \"\"\"\n        Matrix.extend_columns(self, columns)\n\n    @_lift_filters\n    def delete_columns(self, column_indices):\n        \"\"\"delete rows\n\n        :param list row_indices: a list of column indices to be removed\n        \"\"\"\n        Matrix.delete_columns(self, column_indices)\n\n    def region(self, topleft_corner, bottomright_corner):\n        \"\"\"Get a rectangle shaped data out\n\n        :param slice topleft_corner: the top left corner of the rectangle\n        :param slice bottomright_corner: the bottom right\n                                         corner of the rectangle\n\n        example::\n\n            >>> import pyexcel as pe\n            >>> data = [\n            ...     # 0 1  2  3  4 5   6\n            ...     [1, 2, 3, 4, 5, 6, 7], #  0\n            ...     [21, 22, 23, 24, 25, 26, 27],\n            ...     [31, 32, 33, 34, 35, 36, 37],\n            ...     [41, 42, 43, 44, 45, 46, 47],\n            ...     [51, 52, 53, 54, 55, 56, 57]  # 4\n            ... ]\n            >>> s = pe.Sheet(data)\n            >>> data = s.cut([1, 1], [4, 5])\n            >>> s2 = pe.Sheet(data) #  let's present the result\n            >>> s2\n            Sheet Name: pyexcel\n            +----+----+----+----+\n            | 22 | 23 | 24 | 25 |\n            +----+----+----+----+\n            | 32 | 33 | 34 | 35 |\n            +----+----+----+----+\n            | 42 | 43 | 44 | 45 |\n            +----+----+----+----+\n\n        \"\"\"\n        row_slice = slice(topleft_corner[0], bottomright_corner[0], 1)\n        column_slice = slice(topleft_corner[1], bottomright_corner[1], 1)\n        f = RegionFilter(row_slice, column_slice)\n        self.add_filter(f)\n        ret_data = copy.deepcopy(self.to_array())\n        self.remove_filter(f)\n        return ret_data\n\n    def cut(self, topleft_corner, bottomright_corner):\n        \"\"\"Get a rectangle shaped data out and clear them in position\n\n        :param slice topleft_corner: the top left corner of the rectangle\n        :param slice bottomright_corner: the bottom right\n                                         corner of the rectangle\n\n        example::\n\n            >>> import pyexcel as pe\n            >>> data = [\n            ...     # 0 1  2  3  4 5   6\n            ...     [1, 2, 3, 4, 5, 6, 7], #  0\n            ...     [21, 22, 23, 24, 25, 26, 27],\n            ...     [31, 32, 33, 34, 35, 36, 37],\n            ...     [41, 42, 43, 44, 45, 46, 47],\n            ...     [51, 52, 53, 54, 55, 56, 57]  # 4\n            ... ]\n            >>> s = pe.Sheet(data)\n            >>> s\n            Sheet Name: pyexcel\n            +----+----+----+----+----+----+----+\n            | 1  | 2  | 3  | 4  | 5  | 6  | 7  |\n            +----+----+----+----+----+----+----+\n            | 21 | 22 | 23 | 24 | 25 | 26 | 27 |\n            +----+----+----+----+----+----+----+\n            | 31 | 32 | 33 | 34 | 35 | 36 | 37 |\n            +----+----+----+----+----+----+----+\n            | 41 | 42 | 43 | 44 | 45 | 46 | 47 |\n            +----+----+----+----+----+----+----+\n            | 51 | 52 | 53 | 54 | 55 | 56 | 57 |\n            +----+----+----+----+----+----+----+\n            >>> # cut  1<= row < 4, 1<= column < 5\n            >>> data = s.cut([1, 1], [4, 5])\n            >>> s\n            Sheet Name: pyexcel\n            +----+----+----+----+----+----+----+\n            | 1  | 2  | 3  | 4  | 5  | 6  | 7  |\n            +----+----+----+----+----+----+----+\n            | 21 |    |    |    |    | 26 | 27 |\n            +----+----+----+----+----+----+----+\n            | 31 |    |    |    |    | 36 | 37 |\n            +----+----+----+----+----+----+----+\n            | 41 |    |    |    |    | 46 | 47 |\n            +----+----+----+----+----+----+----+\n            | 51 | 52 | 53 | 54 | 55 | 56 | 57 |\n            +----+----+----+----+----+----+----+\n\n        \"\"\"\n        row_slice = slice(topleft_corner[0], bottomright_corner[0], 1)\n        column_slice = slice(topleft_corner[1], bottomright_corner[1], 1)\n        f = RegionFilter(row_slice, column_slice)\n        self.add_filter(f)\n        ret_data = copy.deepcopy(self.to_array())\n        for r in self.row_range():\n            for c in self.column_range():\n                self.cell_value(r, c, '')\n        self.remove_filter(f)\n        return ret_data\n\n    def insert(self, topleft_corner, rows=None, columns=None):\n        \"\"\"Insert a rectangle shaped data after a position\n\n        :param slice topleft_corner: the top left corner of the rectangle\n        example::\n\n            >>> import pyexcel as pe\n            >>> data = [\n            ...     # 0 1  2  3  4 5   6\n            ...     [1, 2, 3, 4, 5, 6, 7], #  0\n            ...     [21, 22, 23, 24, 25, 26, 27],\n            ...     [31, 32, 33, 34, 35, 36, 37],\n            ...     [41, 42, 43, 44, 45, 46, 47],\n            ...     [51, 52, 53, 54, 55, 56, 57]  # 4\n            ... ]\n            >>> s = pe.Sheet(data)\n            >>> data_to_be_inserted = [\n            ...     ['a1', 'b1', 'c1', 'd1'],\n            ...     ['a2', 'b2', 'c2', 'd2'],\n            ...     ['a3', 'b3', 'c3', 'd3'],\n            ...     ['a4', 'b4', 'c4', 'd4'],\n            ...     ['a5', 'b5', 'c5', 'd5'],\n            ... ]\n            >>> s.insert([1, 1], rows=data_to_be_inserted)\n            >>> s\n            Sheet Name: pyexcel\n            +----+----+----+----+----+----+----+\n            | 1  | 2  | 3  | 4  | 5  | 6  | 7  |\n            +----+----+----+----+----+----+----+\n            | 21 | a1 | b1 | c1 | d1 | 26 | 27 |\n            +----+----+----+----+----+----+----+\n            | 31 | a2 | b2 | c2 | d2 | 36 | 37 |\n            +----+----+----+----+----+----+----+\n            | 41 | a3 | b3 | c3 | d3 | 46 | 47 |\n            +----+----+----+----+----+----+----+\n            | 51 | a4 | b4 | c4 | d4 | 56 | 57 |\n            +----+----+----+----+----+----+----+\n            |    | a5 | b5 | c5 | d5 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | 22 | 23 | 24 | 25 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | 32 | 33 | 34 | 35 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | 42 | 43 | 44 | 45 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | 52 | 53 | 54 | 55 |    |    |\n            +----+----+----+----+----+----+----+\n            >>> data_to_be_inserted2 = [\n            ...     ['A1', 'B1', 'C1', 'D1'],\n            ...     ['A2', 'B2', 'C2', 'D2'],\n            ...     ['A3', 'B3', 'C3', 'D3'],\n            ...     ['A4', 'B4', 'C4', 'D4'],\n            ...     ['A5', 'B5', 'C5', 'D5'],\n            ... ]\n            >>> s.insert([1, 1], columns=data_to_be_inserted2)\n            >>> s\n            Sheet Name: pyexcel\n            +----+----+----+----+----+----+----+\n            | 1  | 2  | 3  | 4  | 5  | 6  | 7  |\n            +----+----+----+----+----+----+----+\n            | 21 | A1 | A2 | A3 | A4 | A5 | 27 |\n            +----+----+----+----+----+----+----+\n            | 31 | B1 | B2 | B3 | B4 | B5 | 37 |\n            +----+----+----+----+----+----+----+\n            | 41 | C1 | C2 | C3 | C4 | C5 | 47 |\n            +----+----+----+----+----+----+----+\n            | 51 | D1 | D2 | D3 | D4 | D5 | 57 |\n            +----+----+----+----+----+----+----+\n            |    | a1 | b1 | c1 | d1 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | a2 | b2 | c2 | d2 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | a3 | b3 | c3 | d3 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | a4 | b4 | c4 | d4 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | a5 | b5 | c5 | d5 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | 22 | 23 | 24 | 25 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | 32 | 33 | 34 | 35 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | 42 | 43 | 44 | 45 |    |    |\n            +----+----+----+----+----+----+----+\n            |    | 52 | 53 | 54 | 55 |    |    |\n            +----+----+----+----+----+----+----+\n\n        \"\"\"\n        if rows:\n            self._insert_rows(topleft_corner, rows)\n        elif columns:\n            self._insert_columns(topleft_corner, columns)\n        else:\n            raise ValueError(\"Nothing to be inserted!\")\n\n    def _insert_columns(self, topleft_corner, columns):\n        height, incoming_data = uniform(copy.deepcopy(columns))\n        bottom_right_corner_row = self.number_of_rows()\n        bottom_right_corner_column = min(self.width, topleft_corner[1]+height)\n        relocated_region = self.cut(\n            topleft_corner,\n            (bottom_right_corner_row, bottom_right_corner_column))\n        self.paste(topleft_corner, columns=incoming_data)\n        new_topeft_corner = (topleft_corner[0]+height, topleft_corner[1])\n        self.paste(new_topeft_corner, relocated_region)\n\n    def _insert_rows(self, topleft_corner, rows):\n        width, incoming_data = uniform(copy.deepcopy(rows))\n        height = len(rows)\n        bottom_right_corner_row = self.number_of_rows()\n        bottom_right_corner_column = min(self.number_of_columns(),\n                                         topleft_corner[1]+width)\n        relocated_region = self.cut(\n            topleft_corner,\n            (bottom_right_corner_row, bottom_right_corner_column))\n        self.paste(topleft_corner, rows=incoming_data)\n        new_topeft_corner = (topleft_corner[0]+height, topleft_corner[1])\n        self.paste(new_topeft_corner, rows=relocated_region)\n", 
    "pyexcel.sheets.formattablesheet": "\"\"\"\n    pyexcel.sheets.formattablesheet\n    ~~~~~~~~~~~~~~~~~~~\n\n    Building on top of Matrix, adding formatting feature\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom .matrix import Matrix\nfrom ..formatters import (\n    ColumnFormatter,\n    RowFormatter,\n    SheetFormatter\n)\nfrom ..constants import MESSAGE_NOT_IMPLEMENTED_01\n\n\nclass FormattableSheet(Matrix):\n    \"\"\"\n    A represetation of Matrix that accept custom formatters\n    \"\"\"\n    def __init__(self, array):\n        \"\"\"Constructor\n\n        Example::\n\n            >>> import pyexcel as pe\n            >>> # Given a dictinoary as the following\n            >>> data = {\n            ...     \"1\": [1, 2, 3, 4, 5, 6, 7, 8],\n            ...     \"3\": [1.25, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8],\n            ...     \"5\": [2, 3, 4, 5, 6, 7, 8, 9],\n            ...     \"7\": [1, '',]\n            ...     }\n            >>> sheet = pe.Sheet(pe.dict_to_array(data))\n\n        \"\"\"\n        Matrix.__init__(self, array)\n        self._formatters = []\n\n    def format(self, formatter, on_demand=False):\n        \"\"\"Apply a formatting action for the whole sheet\n\n        Example::\n\n            >>> import pyexcel as pe\n            >>> # Given a dictinoary as the following\n            >>> data = {\n            ...     \"1\": [1, 2, 3, 4, 5, 6, 7, 8],\n            ...     \"3\": [1.25, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8],\n            ...     \"5\": [2, 3, 4, 5, 6, 7, 8, 9],\n            ...     \"7\": [1, '',]\n            ...     }\n            >>> sheet = pe.Sheet(pe.dict_to_array(data))\n            >>> sheet.row[1]\n            [1, 1.25, 2, 1]\n            >>> sheet.format(str)\n            >>> sheet.row[1]\n            ['1', '1.25', '2', '1']\n            >>> sheet.format(int)\n            >>> sheet.row[1]\n            [1, 1, 2, 1]\n\n        \"\"\"\n        sf = SheetFormatter(formatter)\n        if on_demand:\n            self.add_formatter(sf)\n        else:\n            self.apply_formatter(sf)\n\n    def map(self, custom_function):\n        \"\"\"Execute a function across all cells of the sheet\n\n        Example::\n\n            >>> import pyexcel as pe\n            >>> # Given a dictinoary as the following\n            >>> data = {\n            ...     \"1\": [1, 2, 3, 4, 5, 6, 7, 8],\n            ...     \"3\": [1.25, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8],\n            ...     \"5\": [2, 3, 4, 5, 6, 7, 8, 9],\n            ...     \"7\": [1, '',]\n            ...     }\n            >>> sheet = pe.Sheet(pe.dict_to_array(data))\n            >>> sheet.row[1]\n            [1, 1.25, 2, 1]\n            >>> sheet.map(lambda value: (float(value) if value != None else 0)+1 )\n            >>> sheet.row[1]\n            [2.0, 2.25, 3.0, 2.0]\n\n        \"\"\"\n        sf = SheetFormatter(custom_function)\n        self.apply_formatter(sf)\n\n    def apply_formatter(self, aformatter):\n        \"\"\"Apply the formatter immediately. No return ticket\n\n        Example::\n\n            >>> import pyexcel as pe\n            >>> # Given a dictinoary as the following\n            >>> data = {\n            ...     \"1\": [1, 2, 3, 4, 5, 6, 7, 8],\n            ...     \"3\": [1.25, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8],\n            ...     \"5\": [2, 3, 4, 5, 6, 7, 8, 9],\n            ...     \"7\": [1, '',]\n            ...     }\n            >>> sheet = pe.Sheet(pe.dict_to_array(data))\n            >>> sheet.row[1]\n            [1, 1.25, 2, 1]\n            >>> aformatter = pe.SheetFormatter(lambda value: (float(value) if value != None else 0)+1 )\n            >>> sheet.apply_formatter(aformatter)\n            >>> sheet.row[1]\n            [2.0, 2.25, 3.0, 2.0]\n            >>> sheet.clear_formatters() # no return ticket\n            >>> sheet.row[1]\n            [2.0, 2.25, 3.0, 2.0]\n\n        \"\"\"\n        if isinstance(aformatter, ColumnFormatter):\n            self._apply_column_formatter(aformatter)\n        elif isinstance(aformatter, RowFormatter):\n            self._apply_row_formatter(aformatter)\n        else:\n            # to do don't use add_formatter'\n            self.add_formatter(aformatter)\n            self.freeze_formatters()\n\n    def _apply_column_formatter(self, column_formatter):\n        def filter_indices(column_index):\n            return column_formatter.is_my_business(-1, column_index, -1)\n        applicables = [i for i in self.column_range() if filter_indices(i)]\n        # set the values\n        for rindex in self.row_range():\n            for cindex in applicables:\n                value = self.cell_value(rindex, cindex)\n                value = column_formatter.do_format(value)\n                self.cell_value(rindex, cindex, value)\n\n    def _apply_row_formatter(self, row_formatter):\n        def filter_indices(row_index):\n            return row_formatter.is_my_business(row_index, -1, -1)\n        applicables = [i for i in self.row_range() if filter_indices(i)]\n        # set the values\n        for rindex in applicables:\n            for cindex in self.column_range():\n                value = self.cell_value(rindex, cindex)\n                value = row_formatter.do_format(value)\n                self.cell_value(rindex, cindex, value)\n\n    def add_formatter(self, aformatter):\n        \"\"\"Add a lazy formatter.\n\n        The formatter takes effect on the fly when a cell value is read\n        This is cost effective when you have a big data table\n        and you use only a few columns or rows. If you have farily modest\n        data table, you can choose apply_formatter() too.\n\n        :param Formatter aformatter: a custom formatter\n\n        Example::\n\n            >>> import pyexcel as pe\n            >>> # Given a dictinoary as the following\n            >>> data = {\n            ...     \"1\": [1, 2, 3, 4, 5, 6, 7, 8],\n            ...     \"3\": [1.25, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8],\n            ...     \"5\": [2, 3, 4, 5, 6, 7, 8, 9],\n            ...     \"7\": [1, '',]\n            ...     }\n            >>> sheet = pe.Sheet(pe.dict_to_array(data))\n            >>> sheet.row[1]\n            [1, 1.25, 2, 1]\n            >>> aformatter = pe.SheetFormatter(lambda value: (float(value) if value != None else 0)+1 )\n            >>> sheet.add_formatter(aformatter)\n            >>> sheet.row[1]\n            [2.0, 2.25, 3.0, 2.0]\n            >>> sheet.clear_formatters()\n            >>> sheet.row[1]\n            [1, 1.25, 2, 1]\n            >>> aformatter = pe.SheetFormatter(lambda value: (float(value) if value != None else 0)+1 )\n            >>> sheet.apply_formatter(aformatter)\n            >>> sheet.row[1]\n            [2.0, 2.25, 3.0, 2.0]\n            >>> sheet.clear_formatters() # no return ticket\n            >>> sheet.row[1]\n            [2.0, 2.25, 3.0, 2.0]\n\n        \"\"\"\n        self._formatters.append(aformatter)\n\n    def remove_formatter(self, aformatter):\n        \"\"\"Remove a formatter\n\n        :param Formatter aformatter: a custom formatter\n        \"\"\"\n        self._formatters.remove(aformatter)\n\n    def clear_formatters(self):\n        \"\"\"Clear all formatters\n\n        Example::\n\n            >>> import pyexcel as pe\n            >>> # Given a dictinoary as the following\n            >>> data = {\n            ...     \"1\": [1, 2, 3, 4, 5, 6, 7, 8],\n            ...     \"3\": [1.25, 2.2, 3.3, 4.4, 5.5, 6.6, 7.7, 8.8],\n            ...     \"5\": [2, 3, 4, 5, 6, 7, 8, 9],\n            ...     \"7\": [1, '',]\n            ...     }\n            >>> sheet = pe.Sheet(pe.dict_to_array(data))\n            >>> sheet.row[1]\n            [1, 1.25, 2, 1]\n            >>> aformatter = pe.SheetFormatter(lambda value: (float(value) if value != None else 0)+1)\n            >>> sheet.add_formatter(aformatter)\n            >>> sheet.row[1]\n            [2.0, 2.25, 3.0, 2.0]\n            >>> sheet.clear_formatters()\n            >>> sheet.row[1]\n            [1, 1.25, 2, 1]\n\n\n        \"\"\"\n        self._formatters = []\n\n    def freeze_formatters(self):\n        \"\"\"Apply all added formatters and clear them\n\n        The tradeoff here is when you extend the sheet, you won't\n        get the effect of previously applied formatters because they\n        are applied and gone.\n        \"\"\"\n        if len(self._formatters) < 1:\n            return\n        # set the values\n        for rindex in self.row_range():\n            for cindex in self.column_range():\n                value = self.cell_value(rindex, cindex)\n                self.cell_value(rindex, cindex, value)\n        # clear formatters\n        self._formatters = []\n\n    def _cell_value(self, row, column, new_value=None):\n        \"\"\"\n        Random access to the xls cells\n        \"\"\"\n        if new_value is None:\n            try:\n                value = self.array[row][column]\n            except IndexError:\n                value = \"\"\n            if len(self._formatters) > 0:\n                for f in self._formatters:\n                    if f.is_my_business(row, column, value):\n                        value = f.do_format(value)\n            return value\n        else:\n            self.array[row][column] = new_value\n            return new_value\n\n    def cell_value(self, row, column, new_value=None):\n        \"\"\"\n        Random access to the data cells\n        \"\"\"\n        if row in self.row_range() and column in self.column_range():\n            # apply formatting\n            return self._cell_value(row, column, new_value)\n        else:\n            return None\n\n    def __add__(self, other):\n        \"\"\"Overload the + sign\n\n        :returns: a new book\n        \"\"\"\n        from ..book import Book\n        from ..utils import to_dict, local_uuid\n        content = {}\n        content[self.name] = self.array\n        if isinstance(other, Book):\n            b = to_dict(other)\n            for l in b.keys():\n                new_key = l\n                if len(b.keys()) == 1:\n                    new_key = other.filename\n                if new_key in content:\n                    uid = local_uuid()\n                    new_key = \"%s_%s\" % (l, uid)\n                content[new_key] = b[l]\n        elif isinstance(other, Matrix):\n            new_key = other.name\n            if new_key in content:\n                uid = local_uuid()\n                new_key = \"%s_%s\" % (other.name, uid)\n            content[new_key] = other.array\n        else:\n            raise TypeError\n        c = Book()\n        c.load_from_sheets(content)\n        return c\n\n    def __iadd__(self, other):\n        \"\"\"Overload += sign\n\n        :return: self\n        \"\"\"\n        raise NotImplementedError(MESSAGE_NOT_IMPLEMENTED_01)\n", 
    "pyexcel.sheets.matrix": "\"\"\"\n    pyexcel.sheets.matrix\n    ~~~~~~~~~~~~~~~~~~~~~~\n\n    Matrix, a data model that accepts any types, spread sheet style\nof lookup.\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nimport re\nimport copy\nfrom texttable import Texttable\nfrom .._compact import is_array_type, PY2\nfrom ..iterators import (\n    HTLBRIterator,\n    HBRTLIterator,\n    VTLBRIterator,\n    VBRTLIterator,\n    RowIterator,\n    ColumnIterator,\n    RowReverseIterator,\n    ColumnReverseIterator\n)\nfrom ..filters import RowFilter, ColumnFilter\nfrom ..presentation import outsource\nfrom ..constants import (\n    MESSAGE_INDEX_OUT_OF_RANGE,\n    MESSAGE_DATA_ERROR_EMPTY_CONTENT,\n    MESSAGE_DATA_ERROR_DATA_TYPE_MISMATCH,\n    MESSAGE_DEPRECATED_ROW_COLUMN\n)\n\n\ndef _unique(seq):\n    \"\"\"Return a unique list of the incoming list\n\n    Reference:\n    http://stackoverflow.com/questions/480214/\n    how-do-you-remove-duplicates-from-a-list-in-python-whilst-preserving-order\n    \"\"\"\n    seen = set()\n    seen_add = seen.add\n    return [x for x in seq if not (x in seen or seen_add(x))]\n\n\ndef longest_row_number(array):\n    \"\"\"Find the length of the longest row in the array\n\n    :param list in_array: a list of arrays\n    \"\"\"\n    if len(array) > 0:\n        # map runs len() against each member of the array\n        return max(map(len, array))\n    else:\n        return 0\n\n\ndef uniform(array):\n    \"\"\"Fill-in empty strings to empty cells to make it MxN\n\n    :param list in_array: a list of arrays\n    \"\"\"\n    width = longest_row_number(array)\n    if width == 0:\n        return 0, array\n    else:\n        for row in array:\n            row_length = len(row)\n            for index in range(0, row_length):\n                if row[index] is None:\n                    row[index] = \"\"\n            if row_length < width:\n                row += [\"\"] * (width - row_length)\n        return width, array\n\n\ndef transpose(in_array):\n    \"\"\"Rotate clockwise by 90 degrees and flip horizontally\n\n    First column become first row.\n    :param list in_array: a list of arrays\n\n    The transformation is::\n\n        1 2 3       1  4\n        4 5 6 7 ->  2  5\n                    3  6\n                    '' 7\n    \"\"\"\n    max_length = longest_row_number(in_array)\n    new_array = []\n    for i in range(0, max_length):\n        row_data = []\n        for c in in_array:\n            if i < len(c):\n                row_data.append(c[i])\n            else:\n                row_data.append('')\n        new_array.append(row_data)\n    return new_array\n\n\"\"\"\nIn order to easily compute the actual index of 'X' or 'AX', these utility\nfunctions were written\n\"\"\"\n\n_INDICES = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n\n\ndef _get_index(index_chars):\n    length = len(index_chars)\n    if len(index_chars) > 1:\n        return (_get_index(index_chars[0])+1) * (len(_INDICES) ** (length-1)) + _get_index(index_chars[1:])\n    else:\n        return _INDICES.index(index_chars[0])\n\n\ndef _excel_column_index(index_chars):\n    if len(index_chars) < 1:\n        return -1\n    else:\n        return _get_index(index_chars.upper())\n\n\ndef _excel_cell_position(pos_chars):\n    if len(pos_chars) < 2:\n        return -1, -1\n    group = re.match(\"([A-Za-z]+)([0-9]+)\", pos_chars)\n    if group:\n        return int(group.group(2)) - 1, _excel_column_index(group.group(1))\n    else:\n        raise IndexError\n\n\ndef _analyse_slice(aslice, upper_bound):\n    \"\"\"An internal function to analyze a given slice\n    \"\"\"\n    if aslice.start is None:\n        start = 0\n    else:\n        start = max(aslice.start, 0)\n    if aslice.stop is None:\n        stop = upper_bound\n    else:\n        stop = min(aslice.stop, upper_bound)\n    if start > stop:\n        raise ValueError\n    elif start < stop:\n        if aslice.step:\n            my_range = range(start, stop, aslice.step)\n        else:\n            my_range = range(start, stop)\n        if not PY2:\n            # for py3, my_range is a range object\n            my_range = list(my_range)\n    else:\n        my_range = [start]\n    return my_range\n\n\nclass Row:\n    \"\"\"Represet row of a matrix\n\n    .. table:: \"example.csv\"\n\n        = = =\n        1 2 3\n        4 5 6\n        7 8 9\n        = = =\n\n    Above column manipluation can be performed on rows similiarly. This section\n    will not repeat the same example but show some advance usages.\n\n\n        >>> import pyexcel as pe\n        >>> data = [[1,2,3], [4,5,6], [7,8,9]]\n        >>> m = pe.sheets.Matrix(data)\n        >>> m.row[0:2]\n        [[1, 2, 3], [4, 5, 6]]\n        >>> m.row[0:3] = [0, 0, 0]\n        >>> m.row[2]\n        [0, 0, 0]\n        >>> del m.row[0:2]\n        >>> m.row[0]\n        [0, 0, 0]\n\n    \"\"\"\n    def __init__(self, matrix):\n        self.ref = matrix\n\n    def select(self, indices):\n        \"\"\"Delete row indices other than specified\n        \n        Examples:\n\n            >>> import pyexcel as pe\n            >>> data = [[1],[2],[3],[4],[5],[6],[7],[9]]\n            >>> sheet = pe.Sheet(data)\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+\n            | 1 |\n            +---+\n            | 2 |\n            +---+\n            | 3 |\n            +---+\n            | 4 |\n            +---+\n            | 5 |\n            +---+\n            | 6 |\n            +---+\n            | 7 |\n            +---+\n            | 9 |\n            +---+\n            >>> sheet.row.select([1,2,3,5])\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+\n            | 2 |\n            +---+\n            | 3 |\n            +---+\n            | 4 |\n            +---+\n            | 6 |\n            +---+\n\n        \"\"\"\n        self.ref.filter(RowFilter(indices).invert())\n        \n    def __delitem__(self, aslice):\n        \"\"\"Override the operator to delete items\n        \n        Examples:\n\n            >>> import pyexcel as pe\n            >>> data = [[1],[2],[3],[4],[5],[6],[7],[9]]\n            >>> sheet = pe.Sheet(data)\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+\n            | 1 |\n            +---+\n            | 2 |\n            +---+\n            | 3 |\n            +---+\n            | 4 |\n            +---+\n            | 5 |\n            +---+\n            | 6 |\n            +---+\n            | 7 |\n            +---+\n            | 9 |\n            +---+\n            >>> del sheet.row[1,2,3,5]\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+\n            | 1 |\n            +---+\n            | 5 |\n            +---+\n            | 7 |\n            +---+\n            | 9 |\n            +---+\n\n        \"\"\"\n        if isinstance(aslice, slice):\n            my_range = _analyse_slice(aslice, self.ref.number_of_rows())\n            self.ref.delete_rows(my_range)\n        elif isinstance(aslice, tuple):\n            self.ref.filter(RowFilter(list(aslice)))\n        elif isinstance(aslice, list):\n            self.ref.filter(RowFilter(aslice))\n        else:\n            self.ref.delete_rows([aslice])\n\n    def __setitem__(self, aslice, c):\n        \"\"\"Override the operator to set items\"\"\"\n        if isinstance(aslice, slice):\n            my_range = _analyse_slice(aslice, self.ref.number_of_rows())\n            for i in my_range:\n                self.ref.set_row_at(i, c)\n        else:\n            self.ref.set_row_at(aslice, c)\n\n    def __getitem__(self, aslice):\n        \"\"\"By default, this class recognize from top to bottom\n        from left to right\"\"\"\n        index = aslice\n        if isinstance(aslice, slice):\n            my_range = _analyse_slice(aslice, self.ref.number_of_rows())\n            results = []\n            for i in my_range:\n                results.append(self.ref.row_at(i))\n            return results\n        if index in self.ref.row_range():\n            return self.ref.row_at(index)\n        else:\n            raise IndexError\n\n    def __iadd__(self, other):\n        \"\"\"Overload += sign\n\n        :return: self\n        \"\"\"\n        if isinstance(other, list):\n            self.ref.extend_rows(other)\n        elif isinstance(other, Matrix):\n            self.ref.extend_rows(other.array)\n        else:\n            raise TypeError\n        return self\n\n    def __add__(self, other):\n        \"\"\"Overload += sign\n\n        :return: self\n        \"\"\"\n        self.__iadd__(other)\n        return self.ref\n\n\nclass Column:\n    \"\"\"Represet columns of a matrix\n\n    .. table:: \"example.csv\"\n\n        = = =\n        1 2 3\n        4 5 6\n        7 8 9\n        = = =\n\n    Let us manipulate the data columns on the above data matrix::\n\n        >>> import pyexcel as pe\n        >>> data = [[1,2,3], [4,5,6], [7,8,9]]\n        >>> m = pe.sheets.Matrix(data)\n        >>> m.column[0]\n        [1, 4, 7]\n        >>> m.column[2] = [0, 0, 0]\n        >>> m.column[2]\n        [0, 0, 0]\n        >>> del m.column[1]\n        >>> m.column[1]\n        [0, 0, 0]\n        >>> m.column[2]\n        Traceback (most recent call last):\n            ...\n        IndexError\n\n    \"\"\"\n    def __init__(self, matrix):\n        self.ref = matrix\n\n    def select(self, indices):\n        \"\"\"\n        Examples:\n\n            >>> import pyexcel as pe\n            >>> data = [[1,2,3,4,5,6,7,9]]\n            >>> sheet = pe.Sheet(data)\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+---+---+---+---+\n            | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 9 |\n            +---+---+---+---+---+---+---+---+\n            >>> sheet.column.select([1,2,3,5])\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+\n            | 2 | 3 | 4 | 6 |\n            +---+---+---+---+\n        \n        \"\"\"\n        self.ref.filter(ColumnFilter(indices).invert())\n        \n    def __delitem__(self, aslice):\n        \"\"\"Override the operator to delete items\n\n        Examples:\n\n            >>> import pyexcel as pe\n            >>> data = [[1,2,3,4,5,6,7,9]]\n            >>> sheet = pe.Sheet(data)\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+---+---+---+---+\n            | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 9 |\n            +---+---+---+---+---+---+---+---+\n            >>> del sheet.column[1,2,3,5]\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+\n            | 1 | 5 | 7 | 9 |\n            +---+---+---+---+\n        \n        \"\"\"\n        if isinstance(aslice, slice):\n            my_range = _analyse_slice(aslice, self.ref.number_of_columns())\n            self.ref.delete_columns(my_range)\n        elif isinstance(aslice, str):\n            index = _excel_column_index(aslice)\n            self.ref.delete_columns([index])\n        elif isinstance(aslice, tuple):\n            indices = list(aslice)\n            self.ref.filter(ColumnFilter(indices))\n        elif isinstance(aslice, list):\n            indices = aslice\n            self.ref.filter(ColumnFilter(indices))\n        elif isinstance(aslice, int):\n            self.ref.delete_columns([aslice])\n        else:\n            raise IndexError\n\n    def __setitem__(self, aslice, c):\n        \"\"\"Override the operator to set items\"\"\"\n        if isinstance(aslice, slice):\n            my_range = _analyse_slice(aslice, self.ref.number_of_columns())\n            for i in my_range:\n                self.ref.set_column_at(i, c)\n        elif isinstance(aslice, str):\n            index = _excel_column_index(aslice)\n            self.ref.set_column_at(index, c)\n        elif isinstance(aslice, int):\n            self.ref.set_column_at(aslice, c)\n        else:\n            raise IndexError\n\n    def __getitem__(self, aslice):\n        \"\"\"By default, this class recognize from top to bottom\n        from left to right\"\"\"\n        index = aslice\n        if isinstance(aslice, slice):\n            my_range = _analyse_slice(aslice, self.ref.number_of_columns())\n            results = []\n            for i in my_range:\n                results.append(self.ref.column_at(i))\n            return results\n        elif isinstance(aslice, str):\n            index = _excel_column_index(aslice)\n        if index in self.ref.column_range():\n            return self.ref.column_at(index)\n        else:\n            raise IndexError\n\n    def __iadd__(self, other):\n        \"\"\"Overload += sign\n\n        :return: self\n        \"\"\"\n        if isinstance(other, list):\n            self.ref.extend_columns(other)\n        elif isinstance(other, Matrix):\n            self.ref.extend_columns_with_rows(other.array)\n        else:\n            raise TypeError\n        return self\n\n    def __add__(self, other):\n        \"\"\"Overload += sign\n\n        :return: self\n        \"\"\"\n        self.__iadd__(other)\n        return self.ref\n\n\nclass Matrix(object):\n    \"\"\"The internal representation of a sheet data. Each element\n    can be of any python types\n    \"\"\"\n\n    def __init__(self, array):\n        \"\"\"Constructor\n\n        The reason a deep copy was not made here is because\n        the data sheet could be huge. It could be costly to\n        copy every cell to a new memory area\n        :param list array: a list of arrays\n        \"\"\"\n        self.width, self.array = uniform(array)\n\n    def number_of_rows(self):\n        \"\"\"The number of rows\"\"\"\n        return len(self.array)\n\n    def number_of_columns(self):\n        \"\"\"The number of columns\"\"\"\n        if self.number_of_rows() > 0:\n            return self.width\n        else:\n            return 0\n\n    def row_range(self):\n        \"\"\"\n        Utility function to get row range\n        \"\"\"\n        if PY2:\n            return xrange(0, self.number_of_rows())\n        else:\n            return range(0, self.number_of_rows())\n\n    def column_range(self):\n        \"\"\"\n        Utility function to get column range\n        \"\"\"\n        if PY2:\n            return xrange(0, self.number_of_columns())\n        else:\n            return range(0, self.number_of_columns())\n\n    def cell_value(self, row, column, new_value=None):\n        \"\"\"Random access to table cells\n\n        :param int row: row index which starts from 0\n        :param int column: column index which starts from 0\n        :param any new_value: new value if this is to set the value\n        \"\"\"\n        if new_value is None:\n            if row in self.row_range() and column in self.column_range():\n                # apply formatting\n                return self.array[row][column]\n            else:\n                return None\n        else:\n            self.array[row][column] = new_value\n            return new_value\n\n    def __iter__(self):\n        \"\"\"\n        Default iterator to go through each cell one by one from top row to\n        bottom row and from left to right\n        \"\"\"\n        return self.rows()\n\n    def enumerate(self):\n        \"\"\"\n        Iterate cell by cell from top to bottom and from left to right\n\n        .. testcode::\n\n            >>> import pyexcel as pe\n            >>> data = [\n            ...     [1, 2, 3, 4],\n            ...     [5, 6, 7, 8],\n            ...     [9, 10, 11, 12]\n            ... ]\n            >>> m = pe.sheets.Matrix(data)\n            >>> print(pe.utils.to_array(m.enumerate()))\n            [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n\n        More details see :class:`HTLBRIterator`\n        \"\"\"\n        return HTLBRIterator(self)\n\n    def reverse(self):\n        \"\"\"Opposite to enumerate\n\n        each cell one by one from\n        bottom row to top row and from right to left\n        example::\n\n            >>> import pyexcel as pe\n            >>> data = [\n            ...     [1, 2, 3, 4],\n            ...     [5, 6, 7, 8],\n            ...     [9, 10, 11, 12]\n            ... ]\n            >>> m = pe.sheets.Matrix(data)\n            >>> print(pe.utils.to_array(m.reverse()))\n            [12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1]\n\n        More details see :class:`HBRTLIterator`\n        \"\"\"\n        return HBRTLIterator(self)\n\n    def vertical(self):\n        \"\"\"\n        Default iterator to go through each cell one by one from\n        leftmost column to rightmost row and from top to bottom\n        example::\n\n            import pyexcel as pe\n            data = [\n                [1, 2, 3, 4],\n                [5, 6, 7, 8],\n                [9, 10, 11, 12]\n            ]\n            m = pe.Matrix(data)\n            print(pe.utils.to_array(m.vertical()))\n\n        output::\n\n            [1, 5, 9, 2, 6, 10, 3, 7, 11, 4, 8, 12]\n\n        More details see :class:`VTLBRIterator`\n        \"\"\"\n        return VTLBRIterator(self)\n\n    def rvertical(self):\n        \"\"\"\n        Default iterator to go through each cell one by one from rightmost\n        column to leftmost row and from bottom to top\n        example::\n\n            import pyexcel as pe\n            data = [\n                [1, 2, 3, 4],\n                [5, 6, 7, 8],\n                [9, 10, 11, 12]\n            ]\n            m = pe.Matrix(data)\n            print(pe.utils.to_array(m.rvertical())\n\n        output::\n\n            [12, 8, 4, 11, 7, 3, 10, 6, 2, 9, 5, 1]\n\n        More details see :class:`VBRTLIterator`\n        \"\"\"\n        return VBRTLIterator(self)\n\n    def rows(self):\n        \"\"\"\n        Returns a top to bottom row iterator\n\n        example::\n\n            import pyexcel as pe\n            data = [\n                [1, 2, 3, 4],\n                [5, 6, 7, 8],\n                [9, 10, 11, 12]\n            ]\n            m = pe.Matrix(data)\n            print(pe.utils.to_array(m.rows()))\n\n        output::\n\n            [[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]]\n\n        More details see :class:`RowIterator`\n        \"\"\"\n        return RowIterator(self)\n\n    def rrows(self):\n        \"\"\"\n        Returns a bottom to top row iterator\n\n        .. testcode::\n\n            import pyexcel as pe\n            data = [\n                [1, 2, 3, 4],\n                [5, 6, 7, 8],\n                [9, 10, 11, 12]\n            ]\n            m = pe.Matrix(data)\n            print(pe.utils.to_array(m.rrows()))\n\n        .. testoutput::\n\n            [[9, 10, 11, 12], [5, 6, 7, 8], [1, 2, 3, 4]]\n\n        More details see :class:`RowReverseIterator`\n        \"\"\"\n        return RowReverseIterator(self)\n\n    def columns(self):\n        \"\"\"\n        Returns a left to right column iterator\n\n        .. testcode::\n\n            import pyexcel as pe\n            data = [\n                [1, 2, 3, 4],\n                [5, 6, 7, 8],\n                [9, 10, 11, 12]\n            ]\n            m = pe.Matrix(data)\n            print(pe.utils.to_array(m.columns()))\n\n        .. testoutput::\n\n            [[1, 5, 9], [2, 6, 10], [3, 7, 11], [4, 8, 12]]\n\n        More details see :class:`ColumnIterator`\n        \"\"\"\n        return ColumnIterator(self)\n\n    def rcolumns(self):\n        \"\"\"\n        Returns a right to left column iterator\n\n        example::\n\n            import pyexcel as pe\n            data = [\n                [1, 2, 3, 4],\n                [5, 6, 7, 8],\n                [9, 10, 11, 12]\n            ]\n            m = pe.Matrix(data)\n            print(pe.utils.to_array(m.rcolumns()))\n\n        output::\n\n            [[4, 8, 12], [3, 7, 11], [2, 6, 10], [1, 5, 9]]\n\n        More details see :class:`ColumnReverseIterator`\n        \"\"\"\n        return ColumnReverseIterator(self)\n\n    @property\n    def row(self):\n        return Row(self)\n\n    @row.setter\n    def row(self, value):\n        # dummy setter to enable self.column += ..\n        # in py3\n        pass\n\n    @property\n    def column(self):\n        return Column(self)\n\n    @column.setter\n    def column(self, value):\n        # dummy setter to enable self.column += ..\n        # in py3\n        pass\n\n    def row_at(self, index):\n        \"\"\"\n        Gets the data at the specified row\n        \"\"\"\n        if index in self.row_range():\n            cell_array = []\n            for i in self.column_range():\n                cell_array.append(self.cell_value(index, i))\n            return cell_array\n        else:\n            raise IndexError(MESSAGE_INDEX_OUT_OF_RANGE)\n\n    def column_at(self, index):\n        \"\"\"\n        Gets the data at the specified column\n        \"\"\"\n        if index in self.column_range():\n            cell_array = []\n            for i in self.row_range():\n                cell_array.append(self.cell_value(i, index))\n            return cell_array\n        else:\n            raise IndexError(MESSAGE_INDEX_OUT_OF_RANGE)\n\n    def set_column_at(self, column_index, data_array, starting=0):\n        \"\"\"Updates a column data range\n\n        It works like this if the call is:\n        set_column_at(2, ['N','N', 'N'], 1)::\n\n                +--> column_index = 2\n                |\n            A B C\n            1 3 N <- starting = 1\n            2 4 N\n\n        This function will not set element outside the current table range\n\n        :param int column_index: which column to be modified\n        :param list data_array: one dimensional array\n        :param int staring: from which index, the update happens\n        :raises IndexError: if column_index exceeds column range\n                            or starting exceeds row range\n        \"\"\"\n        nrows = self.number_of_rows()\n        ncolumns = self.number_of_columns()\n        if column_index < ncolumns and starting < nrows:\n            real_len = len(data_array)+starting\n            to = min(real_len, nrows)\n            for i in range(starting, to):\n                self.cell_value(i, column_index, data_array[i-starting])\n            if real_len > nrows:\n                for i in range(nrows, real_len):\n                    new_row = [''] * column_index + [data_array[i-starting]]\n                    self.array.append(new_row)\n            self.width, self.array = uniform(self.array)\n        else:\n            raise IndexError(MESSAGE_INDEX_OUT_OF_RANGE)\n\n    def set_row_at(self, row_index, data_array, starting=0):\n        \"\"\"Update a row data range\n\n        It works like this if the call is: set_row_at(2, ['N', 'N', 'N'], 1)::\n\n            A B C\n            1 3 5\n            2 N N <- row_index = 2\n              ^starting = 1\n\n        This function will not set element outside the current table range\n\n        :param int row_index: which row to be modified\n        :param list data_array: one dimensional array\n        :param int starting: from which index, the update happens\n        :raises IndexError: if row_index exceeds row range or starting\n                            exceeds column range\n        \"\"\"\n        nrows = self.number_of_rows()\n        ncolumns = self.number_of_columns()\n        if row_index < nrows and starting < ncolumns:\n            real_len = len(data_array)+starting\n            to = min(real_len, ncolumns)\n            for i in range(starting, to):\n                self.cell_value(row_index, i, data_array[i-starting])\n            if real_len > ncolumns:\n                left = ncolumns - starting\n                self.array[row_index] = self.array[row_index] + data_array[left:]\n            self.width, self.array = uniform(self.array)\n        else:\n            raise IndexError(MESSAGE_INDEX_OUT_OF_RANGE)\n\n    def _extend_row(self, row):\n        array = copy.deepcopy(row)\n        self.array.append(array)\n\n    def extend_rows(self, rows):\n        \"\"\"Inserts two dimensinal data after the bottom row\"\"\"\n        if isinstance(rows, list):\n            if is_array_type(rows, list):\n                for r in rows:\n                    self._extend_row(r)\n            else:\n                self._extend_row(rows)\n            self.width, self.array = uniform(self.array)\n        else:\n            raise TypeError(\"Cannot use %s\" % type(rows))\n\n    def delete_rows(self, row_indices):\n        \"\"\"Deletes specified row indices\"\"\"\n        if isinstance(row_indices, list) is False:\n            raise IndexError\n        if len(row_indices) > 0:\n            unique_list = _unique(row_indices)\n            sorted_list = sorted(unique_list, reverse=True)\n            for i in sorted_list:\n                if i < self.number_of_rows():\n                    del self.array[i]\n\n    def extend_columns(self, columns):\n        \"\"\"Inserts two dimensional data after the rightmost column\n\n        This is how it works:\n\n        Given::\n\n            s s s     t t\n\n        Get::\n\n            s s s  +  t t\n        \"\"\"\n        if not isinstance(columns, list):\n            raise TypeError(MESSAGE_DATA_ERROR_DATA_TYPE_MISMATCH)\n        incoming_data = columns\n        if not is_array_type(columns, list):\n            incoming_data = [columns]\n        incoming_data = transpose(incoming_data)\n        self._extend_columns_with_rows(incoming_data)\n\n    def _extend_columns_with_rows(self, rows):\n        current_nrows = self.number_of_rows()\n        current_ncols = self.number_of_columns()\n        insert_column_nrows = len(rows)\n        array_length = min(current_nrows, insert_column_nrows)\n        for i in range(0, array_length):\n            array = copy.deepcopy(rows[i])\n            self.array[i] += array\n        if current_nrows < insert_column_nrows:\n            delta = insert_column_nrows - current_nrows\n            base = current_nrows\n            for i in range(0, delta):\n                new_array = [\"\"] * current_ncols\n                new_array += rows[base+i]\n                self.array.append(new_array)\n        self.width, self.array = uniform(self.array)\n\n    def extend_columns_with_rows(self, rows):\n        \"\"\"Rows were appended to the rightmost side\n\n        example::\n\n            >>> import pyexcel as pe\n            >>> data = [\n            ...     [1],\n            ...     [2],\n            ...     [3]\n            ... ]\n            >>> matrix = pe.sheets.Matrix(data)\n            >>> matrix\n            +---+\n            | 1 |\n            +---+\n            | 2 |\n            +---+\n            | 3 |\n            +---+\n            >>> rows = [\n            ...      [11, 11],\n            ...      [22, 22]\n            ... ]\n            >>> matrix.extend_columns_with_rows(rows)\n            >>> matrix\n            +---+----+----+\n            | 1 | 11 | 11 |\n            +---+----+----+\n            | 2 | 22 | 22 |\n            +---+----+----+\n            | 3 |    |    |\n            +---+----+----+\n        \"\"\"\n        self._extend_columns_with_rows(rows)\n\n    def paste(self, topleft_corner, rows=None, columns=None):\n        \"\"\"Paste a rectangle shaped data after a position\n\n        :param slice topleft_corner: the top left corner of the rectangle\n\n        example::\n\n            >>> import pyexcel as pe\n            >>> data = [\n            ...     # 0 1  2  3  4 5   6\n            ...     [1, 2, 3, 4, 5, 6, 7], #  0\n            ...     [21, 22, 23, 24, 25, 26, 27],\n            ...     [31, 32, 33, 34, 35, 36, 37],\n            ...     [41, 42, 43, 44, 45, 46, 47],\n            ...     [51, 52, 53, 54, 55, 56, 57]  # 4\n            ... ]\n            >>> s = pe.Sheet(data)\n            >>> # cut  1<= row < 4, 1<= column < 5\n            >>> data = s.cut([1, 1], [4, 5])\n            >>> s.paste([4,6], rows=data)\n            >>> s\n            Sheet Name: pyexcel\n            +----+----+----+----+----+----+----+----+----+----+\n            | 1  | 2  | 3  | 4  | 5  | 6  | 7  |    |    |    |\n            +----+----+----+----+----+----+----+----+----+----+\n            | 21 |    |    |    |    | 26 | 27 |    |    |    |\n            +----+----+----+----+----+----+----+----+----+----+\n            | 31 |    |    |    |    | 36 | 37 |    |    |    |\n            +----+----+----+----+----+----+----+----+----+----+\n            | 41 |    |    |    |    | 46 | 47 |    |    |    |\n            +----+----+----+----+----+----+----+----+----+----+\n            | 51 | 52 | 53 | 54 | 55 | 56 | 22 | 23 | 24 | 25 |\n            +----+----+----+----+----+----+----+----+----+----+\n            |    |    |    |    |    |    | 32 | 33 | 34 | 35 |\n            +----+----+----+----+----+----+----+----+----+----+\n            |    |    |    |    |    |    | 42 | 43 | 44 | 45 |\n            +----+----+----+----+----+----+----+----+----+----+\n            >>> s.paste([6,9], columns=data)\n            >>> s\n            Sheet Name: pyexcel\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            | 1  | 2  | 3  | 4  | 5  | 6  | 7  |    |    |    |    |    |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            | 21 |    |    |    |    | 26 | 27 |    |    |    |    |    |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            | 31 |    |    |    |    | 36 | 37 |    |    |    |    |    |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            | 41 |    |    |    |    | 46 | 47 |    |    |    |    |    |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            | 51 | 52 | 53 | 54 | 55 | 56 | 22 | 23 | 24 | 25 |    |    |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            |    |    |    |    |    |    | 32 | 33 | 34 | 35 |    |    |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            |    |    |    |    |    |    | 42 | 43 | 44 | 22 | 32 | 42 |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            |    |    |    |    |    |    |    |    |    | 23 | 33 | 43 |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            |    |    |    |    |    |    |    |    |    | 24 | 34 | 44 |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n            |    |    |    |    |    |    |    |    |    | 25 | 35 | 45 |\n            +----+----+----+----+----+----+----+----+----+----+----+----+\n\n        \"\"\"\n        if rows:\n            starting_row = topleft_corner[0]\n            number_of_rows = self.number_of_rows()\n            for index, row in enumerate(rows):\n                set_index = starting_row + index\n                if set_index < number_of_rows:\n                    self.set_row_at(set_index, row, starting=topleft_corner[1])\n                else:\n                    real_row = [\"\"] * topleft_corner[1] + row\n                    self._extend_row(real_row)\n            self.width, self.array = uniform(self.array)\n        elif columns:\n            starting_column = topleft_corner[1]\n            number_of_columns = self.number_of_columns()\n            for index, column in enumerate(columns):\n                set_index = starting_column + index\n                if set_index < number_of_columns:\n                    self.set_column_at(set_index,\n                                       column,\n                                       starting=topleft_corner[0])\n                else:\n                    real_column = [\"\"] * topleft_corner[0] + column\n                    self.extend_columns([real_column])\n            self.width, self.array = uniform(self.array)\n        else:\n            raise ValueError(MESSAGE_DATA_ERROR_EMPTY_CONTENT)\n\n    def delete_columns(self, column_indices):\n        \"\"\"Delete columns by specified list of indices\n        \"\"\"\n        if isinstance(column_indices, list) is False:\n            raise TypeError(MESSAGE_DATA_ERROR_DATA_TYPE_MISMATCH)\n        if len(column_indices) > 0:\n            unique_list = _unique(column_indices)\n            sorted_list = sorted(unique_list, reverse=True)\n            for i in range(0, len(self.array)):\n                for j in sorted_list:\n                    del self.array[i][j]\n            self.width = longest_row_number(self.array)\n\n    def __setitem__(self, aset, c):\n        \"\"\"Override the operator to set items\"\"\"\n        if isinstance(aset, tuple):\n            return self.cell_value(aset[0], aset[1], c)\n        elif isinstance(aset, str):\n            row, column = _excel_cell_position(aset)\n            return self.cell_value(row, column, c)\n        else:\n            raise IndexError\n\n    def __getitem__(self, aset):\n        \"\"\"By default, this class recognize from top to bottom\n        from left to right\"\"\"\n        if isinstance(aset, tuple):\n            return self.cell_value(aset[0], aset[1])\n        elif isinstance(aset, str):\n            row, column = _excel_cell_position(aset)\n            return self.cell_value(row, column)\n        elif isinstance(aset, int):\n            print(MESSAGE_DEPRECATED_ROW_COLUMN)\n            return self.row_at(aset)\n        else:\n            raise IndexError\n\n    def contains(self, predicate):\n        \"\"\"Has something in the table\"\"\"\n        for r in self.rows():\n            if predicate(r):\n                return True\n        else:\n            return False\n\n    def transpose(self):\n        \"\"\"Roate the data table by 90 degrees\n\n        Reference :func:`transpose`\n        \"\"\"\n        self.array = transpose(self.array)\n        self.width, self.array = uniform(self.array)\n\n    def to_array(self):\n        \"\"\"Get an array out\n        \"\"\"\n        return self.array\n\n    def __border__(self):\n        return ['-', '|', '+', '-']\n\n    def __repr__(self):\n        return self.__str__()\n\n    @outsource\n    def __str__(self):\n        from ..formatters import to_format\n        table = Texttable(max_width=0)\n        table.set_chars(self.__border__())\n        data = self.to_array()\n        for sub_array in data:\n            new_array = []\n            for item in sub_array:\n                if item == \"\":\n                    new_array.append(\" \")\n                else:\n                    new_array.append(to_format(str, item))\n            table.add_row(new_array)\n        return table.draw()\n", 
    "pyexcel.sheets.nominablesheet": "\"\"\"\n    pyexcel.sheets.nominablesheet\n    ~~~~~~~~~~~~~~~~~~~\n\n    Building on top of filterablesheet, adding named columns and rows support\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom texttable import Texttable\nfrom .matrix import Row, Column, Matrix\nfrom .formattablesheet import FormattableSheet\nfrom .filterablesheet import FilterableSheet\nfrom ..formatters import (\n    ColumnFormatter,\n    RowFormatter,\n    NamedColumnFormatter,\n    NamedRowFormatter)\nfrom .._compact import is_string, OrderedDict, PY2, is_array_type\nfrom ..filters import ColumnIndexFilter, RowIndexFilter\nfrom ..iterators import (\n    ColumnIndexIterator,\n    RowIndexIterator,\n    NamedRowIterator,\n    NamedColumnIterator\n)\nfrom ..presentation import outsource\nfrom ..constants import MESSAGE_NOT_IMPLEMENTED_02, MESSAGE_DATA_ERROR_ORDEREDDICT_IS_EXPECTED, DEFAULT_NAME\n\n\ndef names_to_indices(names, series):\n    if isinstance(names, str):\n        indices = series.index(names)\n    elif (isinstance(names, list) and\n          isinstance(names[0], str)):\n        # translate each row name to index\n        indices = [series.index(astr) for astr in names]\n    else:\n        return names\n    return indices\n\n\ndef make_names_unique(alist):\n    duplicates = {}\n    new_names = []\n    for item in alist:\n        if item in duplicates:\n            duplicates[item] = duplicates[item] + 1\n            new_names.append(\"%s-%d\" % (item, duplicates[item]))\n        else:\n            duplicates[item] = 0\n            new_names.append(str(item))\n    return new_names\n\n\nclass NamedRow(Row):\n    \"\"\"Series Sheet would have Named Row instead of Row\n\n    Here is an example to merge sheets. Suppose we have the\n    following three files::\n\n        >>> import pyexcel as pe\n        >>> data = [[1,2,3],[4,5,6],[7,8,9]]\n        >>> s = pe.Sheet(data)\n        >>> s.save_as(\"1.csv\")\n        >>> data2 = [['a','b','c'],['d','e','f'],['g','h','i']]\n        >>> s2 = pe.Sheet(data2)\n        >>> s2.save_as(\"2.csv\")\n        >>> data3=[[1.1, 2.2, 3.3],[4.4, 5.5, 6.6],[7.7, 8.8, 9.9]]\n        >>> s3=pe.Sheet(data3)\n        >>> s3.save_as(\"3.csv\")\n\n\n        >>> merged = pe.Sheet()\n        >>> for file in [\"1.csv\", \"2.csv\", \"3.csv\"]:\n        ...     r = pe.get_sheet(file_name=file)\n        ...     merged.row += r\n        >>> merged.save_as(\"merged.csv\")\n\n    Now let's verify what we had::\n\n        >>> r=pe.get_sheet(file_name=\"merged.csv\")\n\n    this is added to overcome doctest's inability to handle\n    python 3's unicode::\n\n        >>> r.format(lambda v: str(v))\n        >>> print(pe.utils.to_array(r))\n        [['1', '2', '3'], ['4', '5', '6'], ['7', '8', '9'], ['a', 'b', 'c'], ['d', 'e', 'f'], ['g', 'h', 'i'], ['1.1', '2.2', '3.3'], ['4.4', '5.5', '6.6'], ['7.7', '8.8', '9.9']]\n\n    .. testcleanup::\n        >>> import os\n        >>> os.unlink(\"1.csv\")\n        >>> os.unlink(\"2.csv\")\n        >>> os.unlink(\"3.csv\")\n        >>> os.unlink(\"merged.csv\")\n\n    \"\"\"\n    def select(self, names):\n        \"\"\"Delete row indices other than specified\n        \n        Examples:\n\n            >>> import pyexcel as pe\n            >>> data = [[1],[2],[3],[4],[5],[6],[7],[9]]\n            >>> sheet = pe.Sheet(data)\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+\n            | 1 |\n            +---+\n            | 2 |\n            +---+\n            | 3 |\n            +---+\n            | 4 |\n            +---+\n            | 5 |\n            +---+\n            | 6 |\n            +---+\n            | 7 |\n            +---+\n            | 9 |\n            +---+\n            >>> sheet.row.select([1,2,3,5])\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+\n            | 2 |\n            +---+\n            | 3 |\n            +---+\n            | 4 |\n            +---+\n            | 6 |\n            +---+\n            >>> data = [\n            ...     ['a', 1],\n            ...     ['b', 1],\n            ...     ['c', 1]\n            ... ]\n            >>> sheet = pe.Sheet(data, name_rows_by_column=0)\n            >>> sheet.row.select(['a', 'b'])\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+\n            | a | 1 |\n            +---+---+\n            | b | 1 |\n            +---+---+\n\n        \"\"\"\n        if is_array_type(names, str):\n            indices = names_to_indices(names, self.ref.rownames)\n            Row.select(self, indices)\n        else:\n            Row.select(self, names)\n\n    def __delitem__(self, column_name):\n        \"\"\"\n\n        Examples::\n\n            >>> import pyexcel as pe\n            >>> data = [\n            ...     ['a', 1],\n            ...     ['b', 1],\n            ...     ['c', 1]\n            ... ]\n            >>> sheet = pe.Sheet(data, name_rows_by_column=0)\n            >>> del sheet.row['a', 'b']\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+\n            | c | 1 |\n            +---+---+\n\n        \"\"\"\n        if is_string(type(column_name)):\n            self.ref.delete_named_row_at(column_name)\n        elif isinstance(column_name, tuple) and is_array_type(list(column_name), str):\n            indices = names_to_indices(list(column_name), self.ref.rownames)\n            Row.__delitem__(self, indices)\n        else:\n            Row.__delitem__(self, column_name)\n\n    def __setitem__(self, str_or_aslice, c):\n        if is_string(type(str_or_aslice)):\n            self.ref.set_named_row_at(str_or_aslice, c)\n        else:\n            Row.__setitem__(self, str_or_aslice, c)\n\n    def __getitem__(self, str_or_aslice):\n        if is_string(type(str_or_aslice)):\n            return self.ref.named_row_at(str_or_aslice)\n        else:\n            return Row.__getitem__(self, str_or_aslice)\n\n    def __iadd__(self, other):\n        \"\"\"Overload += sign\n\n        :param list other: the row header must be the first element.\n        :return: self\n        \"\"\"\n        if isinstance(other, OrderedDict):\n            self.ref.extend_rows(other)\n        else:\n            Row.__iadd__(self, other)\n        return self\n\n    def __add__(self, other):\n        \"\"\"Overload += sign\n\n        :return: self\n        \"\"\"\n        self.__iadd__(other)\n        return self.ref\n\n    def format(self,\n               row_index=None, formatter=None,\n               format_specs=None, on_demand=False):\n        \"\"\"Format a row\n        \"\"\"\n        def handle_one_formatter(rows, theformatter, on_demand):\n            new_indices = rows\n            if len(self.ref.rownames) > 0:\n                new_indices = names_to_indices(rows, self.ref.rownames)\n            aformatter = RowFormatter(new_indices, theformatter)\n            if on_demand:\n                self.ref.add_formatter(aformatter)\n            else:\n                self.ref.apply_formatter(aformatter)\n        if row_index is not None:\n            handle_one_formatter(row_index, formatter, on_demand)\n        elif format_specs:\n            for spec in format_specs:\n                if len(spec) == 3:\n                    handle_one_formatter(spec[0], spec[1],\n                                         on_demand)\n                else:\n                    handle_one_formatter(spec[0], spec[1],\n                                         on_demand)\n\n\nclass NamedColumn(Column):\n    \"\"\"Series Sheet would have Named Column instead of Column\n\n    example::\n\n        import pyexcel as pe\n\n        r = pe.SeriesReader(\"example.csv\")\n        print(r.column[\"column 1\"])\n\n    \"\"\"\n    def select(self, names):\n        \"\"\"Delete columns other than specified\n        \n        Examples:\n     \n            >>> import pyexcel as pe\n            >>> data = [[1,2,3,4,5,6,7,9]]\n            >>> sheet = pe.Sheet(data)\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+---+---+---+---+\n            | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 9 |\n            +---+---+---+---+---+---+---+---+\n            >>> sheet.column.select([1,2,3,5])\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+\n            | 2 | 3 | 4 | 6 |\n            +---+---+---+---+\n            >>> data = [\n            ...     ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'],\n            ...     [1,2,3,4,5,6,7,9],\n            ... ]\n            >>> sheet = pe.Sheet(data, name_columns_by_row=0)\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+---+---+---+---+\n            | a | b | c | d | e | f | g | h |\n            +===+===+===+===+===+===+===+===+\n            | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 9 |\n            +---+---+---+---+---+---+---+---+\n            >>> del sheet.column['a', 'b', 'i', 'f'] # doctest:+ELLIPSIS\n            Traceback (most recent call last):\n                ...\n            ValueError: ...\n            >>> sheet.column.select(['a', 'c', 'e', 'h'])\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+\n            | a | c | e | h |\n            +===+===+===+===+\n            | 1 | 3 | 5 | 9 |\n            +---+---+---+---+\n\n        \"\"\"\n        if is_array_type(names, str):\n            indices = names_to_indices(names, self.ref.colnames)\n            Column.select(self, indices)\n        else:\n            Column.select(self, names)\n\n    def __delitem__(self, str_or_aslice):\n        \"\"\"\n\n        Example::\n\n            >>> import pyexcel as pe\n            >>> data = [\n            ...     ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'],\n            ...     [1,2,3,4,5,6,7,9],\n            ... ]\n            >>> sheet = pe.Sheet(data, name_columns_by_row=0)\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+---+---+---+---+\n            | a | b | c | d | e | f | g | h |\n            +===+===+===+===+===+===+===+===+\n            | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 9 |\n            +---+---+---+---+---+---+---+---+\n            >>> del sheet.column['a', 'b', 'i', 'f'] # doctest:+ELLIPSIS\n            Traceback (most recent call last):\n                ...\n            ValueError: ...\n            >>> del sheet.column['a', 'c', 'e', 'h']\n            >>> sheet\n            Sheet Name: pyexcel\n            +---+---+---+---+\n            | b | d | f | g |\n            +===+===+===+===+\n            | 2 | 4 | 6 | 7 |\n            +---+---+---+---+\n\n        \"\"\"\n        if is_string(type(str_or_aslice)):\n            self.ref.delete_named_column_at(str_or_aslice)\n        elif isinstance(str_or_aslice, tuple) and is_array_type(list(str_or_aslice), str):\n            indices = names_to_indices(list(str_or_aslice), self.ref.colnames)\n            Column.__delitem__(self, indices)\n        else:\n            Column.__delitem__(self, str_or_aslice)\n\n    def __setitem__(self, str_or_aslice, c):\n        if is_string(type(str_or_aslice)):\n            self.ref.set_named_column_at(str_or_aslice, c)\n        else:\n            Column.__setitem__(self, str_or_aslice, c)\n\n    def __getitem__(self, str_or_aslice):\n        if is_string(type(str_or_aslice)):\n            return self.ref.named_column_at(str_or_aslice)\n        else:\n            return Column.__getitem__(self, str_or_aslice)\n\n    def __iadd__(self, other):\n        \"\"\"Overload += sign\n\n        :param list other: the column header must be the first element.\n        :return: self\n        \"\"\"\n        if isinstance(other, OrderedDict):\n            self.ref.extend_columns(other)\n        else:\n            Column.__iadd__(self, other)\n        return self\n\n    def __add__(self, other):\n        \"\"\"Overload += sign\n\n        :return: self\n        \"\"\"\n        self.__iadd__(other)\n        return self.ref\n\n    def format(self,\n               column_index=None, formatter=None,\n               format_specs=None, on_demand=False):\n        \"\"\"Format a column\n        \"\"\"\n        def handle_one_formatter(columns, aformatter, on_demand):\n            new_indices = columns\n            if len(self.ref.colnames) > 0:\n                new_indices = names_to_indices(columns, self.ref.colnames)\n            theformatter = ColumnFormatter(new_indices, aformatter)\n            if on_demand:\n                self.ref.add_formatter(theformatter)\n            else:\n                self.ref.apply_formatter(theformatter)\n        if column_index is not None:\n            handle_one_formatter(column_index, formatter, on_demand)\n        elif format_specs:\n            for spec in format_specs:\n                if len(spec) == 3:\n                    handle_one_formatter(spec[0], spec[1],\n                                         on_demand)\n                else:\n                    handle_one_formatter(spec[0], spec[1],\n                                         on_demand)\n\n\nVALID_SHEET_PARAMETERS = ['name_columns_by_row',\n                          'name_rows_by_column',\n                          'colnames',\n                          'rownames',\n                          'transpose_before',\n                          'transpose_after']\n                    \nclass NominableSheet(FilterableSheet):\n    \"\"\"Allow dictionary group of the content\n    \"\"\"\n    def __init__(self, sheet=None, name=DEFAULT_NAME,\n                 name_columns_by_row=-1,\n                 name_rows_by_column=-1,\n                 colnames=None,\n                 rownames=None,\n                 transpose_before=False,\n                 transpose_after=False):\n        \"\"\"Constructor\n\n        :param sheet: two dimensional array\n        :param name: this becomes the sheet name.\n        :param name_columns_by_row: use a row to name all columns\n        :param name_rows_by_column: use a column to name all rows\n        :param colnames: use an external list of strings to name the columns\n        :param rownames: use an external list of strings to name the rows\n        \"\"\"\n        # this get rid of phatom data by not specifying sheet\n        if sheet is None:\n            sheet = []\n        FilterableSheet.__init__(self, sheet)\n        if transpose_before:\n            self.transpose()\n        self.name = name\n        self._column_names = []\n        self._row_names = []\n        self.named_row = NamedRow(self)\n        self.named_column = NamedColumn(self)\n        if name_columns_by_row != -1:\n            if colnames:\n                raise NotImplementedError(MESSAGE_NOT_IMPLEMENTED_02)\n            self.name_columns_by_row(name_columns_by_row)\n        else:\n            if colnames:\n                self._column_names = colnames\n        if name_rows_by_column != -1:\n            if rownames:\n                raise NotImplementedError(MESSAGE_NOT_IMPLEMENTED_02)\n            self.name_rows_by_column(name_rows_by_column)\n        else:\n            if rownames:\n                self._row_names = rownames\n        if transpose_after:\n            self.transpose()\n\n    @property\n    def row(self):\n        \"\"\"Row representation. see :class:`NamedRow`\n\n        examples::\n\n            >>> import pyexcel as pe\n            >>> data = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n            >>> sheet = pe.Sheet(data)\n            >>> sheet.row[1]\n            [4, 5, 6]\n            >>> sheet.row[0:3]\n            [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n            >>> sheet.row += [11, 12, 13]\n            >>> sheet.row[3]\n            [11, 12, 13]\n            >>> sheet.row[0:4] = [0, 0, 0] # set all to zero\n            >>> sheet.row[3]\n            [0, 0, 0]\n            >>> sheet.row[0] = ['a', 'b', 'c'] # set one row\n            >>> sheet.row[0]\n            ['a', 'b', 'c']\n            >>> del sheet.row[0] # delete first row\n            >>> sheet.row[0] # now, second row becomes the first\n            [0, 0, 0]\n            >>> del sheet.row[0:]\n            >>> sheet.row[0]  # nothing left\n            Traceback (most recent call last):\n                ...\n            IndexError\n        \"\"\"\n        return self.named_row\n\n    @row.setter\n    def row(self, value):\n        # dummy setter to enable self.row += ..\n        pass\n\n    @property\n    def column(self):\n        \"\"\"Column representation. see :class:`NamedColumn`\"\"\"\n        return self.named_column\n\n    @column.setter\n    def column(self, value):\n        # dummy setter to enable self.column += ..\n        pass\n\n    def name_columns_by_row(self, row_index):\n        \"\"\"Use the elements of a specified row to represent individual columns\n\n        The specified row will be deleted from the data\n        :param int row_index: the index of the row that has the column names\n        \"\"\"\n        self.row_index = row_index\n        self._column_names = make_names_unique(self.row_at(row_index))\n        del self.row[row_index]\n\n    def name_rows_by_column(self, column_index):\n        \"\"\"Use the elements of a specified column to represent individual rows\n\n        The specified column will be deleted from the data\n        :param int column_index: the index of the column that has the row names\n        \"\"\"\n        self.column_index = column_index\n        self._row_names = make_names_unique(self.column_at(column_index))\n        del self.column[column_index]\n\n    @property\n    def colnames(self):\n        \"\"\"Return column names\"\"\"\n        if len(self._filters) != 0:\n            column_filters = [f for f in self._filters\n                              if isinstance(f, ColumnIndexFilter)]\n            if len(column_filters) != 0:\n                indices = range(0, len(self._column_names))\n                for f in column_filters:\n                    indices = [i for i in indices if i not in f.indices]\n                return [self._column_names[i] for i in indices]\n            else:\n                return self._column_names\n        else:\n            return self._column_names\n\n    @colnames.setter\n    def colnames(self, value):\n        \"\"\"Set column names\"\"\"\n        self._column_names = make_names_unique(value)\n\n    @property\n    def rownames(self):\n        \"\"\"Return row names\"\"\"\n        if len(self._filters) != 0:\n            row_filters = [f for f in self._filters\n                           if isinstance(f, RowIndexFilter)]\n            if len(row_filters) != 0:\n                indices = range(0, len(self._row_names))\n                for f in row_filters:\n                    indices = [i for i in indices if i not in f.indices]\n                return [self._row_names[i] for i in indices]\n            else:\n                return self._row_names\n        else:\n            return self._row_names\n\n    @rownames.setter\n    def rownames(self, value):\n        \"\"\"Set row names\"\"\"\n        self._row_names = make_names_unique(value)\n\n    def named_column_at(self, name):\n        \"\"\"Get a column by its name \"\"\"\n        index = name\n        if is_string(type(index)):\n            index = self.colnames.index(name)\n        column_array = self.column_at(index)\n        return column_array\n\n    def set_named_column_at(self, name, column_array):\n        \"\"\"\n        Take the first row as column names\n\n        Given name to identify the column index, set the column to\n        the given array except the column name.\n        \"\"\"\n        index = name\n        if is_string(type(index)):\n            index = self.colnames.index(name)\n        self.set_column_at(index, column_array)\n\n    def delete_columns(self, column_indices):\n        \"\"\"Delete one or more columns\n\n        :param list column_indices: a list of column indices\n        \"\"\"\n        FilterableSheet.delete_columns(self, column_indices)\n        if len(self._column_names) > 0:\n            new_series = [self._column_names[i]\n                          for i in range(0, len(self._column_names))\n                          if i not in column_indices]\n            self._column_names = new_series\n\n    def delete_rows(self, row_indices):\n        \"\"\"Delete one or more rows\n\n        :param list row_indices: a list of row indices\n        \"\"\"\n        FilterableSheet.delete_rows(self, row_indices)\n        if len(self._row_names) > 0:\n            new_series = [self._row_names[i]\n                          for i in range(0, len(self._row_names))\n                          if i not in row_indices]\n            self._row_names = new_series\n\n    def delete_named_column_at(self, name):\n        \"\"\"Works only after you named columns by a row\n\n        Given name to identify the column index, set the column to\n        the given array except the column name.\n        :param str name: a column name\n        \"\"\"\n        if isinstance(name, int):\n            if len(self.rownames) > 0:\n                self.rownames.pop(name)\n            self.delete_columns([name])\n        else:\n            index = self.colnames.index(name)\n            self.colnames.pop(index)\n            FilterableSheet.delete_columns(self, [index])\n\n    def named_row_at(self, name):\n        \"\"\"Get a row by its name \"\"\"\n        index = name\n        # if is_string(type(index)):\n        index = self.rownames.index(name)\n        row_array = self.row_at(index)\n        return row_array\n\n    def set_named_row_at(self, name, row_array):\n        \"\"\"\n        Take the first column as row names\n\n        Given name to identify the row index, set the row to\n        the given array except the row name.\n        \"\"\"\n        index = name\n        if is_string(type(index)):\n            index = self.rownames.index(name)\n        self.set_row_at(index, row_array)\n\n    def delete_named_row_at(self, name):\n        \"\"\"Take the first column as row names\n\n        Given name to identify the row index, set the row to\n        the given array except the row name.\n        \"\"\"\n        if isinstance(name, int):\n            if len(self.rownames) > 0:\n                self.rownames.pop(name)\n            self.delete_rows([name])\n        else:\n            index = self.rownames.index(name)\n            self.rownames.pop(index)\n            FilterableSheet.delete_rows(self, [index])\n\n    def apply_formatter(self, aformatter):\n        \"\"\"Apply the formatter immediately.\n\n        :param Formatter aformatter: a custom formatter\n        \"\"\"\n        aformatter = self._translate_named_formatter(aformatter)\n        FormattableSheet.apply_formatter(self, aformatter)\n\n    def _translate_named_formatter(self, aformatter):\n        if isinstance(aformatter, NamedColumnFormatter):\n            series = self.colnames\n        elif isinstance(aformatter, NamedRowFormatter):\n            series = self.rownames\n        else:\n            series = None\n        if series:\n            indices = names_to_indices(aformatter.indices, series)\n            aformatter.update_index(indices)\n        return aformatter\n\n    def add_formatter(self, aformatter):\n        \"\"\"Add a lazy formatter.\n\n        The formatter takes effect on the fly when a cell value is read\n        This is cost effective when you have a big data table\n        and you use only a few rows or columns. If you have farily modest\n        data table, you can choose apply_formatter() too.\n\n        :param Formatter aformatter: a custom formatter\n        \"\"\"\n        aformatter = self._translate_named_formatter(aformatter)\n        FormattableSheet.add_formatter(self, aformatter)\n\n    def extend_rows(self, rows):\n        \"\"\"Take ordereddict to extend named rows\n\n        :param ordereddist/list rows: a list of rows.\n        \"\"\"\n        incoming_data = []\n        if isinstance(rows, OrderedDict):\n            keys = rows.keys()\n            for k in keys:\n                self.rownames.append(k)\n                incoming_data.append(rows[k])\n            FilterableSheet.extend_rows(self, incoming_data)\n        elif len(self.rownames) > 0:\n            raise TypeError(MESSAGE_DATA_ERROR_ORDEREDDICT_IS_EXPECTED)\n        else:\n            FilterableSheet.extend_rows(self, rows)\n\n    def extend_columns_with_rows(self, rows):\n        \"\"\"Put rows on the right most side of the data\"\"\"\n        if len(self.colnames) > 0:\n            headers = rows.pop(self.row_index)\n            self._column_names += headers\n        FilterableSheet.extend_columns_with_rows(self, rows)\n\n    def extend_columns(self, columns):\n        \"\"\"Take ordereddict to extend named columns\n\n        :param ordereddist/list columns: a list of columns\n        \"\"\"\n        incoming_data = []\n        if isinstance(columns, OrderedDict):\n            keys = columns.keys()\n            for k in keys:\n                self.colnames.append(k)\n                incoming_data.append(columns[k])\n            FilterableSheet.extend_columns(self, incoming_data)\n        elif len(self.colnames) > 0:\n            raise TypeError(MESSAGE_DATA_ERROR_ORDEREDDICT_IS_EXPECTED)\n        else:\n            FilterableSheet.extend_columns(self, columns)\n\n    def __iter__(self):\n        if len(self._column_names) > 0:\n            return ColumnIndexIterator(self)\n        elif len(self._row_names) > 0:\n            return RowIndexIterator(self)\n        else:\n            return FilterableSheet.__iter__(self)\n\n    def to_array(self):\n        \"\"\"Returns an array after filtering\"\"\"\n        from ..utils import to_array\n        ret = []\n        ret += to_array(self.rows())\n        if len(self.rownames) > 0:\n            ret = map(lambda value: [value[0]] + value[1],\n                      zip(self.rownames, ret))\n            if not PY2:\n                ret = list(ret)\n        if len(self.colnames) > 0:\n            if len(self.rownames) > 0:\n                ret.insert(0, [\"\"] + self.colnames)\n            else:\n                ret.insert(0, self.colnames)\n        return ret\n\n    def to_records(self, custom_headers=None):\n        \"\"\"Returns the content as an array of dictionaries\n\n        \"\"\"\n        from ..utils import to_records\n        return to_records(self, custom_headers)\n\n    def to_dict(self, row=False):\n        \"\"\"Returns a dictionary\"\"\"\n        from ..utils import to_dict\n        if row:\n            return to_dict(RowIndexIterator(self))\n        else:\n            return to_dict(ColumnIndexIterator(self))\n\n    def __getitem__(self, aset):\n        if isinstance(aset, tuple):\n            if isinstance(aset[0], str):\n                row = self.rownames.index(aset[0])\n            else:\n                row = aset[0]\n\n            if isinstance(aset[1], str):\n                column = self.colnames.index(aset[1])\n            else:\n                column = aset[1]\n            return self.cell_value(row, column)\n        else:\n            return Matrix.__getitem__(self, aset)\n\n    def __border__(self):\n        if len(self.colnames) > 0:\n            return ['-', '|', '+', '=']\n        else:\n            return ['-', '|', '+', '-']\n\n    @outsource\n    def __str__(self):\n        from ..formatters import to_format\n        ret = \"Sheet Name: %s\\n\" % self.name\n        if len(self.colnames) > 0:\n            table = Texttable(max_width=0)\n            table.set_chars(self.__border__())\n            data = self.to_array()\n            new_data = []\n            for sub_array in data:\n                new_array = []\n                for item in sub_array:\n                    if item == \"\":\n                        new_array.append(\" \")\n                    else:\n                        new_array.append(to_format(str,item))\n                new_data.append(new_array)\n            table.add_rows(new_data)\n            return ret+table.draw()\n        else:\n            return ret+FilterableSheet.__str__(self)\n\n    def named_rows(self):\n        return NamedRowIterator(self)\n\n    def named_columns(self):\n        return NamedColumnIterator(self)\n", 
    "pyexcel.sheets.sheet": "\"\"\"\n    pyexcel.sheets\n    ~~~~~~~~~~~~~~~~~~~\n\n    Representation of data sheets\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom .nominablesheet import NominableSheet\n\n\nclass Sheet(NominableSheet):\n    \"\"\"Two dimensional data container for filtering, formatting and iteration\n\n    :class:`Sheet` is a container for a two dimensional array, where individual\n    cell can be any Python types. Other than numbers, value of thsee\n    types: string, date, time and boolean can be mixed in the array. This\n    differs from Numpy's matrix where each cell are of the same number type.\n\n    In order to prepare two dimensional data for your computation, formatting\n    functions help convert array cells to required types. Formatting can be\n    applied not only to the whole sheet but also to selected rows or columns.\n    Custom conversion function can be passed to these formatting functions. For\n    example, to remove extra spaces surrounding the content of a cell, a custom\n    function is required.\n\n    Filtering functions are used to reduce the information contained in the\n    array.\n    \"\"\"\n    def save_to(self, source):\n        \"\"\"Save to a writeable data source\"\"\"\n        source.write_data(self)\n\n    def save_as(self, filename, **keywords):\n        \"\"\"Save the content to a named file\"\"\"\n        from ..sources import SheetSource\n        source = SheetSource(file_name=filename, **keywords)\n        return self.save_to(source)\n\n    def save_to_memory(self, file_type, stream, **keywords):\n        \"\"\"Save the content to memory\n\n        :param str file_type: any value of 'csv', 'tsv', 'csvz',\n                              'tsvz', 'xls', 'xlsm', 'xslm', 'ods'\n        :param iostream stream: the memory stream to be written to. Note in\n                                Python 3, for csv  and tsv format, please\n                                pass an instance of StringIO. For xls, xlsx,\n                                and ods, an instance of BytesIO.\n        \"\"\"\n        self.save_as((file_type, stream), **keywords)\n\n    def save_to_django_model(self,\n                             model,\n                             initializer=None,\n                             mapdict=None,\n                             batch_size=None):\n        \"\"\"Save to database table through django model\n        \n        :param model: a database model\n        :param initializer: a intialization functions for your model\n        :param mapdict: custom map dictionary for your data columns\n        :param batch_size: a parameter to Django concerning the size of data base\n                           set\n        \"\"\"\n        from ..sources import SheetDjangoSource\n        source = SheetDjangoSource(model=model, initializer=initializer, mapdict=mapdict, batch_size=batch_size)\n        self.save_to(source)\n\n    def save_to_database(self, session, table,\n                         initializer=None,\n                         mapdict=None,\n                         auto_commit=True):\n        \"\"\"Save data in sheet to database table\n\n        :param session: database session\n        :param table: a database table\n        :param initializer: a intialization functions for your table\n        :param mapdict: custom map dictionary for your data columns\n        :param auto_commit: by default, data is committed.\n\n        \"\"\"\n        from ..sources import SheetSQLAlchemySource\n        source = SheetSQLAlchemySource(\n            session=session,\n            table=table,\n            initializer=initializer,\n            mapdict=mapdict,\n            auto_commit=auto_commit\n        )\n        self.save_to(source)\n\n\n\n", 
    "pyexcel.sources.__init__": "\"\"\"\n    pyexcel.sources\n    ~~~~~~~~~~~~~~~~~~~\n\n    Representation of excel data sources\n\n    :copyright: (c) 2015 by Onni Software Ltd.\n    :license: New BSD License\n\"\"\"\nimport re\nfrom .base import ReadOnlySource, WriteOnlySource\nfrom ..sheets import VALID_SHEET_PARAMETERS, Sheet\nfrom ..book import Book\nfrom ..constants import (\n    KEYWORD_STARTS_WITH_DEST,\n    MESSAGE_DEPRECATED_OUT_FILE,\n    MESSAGE_DEPRECATED_CONTENT,\n    DEPRECATED_KEYWORD_OUT_FILE,\n    DEPRECATED_KEYWORD_CONTENT,\n    KEYWORD_FILE_CONTENT,\n    KEYWORD_FILE_NAME,\n    KEYWORD_FILE_TYPE,\n    MESSAGE_ERROR_02,\n    MESSAGE_ERROR_NO_HANDLER\n)\nfrom .file import (\n    SheetSource,\n    BookSource\n)\nfrom .memory import (\n    ReadOnlySheetSource,\n    WriteOnlySheetSource,\n    DictSource,\n    RecrodsSource,\n    ArraySource,\n    ReadOnlyBookSource,\n    BookDictSource,\n    WriteOnlyBookSource\n)\nfrom .database import (\n    SheetSQLAlchemySource,\n    SheetDjangoSource,\n    SheetQuerySetSource,\n    BookSQLSource,\n    BookDjangoSource\n)\nfrom .http import HttpBookSource, HttpSheetSource\n\nSOURCES = [\n    ReadOnlySource,\n    SheetSource,\n    ReadOnlySheetSource,\n    SheetSQLAlchemySource,\n    SheetDjangoSource,\n    RecrodsSource,\n    DictSource,\n    SheetQuerySetSource,\n    ArraySource,\n    HttpSheetSource\n]\n\nDEST_SOURCES = [\n    WriteOnlySource,\n    SheetSource,\n    WriteOnlySheetSource,\n    SheetSQLAlchemySource,\n    SheetDjangoSource\n]\n\nBOOK_SOURCES = [\n    ReadOnlySource,\n    BookSource,\n    ReadOnlyBookSource,\n    BookSQLSource,\n    BookDjangoSource,\n    BookDictSource,\n    HttpBookSource\n]\n\nDEST_BOOK_SOURCES = [\n    WriteOnlySource,\n    BookSource,\n    WriteOnlyBookSource,\n    BookDjangoSource,\n    BookSQLSource\n]\n\n\nclass SourceFactory:\n    \"\"\"\n    The factory method to support multiple datasources in getters and savers\n    \"\"\"\n    @classmethod\n    def _get_generic_source(self, registry, action, **keywords):\n        for source in registry:\n            if source.is_my_business(action, **keywords):\n                s = source(**keywords)\n                return s\n        return None\n\n    @classmethod\n    def get_source(self, **keywords):\n        return self._get_generic_source(\n            SOURCES,\n            action='read',\n            **keywords)\n\n    @classmethod\n    def get_book_source(self, **keywords):\n        return self._get_generic_source(\n            BOOK_SOURCES,\n            action='read',\n            **keywords)\n\n    @classmethod\n    def get_writeable_source(self, **keywords):\n        return self._get_generic_source(\n            DEST_SOURCES,\n            action='write',\n            **keywords)\n\n    @classmethod\n    def get_writeable_book_source(self, **keywords):\n        return self._get_generic_source(\n            DEST_BOOK_SOURCES,\n            action='write',\n            **keywords)\n\n\ndef get_sheet(**keywords):\n    \"\"\"Get an instance of :class:`Sheet` from an excel source\n\n    :param file_name: a file with supported file extension\n    :param file_content: the file content\n    :param file_stream: the file stream\n    :param file_type: the file type in *content*\n    :param session: database session\n    :param table: database table\n    :param model: a django model\n    :param adict: a dictionary of one dimensional arrays\n    :param url: a download http url for your excel file\n    :param with_keys: load with previous dictionary's keys, default is True\n    :param records: a list of dictionaries that have the same keys\n    :param array: a two dimensional array, a list of lists\n    :param keywords: additional parameters, see :meth:`Sheet.__init__`\n    :param sheet_name: sheet name. if sheet_name is not given,\n                       the default sheet at index 0 is loaded\n\n    Not all parameters are needed. Here is a table\n\n    ========================== =========================================\n    source                     parameters\n    ========================== =========================================\n    loading from file          file_name, sheet_name, keywords\n    loading from memory        file_type, content, sheet_name, keywords\n    loading from sql           session, table\n    loading from sql in django model\n    loading from query sets    any query sets(sqlalchemy or django)\n    loading from dictionary    adict, with_keys\n    loading from records       records\n    loading from array         array\n    ========================== =========================================\n\n    see also :ref:`a-list-of-data-structures`\n    \"\"\"\n    sheet = None\n    sheet_params = {}\n    for field in VALID_SHEET_PARAMETERS:\n        if field in keywords:\n            sheet_params[field] = keywords.pop(field)\n    if DEPRECATED_KEYWORD_CONTENT in keywords:\n        print(MESSAGE_DEPRECATED_CONTENT)\n        keywords[KEYWORD_FILE_CONTENT] = keywords.pop(\n            DEPRECATED_KEYWORD_CONTENT)\n    source = SourceFactory.get_source(**keywords)\n    if source is not None:\n        sheet_name, data = source.get_data()\n        sheet = Sheet(data, sheet_name, **sheet_params)\n        return sheet\n    raise NotImplementedError(MESSAGE_ERROR_NO_HANDLER)\n\n\ndef get_book(**keywords):\n    \"\"\"Get an instance of :class:`Book` from an excel source\n\n    :param file_name: a file with supported file extension\n    :param file_content: the file content\n    :param file_stream: the file stream\n    :param file_type: the file type in *content*\n    :param session: database session\n    :param tables: a list of database table\n    :param models: a list of django models\n    :param bookdict: a dictionary of two dimensional arrays\n    :param url: a download http url for your excel file\n\n    see also :ref:`a-list-of-data-structures`\n\n    Here is a table of parameters:\n\n    ========================== ============================================\n    source                     parameters\n    ========================== ============================================\n    loading from file          file_name, keywords\n    loading from memory        file_type, content, keywords\n    loading from sql           session, tables\n    loading from django models models\n    loading from dictionary    bookdict\n    ========================== ============================================\n\n    Where the dictionary should have text as keys and two dimensional\n    array as values.\n    \"\"\"\n    if DEPRECATED_KEYWORD_CONTENT in keywords:\n        print(MESSAGE_DEPRECATED_CONTENT)\n        keywords[KEYWORD_FILE_CONTENT] = keywords.pop(\n            DEPRECATED_KEYWORD_CONTENT)\n    source = SourceFactory.get_book_source(**keywords)\n    if source is not None:\n        sheets, filename, path = source.get_data()\n        book = Book(sheets, filename=filename, path=path)\n        return book\n    raise NotImplementedError(MESSAGE_ERROR_NO_HANDLER)\n\n\ndef split_keywords(**keywords):\n    dest_keywords = {}\n    source_keywords = {}\n    for key in keywords.keys():\n        result = re.match(KEYWORD_STARTS_WITH_DEST, key)\n        if result:\n            dest_keywords[result.group(1)] = keywords[key]\n        else:\n            source_keywords[key] = keywords[key]\n    if DEPRECATED_KEYWORD_OUT_FILE in keywords:\n        print(MESSAGE_DEPRECATED_OUT_FILE)\n        dest_keywords[KEYWORD_FILE_NAME] = keywords.pop(\n            DEPRECATED_KEYWORD_OUT_FILE)\n    if DEPRECATED_KEYWORD_CONTENT in keywords:\n        print(MESSAGE_DEPRECATED_CONTENT)\n        dest_keywords[KEYWORD_FILE_CONTENT] = keywords.pop(\n            DEPRECATED_KEYWORD_CONTENT)\n    return dest_keywords, source_keywords\n\n\ndef save_as(**keywords):\n    \"\"\"Save a sheet from a data srouce to another one\n\n    :param dest_file_name: another file name. **out_file** is deprecated\n                           though is still accepted.\n    :param dest_file_type: this is needed if you want to save to memory\n    :param dest_session: the target database session\n    :param dest_table: the target destination table\n    :param dest_model: the target django model\n    :param dest_mapdict: a mapping dictionary, see\n                         :meth:`~pyexcel.Sheet.save_to_memory`\n    :param dest_initializer: a custom initializer function for table or model\n    :param dest_mapdict: nominate headers\n    :param dest_batch_size: object creation batch size.\n                            it is Django specific\n    :param keywords: additional keywords can be found at\n                     :meth:`pyexcel.get_sheet`\n    :returns: IO stream if saving to memory. None otherwise\n\n    ================= =============================================\n    Saving to source  parameters\n    ================= =============================================\n    file              dest_file_name, dest_sheet_name,\n                      keywords with prefix 'dest'\n    memory            dest_file_type, dest_content,\n                      dest_sheet_name, keywords with prefix 'dest'\n    sql               dest_session, table,\n                      dest_initializer, dest_mapdict\n    django model      dest_model, dest_initializer,\n                      dest_mapdict, dest_batch_size\n    ================= =============================================\n    \"\"\"\n    dest_keywords, source_keywords = split_keywords(**keywords)\n    dest_source = SourceFactory.get_writeable_source(**dest_keywords)\n    if dest_source is not None:\n        sheet = get_sheet(**source_keywords)\n        sheet.save_to(dest_source)\n        if KEYWORD_FILE_TYPE in dest_source.fields:\n            dest_source.content.seek(0)\n            return dest_source.content\n    else:\n        raise ValueError(MESSAGE_ERROR_02)\n\n\ndef save_book_as(**keywords):\n    \"\"\"Save a book from a data source to another one\n\n    :param dest_file_name: another file name. **out_file** is\n                           deprecated though is still accepted.\n    :param dest_file_type: this is needed if you want to save to memory\n    :param dest_session: the target database session\n    :param dest_tables: the list of target destination tables\n    :param dest_models: the list of target destination django models\n    :param dest_mapdicts: a list of mapping dictionaries\n    :param dest_initializers: table initialization fuctions\n    :param dest_mapdicts: to nominate a model or table fields. Optional\n    :param dest_batch_size: batch creation size. Optional\n    :param keywords: additional keywords can be found at\n                     :meth:`pyexcel.get_sheet`\n    :returns: IO stream if saving to memory. None otherwise\n\n    ================ ============================================\n    Saving to source parameters\n    ================ ============================================\n    file             dest_file_name, dest_sheet_name,\n                     keywords with prefix 'dest'\n    memory           dest_file_type, dest_content,\n                     dest_sheet_name, keywords with prefix 'dest'\n    sql              dest_session, dest_tables,\n                     dest_table_init_func, dest_mapdict\n    django model     dest_models, dest_initializers,\n                     dest_mapdict, dest_batch_size\n    ================ ============================================\n    \"\"\"\n    dest_keywords, source_keywords = split_keywords(**keywords)\n    dest_source = SourceFactory.get_writeable_book_source(**dest_keywords)\n    if dest_source is not None:\n        book = get_book(**source_keywords)\n        book.save_to(dest_source)\n        if KEYWORD_FILE_TYPE in dest_source.fields:\n            dest_source.content.seek(0)\n            return dest_source.content\n    else:\n        raise ValueError(MESSAGE_ERROR_02)\n", 
    "pyexcel.sources.base": "\"\"\"\n    pyexcel.sources.base\n    ~~~~~~~~~~~~~~~~~~~\n\n    Representation of excel data sources\n\n    :copyright: (c) 2015 by Onni Software Ltd.\n    :license: New BSD License\n\"\"\"\nfrom ..constants import (\n    KEYWORD_SOURCE, KEYWORD_FILE_NAME, KEYWORD_FILE_TYPE)\nfrom .._compact import PY2, is_string\nfrom pyexcel_io import READERS, WRITERS\n\n\ndef _has_field(field, keywords):\n    return field in keywords and keywords[field] is not None\n\n\nclass Source:\n    \"\"\" A command source for get_sheet, get_book, save_as and save_book_as\n\n    This can be used to extend the function parameters once the custom\n    class inherit this and register it with corresponding source registry\n    \"\"\"\n    fields = [KEYWORD_SOURCE]\n\n    def __init__(self, source=None, **keywords):\n        self.source = source\n        self.keywords = keywords\n\n    @classmethod\n    def is_my_business(cls, action, **keywords):\n        \"\"\"\n        If all required keys are present, this source is activated\n        \"\"\"\n        statuses = [_has_field(field, keywords) for field in cls.fields]\n        results = filter(lambda status: status is False, statuses)\n        if not PY2:\n            results = list(results)\n        return len(results) == 0\n\n\nclass FileSource(Source):\n    @classmethod\n    def is_my_business(cls, action, **keywords):\n        statuses = [_has_field(field, keywords) for field in cls.fields]\n        results = filter(lambda status: status is False, statuses)\n        if not PY2:\n            results = list(results)\n        status = len(results) == 0\n        if status:\n            file_name = keywords.get(KEYWORD_FILE_NAME, None)\n            if file_name:\n                if is_string(type(file_name)):\n                    file_type = file_name.split(\".\")[-1]\n                else:\n                    raise IOError(\"Wrong file name\")\n            else:\n                file_type = keywords.get(KEYWORD_FILE_TYPE)\n            if action == 'read':\n                status = file_type in READERS\n            elif action == 'write':\n                status = file_type in WRITERS\n            else:\n                raise Exception(\"Illegal IO operation\")\n        return status\n\n\nclass ReadOnlySource(Source):\n    \"\"\"Read Only Data Source\"\"\"\n    def write_data(self, content):\n        \"\"\"This function does nothing \"\"\"\n        pass\n\n\nclass WriteOnlySource(Source):\n    \"\"\"Write Only Data Source\"\"\"\n\n    def get_data(self):\n        \"\"\"This function does nothing\"\"\"\n        return None\n\n\ndef one_sheet_tuple(items):\n    if not PY2:\n        items = list(items)\n    if len(items[0][1]) == 0:\n        return None, None\n    else:\n        return items[0][0], items[0][1]\n", 
    "pyexcel.sources.database": "\"\"\"\n    pyexcel.sources.database\n    ~~~~~~~~~~~~~~~~~~~\n\n    Representation of database sources\n\n    :copyright: (c) 2015 by Onni Software Ltd.\n    :license: New BSD License\n\"\"\"\nfrom .base import ReadOnlySource, Source, one_sheet_tuple\nfrom pyexcel_io import DB_SQL, DB_DJANGO, load_data\nfrom ..constants import (\n    KEYWORD_TABLES,\n    KEYWORD_MODELS,\n    KEYWORD_INITIALIZERS,\n    KEYWORD_MAPDICTS,\n    KEYWORD_COLUMN_NAMES,\n    KEYWORD_BATCH_SIZE,\n    KEYWORD_QUERY_SETS,\n    KEYWORD_SESSION,\n    KEYWORD_TABLE,\n    KEYWORD_MAPDICT,\n    KEYWORD_INITIALIZER,\n    KEYWORD_MODEL,\n    DEFAULT_SHEET_NAME\n)\n\n\nclass SheetQuerySetSource(ReadOnlySource):\n    \"\"\"\n    Database query set as data source\n\n    SQLAlchemy and Django query sets are supported\n    \"\"\"\n    fields = [KEYWORD_COLUMN_NAMES, KEYWORD_QUERY_SETS]\n\n    def __init__(self, column_names, query_sets, sheet_name=None):\n        self.sheet_name = sheet_name\n        if self.sheet_name is None:\n            self.sheet_name = DEFAULT_SHEET_NAME\n        self.column_names = column_names\n        self.query_sets = query_sets\n\n    def get_data(self):\n        from ..utils import from_query_sets\n        return (self.sheet_name,\n                from_query_sets(self.column_names, self.query_sets))\n\n\nclass SheetDatabaseSourceMixin(Source):\n    \"\"\"\n    Generic database source\n\n    It does the general data import and export. \n    \"\"\"\n    def get_sql_book():\n        pass\n\n    def get_writer(self, sheet):\n        pass\n\n    def get_data(self):\n        sheets = self.get_sql_book()\n        return one_sheet_tuple(sheets.items())\n\n    def write_data(self, sheet):\n        if(len(sheet.colnames)) == 0:\n            sheet.name_columns_by_row(0)\n        w = self.get_writer(sheet)\n        w.write_array(sheet.array)\n        w.close()\n\n\nclass SheetSQLAlchemySource(SheetDatabaseSourceMixin):\n    \"\"\"\n    SQLAlchemy channeled sql database as data source\n    \"\"\"\n    fields = [KEYWORD_SESSION, KEYWORD_TABLE]\n\n    def __init__(self, session, table, **keywords):\n        self.session = session\n        self.table = table\n        self.keywords = keywords\n\n    def get_sql_book(self):\n        return load_data(DB_SQL,\n                         session=self.session,\n                         tables=[self.table])\n\n    def get_writer(self, sheet):\n        from ..writers import Writer\n        tables = {\n            sheet.name: (\n                self.table,\n                sheet.colnames,\n                self.keywords.get(KEYWORD_MAPDICT, None),\n                self.keywords.get(KEYWORD_INITIALIZER, None)\n            )\n        }\n        w = Writer(\n            DB_SQL,\n            sheet_name=sheet.name,\n            session=self.session,\n            tables=tables,\n            **self.keywords\n        )\n        return w\n\n\nclass SheetDjangoSource(SheetDatabaseSourceMixin):\n    \"\"\"\n    Django model as data source\n    \"\"\"\n    fields = [KEYWORD_MODEL]\n\n    def __init__(self, model=None, **keywords):\n        self.model = model\n        self.keywords = keywords\n\n    def get_sql_book(self):\n        return load_data(DB_DJANGO, models=[self.model])\n\n    def get_writer(self, sheet):\n        from ..writers import Writer\n        models = {\n            sheet.name: (\n                self.model,\n                sheet.colnames,\n                self.keywords.get(KEYWORD_MAPDICT, None),\n                self.keywords.get(KEYWORD_INITIALIZER, None)\n            )\n        }\n        w = Writer(\n            DB_DJANGO,\n            sheet_name=sheet.name,\n            models=models,\n            batch_size=self.keywords.get(KEYWORD_BATCH_SIZE, None)\n        )\n        return w\n\n\nclass BookSQLSource(Source):\n    \"\"\"\n    SQLAlchemy bridged multiple table data source\n    \"\"\"\n    fields = [KEYWORD_SESSION, KEYWORD_TABLES]\n\n    def __init__(self, session, tables, **keywords):\n        self.session = session\n        self.tables = tables\n        self.keywords = keywords\n\n    def get_data(self):\n        sheets = load_data(DB_SQL, session=self.session, tables=self.tables)\n        return sheets, DB_SQL, None\n\n    def write_data(self, book):\n        from ..writers import BookWriter\n        initializers = self.keywords.get(KEYWORD_INITIALIZERS, None)\n        if initializers is None:\n            initializers = [None] * len(self.tables)\n        mapdicts = self.keywords.get(KEYWORD_MAPDICTS, None)\n        if mapdicts is None:\n            mapdicts = [None] * len(self.tables)\n        for sheet in book:\n            if len(sheet.colnames) == 0:\n                sheet.name_columns_by_row(0)\n        colnames_array = [sheet.colnames for sheet in book]\n        x = zip(self.tables, colnames_array, mapdicts, initializers)\n        table_dict = dict(zip(book.name_array, x))\n        w = BookWriter(DB_SQL, session=self.session, tables=table_dict, **self.keywords)\n        w.write_book_reader_to_db(book)\n        w.close()\n\n\nclass BookDjangoSource(Source):\n    \"\"\"\n    multiple Django table as data source\n    \"\"\"\n    fields = [KEYWORD_MODELS]\n\n    def __init__(self, models, **keywords):\n        self.models = models\n        self.keywords = keywords\n\n    def get_data(self):\n        sheets = load_data(DB_DJANGO, models=self.models)\n        return sheets, DB_DJANGO, None\n\n    def write_data(self, book):\n        from ..writers import BookWriter\n        new_models = [ model for model in self.models if model is not None ]\n        batch_size = self.keywords.get(KEYWORD_BATCH_SIZE, None)\n        initializers = self.keywords.get(KEYWORD_INITIALIZERS, None)\n        if initializers is None:\n            initializers = [None] * len(new_models)\n        mapdicts = self.keywords.get(KEYWORD_MAPDICTS, None)\n        if mapdicts is None:\n            mapdicts = [None] * len(new_models)\n        for sheet in book:\n            if len(sheet.colnames) == 0:\n                sheet.name_columns_by_row(0)\n        colnames_array = [sheet.colnames for sheet in book]\n        x = zip(new_models, colnames_array, mapdicts, initializers)\n        table_dict = dict(zip(book.name_array, x))\n        w = BookWriter(DB_DJANGO, models=table_dict, batch_size=batch_size)\n        w.write_book_reader_to_db(book)\n        w.close()\n", 
    "pyexcel.sources.file": "\"\"\"\n    pyexcel.sources.file\n    ~~~~~~~~~~~~~~~~~~~\n\n    Representation of file sources\n\n    :copyright: (c) 2015 by Onni Software Ltd.\n    :license: New BSD License\n\"\"\"\nimport os\nfrom .base import FileSource, one_sheet_tuple\nfrom ..constants import KEYWORD_FILE_NAME\nfrom pyexcel_io import load_data\n\n\nclass SheetSource(FileSource):\n    \"\"\"Pick up 'file_name' field and do single sheet based read and write\n    \"\"\"\n    fields = [KEYWORD_FILE_NAME]\n\n    def __init__(self, file_name=None, **keywords):\n        self.file_name = file_name\n        self.keywords = keywords\n\n    def get_data(self):\n        \"\"\"\n        Return a dictionary with only one key and one value\n        \"\"\"\n        sheets = load_data(self.file_name, **self.keywords)\n        return one_sheet_tuple(sheets.items())\n\n    def write_data(self, sheet):\n        from ..writers import Writer\n        w = Writer(self.file_name, sheet_name=sheet.name, **self.keywords)\n        w.write_reader(sheet)\n        w.close()\n\n\nclass BookSource(SheetSource):\n    \"\"\"Pick up 'file_name' field and do multiple sheet based read and write\n    \"\"\"\n    def get_data(self):\n        sheets = load_data(self.file_name, **self.keywords)\n        path, filename_alone = os.path.split(self.file_name)\n        return sheets, filename_alone, path\n\n    def write_data(self, book):\n        from ..writers import BookWriter\n        writer = BookWriter(self.file_name, **self.keywords)\n        writer.write_book_reader(book)\n        writer.close()\n", 
    "pyexcel.sources.http": "\"\"\"\n    pyexcel.sources.http\n    ~~~~~~~~~~~~~~~~~~~\n\n    Representation of http sources\n\n    :copyright: (c) 2015 by Onni Software Ltd.\n    :license: New BSD License\n\"\"\"\nfrom .base import ReadOnlySource, one_sheet_tuple\nfrom ..constants import KEYWORD_URL\nfrom pyexcel_io import load_data\nfrom .._compact import request, PY2\nimport js\n\n\nFILE_TYPE_MIME_TABLE = {\n    \"text/csv\": \"csv\",\n    \"text/tab-separated-values\": \"tsv\",\n    \"application/vnd.oasis.opendocument.spreadsheet\": \"ods\",\n    \"application/vnd.ms-excel\": \"xls\",\n    \"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\": \"xlsx\",\n    \"application/vnd.ms-excel.sheet.macroenabled.12\": \"xlsm\"\n}\n\n\ndef get_file_type_from_url(url):\n    extension = url.split('.')\n    return extension[-1]\n\n\nclass HttpBookSource(ReadOnlySource):\n    \"\"\"\n    Multiple sheet data source via http protocol\n    \"\"\"\n    fields = [KEYWORD_URL]\n\n    def __init__(self, url=None, **keywords):\n        self.url = url\n        self.keywords = keywords\n\n    def get_data(self):\n        global SHEETS\n        sheets = None\n        jquery = js.globals['$']\n        xhr = jquery.ajax({\"url\": self.url, \"async\":False})\n        mime_type = xhr.getResponseHeader('content-type')\n        file_type = FILE_TYPE_MIME_TABLE.get(mime_type, None)\n        if file_type is None:\n            file_type = get_file_type_from_url(self.url)\n        sheets = load_data(xhr.responseText,\n                           file_type=file_type,\n                           **self.keywords)\n        return sheets, KEYWORD_URL, None\n\n\nclass HttpSheetSource(HttpBookSource):\n    \"\"\"\n    Single sheet data source via http protocol\n    \"\"\"\n\n    fields = [KEYWORD_URL]\n\n    def __init__(self, url=None, **keywords):\n        self.url = url\n        self.keywords = keywords\n\n    def get_data(self):\n        sheets, unused1, unused2 = HttpBookSource.get_data(self)\n        print type(sheets)\n        print sheets\n        return one_sheet_tuple(sheets.items())\n\n", 
    "pyexcel.sources.memory": "\"\"\"\n    pyexcel.sources.memory\n    ~~~~~~~~~~~~~~~~~~~\n\n    Representation of memory sources\n\n    :copyright: (c) 2015 by Onni Software Ltd.\n    :license: New BSD License\n\"\"\"\nfrom .base import ReadOnlySource, FileSource, one_sheet_tuple\nfrom .file import SheetSource, BookSource\nfrom pyexcel_io import load_data, get_io\nfrom ..constants import (\n    KEYWORD_FILE_TYPE,\n    KEYWORD_RECORDS,\n    KEYWORD_ADICT,\n    KEYWORD_ARRAY,\n    KEYWORD_MEMORY,\n    KEYWORD_BOOKDICT,\n    DEFAULT_SHEET_NAME\n)\n\n\nclass ReadOnlySheetSource(SheetSource):\n    \"\"\"Pick up 'file_type' and read a sheet from memory\"\"\"\n    fields = [KEYWORD_FILE_TYPE]\n\n    def __init__(self,\n                 file_content=None,\n                 file_type=None,\n                 file_stream=None,\n                 **keywords):\n        self.file_type = file_type\n        self.file_stream = file_stream\n        self.file_content = file_content\n        self.keywords = keywords\n\n    def get_data(self):\n        if self.file_stream is not None:\n            sheets = load_data(self.file_stream,\n                               file_type=self.file_type,\n                               **self.keywords)\n        else:\n            sheets = load_data(self.file_content,\n                               file_type=self.file_type,\n                               **self.keywords)\n        return one_sheet_tuple(sheets.items())\n\n    def write_data(self, content):\n        \"\"\"Disable write\"\"\"\n        pass\n\n\nclass WriteOnlySheetSource(SheetSource):\n    fields = [KEYWORD_FILE_TYPE]\n\n    def __init__(self, file_type=None, **keywords):\n        self.content = get_io(file_type)\n        self.file_name = (file_type, self.content)\n        self.keywords = keywords\n\n    def get_data(self):\n        return None\n\n\nclass RecrodsSource(ReadOnlySource):\n    \"\"\"\n    A list of dictionaries as data source\n\n    The dictionaries should have identical fields.\n    \"\"\"\n    fields = [KEYWORD_RECORDS]\n\n    def __init__(self, records):\n        self.records = records\n\n    def get_data(self):\n        from ..utils import from_records\n        return DEFAULT_SHEET_NAME, from_records(self.records)\n\n\nclass DictSource(ReadOnlySource):\n    \"\"\"\n    A dictionary of one dimensional array as sheet source\n    \"\"\"\n    fields = [KEYWORD_ADICT]\n\n    def __init__(self, adict, with_keys=True):\n        self.adict = adict\n        self.with_keys = with_keys\n\n    def get_data(self):\n        from ..utils import dict_to_array\n        tmp_array = dict_to_array(self.adict, self.with_keys)\n        return DEFAULT_SHEET_NAME, tmp_array\n\n\nclass ArraySource(ReadOnlySource):\n    \"\"\"\n    A two dimensional array as sheet source\n    \"\"\"\n    fields = [KEYWORD_ARRAY]\n\n    def __init__(self, array):\n        self.array = array\n\n    def get_data(self):\n        return DEFAULT_SHEET_NAME, self.array\n\n\nclass ReadOnlyBookSource(ReadOnlySource, FileSource):\n    \"\"\"\n    Multiple sheet data source via memory\n    \"\"\"\n    fields = [KEYWORD_FILE_TYPE]\n\n    def __init__(self,\n                 file_content=None,\n                 file_type=None,\n                 file_stream=None,\n                 **keywords):\n        self.file_type = file_type\n        self.file_content = file_content\n        self.file_stream = file_stream\n        self.keywords = keywords\n\n    def get_data(self):\n        if self.file_stream is not None:\n            sheets = load_data(self.file_stream,\n                               file_type=self.file_type,\n                               **self.keywords)\n        else:\n            sheets = load_data(self.file_content,\n                               file_type=self.file_type,\n                               **self.keywords)\n        return sheets, KEYWORD_MEMORY, None\n\n\nclass BookDictSource(ReadOnlySource):\n    \"\"\"\n    Multiple sheet data source via a dictionary of two dimensional arrays\n    \"\"\"\n    fields = [KEYWORD_BOOKDICT]\n\n    def __init__(self, bookdict, **keywords):\n        self.bookdict = bookdict\n\n    def get_data(self):\n        return self.bookdict, KEYWORD_BOOKDICT, None\n\n\nclass WriteOnlyBookSource(BookSource):\n    \"\"\"\n    Multiple sheet data source for writting back to memory\n    \"\"\"\n    fields = [KEYWORD_FILE_TYPE]\n\n    def __init__(self, file_type=None, **keywords):\n        self.content = get_io(file_type)\n        self.file_name = (file_type, self.content)\n        self.keywords = keywords\n", 
    "pyexcel.utils": "\"\"\"\n    pyexcel.utils\n    ~~~~~~~~~~~~~~~~~~~\n\n    Utility functions for pyexcel\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom .sheets import NominableSheet, Sheet\nfrom ._compact import OrderedDict, PY2\nfrom .constants import MESSAGE_DATA_ERROR_NO_SERIES\nimport datetime\n\n\nLOCAL_UUID = 0\n\n\ndef local_uuid():\n    global LOCAL_UUID\n    LOCAL_UUID = LOCAL_UUID + 1\n    return LOCAL_UUID\n\n\ndef to_array(o):\n    \"\"\"convert a reader iterator to an array\"\"\"\n    array = []\n    for i in o:\n        array.append(i)\n    return array\n\n\ndef to_dict(o):\n    \"\"\"convert a reader iterator to a dictionary\"\"\"\n    the_dict = OrderedDict()\n    series = \"Series_%d\"\n    count = 1\n    for c in o:\n        if type(c) == dict:\n            the_dict.update(c)\n        elif isinstance(c, Sheet):\n            the_dict.update({c.name: c.to_array()})\n        else:\n            key = series % count\n            the_dict.update({key: c})\n            count += 1\n    return the_dict\n\n\ndef to_records(reader, custom_headers=None):\n    \"\"\"\n    Make an array of dictionaries\n\n    It takes the first row as keys and the rest of\n    the rows as values. Then zips keys and row values\n    per each row. This is particularly helpful for\n    database operations.\n    \"\"\"\n    ret = []\n    if isinstance(reader, NominableSheet) is False:\n        raise NotImplementedError\n    if len(reader.rownames) > 0:\n        if custom_headers:\n            headers = custom_headers\n        else:\n            headers = reader.rownames\n        for column in reader.columns():\n            the_dict = dict(zip(headers, column))\n            ret.append(the_dict)\n    elif len(reader.colnames) > 0:\n        if custom_headers:\n            headers = custom_headers\n        else:\n            headers = reader.colnames\n        for row in reader.rows():\n            the_dict = dict(zip(headers, row))\n            ret.append(the_dict)\n    else:\n        raise ValueError(MESSAGE_DATA_ERROR_NO_SERIES)\n    return ret\n\n\ndef from_records(records):\n    \"\"\"Reverse function of to_records\n    \"\"\"\n    if len(records) < 1:\n        return None\n\n    keys = sorted(records[0].keys())\n    data = []\n    data.append(list(keys))\n    for r in records:\n        row = []\n        for k in keys:\n            row.append(r[k])\n        data.append(row)\n    return data\n\n\ndef to_one_dimensional_array(iterator):\n    \"\"\"convert a reader to one dimensional array\"\"\"\n    array = []\n    for i in iterator:\n        if type(i) == list:\n            array += i\n        else:\n            array.append(i)\n    return array\n\n\ndef dict_to_array(the_dict, with_keys=True):\n    \"\"\"Convert a dictionary of columns to an array\n\n    The example dict is::\n\n        {\n            \"Column 1\": [1, 2, 3],\n            \"Column 2\": [5, 6, 7, 8],\n            \"Column 3\": [9, 10, 11, 12, 13],\n        }\n\n    The output will be::\n\n        [\n            [\"Column 1\", \"Column 2\", \"Column 3\"],\n            [1, 5, 9],\n            [2, 6, 10],\n            [3, 7, 11],\n            ['', 8, 12],\n            ['', '', 13]\n        ]\n\n    :param dict the_dict: the dictionary to be converted.\n    :param bool with_keys: to write the keys as the first row or not\n    \"\"\"\n    content = []\n    keys = the_dict.keys()\n    if not PY2:\n        keys = list(keys)\n    if not isinstance(the_dict, OrderedDict):\n        keys = sorted(keys)\n    if with_keys:\n        content.append(keys)\n    max_length = -1\n    for k in keys:\n        column_length = len(the_dict[k])\n        if max_length == -1:\n            max_length = column_length\n        elif max_length < column_length:\n            max_length = column_length\n    for i in range(0, max_length):\n        row_data = []\n        for k in keys:\n            if i < len(the_dict[k]):\n                row_data.append(the_dict[k][i])\n            else:\n                row_data.append('')\n        content.append(row_data)\n    return content\n\n\ndef from_query_sets(column_names, query_sets):\n    array = []\n    array.append(column_names)\n    for o in query_sets:\n        new_array = []\n        for column in column_names:\n            value = getattr(o, column)\n            if isinstance(value, (datetime.date, datetime.time)):\n                value = value.isoformat()\n            new_array.append(value)\n        array.append(new_array)\n    return array\n", 
    "pyexcel.writers": "\"\"\"\n    pyexcel.writers\n    ~~~~~~~~~~~~~~~~~~~\n\n    Uniform interface for writing different excel file formats\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom pyexcel_io import get_writer\nfrom .utils import to_array, from_records, dict_to_array\nfrom .sheets import Matrix, transpose\nfrom ._compact import OrderedDict\n\nclass SheetWriter:\n    \"\"\"Single sheet writer for the excel book writer\"\"\"\n\n    def __init__(self, writer):\n        \"\"\"Constructor\n\n        :param CustomWriter writer: format specific writer\n        \"\"\"\n        self.writer = writer\n\n    def write_array(self, table):\n        \"\"\"Write a two dimensional array\n\n        :param list table: two dimensional array\n        \"\"\"\n        self.write_rows(table)\n\n    def write_rows(self, table):\n        \"\"\"\n        Write a table\n\n        table can be two dimensional array or a row iterator\n        :param list table: two dimensional array\n        \"\"\"\n        if len(table) < 1:\n            return\n        rows = len(table)\n        columns = len(table[0])\n        self.writer.set_size((rows, columns))\n        self.writer.write_array(table)\n\n    def write_dict(self, the_dict):\n        \"\"\"Write a dictionary\n\n        :param dict the_dict: the dictionary to be writeen\n        \"\"\"\n        array = dict_to_array(the_dict)\n        self.write_rows(array)\n\n    def write_dict_columns(self, the_dict):\n        \"\"\"Write a dictionary\n\n        :param dict the_dict: the dictionary to be writeen\n        \"\"\"\n        array = dict_to_array(the_dict)\n        self.write_columns(array)\n\n    def write_reader(self, reader):\n        \"\"\"Write a reader/sheet\n\n        :param Matrix reader: a Matrix instance\n        \"\"\"\n        if not isinstance(reader, Matrix):\n            raise TypeError\n        if len(reader.rownames) > 0:\n            self.write_dict_columns(reader.to_dict(True))\n        elif len(reader.colnames) > 0:\n            self.write_dict(reader.to_dict())\n        else:\n            self.write_rows(to_array(reader))\n\n    def write_columns(self, in_array):\n        \"\"\"Write columns in reference to rows\n\n        It was seen always to write rows horizontally. This\n        method write data vertically.\n        :param list in_array: a two dimensional array\n        \"\"\"\n        out_array = transpose(in_array)\n        self.write_rows(out_array)\n\n    def write_records(self, records):\n        \"\"\"Write records to rows\n\n        key will become the column header and all data\n        will be stacked one by one as rows\n        :param list of dictionary records: the incoming data\n        \"\"\"\n        out_array = from_records(records)\n        self.write_rows(out_array)\n\n    def close(self):\n        \"\"\"\n        Close the writer\n\n        Please remember to call close function\n        \"\"\"\n        self.writer.close()\n\n\nclass BookWriter:\n    \"\"\"\n    A generic book writer.\n\n    It provides one interface for writing any supported file formats.\n    A book refers to the excel file that has many sheets. csv file\n    format does not support such a concept, hence this writer will\n    write a csv book in theory to scattered csv files which share\n    similiar file names.\n    \"\"\"\n\n    def __init__(self, file, **keywords):\n        \"\"\"Constructor\n        :param str file: file name\n        :param dict keywords: extra parameters for format specific writer\n        \"\"\"\n        if isinstance(file, tuple):\n            self.writer = get_writer(file[1], file[0], **keywords)\n        else:\n            self.writer = get_writer(file, **keywords)\n\n    def create_sheet(self, sheet_name):\n        \"\"\"Create a new sheet\n\n        :param str name: the new sheet name\n        \"\"\"\n        return SheetWriter(self.writer.create_sheet(sheet_name))\n\n    def write_book_from_dict(self, sheet_dicts):\n        \"\"\"Write a dictionary to a multi-sheet file\n\n        Requirements for the dictionary is: key is the sheet name,\n        its value must be two dimensional array\n        :param dict sheet_dicts: a dictionary of two dimensional array,\n        for example::\n\n            {\n                \"Sheet1\": [[1, 2, 3], [4, 5, 6]],\n                \"Sheet2\": [[7, 8, 9], [10, 11, 12]]\n            }\n        \"\"\"\n        self.writer.write(sheet_dicts)\n\n    def write_book_reader(self, bookreader):\n        \"\"\"\n        Write a book reader\n\n        Easy implementiation. Dump a book into a dictionary of\n        two dimensional arrays. Then write book from this dictionary\n        :param Book bookreader: a book object to be written\n        \"\"\"\n        self.writer.write(bookreader.to_dict())\n\n    def write_book_reader_to_db(self, bookreader):\n        \"\"\"\n        Write a book reader\n\n        Easy implementiation. Dump a book into a dictionary of\n        two dimensional arrays. Then write book from this dictionary\n        :param Book bookreader: a book object to be written\n        \"\"\"\n        the_dict = OrderedDict()\n        keys = bookreader.sheet_names()\n        for name in keys:\n            the_dict.update({name: bookreader[name].array})\n        self.writer.write(the_dict)\n\n    def close(self):\n        \"\"\"close the writer\"\"\"\n        self.writer.close()\n\n\nclass Writer(SheetWriter):\n    \"\"\"\n    A single sheet excel file writer\n\n    It writes only one sheet to an excel file. It is a quick way to handle most\n    of the data files\n    \"\"\"\n\n    def __init__(self, file, sheet_name=None,  **keywords):\n        \"\"\"Constructor for single sheet writer\n\n        This class creates only one sheet writer and stick with it\n        \"\"\"\n        self.bookwriter = BookWriter(file,\n                                     single_sheet_in_book=True,\n                                     **keywords)\n        self.writer = self.bookwriter.create_sheet(sheet_name).writer\n\n    def close(self):\n        \"\"\"\n        Close the writer\n        \"\"\"\n        SheetWriter.close(self)\n        self.bookwriter.close()\n", 
    "pyexcel_io.__init__": "\"\"\"\n    pyexcel_io\n    ~~~~~~~~~~~~~~~~~~~\n\n    Uniform interface for reading/writing different excel file formats\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom functools import partial\nfrom .base import(\n    NamedContent,\n    SheetReaderBase,\n    SheetReader,\n    BookReaderBase,\n    BookReader,\n    SheetWriterBase,\n    SheetWriter,\n    BookWriter,\n    from_query_sets\n)\nfrom .csvbook import CSVBook, CSVWriter\nfrom .csvzipbook import CSVZipWriter, CSVZipBook\nfrom .sqlbook import SQLBookReader, SQLBookWriter\nfrom .djangobook import DjangoBookReader, DjangoBookWriter\nfrom ._compact import is_string, BytesIO, StringIO, isstream, OrderedDict, PY2\nfrom .constants import (\n    MESSAGE_LOADING_FORMATTER,\n    MESSAGE_ERROR_02,\n    MESSAGE_ERROR_03,\n    MESSAGE_WRONG_IO_INSTANCE,\n    MESSAGE_CANNOT_WRITE_STREAM_FORMATTER,\n    MESSAGE_CANNOT_READ_STREAM_FORMATTER,\n    MESSAGE_CANNOT_WRITE_FILE_TYPE_FORMATTER,\n    MESSAGE_CANNOT_READ_FILE_TYPE_FORMATTER,\n    FILE_FORMAT_CSV,\n    FILE_FORMAT_TSV,\n    FILE_FORMAT_CSVZ,\n    FILE_FORMAT_TSVZ,\n    FILE_FORMAT_ODS,\n    FILE_FORMAT_XLS,\n    FILE_FORMAT_XLSX,\n    FILE_FORMAT_XLSM,\n    DB_SQL,\n    DB_DJANGO,\n    DEFAULT_SHEET_NAME\n)\n\n# Please also register here\nTEXT_STREAM_TYPES = [FILE_FORMAT_CSV, FILE_FORMAT_TSV]\n\n# Please also register here\nBINARY_STREAM_TYPES = [FILE_FORMAT_CSVZ, FILE_FORMAT_TSVZ,\n                       FILE_FORMAT_ODS, FILE_FORMAT_XLS,\n                       FILE_FORMAT_XLSX, FILE_FORMAT_XLSM]\n\n# A list of registered readers\nREADERS = {\n    FILE_FORMAT_CSV: CSVBook,\n    FILE_FORMAT_TSV: partial(CSVBook, dialect=\"excel-tab\"),\n    FILE_FORMAT_CSVZ: CSVZipBook,\n    FILE_FORMAT_TSVZ: partial(CSVZipBook, dialect=\"excel-tab\"),\n    DB_SQL: SQLBookReader,\n    DB_DJANGO: DjangoBookReader\n}\n\nAVAILABLE_READERS = {\n    FILE_FORMAT_XLS: 'pyexcel-xls',\n    FILE_FORMAT_XLSX: ('pyexcel-xls', 'pyexcel-xlsx'),\n    FILE_FORMAT_XLSM: ('pyexcel-xls', 'pyexcel-xlsx'),\n    FILE_FORMAT_ODS: ('pyexcel-ods', 'pyexcel-ods3')\n}\n\n# A list of registered writers\nWRITERS = {\n    FILE_FORMAT_CSV: CSVWriter,\n    FILE_FORMAT_TSV: partial(CSVWriter, dialect=\"excel-tab\"),\n    FILE_FORMAT_CSVZ: CSVZipWriter,\n    FILE_FORMAT_TSVZ: partial(CSVZipWriter, dialect=\"excel-tab\"),\n    DB_SQL: SQLBookWriter,\n    DB_DJANGO: DjangoBookWriter\n}\n\nAVAILABLE_WRITERS = {\n    FILE_FORMAT_XLS: 'pyexcel-xls',\n    FILE_FORMAT_XLSX: 'pyexcel-xlsx',\n    FILE_FORMAT_XLSM: 'pyexcel-xlsx',\n    FILE_FORMAT_ODS: ('pyexcel-ods', 'pyexcel-ods3')\n}\n\n\ndef resolve_missing_extensions(extension, available_list):\n    handler = available_list.get(extension)\n    message = \"\"\n    if handler:\n        if is_string(type(handler)):\n            message = MESSAGE_LOADING_FORMATTER % (extension, handler)\n        else:\n            merged = \"%s or %s\" % (handler[0], handler[1])\n            message = MESSAGE_LOADING_FORMATTER % (extension, merged)\n        raise NotImplementedError(message)\n\n\ndef load_data(filename,\n              file_type=None,\n              sheet_name=None,\n              sheet_index=None,\n              **keywords):\n    \"\"\"Load data from any supported excel formats\n\n    :param filename: actual file name, a file stream or actual content\n    :param file_type: used only when filename is not a physial file name\n    :param sheet_name: the name of the sheet to be loaded\n    :param sheet_index: the index of the sheet to be loaded\n    :param keywords: any other parameters\n    \"\"\"\n    extension = None\n    book = None\n    from_memory = False\n    content = None\n    if filename is None:\n        raise IOError(MESSAGE_ERROR_02)\n    if filename in READERS:\n        book_class = READERS[filename]\n        book = book_class(**keywords)\n        book.set_type(filename)\n    else:\n        if file_type is not None:\n            from_memory = True\n            extension = file_type\n        elif is_string(type(filename)):\n            extension = filename.split(\".\")[-1]\n        else:\n            raise IOError(MESSAGE_ERROR_03)\n        if extension in READERS:\n            book_class = READERS[extension]\n            if from_memory:\n                if isstream(filename):\n                    if validate_io(file_type, filename):\n                        content = filename\n                    else:\n                        raise IOError(MESSAGE_WRONG_IO_INSTANCE)\n                else:\n                    io = get_io(file_type)\n                    if not PY2:\n                        if (isinstance(io, StringIO) and isinstance(filename, bytes)):\n                            content = filename.decode('utf-8')\n                        else:\n                            content = filename\n                        io.write(content)\n                    else:\n                        io.write(filename)\n                    io.seek(0)\n                    content = io\n                book = book_class(None, file_content=content,\n                                  load_sheet_with_name=sheet_name,\n                                  load_sheet_at_index=sheet_index,\n                                  **keywords)\n            else:\n                book = book_class(filename,\n                                  load_sheet_with_name=sheet_name,\n                                  load_sheet_at_index=sheet_index,\n                                  **keywords)\n            book.set_type(extension)\n        else:\n            resolve_missing_extensions(extension, AVAILABLE_READERS)\n            if from_memory:\n                raise NotImplementedError(\n                    MESSAGE_CANNOT_READ_STREAM_FORMATTER % extension)\n            else:\n                raise NotImplementedError(\n                    MESSAGE_CANNOT_READ_FILE_TYPE_FORMATTER % (extension,\n                                                               filename))\n    return book.sheets()\n\n\ndef get_writer(filename, file_type=None, **keywords):\n    \"\"\"Create a writer from any supported excel formats\n\n    :param filename: actual file name or a file stream\n    :param file_type: used only when filename is not a physial file name\n    :param keywords: any other parameters\n    \"\"\"\n    extension = None\n    writer = None\n    to_memory = False\n    if filename is None:\n        raise IOError(MESSAGE_ERROR_02)\n    if filename in WRITERS:\n        writer_class = WRITERS[filename]\n        writer = writer_class(filename, **keywords)\n        writer.set_type(filename)\n    else:\n        if file_type is not None:\n            if isstream(filename):\n                extension = file_type\n                to_memory = True\n                if not validate_io(file_type, filename):\n                    raise IOError(MESSAGE_WRONG_IO_INSTANCE)\n            else:\n                raise IOError(MESSAGE_ERROR_03)\n        elif is_string(type(filename)):\n            extension = filename.split(\".\")[-1]\n        else:\n            raise IOError(MESSAGE_ERROR_03)\n        if extension in WRITERS:\n            writer_class = WRITERS[extension]\n            writer = writer_class(filename, **keywords)\n            writer.set_type(extension)\n        else:\n            resolve_missing_extensions(extension, AVAILABLE_WRITERS)\n            if to_memory:\n                raise NotImplementedError(\n                    MESSAGE_CANNOT_WRITE_STREAM_FORMATTER % extension)\n            else:\n                raise NotImplementedError(\n                    MESSAGE_CANNOT_WRITE_FILE_TYPE_FORMATTER % (extension,\n                                                                filename))\n    return writer\n\n\ndef get_io(file_type):\n    \"\"\"A utility function to help you generate a correct io stream\n\n    :param file_type: a supported file type\n    :returns: a appropriate io stream, None otherwise\n    \"\"\"\n    if file_type in TEXT_STREAM_TYPES:\n        return StringIO()\n    elif file_type in BINARY_STREAM_TYPES:\n        return BytesIO()\n    else:\n        return None\n\n\ndef validate_io(file_type, io):\n    if file_type in TEXT_STREAM_TYPES:\n        return isinstance(io, StringIO)\n    elif file_type in BINARY_STREAM_TYPES:\n        return isinstance(io, BytesIO)\n    else:\n        return False\n\n\ndef store_data(afile, data, file_type=None, **keywords):\n    \"\"\"Non public function to store data to afile\n\n    :param filename: actual file name, a file stream or actual content\n    :param data: the data to be written\n    :param file_type: used only when filename is not a physial file name\n    :param keywords: any other parameters\n    \"\"\"\n    writer = get_writer(\n        afile,\n        file_type=file_type,\n        **keywords)\n    writer.write(data)\n    writer.close()\n\n\ndef save_data(afile, data, file_type=None, **keywords):\n    \"\"\"Save data to an excel file source\n\n    Your data can be an array or an ordered dictionary\n\n    :param filename: actual file name, a file stream or actual content\n    :param data: the data to be saved\n    :param file_type: used only when filename is not a physial file name\n    :param keywords: any other parameters\n    \"\"\"\n    to_store = data\n    if isinstance(data, list):\n        single_sheet_in_book = True\n        to_store = {DEFAULT_SHEET_NAME: data}\n    else:\n        single_sheet_in_book = False\n\n    if isstream(afile) and file_type is None:\n        file_type = FILE_FORMAT_CSV\n\n    store_data(afile, to_store,\n               file_type=file_type,\n               single_sheet_in_book=single_sheet_in_book,\n               **keywords)\n\n\ndef get_data(afile, file_type=None, **keywords):\n    \"\"\"Get data from an excel file source\n\n    :param filename: actual file name, a file stream or actual content\n    :param sheet_name: the name of the sheet to be loaded\n    :param sheet_index: the index of the sheet to be loaded\n    :param file_type: used only when filename is not a physial file name\n    :param keywords: any other parameters\n    :returns: an array if it is a single sheet, an ordered dictionary otherwise\n    \"\"\"\n    if isstream(afile) and file_type is None:\n        file_type = FILE_FORMAT_CSV\n    data = load_data(afile, file_type=file_type, **keywords)\n    if len(list(data.keys())) == 1:\n        return list(data.values())[0]\n    else:\n        return data\n", 
    "pyexcel_io._compact": "\"\"\"\n    pyexcel_io._compact\n    ~~~~~~~~~~~~~~~~~~~\n\n    Compatibles\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nimport sys\n\n\nif sys.version_info[0] == 2 and sys.version_info[1] < 7:\n    from ordereddict import OrderedDict\nelse:\n    from collections import OrderedDict\n\nPY2 = sys.version_info[0] == 2\n\nif PY2:\n    from StringIO import StringIO\n    from StringIO import StringIO as BytesIO\n    text_type = unicode\n\n    class Iterator(object):\n        def next(self):\n            return type(self).__next__(self)\n\n    def isstream(instance):\n        return isinstance(instance, StringIO)\n\nelse:\n    from io import StringIO, BytesIO\n    text_type = str\n    Iterator = object\n\n    def isstream(instance):\n        return isinstance(instance, StringIO) or isinstance(instance, BytesIO)\n\n\ndef is_string(atype):\n    \"\"\"find out if a type is str or not\"\"\"\n    if atype == str:\n            return True\n    elif PY2:\n        if atype == unicode:\n            return True\n    return False\n", 
    "pyexcel_io.base": "\"\"\"\n    pyexcel_io.base\n    ~~~~~~~~~~~~~~~~~~~\n\n    The io interface to file extensions\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nimport sys\nimport datetime\nfrom abc import ABCMeta, abstractmethod, abstractproperty\nif sys.version_info[0] == 2 and sys.version_info[1] < 7:\n    from ordereddict import OrderedDict\nelse:\n    from collections import OrderedDict\nfrom .constants import DEFAULT_SHEET_NAME\n\n\ndef add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper\n\n\nclass NamedContent:\n    \"\"\"Helper class for content that does not have a name\"\"\"\n    def __init__(self, name, payload):\n        self.name = name\n        self.payload = payload\n\n\n@add_metaclass(ABCMeta)\nclass SheetReaderBase(object):\n    \"\"\"\n    sheet\n    \"\"\"\n    def __init__(self, sheet, **keywords):\n        self.native_sheet = sheet\n        self.keywords = keywords\n\n    @abstractproperty\n    def name(self):\n        pass\n\n    @abstractmethod\n    def to_array(self):\n        \"\"\"2 dimentional repsentation of the content\n        \"\"\"\n        pass\n\n\nclass SheetReader(SheetReaderBase):\n\n    @abstractmethod\n    def number_of_rows(self):\n        \"\"\"\n        Number of rows in the sheet\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def number_of_columns(self):\n        \"\"\"\n        Number of columns in the sheet\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def cell_value(self, row, column):\n        \"\"\"\n        Random access to the cells\n        \"\"\"\n        pass\n\n    def to_array(self):\n        array = []\n        for r in range(0, self.number_of_rows()):\n            row = []\n            tmp_row = []\n            for c in range(0, self.number_of_columns()):\n                cell_value = self.cell_value(r, c)\n                tmp_row.append(cell_value)\n                if cell_value is not None and cell_value != '':\n                    row += tmp_row\n                    tmp_row = []\n            array.append(row)\n        return array\n\n\n@add_metaclass(ABCMeta)\nclass BookReaderBase(object):\n\n    def set_type(self, file_type):\n        self.file_type = file_type\n\n    @abstractmethod\n    def sheets(self):\n        \"\"\"Get sheets in a dictionary\"\"\"\n        pass\n\n\nclass BookReader(BookReaderBase):\n    \"\"\"\n    XLSBook reader\n\n    It reads xls, xlsm, xlsx work book\n    \"\"\"\n\n    def __init__(self, filename, file_content=None,\n                 load_sheet_with_name=None,\n                 load_sheet_at_index=None,\n                 **keywords):\n        self.load_from_memory_flag = False\n        self.keywords = keywords\n        self.sheet_name = load_sheet_with_name\n        self.sheet_index = load_sheet_at_index\n        if file_content:\n            self.load_from_memory_flag = True\n            self.native_book = self.load_from_memory(file_content, **keywords)\n        else:\n            self.native_book = self.load_from_file(filename, **keywords)\n        self.mysheets = OrderedDict()\n        for native_sheet in self.sheet_iterator():\n            sheet = self.get_sheet(native_sheet)\n            self.mysheets[sheet.name] = sheet.to_array()\n\n    @abstractmethod\n    def sheet_iterator(self):\n        pass\n\n    @abstractmethod\n    def get_sheet(self, native_sheet, **keywords):\n        \"\"\"Return a context specific sheet from a native sheet\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_from_memory(self, file_content, **keywords):\n        \"\"\"Load content from memory\n\n        :params stream file_content: the actual file content in memory\n        :returns: a book\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def load_from_file(self, filename, **keywords):\n        \"\"\"Load content from a file\n\n        :params str filename: an accessible file path\n        :returns: a book\n        \"\"\"\n        pass\n\n    def sheets(self):\n        \"\"\"Get sheets in a dictionary\"\"\"\n        return self.mysheets\n\n\n@add_metaclass(ABCMeta)\nclass SheetWriterBase(object):\n    @abstractmethod\n    def set_size(self, size):\n        \"\"\"size of the content will be given\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_array(self, table):\n        \"\"\"For standalone usage, write an array\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def close(self):\n        \"\"\"\n        This call actually save the file\n        \"\"\"\n        pass\n\n\n@add_metaclass(ABCMeta)\nclass SheetWriter(SheetWriterBase):\n    \"\"\"\n    xls, xlsx and xlsm sheet writer\n    \"\"\"\n    def __init__(self, native_book, native_sheet, name, **keywords):\n        if name:\n            sheet_name = name\n        else:\n            sheet_name = DEFAULT_SHEET_NAME\n        self.native_book = native_book\n        self.native_sheet = native_sheet\n        self.keywords = keywords\n        self.set_sheet_name(sheet_name)\n\n    def set_sheet_name(self, name):\n        pass\n\n    def set_size(self, size):\n        \"\"\"size of the content will be given\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def write_row(self, array):\n        \"\"\"\n        write a row into the file\n        \"\"\"\n        pass\n\n    def write_array(self, table):\n        \"\"\"For standalone usage, write an array\n        \"\"\"\n        for r in table:\n            self.write_row(r)\n\n    def close(self):\n        \"\"\"\n        This call actually save the file\n        \"\"\"\n        pass\n\n\n@add_metaclass(ABCMeta)\nclass BookWriter(object):\n    \"\"\"\n    xls, xlsx and xlsm writer\n    \"\"\"\n    def __init__(self, file, **keywords):\n        self.file = file\n        self.keywords = keywords\n\n    @abstractmethod\n    def create_sheet(self, name):\n        \"\"\"Get a native sheet out\"\"\"\n        pass\n\n    def set_type(self, file_type):\n        self.file_type = file_type\n\n    def write(self, sheet_dicts):\n        \"\"\"Write a dictionary to a multi-sheet file\n\n        Requirements for the dictionary is: key is the sheet name,\n        its value must be two dimensional array\n        \"\"\"\n        keys = sheet_dicts.keys()\n        for name in keys:\n            sheet = self.create_sheet(name)\n            if sheet is not None:\n                sheet.write_array(sheet_dicts[name])\n                sheet.close()\n\n    @abstractmethod\n    def close(self):\n        \"\"\"\n        This call actually save the file\n        \"\"\"\n        pass\n\n\ndef from_query_sets(column_names, query_sets):\n    array = []\n    array.append(column_names)\n    for o in query_sets:\n        new_array = []\n        for column in column_names:\n            value = getattr(o, column)\n            if isinstance(value, (datetime.date, datetime.time)):\n                value = value.isoformat()\n            new_array.append(value)\n        array.append(new_array)\n    return array\n\n\ndef is_empty_array(array):\n    return len([x for x in array if x != '']) == 0\n\n\ndef swap_empty_string_for_none(array):\n    def swap(x):\n        if x == '':\n            return None\n        else:\n            return x\n    return [swap(x) for x in array]\n", 
    "pyexcel_io.constants": "\"\"\"\n    pyexcel_io.constants\n    ~~~~~~~~~~~~~~~~~~~\n\n    Constants appeared in pyexcel\n\n    :copyright: (c) 2015 by Onni Software Ltd.\n    :license: New BSD License\n\"\"\"\nDEFAULT_NAME = 'pyexcel'\nDEFAULT_SHEET_NAME = 'pyexcel_sheet1'\nDEFAULT_SEPARATOR = '__'\n\nMESSAGE_INVALID_PARAMETERS = \"Invalid parameters\"\nMESSAGE_ERROR_02 = \"No content, file name. Nothing is given\"\nMESSAGE_ERROR_03 = \"cannot handle unknown content\"\nMESSAGE_WRONG_IO_INSTANCE = \"Wrong io instance is passed for your file format.\"\nMESSAGE_CANNOT_WRITE_STREAM_FORMATTER = \"Cannot write content of file type %s to stream\"\nMESSAGE_CANNOT_READ_STREAM_FORMATTER = \"Cannot read content of file type %s from stream\"\nMESSAGE_CANNOT_WRITE_FILE_TYPE_FORMATTER = \"Cannot write content of file type %s to file %s\"\nMESSAGE_CANNOT_READ_FILE_TYPE_FORMATTER = \"Cannot read content of file type %s from file %s\"\nMESSAGE_LOADING_FORMATTER = \"The plugin for file type %s is not installed. Please install %s\"\nMESSAGE_EMPTY_ARRAY = \"One empty row is found\"\nMESSAGE_IGNORE_ROW = \"One row is ignored\"\nMESSAGE_DB_EXCEPTION = \"Warning: Bulk insertion got below exception. Trying to do it one by one slowly.\"\n\nFILE_FORMAT_CSV = 'csv'\nFILE_FORMAT_TSV = 'tsv'\nFILE_FORMAT_CSVZ = 'csvz'\nFILE_FORMAT_TSVZ = 'tsvz'\nFILE_FORMAT_ODS = 'ods'\nFILE_FORMAT_XLS = 'xls'\nFILE_FORMAT_XLSX = 'xlsx'\nFILE_FORMAT_XLSM = 'xlsm'\nDB_SQL = 'sql'\nDB_DJANGO = 'django'\n", 
    "pyexcel_io.csvbook": "\"\"\"\n    pyexcel_io.csvbook\n    ~~~~~~~~~~~~~~~~~~~\n\n    The lower level csv file format handler.\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nimport re\nimport os\nimport csv\nimport codecs\nimport glob\nfrom abc import abstractmethod\nfrom .base import (\n    BookReader,\n    SheetReaderBase,\n    SheetWriter,\n    BookWriter,\n    NamedContent\n)\nfrom ._compact import (\n    is_string,\n    StringIO,\n    BytesIO,\n    PY2,\n    text_type,\n    Iterator,\n    isstream\n)\nfrom .constants import DEFAULT_SEPARATOR, DEFAULT_SHEET_NAME\n\n\nclass UTF8Recorder(Iterator):\n    \"\"\"\n    Iterator that reads an encoded stream and reencodes the input to UTF-8.\n    \"\"\"\n    def __init__(self, f, encoding):\n        self.reader = codecs.getreader(encoding)(f)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return next(self.reader).encode('utf-8')\n\n\nclass CSVSheetReader(SheetReaderBase):\n    def __init__(self, sheet, encoding=\"utf-8\", **keywords):\n        SheetReaderBase.__init__(self, sheet, **keywords)\n        self.encoding = encoding\n\n    @property\n    def name(self):\n        return self.native_sheet.name\n\n    @abstractmethod\n    def get_file_handle(self):\n        pass\n\n    def to_array(self):\n        reader = csv.reader(self.get_file_handle(), **self.keywords)\n        array = []\n        for row in reader:\n            myrow = []\n            tmp_row = []\n            for element in row:\n                if PY2:\n                    element = element.decode(self.encoding)\n                tmp_row.append(element)\n                if element is not None and element != '':\n                    myrow += tmp_row\n                    tmp_row = []\n            array.append(myrow)\n        return array\n\n\nclass CSVFileReader(CSVSheetReader):\n    def get_file_handle(self):\n        if PY2:\n            f1 = open(self.native_sheet.payload, 'rb')\n            f = UTF8Recorder(f1, self.encoding)\n        else:\n            f = open(self.native_sheet.payload, 'r')\n        return f\n\n\nclass CSVinMemoryReader(CSVSheetReader):\n    def get_file_handle(self):\n        if PY2:\n            f = UTF8Recorder(self.native_sheet.payload,\n                             self.encoding)\n        else:\n            f = self.native_sheet.payload\n        return f\n\n\nclass CSVBook(BookReader):\n    \"\"\"\n    CSVBook reader\n\n    It simply return one sheet\n    \"\"\"\n    def __init__(self, filename, file_content=None,\n                 load_sheet_with_name=None,\n                 load_sheet_at_index=None, **keywords):\n        if filename is None and file_content is None:\n            self.keywords = keywords\n            self.mysheets = {\"csv\": []}\n        else:\n            BookReader.__init__(self, filename, file_content=file_content,\n                                load_sheet_with_name=load_sheet_with_name,\n                                load_sheet_at_index=load_sheet_at_index,\n                                **keywords)\n\n    def load_from_memory(self, file_content, **keywords):\n        content = file_content.getvalue()\n        if \"---pyexcel---\\r\\n\" in content:\n            sheets = content.split(\"---pyexcel---\\r\\n\")\n            named_contents = []\n            matcher = \"---pyexcel:(.*)---\"\n            for sheet in sheets:\n                if sheet != '':\n                    lines = sheet.split('\\r\\n')\n                    result = re.match(matcher, lines[0])\n                    new_content = '\\n'.join(lines[1:])\n                    new_sheet = NamedContent(result.group(1),\n                                             StringIO(new_content))\n                    named_contents.append(new_sheet)\n            return named_contents\n        else:\n            file_content.seek(0)\n            return [NamedContent('csv', file_content)]\n\n    def load_from_file(self, filename, **keywords):\n        names = filename.split('.')\n        filepattern = \"%s%s*%s*.%s\" % (names[0],\n                                       DEFAULT_SEPARATOR,\n                                       DEFAULT_SEPARATOR,\n                                       names[1])\n        filelist = glob.glob(filepattern)\n        if len(filelist) == 0:\n            file_parts = os.path.split(filename)\n            return [NamedContent(file_parts[-1], filename)]\n        else:\n            matcher = \"%s%s(.*)%s(.*).%s\" % (names[0],\n                                             DEFAULT_SEPARATOR,\n                                             DEFAULT_SEPARATOR,\n                                             names[1])\n            tmp_file_list = []\n            for filen in filelist:\n                result = re.match(matcher, filen)\n                tmp_file_list.append((result.group(1), result.group(2), filen))\n            ret = []\n            for lsheetname, index, filen in sorted(tmp_file_list,\n                                                   key=lambda row: row[1]):\n                if self.sheet_name is not None:\n                    if self.sheet_name == lsheetname:\n                        ret.append(NamedContent(lsheetname, filen))\n                elif self.sheet_index is not None:\n                    if self.sheet_index == int(index):\n                        ret.append(NamedContent(lsheetname, filen))\n                else:\n                    ret.append(NamedContent(lsheetname, filen))\n            if len(ret) == 0:\n                if self.sheet_name is not None:\n                    raise ValueError(\"%s cannot be found\" % self.sheet_name)\n                elif self.sheet_index is not None:\n                    raise IndexError(\n                        \"Index %d of out bound %d.\" % (self.sheet_index,\n                                                       len(filelist)))\n            return ret\n\n    def sheet_iterator(self):\n        return self.native_book\n\n    def get_sheet(self, native_sheet):\n        if self.load_from_memory_flag:\n            return CSVinMemoryReader(native_sheet, **self.keywords)\n        else:\n            return CSVFileReader(native_sheet, **self.keywords)\n\n\nclass CSVSheetWriter(SheetWriter):\n    \"\"\"\n    csv file writer\n\n    \"\"\"\n    def __init__(self, filename, name,\n                 encoding=\"utf-8\", single_sheet_in_book=False,\n                 sheet_index=None, **keywords):\n        self.encoding = encoding\n        sheet_name = name\n        self.single_sheet_in_book = single_sheet_in_book\n        if single_sheet_in_book:\n            sheet_name = None\n        elif isstream(filename):\n            filename.write(\"---pyexcel:%s---\\r\\n\" % sheet_name)\n        self.sheet_index = sheet_index\n        SheetWriter.__init__(self, filename,\n                             sheet_name, sheet_name,\n                             **keywords)\n\n    def set_sheet_name(self, name):\n        if is_string(type(self.native_book)):\n            if name != DEFAULT_SHEET_NAME:\n                names = self.native_book.split(\".\")\n                file_name = \"%s%s%s%s%s.%s\" % (names[0],\n                                               DEFAULT_SEPARATOR,\n                                               name,              # sheet name\n                                               DEFAULT_SEPARATOR,\n                                               self.sheet_index,  # sheet index\n                                               names[1])\n            else:\n                file_name = self.native_book\n            if PY2:\n                self.f = open(file_name, \"wb\")\n            else:\n                self.f = open(file_name, \"w\", newline=\"\")\n        else:\n            self.f = self.native_book\n        self.writer = csv.writer(self.f, **self.keywords)\n\n    def write_row(self, array):\n        \"\"\"\n        write a row into the file\n        \"\"\"\n        if PY2:\n            self.writer.writerow(\n                [text_type(s if s is not None else '').encode(self.encoding)\n                 for s in array])\n        else:\n            self.writer.writerow(array)\n\n    def close(self):\n        \"\"\"\n        This call close the file handle\n        \"\"\"\n        if not isinstance(self.f, StringIO) and not isinstance(self.f, BytesIO):\n            self.f.close()\n        elif not self.single_sheet_in_book:\n            self.f.write(\"---pyexcel---\\r\\n\")\n\n\nclass CSVWriter(BookWriter):\n    \"\"\"\n    csv file writer\n\n    if there is multiple sheets for csv file, it simpily writes\n    multiple csv files\n    \"\"\"\n    def __init__(self, file, **keywords):\n        self.index = 0\n        BookWriter.__init__(self, file, **keywords)\n\n    def create_sheet(self, name):\n        self.index = self.index + 1\n        return CSVSheetWriter(self.file, name,\n                              sheet_index=(self.index-1),\n                              **self.keywords)\n\n    def close(self):\n        \"\"\"\n        This call close the file handle\n        \"\"\"\n        pass\n", 
    "pyexcel_io.csvzipbook": "\"\"\"\n    pyexcel_io.csvzipbook\n    ~~~~~~~~~~~~~~~~~~~\n\n    The lower level csv file format handler.\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nimport os\nimport csv\nimport zipfile\nfrom .base import BookReader, BookWriter\nfrom ._compact import StringIO, PY2, is_string\nfrom .csvbook import (\n    CSVinMemoryReader,\n    NamedContent,\n    CSVSheetWriter,\n)\nfrom .constants import DEFAULT_SHEET_NAME, FILE_FORMAT_CSV, FILE_FORMAT_TSV\n\n\nclass CSVZipBook(BookReader):\n    \"\"\"\n    CSVBook reader\n\n    It simply return one sheet\n    \"\"\"\n    def __init__(self, filename,\n                 file_content=None,\n                 load_sheet_with_name=None,\n                 load_sheet_at_index=None,\n                 **keywords):\n        BookReader.__init__(self, filename,\n                            file_content=file_content,\n                            load_sheet_with_name=load_sheet_with_name,\n                            load_sheet_at_index=load_sheet_at_index,\n                            **keywords)\n        self.native_book.close()\n\n    def load_from_memory(self, file_content, **keywords):\n        return zipfile.ZipFile(file_content, 'r')\n\n    def load_from_file(self, filename, **keywords):\n        return zipfile.ZipFile(filename, 'r')\n\n    def sheet_iterator(self):\n        if self.sheet_name:\n            rets = [sheet for sheet\n                    in self.native_book.namelist()\n                    if self._get_sheet_name(sheet) == self.sheet_name]\n            if len(rets) == 0:\n                raise ValueError(\"%s cannot be found\" % self.sheet_name)\n            else:\n                return rets\n        elif self.sheet_index is not None:\n            file_list = self.native_book.namelist()\n            length = len(file_list)\n            if self.sheet_index < length:\n                return [file_list[self.sheet_index]]\n            else:\n                raise IndexError(\n                    \"Index %d of out bound %d\" % (self.sheet_index,\n                                                  length))\n        else:\n            return self.native_book.namelist()\n\n    def _get_sheet_name(self, filename):\n        name_len = len(filename) - 4\n        return filename[:name_len]\n\n    def get_sheet(self, native_sheet):\n        content = self.native_book.read(native_sheet)\n        if PY2:\n            sheet = StringIO(content)\n        else:\n            sheet = StringIO(content.decode('utf-8'))\n\n        return CSVinMemoryReader(\n            NamedContent(\n                self._get_sheet_name(native_sheet),\n                sheet\n            ),\n            **self.keywords\n        )\n\n\nclass CSVZipSheetWriter(CSVSheetWriter):\n    def __init__(self, zipfile, sheetname, file_extension, **keywords):\n        self.file_extension = file_extension\n        keywords['single_sheet_in_book'] = False\n        CSVSheetWriter.__init__(self, zipfile, sheetname, **keywords)\n\n    def set_sheet_name(self, name):\n        self.content = StringIO()\n        self.writer = csv.writer(self.content, **self.keywords)\n\n    def close(self):\n        file_name = \"%s.%s\" % (self.native_sheet, self.file_extension)\n        self.native_book.writestr(file_name, self.content.getvalue())\n        self.content.close()\n\n\nclass CSVZipWriter(BookWriter):\n    \"\"\"\n    csv file writer\n\n    if there is multiple sheets for csv file, it simpily writes\n    multiple csv files\n    \"\"\"\n    def __init__(self, filename, **keywords):\n        BookWriter.__init__(self, filename, **keywords)\n        self.myzip = zipfile.ZipFile(self.file, 'w')\n        if 'dialect' in keywords:\n            self.file_extension = FILE_FORMAT_TSV\n        else:\n            self.file_extension = FILE_FORMAT_CSV\n\n    def create_sheet(self, name):\n        given_name = name\n        if given_name is None:\n            given_name = DEFAULT_SHEET_NAME\n        return CSVZipSheetWriter(self.myzip,\n                                 given_name,\n                                 self.file_extension,\n                                 **self.keywords)\n\n    def close(self):\n        \"\"\"\n        This call close the file handle\n        \"\"\"\n        self.myzip.close()\n", 
    "pyexcel_io.djangobook": "\"\"\"\n    pyexcel_io.djangobook\n    ~~~~~~~~~~~~~~~~~~~\n\n    The lower level handler for django import and export\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom ._compact import OrderedDict\nfrom .constants import (\n    MESSAGE_EMPTY_ARRAY,\n    MESSAGE_DB_EXCEPTION,\n    MESSAGE_IGNORE_ROW\n)\nfrom .base import (\n    BookReaderBase,\n    SheetReaderBase,\n    BookWriter,\n    SheetWriter,\n    from_query_sets,\n    is_empty_array,\n    swap_empty_string_for_none\n)\n\n\nclass DjangoModelReader(SheetReaderBase):\n    \"\"\"Read from django model\n    \"\"\"\n    def __init__(self, model):\n        self.model = model\n\n    @property\n    def name(self):\n        return self.model._meta.model_name\n\n    def to_array(self):\n        objects = self.model.objects.all()\n        if len(objects) == 0:\n            return []\n        else:\n            column_names = sorted(\n                [field.attname\n                 for field in self.model._meta.concrete_fields])\n            return from_query_sets(column_names, objects)\n\n\nclass DjangoBookReader(BookReaderBase):\n    \"\"\"Read from a list of django models\n    \"\"\"\n    def __init__(self, models):\n        self.my_sheets = OrderedDict()\n        for model in models:\n            djangomodelreader = DjangoModelReader(model)\n            self.my_sheets[djangomodelreader.name] = djangomodelreader.to_array()\n\n    def sheets(self):\n        return self.my_sheets\n\n\nclass DjangoModelWriter(SheetWriter):\n    def __init__(self, model, batch_size=None):\n        self.batch_size = batch_size\n        self.mymodel = None\n        self.column_names = None\n        self.mapdict = None\n        self.initializer = None\n\n        self.mymodel, self.column_names, self.mapdict, self.initializer = model\n\n        if self.initializer is None:\n            self.initializer = lambda row: row\n        if isinstance(self.mapdict, list):\n            self.column_names = self.mapdict\n            self.mapdict = None\n        elif isinstance(self.mapdict, dict):\n            self.column_names = [self.mapdict[name]\n                                 for name in self.column_names]\n        self.objs = []\n\n    def write_row(self, array):\n        if is_empty_array(array):\n            print(MESSAGE_EMPTY_ARRAY)\n        else:\n            new_array = swap_empty_string_for_none(array)\n            self.objs.append(self.mymodel(**dict(\n                zip(self.column_names, self.initializer(new_array))\n            )))\n\n    def close(self):\n        try:\n            self.mymodel.objects.bulk_create(self.objs, batch_size=self.batch_size)\n        except Exception as e:\n            print(MESSAGE_DB_EXCEPTION)\n            print(e)\n            for object in self.objs:\n                try:\n                    object.save()\n                except Exception as e2:\n                    print(MESSAGE_IGNORE_ROW)\n                    print(e2)\n                    print(object)\n                    continue\n\n\nclass DjangoBookWriter(BookWriter):\n    \"\"\"Write to alist of tables\n    \"\"\"\n    def __init__(self, file, models=None, batch_size=None, **keywords):\n        BookWriter.__init__(self, file, **keywords)\n        self.models = models\n        self.batch_size = batch_size\n\n    def create_sheet(self, name):\n        if name in self.models:\n            model_params = self.models[name]\n            return DjangoModelWriter(model_params, batch_size=self.batch_size)\n        else:\n            return None\n\n    def close(self):\n        pass\n", 
    "pyexcel_io.sqlbook": "\"\"\"\n    pyexcel_io.sqlbook\n    ~~~~~~~~~~~~~~~~~~~\n\n    The lower level handler for database import and export\n\n    :copyright: (c) 2014-2015 by Onni Software Ltd.\n    :license: New BSD License, see LICENSE for more details\n\"\"\"\nfrom ._compact import OrderedDict\nfrom .constants import (\n    MESSAGE_INVALID_PARAMETERS,\n    MESSAGE_EMPTY_ARRAY\n)\nfrom .base import (\n    BookReaderBase,\n    SheetReaderBase,\n    BookWriter,\n    SheetWriter,\n    from_query_sets,\n    is_empty_array,\n    swap_empty_string_for_none\n)\n\n\nclass SQLTableReader(SheetReaderBase):\n    \"\"\"Read a table\n    \"\"\"\n    def __init__(self, session, table):\n        self.session = session\n        self.table = table\n\n    @property\n    def name(self):\n        return getattr(self.table, '__tablename__', None)\n\n    def to_array(self):\n        objects = self.session.query(self.table).all()\n        if len(objects) == 0:\n            return []\n        else:\n            column_names = sorted([column for column in objects[0].__dict__\n                                   if column != '_sa_instance_state'])\n            return from_query_sets(column_names, objects)\n\n\nclass SQLBookReader(BookReaderBase):\n    \"\"\"Read a list of tables\n    \"\"\"\n    def __init__(self, session=None, tables=None):\n        self.my_sheets = OrderedDict()\n        for table in tables:\n            sqltablereader = SQLTableReader(session, table)\n            self.my_sheets[sqltablereader.name] = sqltablereader.to_array()\n\n    def sheets(self):\n        return self.my_sheets\n\n\nclass SQLTableWriter(SheetWriter):\n    \"\"\"Write to a table\n    \"\"\"\n    def __init__(self, session, table_params, auto_commit=True, **keywords):\n        self.session = session\n        self.table = None\n        self.initializer = None\n        self.mapdict = None\n        self.column_names = None\n        self.auto_commit = auto_commit\n        self.keywords = keywords\n        if len(table_params) == 4:\n            self.table, self.column_names, self.mapdict, self.initializer = table_params\n        else:\n            raise ValueError(MESSAGE_INVALID_PARAMETERS)\n\n        if isinstance(self.mapdict, list):\n            self.column_names = self.mapdict\n            self.mapdict = None\n\n    def write_row(self, array):\n        if is_empty_array(array):\n            print(MESSAGE_EMPTY_ARRAY)\n        else:\n            new_array = swap_empty_string_for_none(array)\n            self._write_row(new_array)\n\n    def _write_row(self, array):\n        row = dict(zip(self.column_names, array))\n        if self.initializer:\n            o = self.initializer(row)\n        else:\n            o = self.table()\n            for name in self.column_names:\n                if self.mapdict is not None:\n                    key = self.mapdict[name]\n                else:\n                    key = name\n                setattr(o, key, row[name])\n        self.session.add(o)\n\n    def close(self):\n        if self.auto_commit:\n            self.session.commit()\n\n\nclass SQLBookWriter(BookWriter):\n    \"\"\"Write to alist of tables\n    \"\"\"\n    def __init__(self, file, session=None, tables=None, **keywords):\n        BookWriter.__init__(self, file, **keywords)\n        self.session = session\n        self.tables = tables\n\n    def create_sheet(self, name):\n        table_params = self.tables[name]\n        return SQLTableWriter(self.session, table_params, **self.keywords)\n\n    def close(self):\n        pass\n", 
    "quopri": "#! /usr/bin/env python\n\n\"\"\"Conversions to/from quoted-printable transport encoding as per RFC 1521.\"\"\"\n\n# (Dec 1991 version).\n\n__all__ = [\"encode\", \"decode\", \"encodestring\", \"decodestring\"]\n\nESCAPE = '='\nMAXLINESIZE = 76\nHEX = '0123456789ABCDEF'\nEMPTYSTRING = ''\n\ntry:\n    from binascii import a2b_qp, b2a_qp\nexcept ImportError:\n    a2b_qp = None\n    b2a_qp = None\n\n\ndef needsquoting(c, quotetabs, header):\n    \"\"\"Decide whether a particular character needs to be quoted.\n\n    The 'quotetabs' flag indicates whether embedded tabs and spaces should be\n    quoted.  Note that line-ending tabs and spaces are always encoded, as per\n    RFC 1521.\n    \"\"\"\n    if c in ' \\t':\n        return quotetabs\n    # if header, we have to escape _ because _ is used to escape space\n    if c == '_':\n        return header\n    return c == ESCAPE or not (' ' <= c <= '~')\n\ndef quote(c):\n    \"\"\"Quote a single character.\"\"\"\n    i = ord(c)\n    return ESCAPE + HEX[i//16] + HEX[i%16]\n\n\n\ndef encode(input, output, quotetabs, header = 0):\n    \"\"\"Read 'input', apply quoted-printable encoding, and write to 'output'.\n\n    'input' and 'output' are files with readline() and write() methods.\n    The 'quotetabs' flag indicates whether embedded tabs and spaces should be\n    quoted.  Note that line-ending tabs and spaces are always encoded, as per\n    RFC 1521.\n    The 'header' flag indicates whether we are encoding spaces as _ as per\n    RFC 1522.\n    \"\"\"\n\n    if b2a_qp is not None:\n        data = input.read()\n        odata = b2a_qp(data, quotetabs = quotetabs, header = header)\n        output.write(odata)\n        return\n\n    def write(s, output=output, lineEnd='\\n'):\n        # RFC 1521 requires that the line ending in a space or tab must have\n        # that trailing character encoded.\n        if s and s[-1:] in ' \\t':\n            output.write(s[:-1] + quote(s[-1]) + lineEnd)\n        elif s == '.':\n            output.write(quote(s) + lineEnd)\n        else:\n            output.write(s + lineEnd)\n\n    prevline = None\n    while 1:\n        line = input.readline()\n        if not line:\n            break\n        outline = []\n        # Strip off any readline induced trailing newline\n        stripped = ''\n        if line[-1:] == '\\n':\n            line = line[:-1]\n            stripped = '\\n'\n        # Calculate the un-length-limited encoded line\n        for c in line:\n            if needsquoting(c, quotetabs, header):\n                c = quote(c)\n            if header and c == ' ':\n                outline.append('_')\n            else:\n                outline.append(c)\n        # First, write out the previous line\n        if prevline is not None:\n            write(prevline)\n        # Now see if we need any soft line breaks because of RFC-imposed\n        # length limitations.  Then do the thisline->prevline dance.\n        thisline = EMPTYSTRING.join(outline)\n        while len(thisline) > MAXLINESIZE:\n            # Don't forget to include the soft line break `=' sign in the\n            # length calculation!\n            write(thisline[:MAXLINESIZE-1], lineEnd='=\\n')\n            thisline = thisline[MAXLINESIZE-1:]\n        # Write out the current line\n        prevline = thisline\n    # Write out the last line, without a trailing newline\n    if prevline is not None:\n        write(prevline, lineEnd=stripped)\n\ndef encodestring(s, quotetabs = 0, header = 0):\n    if b2a_qp is not None:\n        return b2a_qp(s, quotetabs = quotetabs, header = header)\n    from cStringIO import StringIO\n    infp = StringIO(s)\n    outfp = StringIO()\n    encode(infp, outfp, quotetabs, header)\n    return outfp.getvalue()\n\n\n\ndef decode(input, output, header = 0):\n    \"\"\"Read 'input', apply quoted-printable decoding, and write to 'output'.\n    'input' and 'output' are files with readline() and write() methods.\n    If 'header' is true, decode underscore as space (per RFC 1522).\"\"\"\n\n    if a2b_qp is not None:\n        data = input.read()\n        odata = a2b_qp(data, header = header)\n        output.write(odata)\n        return\n\n    new = ''\n    while 1:\n        line = input.readline()\n        if not line: break\n        i, n = 0, len(line)\n        if n > 0 and line[n-1] == '\\n':\n            partial = 0; n = n-1\n            # Strip trailing whitespace\n            while n > 0 and line[n-1] in \" \\t\\r\":\n                n = n-1\n        else:\n            partial = 1\n        while i < n:\n            c = line[i]\n            if c == '_' and header:\n                new = new + ' '; i = i+1\n            elif c != ESCAPE:\n                new = new + c; i = i+1\n            elif i+1 == n and not partial:\n                partial = 1; break\n            elif i+1 < n and line[i+1] == ESCAPE:\n                new = new + ESCAPE; i = i+2\n            elif i+2 < n and ishex(line[i+1]) and ishex(line[i+2]):\n                new = new + chr(unhex(line[i+1:i+3])); i = i+3\n            else: # Bad escape sequence -- leave it in\n                new = new + c; i = i+1\n        if not partial:\n            output.write(new + '\\n')\n            new = ''\n    if new:\n        output.write(new)\n\ndef decodestring(s, header = 0):\n    if a2b_qp is not None:\n        return a2b_qp(s, header = header)\n    from cStringIO import StringIO\n    infp = StringIO(s)\n    outfp = StringIO()\n    decode(infp, outfp, header = header)\n    return outfp.getvalue()\n\n\n\n# Other helper functions\ndef ishex(c):\n    \"\"\"Return true if the character 'c' is a hexadecimal digit.\"\"\"\n    return '0' <= c <= '9' or 'a' <= c <= 'f' or 'A' <= c <= 'F'\n\ndef unhex(s):\n    \"\"\"Get the integer value of a hexadecimal number.\"\"\"\n    bits = 0\n    for c in s:\n        if '0' <= c <= '9':\n            i = ord('0')\n        elif 'a' <= c <= 'f':\n            i = ord('a')-10\n        elif 'A' <= c <= 'F':\n            i = ord('A')-10\n        else:\n            break\n        bits = bits*16 + (ord(c) - i)\n    return bits\n\n\n\ndef main():\n    import sys\n    import getopt\n    try:\n        opts, args = getopt.getopt(sys.argv[1:], 'td')\n    except getopt.error, msg:\n        sys.stdout = sys.stderr\n        print msg\n        print \"usage: quopri [-t | -d] [file] ...\"\n        print \"-t: quote tabs\"\n        print \"-d: decode; default encode\"\n        sys.exit(2)\n    deco = 0\n    tabs = 0\n    for o, a in opts:\n        if o == '-t': tabs = 1\n        if o == '-d': deco = 1\n    if tabs and deco:\n        sys.stdout = sys.stderr\n        print \"-t and -d are mutually exclusive\"\n        sys.exit(2)\n    if not args: args = ['-']\n    sts = 0\n    for file in args:\n        if file == '-':\n            fp = sys.stdin\n        else:\n            try:\n                fp = open(file)\n            except IOError, msg:\n                sys.stderr.write(\"%s: can't open (%s)\\n\" % (file, msg))\n                sts = 1\n                continue\n        if deco:\n            decode(fp, sys.stdout)\n        else:\n            encode(fp, sys.stdout, tabs)\n        if fp is not sys.stdin:\n            fp.close()\n    if sts:\n        sys.exit(sts)\n\n\n\nif __name__ == '__main__':\n    main()\n", 
    "random": "\"\"\"Random variable generators.\n\n    integers\n    --------\n           uniform within range\n\n    sequences\n    ---------\n           pick random element\n           pick random sample\n           generate random permutation\n\n    distributions on the real line:\n    ------------------------------\n           uniform\n           triangular\n           normal (Gaussian)\n           lognormal\n           negative exponential\n           gamma\n           beta\n           pareto\n           Weibull\n\n    distributions on the circle (angles 0 to 2pi)\n    ---------------------------------------------\n           circular uniform\n           von Mises\n\nGeneral notes on the underlying Mersenne Twister core generator:\n\n* The period is 2**19937-1.\n* It is one of the most extensively tested generators in existence.\n* Without a direct way to compute N steps forward, the semantics of\n  jumpahead(n) are weakened to simply jump to another distant state and rely\n  on the large period to avoid overlapping sequences.\n* The random() method is implemented in C, executes in a single Python step,\n  and is, therefore, threadsafe.\n\n\"\"\"\n\nfrom __future__ import division\nfrom warnings import warn as _warn\nfrom math import log as _log, exp as _exp, pi as _pi, e as _e, ceil as _ceil\nfrom math import sqrt as _sqrt, acos as _acos, cos as _cos, sin as _sin\nfrom os import urandom as _urandom\nfrom binascii import hexlify as _hexlify\nimport hashlib as _hashlib\n\n__all__ = [\"Random\",\"seed\",\"random\",\"uniform\",\"randint\",\"choice\",\"sample\",\n           \"randrange\",\"shuffle\",\"normalvariate\",\"lognormvariate\",\n           \"expovariate\",\"vonmisesvariate\",\"gammavariate\",\"triangular\",\n           \"gauss\",\"betavariate\",\"paretovariate\",\"weibullvariate\",\n           \"getstate\",\"setstate\",\"jumpahead\", \"WichmannHill\", \"getrandbits\",\n           \"SystemRandom\"]\n\nNV_MAGICCONST = 4 * _exp(-0.5)/_sqrt(2.0)\nTWOPI = 2.0*_pi\nLOG4 = _log(4.0)\nSG_MAGICCONST = 1.0 + _log(4.5)\nBPF = 53        # Number of bits in a float\nRECIP_BPF = 2**-BPF\n\n\n# Translated by Guido van Rossum from C source provided by\n# Adrian Baddeley.  Adapted by Raymond Hettinger for use with\n# the Mersenne Twister  and os.urandom() core generators.\n\nimport _random\n\nclass Random(_random.Random):\n    \"\"\"Random number generator base class used by bound module functions.\n\n    Used to instantiate instances of Random to get generators that don't\n    share state.  Especially useful for multi-threaded programs, creating\n    a different instance of Random for each thread, and using the jumpahead()\n    method to ensure that the generated sequences seen by each thread don't\n    overlap.\n\n    Class Random can also be subclassed if you want to use a different basic\n    generator of your own devising: in that case, override the following\n    methods: random(), seed(), getstate(), setstate() and jumpahead().\n    Optionally, implement a getrandbits() method so that randrange() can cover\n    arbitrarily large ranges.\n\n    \"\"\"\n\n    VERSION = 3     # used by getstate/setstate\n\n    def __init__(self, x=None):\n        \"\"\"Initialize an instance.\n\n        Optional argument x controls seeding, as for Random.seed().\n        \"\"\"\n\n        self.seed(x)\n        self.gauss_next = None\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n        \"\"\"\n\n        if a is None:\n            try:\n                # Seed with enough bytes to span the 19937 bit\n                # state space for the Mersenne Twister\n                a = long(_hexlify(_urandom(2500)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        super(Random, self).seed(a)\n        self.gauss_next = None\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, super(Random, self).getstate(), self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 3:\n            version, internalstate, self.gauss_next = state\n            super(Random, self).setstate(internalstate)\n        elif version == 2:\n            version, internalstate, self.gauss_next = state\n            # In version 2, the state was saved as signed ints, which causes\n            #   inconsistencies between 32/64-bit systems. The state is\n            #   really unsigned 32-bit ints, so we convert negative ints from\n            #   version 2 to positive longs for version 3.\n            try:\n                internalstate = tuple( long(x) % (2**32) for x in internalstate )\n            except ValueError, e:\n                raise TypeError, e\n            super(Random, self).setstate(internalstate)\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Change the internal state to one that is likely far away\n        from the current state.  This method will not be in Py3.x,\n        so it is better to simply reseed.\n        \"\"\"\n        # The super.jumpahead() method uses shuffling to change state,\n        # so it needs a large and \"interesting\" n to work with.  Here,\n        # we use hashing to create a large n for the shuffle.\n        s = repr(n) + repr(self.getstate())\n        n = int(_hashlib.new('sha512', s).hexdigest(), 16)\n        super(Random, self).jumpahead(n)\n\n## ---- Methods below this point do not need to be overridden when\n## ---- subclassing for the purpose of using a different core generator.\n\n## -------------------- pickle support  -------------------\n\n    def __getstate__(self): # for pickle\n        return self.getstate()\n\n    def __setstate__(self, state):  # for pickle\n        self.setstate(state)\n\n    def __reduce__(self):\n        return self.__class__, (), self.getstate()\n\n## -------------------- integer methods  -------------------\n\n    def randrange(self, start, stop=None, step=1, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Choose a random item from range(start, stop[, step]).\n\n        This fixes the problem with randint() which includes the\n        endpoint; in Python this is usually not what you want.\n\n        \"\"\"\n\n        # This code is a bit messy to make it fast for the\n        # common case while still doing adequate error checking.\n        istart = _int(start)\n        if istart != start:\n            raise ValueError, \"non-integer arg 1 for randrange()\"\n        if stop is None:\n            if istart > 0:\n                if istart >= _maxwidth:\n                    return self._randbelow(istart)\n                return _int(self.random() * istart)\n            raise ValueError, \"empty range for randrange()\"\n\n        # stop argument supplied.\n        istop = _int(stop)\n        if istop != stop:\n            raise ValueError, \"non-integer stop for randrange()\"\n        width = istop - istart\n        if step == 1 and width > 0:\n            # Note that\n            #     int(istart + self.random()*width)\n            # instead would be incorrect.  For example, consider istart\n            # = -2 and istop = 0.  Then the guts would be in\n            # -2.0 to 0.0 exclusive on both ends (ignoring that random()\n            # might return 0.0), and because int() truncates toward 0, the\n            # final result would be -1 or 0 (instead of -2 or -1).\n            #     istart + int(self.random()*width)\n            # would also be incorrect, for a subtler reason:  the RHS\n            # can return a long, and then randrange() would also return\n            # a long, but we're supposed to return an int (for backward\n            # compatibility).\n\n            if width >= _maxwidth:\n                return _int(istart + self._randbelow(width))\n            return _int(istart + _int(self.random()*width))\n        if step == 1:\n            raise ValueError, \"empty range for randrange() (%d,%d, %d)\" % (istart, istop, width)\n\n        # Non-unit step argument supplied.\n        istep = _int(step)\n        if istep != step:\n            raise ValueError, \"non-integer step for randrange()\"\n        if istep > 0:\n            n = (width + istep - 1) // istep\n        elif istep < 0:\n            n = (width + istep + 1) // istep\n        else:\n            raise ValueError, \"zero step for randrange()\"\n\n        if n <= 0:\n            raise ValueError, \"empty range for randrange()\"\n\n        if n >= _maxwidth:\n            return istart + istep*self._randbelow(n)\n        return istart + istep*_int(self.random() * n)\n\n    def randint(self, a, b):\n        \"\"\"Return random integer in range [a, b], including both end points.\n        \"\"\"\n\n        return self.randrange(a, b+1)\n\n    def _randbelow(self, n, _log=_log, _int=int, _maxwidth=1L<<BPF):\n        \"\"\"Return a random int in the range [0,n)\n\n        Handles the case where n has more bits than returned\n        by a single call to the underlying generator.\n        \"\"\"\n\n        try:\n            getrandbits = self.getrandbits\n        except AttributeError:\n            pass\n        else:\n            # Only call self.getrandbits if the original random() builtin method\n            # has not been overridden or if a new getrandbits() was supplied.\n            # This assures that the two methods correspond.\n            if (self.random == super(Random, self).random or\n                    getrandbits != super(Random, self).getrandbits):\n                k = _int(1.00001 + _log(n-1, 2.0))   # 2**k > n-1 > 2**(k-2)\n                r = getrandbits(k)\n                while r >= n:\n                    r = getrandbits(k)\n                return r\n        if n >= _maxwidth:\n            _warn(\"Underlying random() generator does not supply \\n\"\n                \"enough bits to choose from a population range this large\")\n        return _int(self.random() * n)\n\n## -------------------- sequence methods  -------------------\n\n    def choice(self, seq):\n        \"\"\"Choose a random element from a non-empty sequence.\"\"\"\n        return seq[int(self.random() * len(seq))]  # raises IndexError if seq is empty\n\n    def shuffle(self, x, random=None):\n        \"\"\"x, random=random.random -> shuffle list x in place; return None.\n\n        Optional arg random is a 0-argument function returning a random\n        float in [0.0, 1.0); by default, the standard random.random.\n\n        \"\"\"\n\n        if random is None:\n            random = self.random\n        _int = int\n        for i in reversed(xrange(1, len(x))):\n            # pick an element in x[:i+1] with which to exchange x[i]\n            j = _int(random() * (i+1))\n            x[i], x[j] = x[j], x[i]\n\n    def sample(self, population, k):\n        \"\"\"Chooses k unique random elements from a population sequence.\n\n        Returns a new list containing elements from the population while\n        leaving the original population unchanged.  The resulting list is\n        in selection order so that all sub-slices will also be valid random\n        samples.  This allows raffle winners (the sample) to be partitioned\n        into grand prize and second place winners (the subslices).\n\n        Members of the population need not be hashable or unique.  If the\n        population contains repeats, then each occurrence is a possible\n        selection in the sample.\n\n        To choose a sample in a range of integers, use xrange as an argument.\n        This is especially fast and space efficient for sampling from a\n        large population:   sample(xrange(10000000), 60)\n        \"\"\"\n\n        # Sampling without replacement entails tracking either potential\n        # selections (the pool) in a list or previous selections in a set.\n\n        # When the number of selections is small compared to the\n        # population, then tracking selections is efficient, requiring\n        # only a small set and an occasional reselection.  For\n        # a larger number of selections, the pool tracking method is\n        # preferred since the list takes less space than the\n        # set and it doesn't suffer from frequent reselections.\n\n        n = len(population)\n        if not 0 <= k <= n:\n            raise ValueError(\"sample larger than population\")\n        random = self.random\n        _int = int\n        result = [None] * k\n        setsize = 21        # size of a small set minus size of an empty list\n        if k > 5:\n            setsize += 4 ** _ceil(_log(k * 3, 4)) # table size for big sets\n        if n <= setsize or hasattr(population, \"keys\"):\n            # An n-length list is smaller than a k-length set, or this is a\n            # mapping type so the other algorithm wouldn't work.\n            pool = list(population)\n            for i in xrange(k):         # invariant:  non-selected at [0,n-i)\n                j = _int(random() * (n-i))\n                result[i] = pool[j]\n                pool[j] = pool[n-i-1]   # move non-selected item into vacancy\n        else:\n            try:\n                selected = set()\n                selected_add = selected.add\n                for i in xrange(k):\n                    j = _int(random() * n)\n                    while j in selected:\n                        j = _int(random() * n)\n                    selected_add(j)\n                    result[i] = population[j]\n            except (TypeError, KeyError):   # handle (at least) sets\n                if isinstance(population, list):\n                    raise\n                return self.sample(tuple(population), k)\n        return result\n\n## -------------------- real-valued distributions  -------------------\n\n## -------------------- uniform distribution -------------------\n\n    def uniform(self, a, b):\n        \"Get a random number in the range [a, b) or [a, b] depending on rounding.\"\n        return a + (b-a) * self.random()\n\n## -------------------- triangular --------------------\n\n    def triangular(self, low=0.0, high=1.0, mode=None):\n        \"\"\"Triangular distribution.\n\n        Continuous distribution bounded by given lower and upper limits,\n        and having a given mode value in-between.\n\n        http://en.wikipedia.org/wiki/Triangular_distribution\n\n        \"\"\"\n        u = self.random()\n        try:\n            c = 0.5 if mode is None else (mode - low) / (high - low)\n        except ZeroDivisionError:\n            return low\n        if u > c:\n            u = 1.0 - u\n            c = 1.0 - c\n            low, high = high, low\n        return low + (high - low) * (u * c) ** 0.5\n\n## -------------------- normal distribution --------------------\n\n    def normalvariate(self, mu, sigma):\n        \"\"\"Normal distribution.\n\n        mu is the mean, and sigma is the standard deviation.\n\n        \"\"\"\n        # mu = mean, sigma = standard deviation\n\n        # Uses Kinderman and Monahan method. Reference: Kinderman,\n        # A.J. and Monahan, J.F., \"Computer generation of random\n        # variables using the ratio of uniform deviates\", ACM Trans\n        # Math Software, 3, (1977), pp257-260.\n\n        random = self.random\n        while 1:\n            u1 = random()\n            u2 = 1.0 - random()\n            z = NV_MAGICCONST*(u1-0.5)/u2\n            zz = z*z/4.0\n            if zz <= -_log(u2):\n                break\n        return mu + z*sigma\n\n## -------------------- lognormal distribution --------------------\n\n    def lognormvariate(self, mu, sigma):\n        \"\"\"Log normal distribution.\n\n        If you take the natural logarithm of this distribution, you'll get a\n        normal distribution with mean mu and standard deviation sigma.\n        mu can have any value, and sigma must be greater than zero.\n\n        \"\"\"\n        return _exp(self.normalvariate(mu, sigma))\n\n## -------------------- exponential distribution --------------------\n\n    def expovariate(self, lambd):\n        \"\"\"Exponential distribution.\n\n        lambd is 1.0 divided by the desired mean.  It should be\n        nonzero.  (The parameter would be called \"lambda\", but that is\n        a reserved word in Python.)  Returned values range from 0 to\n        positive infinity if lambd is positive, and from negative\n        infinity to 0 if lambd is negative.\n\n        \"\"\"\n        # lambd: rate lambd = 1/mean\n        # ('lambda' is a Python reserved word)\n\n        # we use 1-random() instead of random() to preclude the\n        # possibility of taking the log of zero.\n        return -_log(1.0 - self.random())/lambd\n\n## -------------------- von Mises distribution --------------------\n\n    def vonmisesvariate(self, mu, kappa):\n        \"\"\"Circular data distribution.\n\n        mu is the mean angle, expressed in radians between 0 and 2*pi, and\n        kappa is the concentration parameter, which must be greater than or\n        equal to zero.  If kappa is equal to zero, this distribution reduces\n        to a uniform random angle over the range 0 to 2*pi.\n\n        \"\"\"\n        # mu:    mean angle (in radians between 0 and 2*pi)\n        # kappa: concentration parameter kappa (>= 0)\n        # if kappa = 0 generate uniform random angle\n\n        # Based upon an algorithm published in: Fisher, N.I.,\n        # \"Statistical Analysis of Circular Data\", Cambridge\n        # University Press, 1993.\n\n        # Thanks to Magnus Kessler for a correction to the\n        # implementation of step 4.\n\n        random = self.random\n        if kappa <= 1e-6:\n            return TWOPI * random()\n\n        s = 0.5 / kappa\n        r = s + _sqrt(1.0 + s * s)\n\n        while 1:\n            u1 = random()\n            z = _cos(_pi * u1)\n\n            d = z / (r + z)\n            u2 = random()\n            if u2 < 1.0 - d * d or u2 <= (1.0 - d) * _exp(d):\n                break\n\n        q = 1.0 / r\n        f = (q + z) / (1.0 + q * z)\n        u3 = random()\n        if u3 > 0.5:\n            theta = (mu + _acos(f)) % TWOPI\n        else:\n            theta = (mu - _acos(f)) % TWOPI\n\n        return theta\n\n## -------------------- gamma distribution --------------------\n\n    def gammavariate(self, alpha, beta):\n        \"\"\"Gamma distribution.  Not the gamma function!\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n\n        The probability distribution function is:\n\n                    x ** (alpha - 1) * math.exp(-x / beta)\n          pdf(x) =  --------------------------------------\n                      math.gamma(alpha) * beta ** alpha\n\n        \"\"\"\n\n        # alpha > 0, beta > 0, mean is alpha*beta, variance is alpha*beta**2\n\n        # Warning: a few older sources define the gamma distribution in terms\n        # of alpha > -1.0\n        if alpha <= 0.0 or beta <= 0.0:\n            raise ValueError, 'gammavariate: alpha and beta must be > 0.0'\n\n        random = self.random\n        if alpha > 1.0:\n\n            # Uses R.C.H. Cheng, \"The generation of Gamma\n            # variables with non-integral shape parameters\",\n            # Applied Statistics, (1977), 26, No. 1, p71-74\n\n            ainv = _sqrt(2.0 * alpha - 1.0)\n            bbb = alpha - LOG4\n            ccc = alpha + ainv\n\n            while 1:\n                u1 = random()\n                if not 1e-7 < u1 < .9999999:\n                    continue\n                u2 = 1.0 - random()\n                v = _log(u1/(1.0-u1))/ainv\n                x = alpha*_exp(v)\n                z = u1*u1*u2\n                r = bbb+ccc*v-x\n                if r + SG_MAGICCONST - 4.5*z >= 0.0 or r >= _log(z):\n                    return x * beta\n\n        elif alpha == 1.0:\n            # expovariate(1)\n            u = random()\n            while u <= 1e-7:\n                u = random()\n            return -_log(u) * beta\n\n        else:   # alpha is between 0 and 1 (exclusive)\n\n            # Uses ALGORITHM GS of Statistical Computing - Kennedy & Gentle\n\n            while 1:\n                u = random()\n                b = (_e + alpha)/_e\n                p = b*u\n                if p <= 1.0:\n                    x = p ** (1.0/alpha)\n                else:\n                    x = -_log((b-p)/alpha)\n                u1 = random()\n                if p > 1.0:\n                    if u1 <= x ** (alpha - 1.0):\n                        break\n                elif u1 <= _exp(-x):\n                    break\n            return x * beta\n\n## -------------------- Gauss (faster alternative) --------------------\n\n    def gauss(self, mu, sigma):\n        \"\"\"Gaussian distribution.\n\n        mu is the mean, and sigma is the standard deviation.  This is\n        slightly faster than the normalvariate() function.\n\n        Not thread-safe without a lock around calls.\n\n        \"\"\"\n\n        # When x and y are two variables from [0, 1), uniformly\n        # distributed, then\n        #\n        #    cos(2*pi*x)*sqrt(-2*log(1-y))\n        #    sin(2*pi*x)*sqrt(-2*log(1-y))\n        #\n        # are two *independent* variables with normal distribution\n        # (mu = 0, sigma = 1).\n        # (Lambert Meertens)\n        # (corrected version; bug discovered by Mike Miller, fixed by LM)\n\n        # Multithreading note: When two threads call this function\n        # simultaneously, it is possible that they will receive the\n        # same return value.  The window is very small though.  To\n        # avoid this, you have to use a lock around all calls.  (I\n        # didn't want to slow this down in the serial case by using a\n        # lock here.)\n\n        random = self.random\n        z = self.gauss_next\n        self.gauss_next = None\n        if z is None:\n            x2pi = random() * TWOPI\n            g2rad = _sqrt(-2.0 * _log(1.0 - random()))\n            z = _cos(x2pi) * g2rad\n            self.gauss_next = _sin(x2pi) * g2rad\n\n        return mu + z*sigma\n\n## -------------------- beta --------------------\n## See\n## http://mail.python.org/pipermail/python-bugs-list/2001-January/003752.html\n## for Ivan Frohne's insightful analysis of why the original implementation:\n##\n##    def betavariate(self, alpha, beta):\n##        # Discrete Event Simulation in C, pp 87-88.\n##\n##        y = self.expovariate(alpha)\n##        z = self.expovariate(1.0/beta)\n##        return z/(y+z)\n##\n## was dead wrong, and how it probably got that way.\n\n    def betavariate(self, alpha, beta):\n        \"\"\"Beta distribution.\n\n        Conditions on the parameters are alpha > 0 and beta > 0.\n        Returned values range between 0 and 1.\n\n        \"\"\"\n\n        # This version due to Janne Sinkkonen, and matches all the std\n        # texts (e.g., Knuth Vol 2 Ed 3 pg 134 \"the beta distribution\").\n        y = self.gammavariate(alpha, 1.)\n        if y == 0:\n            return 0.0\n        else:\n            return y / (y + self.gammavariate(beta, 1.))\n\n## -------------------- Pareto --------------------\n\n    def paretovariate(self, alpha):\n        \"\"\"Pareto distribution.  alpha is the shape parameter.\"\"\"\n        # Jain, pg. 495\n\n        u = 1.0 - self.random()\n        return 1.0 / pow(u, 1.0/alpha)\n\n## -------------------- Weibull --------------------\n\n    def weibullvariate(self, alpha, beta):\n        \"\"\"Weibull distribution.\n\n        alpha is the scale parameter and beta is the shape parameter.\n\n        \"\"\"\n        # Jain, pg. 499; bug fix courtesy Bill Arms\n\n        u = 1.0 - self.random()\n        return alpha * pow(-_log(u), 1.0/beta)\n\n## -------------------- Wichmann-Hill -------------------\n\nclass WichmannHill(Random):\n\n    VERSION = 1     # used by getstate/setstate\n\n    def seed(self, a=None):\n        \"\"\"Initialize internal state from hashable object.\n\n        None or no argument seeds from current time or from an operating\n        system specific randomness source if available.\n\n        If a is not None or an int or long, hash(a) is used instead.\n\n        If a is an int or long, a is used directly.  Distinct values between\n        0 and 27814431486575L inclusive are guaranteed to yield distinct\n        internal states (this guarantee is specific to the default\n        Wichmann-Hill generator).\n        \"\"\"\n\n        if a is None:\n            try:\n                a = long(_hexlify(_urandom(16)), 16)\n            except NotImplementedError:\n                import time\n                a = long(time.time() * 256) # use fractional seconds\n\n        if not isinstance(a, (int, long)):\n            a = hash(a)\n\n        a, x = divmod(a, 30268)\n        a, y = divmod(a, 30306)\n        a, z = divmod(a, 30322)\n        self._seed = int(x)+1, int(y)+1, int(z)+1\n\n        self.gauss_next = None\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n\n        # Wichman-Hill random number generator.\n        #\n        # Wichmann, B. A. & Hill, I. D. (1982)\n        # Algorithm AS 183:\n        # An efficient and portable pseudo-random number generator\n        # Applied Statistics 31 (1982) 188-190\n        #\n        # see also:\n        #        Correction to Algorithm AS 183\n        #        Applied Statistics 33 (1984) 123\n        #\n        #        McLeod, A. I. (1985)\n        #        A remark on Algorithm AS 183\n        #        Applied Statistics 34 (1985),198-200\n\n        # This part is thread-unsafe:\n        # BEGIN CRITICAL SECTION\n        x, y, z = self._seed\n        x = (171 * x) % 30269\n        y = (172 * y) % 30307\n        z = (170 * z) % 30323\n        self._seed = x, y, z\n        # END CRITICAL SECTION\n\n        # Note:  on a platform using IEEE-754 double arithmetic, this can\n        # never return 0.0 (asserted by Tim; proof too long for a comment).\n        return (x/30269.0 + y/30307.0 + z/30323.0) % 1.0\n\n    def getstate(self):\n        \"\"\"Return internal state; can be passed to setstate() later.\"\"\"\n        return self.VERSION, self._seed, self.gauss_next\n\n    def setstate(self, state):\n        \"\"\"Restore internal state from object returned by getstate().\"\"\"\n        version = state[0]\n        if version == 1:\n            version, self._seed, self.gauss_next = state\n        else:\n            raise ValueError(\"state with version %s passed to \"\n                             \"Random.setstate() of version %s\" %\n                             (version, self.VERSION))\n\n    def jumpahead(self, n):\n        \"\"\"Act as if n calls to random() were made, but quickly.\n\n        n is an int, greater than or equal to 0.\n\n        Example use:  If you have 2 threads and know that each will\n        consume no more than a million random numbers, create two Random\n        objects r1 and r2, then do\n            r2.setstate(r1.getstate())\n            r2.jumpahead(1000000)\n        Then r1 and r2 will use guaranteed-disjoint segments of the full\n        period.\n        \"\"\"\n\n        if not n >= 0:\n            raise ValueError(\"n must be >= 0\")\n        x, y, z = self._seed\n        x = int(x * pow(171, n, 30269)) % 30269\n        y = int(y * pow(172, n, 30307)) % 30307\n        z = int(z * pow(170, n, 30323)) % 30323\n        self._seed = x, y, z\n\n    def __whseed(self, x=0, y=0, z=0):\n        \"\"\"Set the Wichmann-Hill seed from (x, y, z).\n\n        These must be integers in the range [0, 256).\n        \"\"\"\n\n        if not type(x) == type(y) == type(z) == int:\n            raise TypeError('seeds must be integers')\n        if not (0 <= x < 256 and 0 <= y < 256 and 0 <= z < 256):\n            raise ValueError('seeds must be in range(0, 256)')\n        if 0 == x == y == z:\n            # Initialize from current time\n            import time\n            t = long(time.time() * 256)\n            t = int((t&0xffffff) ^ (t>>24))\n            t, x = divmod(t, 256)\n            t, y = divmod(t, 256)\n            t, z = divmod(t, 256)\n        # Zero is a poor seed, so substitute 1\n        self._seed = (x or 1, y or 1, z or 1)\n\n        self.gauss_next = None\n\n    def whseed(self, a=None):\n        \"\"\"Seed from hashable object's hash code.\n\n        None or no argument seeds from current time.  It is not guaranteed\n        that objects with distinct hash codes lead to distinct internal\n        states.\n\n        This is obsolete, provided for compatibility with the seed routine\n        used prior to Python 2.1.  Use the .seed() method instead.\n        \"\"\"\n\n        if a is None:\n            self.__whseed()\n            return\n        a = hash(a)\n        a, x = divmod(a, 256)\n        a, y = divmod(a, 256)\n        a, z = divmod(a, 256)\n        x = (x + a) % 256 or 1\n        y = (y + a) % 256 or 1\n        z = (z + a) % 256 or 1\n        self.__whseed(x, y, z)\n\n## --------------- Operating System Random Source  ------------------\n\nclass SystemRandom(Random):\n    \"\"\"Alternate random number generator using sources provided\n    by the operating system (such as /dev/urandom on Unix or\n    CryptGenRandom on Windows).\n\n     Not available on all systems (see os.urandom() for details).\n    \"\"\"\n\n    def random(self):\n        \"\"\"Get the next random number in the range [0.0, 1.0).\"\"\"\n        return (long(_hexlify(_urandom(7)), 16) >> 3) * RECIP_BPF\n\n    def getrandbits(self, k):\n        \"\"\"getrandbits(k) -> x.  Generates a long int with k random bits.\"\"\"\n        if k <= 0:\n            raise ValueError('number of bits must be greater than zero')\n        if k != int(k):\n            raise TypeError('number of bits should be an integer')\n        bytes = (k + 7) // 8                    # bits / 8 and rounded up\n        x = long(_hexlify(_urandom(bytes)), 16)\n        return x >> (bytes * 8 - k)             # trim excess bits\n\n    def _stub(self, *args, **kwds):\n        \"Stub method.  Not used for a system random number generator.\"\n        return None\n    seed = jumpahead = _stub\n\n    def _notimplemented(self, *args, **kwds):\n        \"Method should not be called for a system random number generator.\"\n        raise NotImplementedError('System entropy source does not have state.')\n    getstate = setstate = _notimplemented\n\n## -------------------- test program --------------------\n\ndef _test_generator(n, func, args):\n    import time\n    print n, 'times', func.__name__\n    total = 0.0\n    sqsum = 0.0\n    smallest = 1e10\n    largest = -1e10\n    t0 = time.time()\n    for i in range(n):\n        x = func(*args)\n        total += x\n        sqsum = sqsum + x*x\n        smallest = min(x, smallest)\n        largest = max(x, largest)\n    t1 = time.time()\n    print round(t1-t0, 3), 'sec,',\n    avg = total/n\n    stddev = _sqrt(sqsum/n - avg*avg)\n    print 'avg %g, stddev %g, min %g, max %g' % \\\n              (avg, stddev, smallest, largest)\n\n\ndef _test(N=2000):\n    _test_generator(N, random, ())\n    _test_generator(N, normalvariate, (0.0, 1.0))\n    _test_generator(N, lognormvariate, (0.0, 1.0))\n    _test_generator(N, vonmisesvariate, (0.0, 1.0))\n    _test_generator(N, gammavariate, (0.01, 1.0))\n    _test_generator(N, gammavariate, (0.1, 1.0))\n    _test_generator(N, gammavariate, (0.1, 2.0))\n    _test_generator(N, gammavariate, (0.5, 1.0))\n    _test_generator(N, gammavariate, (0.9, 1.0))\n    _test_generator(N, gammavariate, (1.0, 1.0))\n    _test_generator(N, gammavariate, (2.0, 1.0))\n    _test_generator(N, gammavariate, (20.0, 1.0))\n    _test_generator(N, gammavariate, (200.0, 1.0))\n    _test_generator(N, gauss, (0.0, 1.0))\n    _test_generator(N, betavariate, (3.0, 3.0))\n    _test_generator(N, triangular, (0.0, 1.0, 1.0/3.0))\n\n# Create one instance, seeded from current time, and export its methods\n# as module-level functions.  The functions share state across all uses\n#(both in the user's code and in the Python libraries), but that's fine\n# for most programs and is easier for the casual user than making them\n# instantiate their own Random() instance.\n\n_inst = Random()\nseed = _inst.seed\nrandom = _inst.random\nuniform = _inst.uniform\ntriangular = _inst.triangular\nrandint = _inst.randint\nchoice = _inst.choice\nrandrange = _inst.randrange\nsample = _inst.sample\nshuffle = _inst.shuffle\nnormalvariate = _inst.normalvariate\nlognormvariate = _inst.lognormvariate\nexpovariate = _inst.expovariate\nvonmisesvariate = _inst.vonmisesvariate\ngammavariate = _inst.gammavariate\ngauss = _inst.gauss\nbetavariate = _inst.betavariate\nparetovariate = _inst.paretovariate\nweibullvariate = _inst.weibullvariate\ngetstate = _inst.getstate\nsetstate = _inst.setstate\njumpahead = _inst.jumpahead\ngetrandbits = _inst.getrandbits\n\nif __name__ == '__main__':\n    _test()\n", 
    "re": "#\n# Secret Labs' Regular Expression Engine\n#\n# re-compatible interface for the sre matching engine\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# This version of the SRE library can be redistributed under CNRI's\n# Python 1.6 license.  For any other use, please contact Secret Labs\n# AB (info@pythonware.com).\n#\n# Portions of this engine have been developed in cooperation with\n# CNRI.  Hewlett-Packard provided funding for 1.6 integration and\n# other compatibility work.\n#\n\nr\"\"\"Support for regular expressions (RE).\n\nThis module provides regular expression matching operations similar to\nthose found in Perl.  It supports both 8-bit and Unicode strings; both\nthe pattern and the strings being processed can contain null bytes and\ncharacters outside the US ASCII range.\n\nRegular expressions can contain both special and ordinary characters.\nMost ordinary characters, like \"A\", \"a\", or \"0\", are the simplest\nregular expressions; they simply match themselves.  You can\nconcatenate ordinary characters, so last matches the string 'last'.\n\nThe special characters are:\n    \".\"      Matches any character except a newline.\n    \"^\"      Matches the start of the string.\n    \"$\"      Matches the end of the string or just before the newline at\n             the end of the string.\n    \"*\"      Matches 0 or more (greedy) repetitions of the preceding RE.\n             Greedy means that it will match as many repetitions as possible.\n    \"+\"      Matches 1 or more (greedy) repetitions of the preceding RE.\n    \"?\"      Matches 0 or 1 (greedy) of the preceding RE.\n    *?,+?,?? Non-greedy versions of the previous three special characters.\n    {m,n}    Matches from m to n repetitions of the preceding RE.\n    {m,n}?   Non-greedy version of the above.\n    \"\\\\\"     Either escapes special characters or signals a special sequence.\n    []       Indicates a set of characters.\n             A \"^\" as the first character indicates a complementing set.\n    \"|\"      A|B, creates an RE that will match either A or B.\n    (...)    Matches the RE inside the parentheses.\n             The contents can be retrieved or matched later in the string.\n    (?iLmsux) Set the I, L, M, S, U, or X flag for the RE (see below).\n    (?:...)  Non-grouping version of regular parentheses.\n    (?P<name>...) The substring matched by the group is accessible by name.\n    (?P=name)     Matches the text matched earlier by the group named name.\n    (?#...)  A comment; ignored.\n    (?=...)  Matches if ... matches next, but doesn't consume the string.\n    (?!...)  Matches if ... doesn't match next.\n    (?<=...) Matches if preceded by ... (must be fixed length).\n    (?<!...) Matches if not preceded by ... (must be fixed length).\n    (?(id/name)yes|no) Matches yes pattern if the group with id/name matched,\n                       the (optional) no pattern otherwise.\n\nThe special sequences consist of \"\\\\\" and a character from the list\nbelow.  If the ordinary character is not on the list, then the\nresulting RE will match the second character.\n    \\number  Matches the contents of the group of the same number.\n    \\A       Matches only at the start of the string.\n    \\Z       Matches only at the end of the string.\n    \\b       Matches the empty string, but only at the start or end of a word.\n    \\B       Matches the empty string, but not at the start or end of a word.\n    \\d       Matches any decimal digit; equivalent to the set [0-9].\n    \\D       Matches any non-digit character; equivalent to the set [^0-9].\n    \\s       Matches any whitespace character; equivalent to [ \\t\\n\\r\\f\\v].\n    \\S       Matches any non-whitespace character; equiv. to [^ \\t\\n\\r\\f\\v].\n    \\w       Matches any alphanumeric character; equivalent to [a-zA-Z0-9_].\n             With LOCALE, it will match the set [0-9_] plus characters defined\n             as letters for the current locale.\n    \\W       Matches the complement of \\w.\n    \\\\       Matches a literal backslash.\n\nThis module exports the following functions:\n    match    Match a regular expression pattern to the beginning of a string.\n    search   Search a string for the presence of a pattern.\n    sub      Substitute occurrences of a pattern found in a string.\n    subn     Same as sub, but also return the number of substitutions made.\n    split    Split a string by the occurrences of a pattern.\n    findall  Find all occurrences of a pattern in a string.\n    finditer Return an iterator yielding a match object for each match.\n    compile  Compile a pattern into a RegexObject.\n    purge    Clear the regular expression cache.\n    escape   Backslash all non-alphanumerics in a string.\n\nSome of the functions in this module takes flags as optional parameters:\n    I  IGNORECASE  Perform case-insensitive matching.\n    L  LOCALE      Make \\w, \\W, \\b, \\B, dependent on the current locale.\n    M  MULTILINE   \"^\" matches the beginning of lines (after a newline)\n                   as well as the string.\n                   \"$\" matches the end of lines (before a newline) as well\n                   as the end of the string.\n    S  DOTALL      \".\" matches any character at all, including the newline.\n    X  VERBOSE     Ignore whitespace and comments for nicer looking RE's.\n    U  UNICODE     Make \\w, \\W, \\b, \\B, dependent on the Unicode locale.\n\nThis module also defines an exception 'error'.\n\n\"\"\"\n\nimport sys\nimport sre_compile\nimport sre_parse\ntry:\n    import _locale\nexcept ImportError:\n    _locale = None\n\n# public symbols\n__all__ = [ \"match\", \"search\", \"sub\", \"subn\", \"split\", \"findall\",\n    \"compile\", \"purge\", \"template\", \"escape\", \"I\", \"L\", \"M\", \"S\", \"X\",\n    \"U\", \"IGNORECASE\", \"LOCALE\", \"MULTILINE\", \"DOTALL\", \"VERBOSE\",\n    \"UNICODE\", \"error\" ]\n\n__version__ = \"2.2.1\"\n\n# flags\nI = IGNORECASE = sre_compile.SRE_FLAG_IGNORECASE # ignore case\nL = LOCALE = sre_compile.SRE_FLAG_LOCALE # assume current 8-bit locale\nU = UNICODE = sre_compile.SRE_FLAG_UNICODE # assume unicode locale\nM = MULTILINE = sre_compile.SRE_FLAG_MULTILINE # make anchors look for newline\nS = DOTALL = sre_compile.SRE_FLAG_DOTALL # make dot match newline\nX = VERBOSE = sre_compile.SRE_FLAG_VERBOSE # ignore whitespace and comments\n\n# sre extensions (experimental, don't rely on these)\nT = TEMPLATE = sre_compile.SRE_FLAG_TEMPLATE # disable backtracking\nDEBUG = sre_compile.SRE_FLAG_DEBUG # dump pattern after compilation\n\n# sre exception\nerror = sre_compile.error\n\n# --------------------------------------------------------------------\n# public interface\n\ndef match(pattern, string, flags=0):\n    \"\"\"Try to apply the pattern at the start of the string, returning\n    a match object, or None if no match was found.\"\"\"\n    return _compile(pattern, flags).match(string)\n\ndef search(pattern, string, flags=0):\n    \"\"\"Scan through string looking for a match to the pattern, returning\n    a match object, or None if no match was found.\"\"\"\n    return _compile(pattern, flags).search(string)\n\ndef sub(pattern, repl, string, count=0, flags=0):\n    \"\"\"Return the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in string by the\n    replacement repl.  repl can be either a string or a callable;\n    if a string, backslash escapes in it are processed.  If it is\n    a callable, it's passed the match object and must return\n    a replacement string to be used.\"\"\"\n    return _compile(pattern, flags).sub(repl, string, count)\n\ndef subn(pattern, repl, string, count=0, flags=0):\n    \"\"\"Return a 2-tuple containing (new_string, number).\n    new_string is the string obtained by replacing the leftmost\n    non-overlapping occurrences of the pattern in the source\n    string by the replacement repl.  number is the number of\n    substitutions that were made. repl can be either a string or a\n    callable; if a string, backslash escapes in it are processed.\n    If it is a callable, it's passed the match object and must\n    return a replacement string to be used.\"\"\"\n    return _compile(pattern, flags).subn(repl, string, count)\n\ndef split(pattern, string, maxsplit=0, flags=0):\n    \"\"\"Split the source string by the occurrences of the pattern,\n    returning a list containing the resulting substrings.\"\"\"\n    return _compile(pattern, flags).split(string, maxsplit)\n\ndef findall(pattern, string, flags=0):\n    \"\"\"Return a list of all non-overlapping matches in the string.\n\n    If one or more groups are present in the pattern, return a\n    list of groups; this will be a list of tuples if the pattern\n    has more than one group.\n\n    Empty matches are included in the result.\"\"\"\n    return _compile(pattern, flags).findall(string)\n\nif sys.hexversion >= 0x02020000:\n    __all__.append(\"finditer\")\n    def finditer(pattern, string, flags=0):\n        \"\"\"Return an iterator over all non-overlapping matches in the\n        string.  For each match, the iterator returns a match object.\n\n        Empty matches are included in the result.\"\"\"\n        return _compile(pattern, flags).finditer(string)\n\ndef compile(pattern, flags=0):\n    \"Compile a regular expression pattern, returning a pattern object.\"\n    return _compile(pattern, flags)\n\ndef purge():\n    \"Clear the regular expression cache\"\n    _cache.clear()\n    _cache_repl.clear()\n\ndef template(pattern, flags=0):\n    \"Compile a template pattern, returning a pattern object\"\n    return _compile(pattern, flags|T)\n\n_alphanum = frozenset(\n    \"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\")\n\ndef escape(pattern):\n    \"Escape all non-alphanumeric characters in pattern.\"\n    s = list(pattern)\n    alphanum = _alphanum\n    for i, c in enumerate(pattern):\n        if c not in alphanum:\n            if c == \"\\000\":\n                s[i] = \"\\\\000\"\n            else:\n                s[i] = \"\\\\\" + c\n    return pattern[:0].join(s)\n\n# --------------------------------------------------------------------\n# internals\n\n_cache = {}\n_cache_repl = {}\n\n_pattern_type = type(sre_compile.compile(\"\", 0))\n\n_MAXCACHE = 100\n\ndef _compile(*key):\n    # internal: compile pattern\n    pattern, flags = key\n    bypass_cache = flags & DEBUG\n    if not bypass_cache:\n        cachekey = (type(key[0]),) + key\n        try:\n            p, loc = _cache[cachekey]\n            if loc is None or loc == _locale.setlocale(_locale.LC_CTYPE):\n                return p\n        except KeyError:\n            pass\n    if isinstance(pattern, _pattern_type):\n        if flags:\n            raise ValueError('Cannot process flags argument with a compiled pattern')\n        return pattern\n    if not sre_compile.isstring(pattern):\n        raise TypeError, \"first argument must be string or compiled pattern\"\n    try:\n        p = sre_compile.compile(pattern, flags)\n    except error, v:\n        raise error, v # invalid expression\n    if not bypass_cache:\n        if len(_cache) >= _MAXCACHE:\n            _cache.clear()\n        if p.flags & LOCALE:\n            if not _locale:\n                return p\n            loc = _locale.setlocale(_locale.LC_CTYPE)\n        else:\n            loc = None\n        _cache[cachekey] = p, loc\n    return p\n\ndef _compile_repl(*key):\n    # internal: compile replacement pattern\n    p = _cache_repl.get(key)\n    if p is not None:\n        return p\n    repl, pattern = key\n    try:\n        p = sre_parse.parse_template(repl, pattern)\n    except error, v:\n        raise error, v # invalid expression\n    if len(_cache_repl) >= _MAXCACHE:\n        _cache_repl.clear()\n    _cache_repl[key] = p\n    return p\n\ndef _expand(pattern, match, template):\n    # internal: match.expand implementation hook\n    template = sre_parse.parse_template(template, pattern)\n    return sre_parse.expand_template(template, match)\n\ndef _subx(pattern, template):\n    # internal: pattern.sub/subn implementation helper\n    template = _compile_repl(template, pattern)\n    if not template[0] and len(template[1]) == 1:\n        # literal replacement\n        return template[1][0]\n    def filter(match, template=template):\n        return sre_parse.expand_template(template, match)\n    return filter\n\n# register myself for pickling\n\nimport copy_reg\n\ndef _pickle(p):\n    return _compile, (p.pattern, p.flags)\n\ncopy_reg.pickle(_pattern_type, _pickle, _compile)\n\n# --------------------------------------------------------------------\n# experimental stuff (see python-dev discussions for details)\n\nclass Scanner:\n    def __init__(self, lexicon, flags=0):\n        from sre_constants import BRANCH, SUBPATTERN\n        self.lexicon = lexicon\n        # combine phrases into a compound pattern\n        p = []\n        s = sre_parse.Pattern()\n        s.flags = flags\n        for phrase, action in lexicon:\n            p.append(sre_parse.SubPattern(s, [\n                (SUBPATTERN, (len(p)+1, sre_parse.parse(phrase, flags))),\n                ]))\n        s.groups = len(p)+1\n        p = sre_parse.SubPattern(s, [(BRANCH, (None, p))])\n        self.scanner = sre_compile.compile(p)\n    def scan(self, string):\n        result = []\n        append = result.append\n        match = self.scanner.scanner(string).match\n        i = 0\n        while 1:\n            m = match()\n            if not m:\n                break\n            j = m.end()\n            if i == j:\n                break\n            action = self.lexicon[m.lastindex-1][1]\n            if hasattr(action, '__call__'):\n                self.match = m\n                action = action(self, m.group())\n            if action is not None:\n                append(action)\n            i = j\n        return result, string[i:]\n", 
    "repr": "\"\"\"Redo the builtin repr() (representation) but with limits on most sizes.\"\"\"\n\n__all__ = [\"Repr\",\"repr\"]\n\nimport __builtin__\nfrom itertools import islice\n\nclass Repr:\n\n    def __init__(self):\n        self.maxlevel = 6\n        self.maxtuple = 6\n        self.maxlist = 6\n        self.maxarray = 5\n        self.maxdict = 4\n        self.maxset = 6\n        self.maxfrozenset = 6\n        self.maxdeque = 6\n        self.maxstring = 30\n        self.maxlong = 40\n        self.maxother = 20\n\n    def repr(self, x):\n        return self.repr1(x, self.maxlevel)\n\n    def repr1(self, x, level):\n        typename = type(x).__name__\n        if ' ' in typename:\n            parts = typename.split()\n            typename = '_'.join(parts)\n        if hasattr(self, 'repr_' + typename):\n            return getattr(self, 'repr_' + typename)(x, level)\n        else:\n            s = __builtin__.repr(x)\n            if len(s) > self.maxother:\n                i = max(0, (self.maxother-3)//2)\n                j = max(0, self.maxother-3-i)\n                s = s[:i] + '...' + s[len(s)-j:]\n            return s\n\n    def _repr_iterable(self, x, level, left, right, maxiter, trail=''):\n        n = len(x)\n        if level <= 0 and n:\n            s = '...'\n        else:\n            newlevel = level - 1\n            repr1 = self.repr1\n            pieces = [repr1(elem, newlevel) for elem in islice(x, maxiter)]\n            if n > maxiter:  pieces.append('...')\n            s = ', '.join(pieces)\n            if n == 1 and trail:  right = trail + right\n        return '%s%s%s' % (left, s, right)\n\n    def repr_tuple(self, x, level):\n        return self._repr_iterable(x, level, '(', ')', self.maxtuple, ',')\n\n    def repr_list(self, x, level):\n        return self._repr_iterable(x, level, '[', ']', self.maxlist)\n\n    def repr_array(self, x, level):\n        header = \"array('%s', [\" % x.typecode\n        return self._repr_iterable(x, level, header, '])', self.maxarray)\n\n    def repr_set(self, x, level):\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'set([', '])', self.maxset)\n\n    def repr_frozenset(self, x, level):\n        x = _possibly_sorted(x)\n        return self._repr_iterable(x, level, 'frozenset([', '])',\n                                   self.maxfrozenset)\n\n    def repr_deque(self, x, level):\n        return self._repr_iterable(x, level, 'deque([', '])', self.maxdeque)\n\n    def repr_dict(self, x, level):\n        n = len(x)\n        if n == 0: return '{}'\n        if level <= 0: return '{...}'\n        newlevel = level - 1\n        repr1 = self.repr1\n        pieces = []\n        for key in islice(_possibly_sorted(x), self.maxdict):\n            keyrepr = repr1(key, newlevel)\n            valrepr = repr1(x[key], newlevel)\n            pieces.append('%s: %s' % (keyrepr, valrepr))\n        if n > self.maxdict: pieces.append('...')\n        s = ', '.join(pieces)\n        return '{%s}' % (s,)\n\n    def repr_str(self, x, level):\n        s = __builtin__.repr(x[:self.maxstring])\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = __builtin__.repr(x[:i] + x[len(x)-j:])\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n    def repr_long(self, x, level):\n        s = __builtin__.repr(x) # XXX Hope this isn't too slow...\n        if len(s) > self.maxlong:\n            i = max(0, (self.maxlong-3)//2)\n            j = max(0, self.maxlong-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n    def repr_instance(self, x, level):\n        try:\n            s = __builtin__.repr(x)\n            # Bugs in x.__repr__() can cause arbitrary\n            # exceptions -- then make up something\n        except Exception:\n            return '<%s instance at %x>' % (x.__class__.__name__, id(x))\n        if len(s) > self.maxstring:\n            i = max(0, (self.maxstring-3)//2)\n            j = max(0, self.maxstring-3-i)\n            s = s[:i] + '...' + s[len(s)-j:]\n        return s\n\n\ndef _possibly_sorted(x):\n    # Since not all sequences of items can be sorted and comparison\n    # functions may raise arbitrary exceptions, return an unsorted\n    # sequence in that case.\n    try:\n        return sorted(x)\n    except Exception:\n        return list(x)\n\naRepr = Repr()\nrepr = aRepr.repr\n", 
    "rfc822": "\"\"\"RFC 2822 message manipulation.\n\nNote: This is only a very rough sketch of a full RFC-822 parser; in particular\nthe tokenizing of addresses does not adhere to all the quoting rules.\n\nNote: RFC 2822 is a long awaited update to RFC 822.  This module should\nconform to RFC 2822, and is thus mis-named (it's not worth renaming it).  Some\neffort at RFC 2822 updates have been made, but a thorough audit has not been\nperformed.  Consider any RFC 2822 non-conformance to be a bug.\n\n    RFC 2822: http://www.faqs.org/rfcs/rfc2822.html\n    RFC 822 : http://www.faqs.org/rfcs/rfc822.html (obsolete)\n\nDirections for use:\n\nTo create a Message object: first open a file, e.g.:\n\n  fp = open(file, 'r')\n\nYou can use any other legal way of getting an open file object, e.g. use\nsys.stdin or call os.popen().  Then pass the open file object to the Message()\nconstructor:\n\n  m = Message(fp)\n\nThis class can work with any input object that supports a readline method.  If\nthe input object has seek and tell capability, the rewindbody method will\nwork; also illegal lines will be pushed back onto the input stream.  If the\ninput object lacks seek but has an `unread' method that can push back a line\nof input, Message will use that to push back illegal lines.  Thus this class\ncan be used to parse messages coming from a buffered stream.\n\nThe optional `seekable' argument is provided as a workaround for certain stdio\nlibraries in which tell() discards buffered data before discovering that the\nlseek() system call doesn't work.  For maximum portability, you should set the\nseekable argument to zero to prevent that initial \\code{tell} when passing in\nan unseekable object such as a file object created from a socket object.  If\nit is 1 on entry -- which it is by default -- the tell() method of the open\nfile object is called once; if this raises an exception, seekable is reset to\n0.  For other nonzero values of seekable, this test is not made.\n\nTo get the text of a particular header there are several methods:\n\n  str = m.getheader(name)\n  str = m.getrawheader(name)\n\nwhere name is the name of the header, e.g. 'Subject'.  The difference is that\ngetheader() strips the leading and trailing whitespace, while getrawheader()\ndoesn't.  Both functions retain embedded whitespace (including newlines)\nexactly as they are specified in the header, and leave the case of the text\nunchanged.\n\nFor addresses and address lists there are functions\n\n  realname, mailaddress = m.getaddr(name)\n  list = m.getaddrlist(name)\n\nwhere the latter returns a list of (realname, mailaddr) tuples.\n\nThere is also a method\n\n  time = m.getdate(name)\n\nwhich parses a Date-like field and returns a time-compatible tuple,\ni.e. a tuple such as returned by time.localtime() or accepted by\ntime.mktime().\n\nSee the class definition for lower level access methods.\n\nThere are also some utility functions here.\n\"\"\"\n# Cleanup and extensions by Eric S. Raymond <esr@thyrsus.com>\n\nimport time\n\nfrom warnings import warnpy3k\nwarnpy3k(\"in 3.x, rfc822 has been removed in favor of the email package\",\n         stacklevel=2)\n\n__all__ = [\"Message\",\"AddressList\",\"parsedate\",\"parsedate_tz\",\"mktime_tz\"]\n\n_blanklines = ('\\r\\n', '\\n')            # Optimization for islast()\n\n\nclass Message:\n    \"\"\"Represents a single RFC 2822-compliant message.\"\"\"\n\n    def __init__(self, fp, seekable = 1):\n        \"\"\"Initialize the class instance and read the headers.\"\"\"\n        if seekable == 1:\n            # Exercise tell() to make sure it works\n            # (and then assume seek() works, too)\n            try:\n                fp.tell()\n            except (AttributeError, IOError):\n                seekable = 0\n        self.fp = fp\n        self.seekable = seekable\n        self.startofheaders = None\n        self.startofbody = None\n        #\n        if self.seekable:\n            try:\n                self.startofheaders = self.fp.tell()\n            except IOError:\n                self.seekable = 0\n        #\n        self.readheaders()\n        #\n        if self.seekable:\n            try:\n                self.startofbody = self.fp.tell()\n            except IOError:\n                self.seekable = 0\n\n    def rewindbody(self):\n        \"\"\"Rewind the file to the start of the body (if seekable).\"\"\"\n        if not self.seekable:\n            raise IOError, \"unseekable file\"\n        self.fp.seek(self.startofbody)\n\n    def readheaders(self):\n        \"\"\"Read header lines.\n\n        Read header lines up to the entirely blank line that terminates them.\n        The (normally blank) line that ends the headers is skipped, but not\n        included in the returned list.  If a non-header line ends the headers,\n        (which is an error), an attempt is made to backspace over it; it is\n        never included in the returned list.\n\n        The variable self.status is set to the empty string if all went well,\n        otherwise it is an error message.  The variable self.headers is a\n        completely uninterpreted list of lines contained in the header (so\n        printing them will reproduce the header exactly as it appears in the\n        file).\n        \"\"\"\n        self.dict = {}\n        self.unixfrom = ''\n        self.headers = lst = []\n        self.status = ''\n        headerseen = \"\"\n        firstline = 1\n        startofline = unread = tell = None\n        if hasattr(self.fp, 'unread'):\n            unread = self.fp.unread\n        elif self.seekable:\n            tell = self.fp.tell\n        while 1:\n            if tell:\n                try:\n                    startofline = tell()\n                except IOError:\n                    startofline = tell = None\n                    self.seekable = 0\n            line = self.fp.readline()\n            if not line:\n                self.status = 'EOF in headers'\n                break\n            # Skip unix From name time lines\n            if firstline and line.startswith('From '):\n                self.unixfrom = self.unixfrom + line\n                continue\n            firstline = 0\n            if headerseen and line[0] in ' \\t':\n                # It's a continuation line.\n                lst.append(line)\n                x = (self.dict[headerseen] + \"\\n \" + line.strip())\n                self.dict[headerseen] = x.strip()\n                continue\n            elif self.iscomment(line):\n                # It's a comment.  Ignore it.\n                continue\n            elif self.islast(line):\n                # Note! No pushback here!  The delimiter line gets eaten.\n                break\n            headerseen = self.isheader(line)\n            if headerseen:\n                # It's a legal header line, save it.\n                lst.append(line)\n                self.dict[headerseen] = line[len(headerseen)+1:].strip()\n                continue\n            else:\n                # It's not a header line; throw it back and stop here.\n                if not self.dict:\n                    self.status = 'No headers'\n                else:\n                    self.status = 'Non-header line where header expected'\n                # Try to undo the read.\n                if unread:\n                    unread(line)\n                elif tell:\n                    self.fp.seek(startofline)\n                else:\n                    self.status = self.status + '; bad seek'\n                break\n\n    def isheader(self, line):\n        \"\"\"Determine whether a given line is a legal header.\n\n        This method should return the header name, suitably canonicalized.\n        You may override this method in order to use Message parsing on tagged\n        data in RFC 2822-like formats with special header formats.\n        \"\"\"\n        i = line.find(':')\n        if i > 0:\n            return line[:i].lower()\n        return None\n\n    def islast(self, line):\n        \"\"\"Determine whether a line is a legal end of RFC 2822 headers.\n\n        You may override this method if your application wants to bend the\n        rules, e.g. to strip trailing whitespace, or to recognize MH template\n        separators ('--------').  For convenience (e.g. for code reading from\n        sockets) a line consisting of \\\\r\\\\n also matches.\n        \"\"\"\n        return line in _blanklines\n\n    def iscomment(self, line):\n        \"\"\"Determine whether a line should be skipped entirely.\n\n        You may override this method in order to use Message parsing on tagged\n        data in RFC 2822-like formats that support embedded comments or\n        free-text data.\n        \"\"\"\n        return False\n\n    def getallmatchingheaders(self, name):\n        \"\"\"Find all header lines matching a given header name.\n\n        Look through the list of headers and find all lines matching a given\n        header name (and their continuation lines).  A list of the lines is\n        returned, without interpretation.  If the header does not occur, an\n        empty list is returned.  If the header occurs multiple times, all\n        occurrences are returned.  Case is not important in the header name.\n        \"\"\"\n        name = name.lower() + ':'\n        n = len(name)\n        lst = []\n        hit = 0\n        for line in self.headers:\n            if line[:n].lower() == name:\n                hit = 1\n            elif not line[:1].isspace():\n                hit = 0\n            if hit:\n                lst.append(line)\n        return lst\n\n    def getfirstmatchingheader(self, name):\n        \"\"\"Get the first header line matching name.\n\n        This is similar to getallmatchingheaders, but it returns only the\n        first matching header (and its continuation lines).\n        \"\"\"\n        name = name.lower() + ':'\n        n = len(name)\n        lst = []\n        hit = 0\n        for line in self.headers:\n            if hit:\n                if not line[:1].isspace():\n                    break\n            elif line[:n].lower() == name:\n                hit = 1\n            if hit:\n                lst.append(line)\n        return lst\n\n    def getrawheader(self, name):\n        \"\"\"A higher-level interface to getfirstmatchingheader().\n\n        Return a string containing the literal text of the header but with the\n        keyword stripped.  All leading, trailing and embedded whitespace is\n        kept in the string, however.  Return None if the header does not\n        occur.\n        \"\"\"\n\n        lst = self.getfirstmatchingheader(name)\n        if not lst:\n            return None\n        lst[0] = lst[0][len(name) + 1:]\n        return ''.join(lst)\n\n    def getheader(self, name, default=None):\n        \"\"\"Get the header value for a name.\n\n        This is the normal interface: it returns a stripped version of the\n        header value for a given header name, or None if it doesn't exist.\n        This uses the dictionary version which finds the *last* such header.\n        \"\"\"\n        return self.dict.get(name.lower(), default)\n    get = getheader\n\n    def getheaders(self, name):\n        \"\"\"Get all values for a header.\n\n        This returns a list of values for headers given more than once; each\n        value in the result list is stripped in the same way as the result of\n        getheader().  If the header is not given, return an empty list.\n        \"\"\"\n        result = []\n        current = ''\n        have_header = 0\n        for s in self.getallmatchingheaders(name):\n            if s[0].isspace():\n                if current:\n                    current = \"%s\\n %s\" % (current, s.strip())\n                else:\n                    current = s.strip()\n            else:\n                if have_header:\n                    result.append(current)\n                current = s[s.find(\":\") + 1:].strip()\n                have_header = 1\n        if have_header:\n            result.append(current)\n        return result\n\n    def getaddr(self, name):\n        \"\"\"Get a single address from a header, as a tuple.\n\n        An example return value:\n        ('Guido van Rossum', 'guido@cwi.nl')\n        \"\"\"\n        # New, by Ben Escoto\n        alist = self.getaddrlist(name)\n        if alist:\n            return alist[0]\n        else:\n            return (None, None)\n\n    def getaddrlist(self, name):\n        \"\"\"Get a list of addresses from a header.\n\n        Retrieves a list of addresses from a header, where each address is a\n        tuple as returned by getaddr().  Scans all named headers, so it works\n        properly with multiple To: or Cc: headers for example.\n        \"\"\"\n        raw = []\n        for h in self.getallmatchingheaders(name):\n            if h[0] in ' \\t':\n                raw.append(h)\n            else:\n                if raw:\n                    raw.append(', ')\n                i = h.find(':')\n                if i > 0:\n                    addr = h[i+1:]\n                raw.append(addr)\n        alladdrs = ''.join(raw)\n        a = AddressList(alladdrs)\n        return a.addresslist\n\n    def getdate(self, name):\n        \"\"\"Retrieve a date field from a header.\n\n        Retrieves a date field from the named header, returning a tuple\n        compatible with time.mktime().\n        \"\"\"\n        try:\n            data = self[name]\n        except KeyError:\n            return None\n        return parsedate(data)\n\n    def getdate_tz(self, name):\n        \"\"\"Retrieve a date field from a header as a 10-tuple.\n\n        The first 9 elements make up a tuple compatible with time.mktime(),\n        and the 10th is the offset of the poster's time zone from GMT/UTC.\n        \"\"\"\n        try:\n            data = self[name]\n        except KeyError:\n            return None\n        return parsedate_tz(data)\n\n\n    # Access as a dictionary (only finds *last* header of each type):\n\n    def __len__(self):\n        \"\"\"Get the number of headers in a message.\"\"\"\n        return len(self.dict)\n\n    def __getitem__(self, name):\n        \"\"\"Get a specific header, as from a dictionary.\"\"\"\n        return self.dict[name.lower()]\n\n    def __setitem__(self, name, value):\n        \"\"\"Set the value of a header.\n\n        Note: This is not a perfect inversion of __getitem__, because any\n        changed headers get stuck at the end of the raw-headers list rather\n        than where the altered header was.\n        \"\"\"\n        del self[name] # Won't fail if it doesn't exist\n        self.dict[name.lower()] = value\n        text = name + \": \" + value\n        for line in text.split(\"\\n\"):\n            self.headers.append(line + \"\\n\")\n\n    def __delitem__(self, name):\n        \"\"\"Delete all occurrences of a specific header, if it is present.\"\"\"\n        name = name.lower()\n        if not name in self.dict:\n            return\n        del self.dict[name]\n        name = name + ':'\n        n = len(name)\n        lst = []\n        hit = 0\n        for i in range(len(self.headers)):\n            line = self.headers[i]\n            if line[:n].lower() == name:\n                hit = 1\n            elif not line[:1].isspace():\n                hit = 0\n            if hit:\n                lst.append(i)\n        for i in reversed(lst):\n            del self.headers[i]\n\n    def setdefault(self, name, default=\"\"):\n        lowername = name.lower()\n        if lowername in self.dict:\n            return self.dict[lowername]\n        else:\n            text = name + \": \" + default\n            for line in text.split(\"\\n\"):\n                self.headers.append(line + \"\\n\")\n            self.dict[lowername] = default\n            return default\n\n    def has_key(self, name):\n        \"\"\"Determine whether a message contains the named header.\"\"\"\n        return name.lower() in self.dict\n\n    def __contains__(self, name):\n        \"\"\"Determine whether a message contains the named header.\"\"\"\n        return name.lower() in self.dict\n\n    def __iter__(self):\n        return iter(self.dict)\n\n    def keys(self):\n        \"\"\"Get all of a message's header field names.\"\"\"\n        return self.dict.keys()\n\n    def values(self):\n        \"\"\"Get all of a message's header field values.\"\"\"\n        return self.dict.values()\n\n    def items(self):\n        \"\"\"Get all of a message's headers.\n\n        Returns a list of name, value tuples.\n        \"\"\"\n        return self.dict.items()\n\n    def __str__(self):\n        return ''.join(self.headers)\n\n\n# Utility functions\n# -----------------\n\n# XXX Should fix unquote() and quote() to be really conformant.\n# XXX The inverses of the parse functions may also be useful.\n\n\ndef unquote(s):\n    \"\"\"Remove quotes from a string.\"\"\"\n    if len(s) > 1:\n        if s.startswith('\"') and s.endswith('\"'):\n            return s[1:-1].replace('\\\\\\\\', '\\\\').replace('\\\\\"', '\"')\n        if s.startswith('<') and s.endswith('>'):\n            return s[1:-1]\n    return s\n\n\ndef quote(s):\n    \"\"\"Add quotes around a string.\"\"\"\n    return s.replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')\n\n\ndef parseaddr(address):\n    \"\"\"Parse an address into a (realname, mailaddr) tuple.\"\"\"\n    a = AddressList(address)\n    lst = a.addresslist\n    if not lst:\n        return (None, None)\n    return lst[0]\n\n\nclass AddrlistClass:\n    \"\"\"Address parser class by Ben Escoto.\n\n    To understand what this class does, it helps to have a copy of\n    RFC 2822 in front of you.\n\n    http://www.faqs.org/rfcs/rfc2822.html\n\n    Note: this class interface is deprecated and may be removed in the future.\n    Use rfc822.AddressList instead.\n    \"\"\"\n\n    def __init__(self, field):\n        \"\"\"Initialize a new instance.\n\n        `field' is an unparsed address header field, containing one or more\n        addresses.\n        \"\"\"\n        self.specials = '()<>@,:;.\\\"[]'\n        self.pos = 0\n        self.LWS = ' \\t'\n        self.CR = '\\r\\n'\n        self.atomends = self.specials + self.LWS + self.CR\n        # Note that RFC 2822 now specifies `.' as obs-phrase, meaning that it\n        # is obsolete syntax.  RFC 2822 requires that we recognize obsolete\n        # syntax, so allow dots in phrases.\n        self.phraseends = self.atomends.replace('.', '')\n        self.field = field\n        self.commentlist = []\n\n    def gotonext(self):\n        \"\"\"Parse up to the start of the next address.\"\"\"\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS + '\\n\\r':\n                self.pos = self.pos + 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            else: break\n\n    def getaddrlist(self):\n        \"\"\"Parse all addresses.\n\n        Returns a list containing all of the addresses.\n        \"\"\"\n        result = []\n        ad = self.getaddress()\n        while ad:\n            result += ad\n            ad = self.getaddress()\n        return result\n\n    def getaddress(self):\n        \"\"\"Parse the next address.\"\"\"\n        self.commentlist = []\n        self.gotonext()\n\n        oldpos = self.pos\n        oldcl = self.commentlist\n        plist = self.getphraselist()\n\n        self.gotonext()\n        returnlist = []\n\n        if self.pos >= len(self.field):\n            # Bad email address technically, no domain.\n            if plist:\n                returnlist = [(' '.join(self.commentlist), plist[0])]\n\n        elif self.field[self.pos] in '.@':\n            # email address is just an addrspec\n            # this isn't very efficient since we start over\n            self.pos = oldpos\n            self.commentlist = oldcl\n            addrspec = self.getaddrspec()\n            returnlist = [(' '.join(self.commentlist), addrspec)]\n\n        elif self.field[self.pos] == ':':\n            # address is a group\n            returnlist = []\n\n            fieldlen = len(self.field)\n            self.pos += 1\n            while self.pos < len(self.field):\n                self.gotonext()\n                if self.pos < fieldlen and self.field[self.pos] == ';':\n                    self.pos += 1\n                    break\n                returnlist = returnlist + self.getaddress()\n\n        elif self.field[self.pos] == '<':\n            # Address is a phrase then a route addr\n            routeaddr = self.getrouteaddr()\n\n            if self.commentlist:\n                returnlist = [(' '.join(plist) + ' (' + \\\n                         ' '.join(self.commentlist) + ')', routeaddr)]\n            else: returnlist = [(' '.join(plist), routeaddr)]\n\n        else:\n            if plist:\n                returnlist = [(' '.join(self.commentlist), plist[0])]\n            elif self.field[self.pos] in self.specials:\n                self.pos += 1\n\n        self.gotonext()\n        if self.pos < len(self.field) and self.field[self.pos] == ',':\n            self.pos += 1\n        return returnlist\n\n    def getrouteaddr(self):\n        \"\"\"Parse a route address (Return-path value).\n\n        This method just skips all the route stuff and returns the addrspec.\n        \"\"\"\n        if self.field[self.pos] != '<':\n            return\n\n        expectroute = 0\n        self.pos += 1\n        self.gotonext()\n        adlist = \"\"\n        while self.pos < len(self.field):\n            if expectroute:\n                self.getdomain()\n                expectroute = 0\n            elif self.field[self.pos] == '>':\n                self.pos += 1\n                break\n            elif self.field[self.pos] == '@':\n                self.pos += 1\n                expectroute = 1\n            elif self.field[self.pos] == ':':\n                self.pos += 1\n            else:\n                adlist = self.getaddrspec()\n                self.pos += 1\n                break\n            self.gotonext()\n\n        return adlist\n\n    def getaddrspec(self):\n        \"\"\"Parse an RFC 2822 addr-spec.\"\"\"\n        aslist = []\n\n        self.gotonext()\n        while self.pos < len(self.field):\n            if self.field[self.pos] == '.':\n                aslist.append('.')\n                self.pos += 1\n            elif self.field[self.pos] == '\"':\n                aslist.append('\"%s\"' % self.getquote())\n            elif self.field[self.pos] in self.atomends:\n                break\n            else: aslist.append(self.getatom())\n            self.gotonext()\n\n        if self.pos >= len(self.field) or self.field[self.pos] != '@':\n            return ''.join(aslist)\n\n        aslist.append('@')\n        self.pos += 1\n        self.gotonext()\n        return ''.join(aslist) + self.getdomain()\n\n    def getdomain(self):\n        \"\"\"Get the complete domain name from an address.\"\"\"\n        sdlist = []\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS:\n                self.pos += 1\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] == '[':\n                sdlist.append(self.getdomainliteral())\n            elif self.field[self.pos] == '.':\n                self.pos += 1\n                sdlist.append('.')\n            elif self.field[self.pos] in self.atomends:\n                break\n            else: sdlist.append(self.getatom())\n        return ''.join(sdlist)\n\n    def getdelimited(self, beginchar, endchars, allowcomments = 1):\n        \"\"\"Parse a header fragment delimited by special characters.\n\n        `beginchar' is the start character for the fragment.  If self is not\n        looking at an instance of `beginchar' then getdelimited returns the\n        empty string.\n\n        `endchars' is a sequence of allowable end-delimiting characters.\n        Parsing stops when one of these is encountered.\n\n        If `allowcomments' is non-zero, embedded RFC 2822 comments are allowed\n        within the parsed fragment.\n        \"\"\"\n        if self.field[self.pos] != beginchar:\n            return ''\n\n        slist = ['']\n        quote = 0\n        self.pos += 1\n        while self.pos < len(self.field):\n            if quote == 1:\n                slist.append(self.field[self.pos])\n                quote = 0\n            elif self.field[self.pos] in endchars:\n                self.pos += 1\n                break\n            elif allowcomments and self.field[self.pos] == '(':\n                slist.append(self.getcomment())\n                continue        # have already advanced pos from getcomment\n            elif self.field[self.pos] == '\\\\':\n                quote = 1\n            else:\n                slist.append(self.field[self.pos])\n            self.pos += 1\n\n        return ''.join(slist)\n\n    def getquote(self):\n        \"\"\"Get a quote-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('\"', '\"\\r', 0)\n\n    def getcomment(self):\n        \"\"\"Get a parenthesis-delimited fragment from self's field.\"\"\"\n        return self.getdelimited('(', ')\\r', 1)\n\n    def getdomainliteral(self):\n        \"\"\"Parse an RFC 2822 domain-literal.\"\"\"\n        return '[%s]' % self.getdelimited('[', ']\\r', 0)\n\n    def getatom(self, atomends=None):\n        \"\"\"Parse an RFC 2822 atom.\n\n        Optional atomends specifies a different set of end token delimiters\n        (the default is to use self.atomends).  This is used e.g. in\n        getphraselist() since phrase endings must not include the `.' (which\n        is legal in phrases).\"\"\"\n        atomlist = ['']\n        if atomends is None:\n            atomends = self.atomends\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in atomends:\n                break\n            else: atomlist.append(self.field[self.pos])\n            self.pos += 1\n\n        return ''.join(atomlist)\n\n    def getphraselist(self):\n        \"\"\"Parse a sequence of RFC 2822 phrases.\n\n        A phrase is a sequence of words, which are in turn either RFC 2822\n        atoms or quoted-strings.  Phrases are canonicalized by squeezing all\n        runs of continuous whitespace into one space.\n        \"\"\"\n        plist = []\n\n        while self.pos < len(self.field):\n            if self.field[self.pos] in self.LWS:\n                self.pos += 1\n            elif self.field[self.pos] == '\"':\n                plist.append(self.getquote())\n            elif self.field[self.pos] == '(':\n                self.commentlist.append(self.getcomment())\n            elif self.field[self.pos] in self.phraseends:\n                break\n            else:\n                plist.append(self.getatom(self.phraseends))\n\n        return plist\n\nclass AddressList(AddrlistClass):\n    \"\"\"An AddressList encapsulates a list of parsed RFC 2822 addresses.\"\"\"\n    def __init__(self, field):\n        AddrlistClass.__init__(self, field)\n        if field:\n            self.addresslist = self.getaddrlist()\n        else:\n            self.addresslist = []\n\n    def __len__(self):\n        return len(self.addresslist)\n\n    def __str__(self):\n        return \", \".join(map(dump_address_pair, self.addresslist))\n\n    def __add__(self, other):\n        # Set union\n        newaddr = AddressList(None)\n        newaddr.addresslist = self.addresslist[:]\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __iadd__(self, other):\n        # Set union, in-place\n        for x in other.addresslist:\n            if not x in self.addresslist:\n                self.addresslist.append(x)\n        return self\n\n    def __sub__(self, other):\n        # Set difference\n        newaddr = AddressList(None)\n        for x in self.addresslist:\n            if not x in other.addresslist:\n                newaddr.addresslist.append(x)\n        return newaddr\n\n    def __isub__(self, other):\n        # Set difference, in-place\n        for x in other.addresslist:\n            if x in self.addresslist:\n                self.addresslist.remove(x)\n        return self\n\n    def __getitem__(self, index):\n        # Make indexing, slices, and 'in' work\n        return self.addresslist[index]\n\ndef dump_address_pair(pair):\n    \"\"\"Dump a (name, address) pair in a canonicalized form.\"\"\"\n    if pair[0]:\n        return '\"' + pair[0] + '\" <' + pair[1] + '>'\n    else:\n        return pair[1]\n\n# Parse a date field\n\n_monthnames = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul',\n               'aug', 'sep', 'oct', 'nov', 'dec',\n               'january', 'february', 'march', 'april', 'may', 'june', 'july',\n               'august', 'september', 'october', 'november', 'december']\n_daynames = ['mon', 'tue', 'wed', 'thu', 'fri', 'sat', 'sun']\n\n# The timezone table does not include the military time zones defined\n# in RFC822, other than Z.  According to RFC1123, the description in\n# RFC822 gets the signs wrong, so we can't rely on any such time\n# zones.  RFC1123 recommends that numeric timezone indicators be used\n# instead of timezone names.\n\n_timezones = {'UT':0, 'UTC':0, 'GMT':0, 'Z':0,\n              'AST': -400, 'ADT': -300,  # Atlantic (used in Canada)\n              'EST': -500, 'EDT': -400,  # Eastern\n              'CST': -600, 'CDT': -500,  # Central\n              'MST': -700, 'MDT': -600,  # Mountain\n              'PST': -800, 'PDT': -700   # Pacific\n              }\n\n\ndef parsedate_tz(data):\n    \"\"\"Convert a date string to a time tuple.\n\n    Accounts for military timezones.\n    \"\"\"\n    if not data:\n        return None\n    data = data.split()\n    if data[0][-1] in (',', '.') or data[0].lower() in _daynames:\n        # There's a dayname here. Skip it\n        del data[0]\n    else:\n        # no space after the \"weekday,\"?\n        i = data[0].rfind(',')\n        if i >= 0:\n            data[0] = data[0][i+1:]\n    if len(data) == 3: # RFC 850 date, deprecated\n        stuff = data[0].split('-')\n        if len(stuff) == 3:\n            data = stuff + data[1:]\n    if len(data) == 4:\n        s = data[3]\n        i = s.find('+')\n        if i > 0:\n            data[3:] = [s[:i], s[i+1:]]\n        else:\n            data.append('') # Dummy tz\n    if len(data) < 5:\n        return None\n    data = data[:5]\n    [dd, mm, yy, tm, tz] = data\n    mm = mm.lower()\n    if not mm in _monthnames:\n        dd, mm = mm, dd.lower()\n        if not mm in _monthnames:\n            return None\n    mm = _monthnames.index(mm)+1\n    if mm > 12: mm = mm - 12\n    if dd[-1] == ',':\n        dd = dd[:-1]\n    i = yy.find(':')\n    if i > 0:\n        yy, tm = tm, yy\n    if yy[-1] == ',':\n        yy = yy[:-1]\n    if not yy[0].isdigit():\n        yy, tz = tz, yy\n    if tm[-1] == ',':\n        tm = tm[:-1]\n    tm = tm.split(':')\n    if len(tm) == 2:\n        [thh, tmm] = tm\n        tss = '0'\n    elif len(tm) == 3:\n        [thh, tmm, tss] = tm\n    else:\n        return None\n    try:\n        yy = int(yy)\n        dd = int(dd)\n        thh = int(thh)\n        tmm = int(tmm)\n        tss = int(tss)\n    except ValueError:\n        return None\n    tzoffset = None\n    tz = tz.upper()\n    if tz in _timezones:\n        tzoffset = _timezones[tz]\n    else:\n        try:\n            tzoffset = int(tz)\n        except ValueError:\n            pass\n    # Convert a timezone offset into seconds ; -0500 -> -18000\n    if tzoffset:\n        if tzoffset < 0:\n            tzsign = -1\n            tzoffset = -tzoffset\n        else:\n            tzsign = 1\n        tzoffset = tzsign * ( (tzoffset//100)*3600 + (tzoffset % 100)*60)\n    return (yy, mm, dd, thh, tmm, tss, 0, 1, 0, tzoffset)\n\n\ndef parsedate(data):\n    \"\"\"Convert a time string to a time tuple.\"\"\"\n    t = parsedate_tz(data)\n    if t is None:\n        return t\n    return t[:9]\n\n\ndef mktime_tz(data):\n    \"\"\"Turn a 10-tuple as returned by parsedate_tz() into a UTC timestamp.\"\"\"\n    if data[9] is None:\n        # No zone info, so localtime is better assumption than GMT\n        return time.mktime(data[:8] + (-1,))\n    else:\n        t = time.mktime(data[:8] + (0,))\n        return t - data[9] - time.timezone\n\ndef formatdate(timeval=None):\n    \"\"\"Returns time format preferred for Internet standards.\n\n    Sun, 06 Nov 1994 08:49:37 GMT  ; RFC 822, updated by RFC 1123\n\n    According to RFC 1123, day and month names must always be in\n    English.  If not for that, this code could use strftime().  It\n    can't because strftime() honors the locale and could generated\n    non-English names.\n    \"\"\"\n    if timeval is None:\n        timeval = time.time()\n    timeval = time.gmtime(timeval)\n    return \"%s, %02d %s %04d %02d:%02d:%02d GMT\" % (\n            (\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")[timeval[6]],\n            timeval[2],\n            (\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n             \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")[timeval[1]-1],\n                                timeval[0], timeval[3], timeval[4], timeval[5])\n\n\n# When used as script, run a small test program.\n# The first command line argument must be a filename containing one\n# message in RFC-822 format.\n\nif __name__ == '__main__':\n    import sys, os\n    file = os.path.join(os.environ['HOME'], 'Mail/inbox/1')\n    if sys.argv[1:]: file = sys.argv[1]\n    f = open(file, 'r')\n    m = Message(f)\n    print 'From:', m.getaddr('from')\n    print 'To:', m.getaddrlist('to')\n    print 'Subject:', m.getheader('subject')\n    print 'Date:', m.getheader('date')\n    date = m.getdate_tz('date')\n    tz = date[-1]\n    date = time.localtime(mktime_tz(date))\n    if date:\n        print 'ParsedDate:', time.asctime(date),\n        hhmmss = tz\n        hhmm, ss = divmod(hhmmss, 60)\n        hh, mm = divmod(hhmm, 60)\n        print \"%+03d%02d\" % (hh, mm),\n        if ss: print \".%02d\" % ss,\n        print\n    else:\n        print 'ParsedDate:', None\n    m.rewindbody()\n    n = 0\n    while f.readline():\n        n += 1\n    print 'Lines:', n\n    print '-'*70\n    print 'len =', len(m)\n    if 'Date' in m: print 'Date =', m['Date']\n    if 'X-Nonsense' in m: pass\n    print 'keys =', m.keys()\n    print 'values =', m.values()\n    print 'items =', m.items()\n", 
    "select": "import time\nimport _socket\nimport os\n\n\ndef select(readlist, writelist, xlist, timeout=None):\n    readlist=list(readlist)\n    writelist=list(writelist)\n    xlist=list(xlist)\n    endtime=time.time()+timeout\n    read_out=[]\n    write_out=[]\n    xlist_out=[]\n    while True:\n        for i in (readlist):\n            if isinstance(i, int):\n                fno=i\n            else:\n                fno=i.fileno()\n            if fno==0:\n                if (hasattr(os.fileNumbers[fno], 'buffer') and hasattr(os.fileNumbers[fno].buffer,'len')):\n                    if os.fileNumbers[fno].buffer.len>0:\n                        read_out.append(fno)\n                        continue\n            if fno >= len(os.fileNumbers):\n                continue\n            so=os.fileNumbers[fno]\n            if isinstance(so, _socket._fileobject):\n                if so.pullFromJS():\n                    read_out.append(i)\n            else:\n                if getattr(so, 'buffer', None) and so.buffer.buffer:\n                    read_out.append(i)\n        for i in (xlist):\n            if isinstance(i, int):\n                fno=i\n            else:\n                fno=i.fileno()\n            if fno<3:\n                continue\n            if fno >= len(os.fileNumbers):\n                continue\n            so=os.fileNumbers[fno]\n            if isinstance(so, _socket._socket):\n                if so._sock.closed:\n                    xlist_out.append(i)\n        for i in (writelist):\n            if isinstance(i, int):\n                fno=i\n            else:\n                fno=i.fileno()\n            if fno >= len(os.fileNumbers):\n                continue\n            so=os.fileNumbers[fno]\n            if isinstance(so, _socket._fileobject):\n                if not so._sock.ready:\n                    continue\n            write_out.append(i)\n        if read_out or write_out or xlist_out or time.time() > timeout:\n            return read_out, write_out, xlist_out\n        time.sleep(0.1)\n        \n\nclass error(Exception): pass\n", 
    "shlex": "# -*- coding: utf-8 -*-\n\"\"\"A lexical analyzer class for simple shell-like syntaxes.\"\"\"\n\n# Module and documentation by Eric S. Raymond, 21 Dec 1998\n# Input stacking and error message cleanup added by ESR, March 2000\n# push_source() and pop_source() made explicit by ESR, January 2001.\n# Posix compliance, split(), string arguments, and\n# iterator interface by Gustavo Niemeyer, April 2003.\n\nimport os.path\nimport sys\nfrom collections import deque\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\n__all__ = [\"shlex\", \"split\"]\n\nclass shlex:\n    \"A lexical analyzer class for simple shell-like syntaxes.\"\n    def __init__(self, instream=None, infile=None, posix=False):\n        if isinstance(instream, basestring):\n            instream = StringIO(instream)\n        if instream is not None:\n            self.instream = instream\n            self.infile = infile\n        else:\n            self.instream = sys.stdin\n            self.infile = None\n        self.posix = posix\n        if posix:\n            self.eof = None\n        else:\n            self.eof = ''\n        self.commenters = '#'\n        self.wordchars = ('abcdfeghijklmnopqrstuvwxyz'\n                          'ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789_')\n        if self.posix:\n            self.wordchars += ('\u00df\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e6\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f1\u00f2\u00f3\u00f4\u00f5\u00f6\u00f8\u00f9\u00fa\u00fb\u00fc\u00fd\u00fe\u00ff'\n                               '\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00c6\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d0\u00d1\u00d2\u00d3\u00d4\u00d5\u00d6\u00d8\u00d9\u00da\u00db\u00dc\u00dd\u00de')\n        self.whitespace = ' \\t\\r\\n'\n        self.whitespace_split = False\n        self.quotes = '\\'\"'\n        self.escape = '\\\\'\n        self.escapedquotes = '\"'\n        self.state = ' '\n        self.pushback = deque()\n        self.lineno = 1\n        self.debug = 0\n        self.token = ''\n        self.filestack = deque()\n        self.source = None\n        if self.debug:\n            print 'shlex: reading from %s, line %d' \\\n                  % (self.instream, self.lineno)\n\n    def push_token(self, tok):\n        \"Push a token onto the stack popped by the get_token method\"\n        if self.debug >= 1:\n            print \"shlex: pushing token \" + repr(tok)\n        self.pushback.appendleft(tok)\n\n    def push_source(self, newstream, newfile=None):\n        \"Push an input source onto the lexer's input source stack.\"\n        if isinstance(newstream, basestring):\n            newstream = StringIO(newstream)\n        self.filestack.appendleft((self.infile, self.instream, self.lineno))\n        self.infile = newfile\n        self.instream = newstream\n        self.lineno = 1\n        if self.debug:\n            if newfile is not None:\n                print 'shlex: pushing to file %s' % (self.infile,)\n            else:\n                print 'shlex: pushing to stream %s' % (self.instream,)\n\n    def pop_source(self):\n        \"Pop the input source stack.\"\n        self.instream.close()\n        (self.infile, self.instream, self.lineno) = self.filestack.popleft()\n        if self.debug:\n            print 'shlex: popping to %s, line %d' \\\n                  % (self.instream, self.lineno)\n        self.state = ' '\n\n    def get_token(self):\n        \"Get a token from the input stream (or from stack if it's nonempty)\"\n        if self.pushback:\n            tok = self.pushback.popleft()\n            if self.debug >= 1:\n                print \"shlex: popping token \" + repr(tok)\n            return tok\n        # No pushback.  Get a token.\n        raw = self.read_token()\n        # Handle inclusions\n        if self.source is not None:\n            while raw == self.source:\n                spec = self.sourcehook(self.read_token())\n                if spec:\n                    (newfile, newstream) = spec\n                    self.push_source(newstream, newfile)\n                raw = self.get_token()\n        # Maybe we got EOF instead?\n        while raw == self.eof:\n            if not self.filestack:\n                return self.eof\n            else:\n                self.pop_source()\n                raw = self.get_token()\n        # Neither inclusion nor EOF\n        if self.debug >= 1:\n            if raw != self.eof:\n                print \"shlex: token=\" + repr(raw)\n            else:\n                print \"shlex: token=EOF\"\n        return raw\n\n    def read_token(self):\n        quoted = False\n        escapedstate = ' '\n        while True:\n            nextchar = self.instream.read(1)\n            if nextchar == '\\n':\n                self.lineno = self.lineno + 1\n            if self.debug >= 3:\n                print \"shlex: in state\", repr(self.state), \\\n                      \"I see character:\", repr(nextchar)\n            if self.state is None:\n                self.token = ''        # past end of file\n                break\n            elif self.state == ' ':\n                if not nextchar:\n                    self.state = None  # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in whitespace state\"\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars:\n                    self.token = nextchar\n                    self.state = 'a'\n                elif nextchar in self.quotes:\n                    if not self.posix:\n                        self.token = nextchar\n                    self.state = nextchar\n                elif self.whitespace_split:\n                    self.token = nextchar\n                    self.state = 'a'\n                else:\n                    self.token = nextchar\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n            elif self.state in self.quotes:\n                quoted = True\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in quotes state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No closing quotation\"\n                if nextchar == self.state:\n                    if not self.posix:\n                        self.token = self.token + nextchar\n                        self.state = ' '\n                        break\n                    else:\n                        self.state = 'a'\n                elif self.posix and nextchar in self.escape and \\\n                     self.state in self.escapedquotes:\n                    escapedstate = self.state\n                    self.state = nextchar\n                else:\n                    self.token = self.token + nextchar\n            elif self.state in self.escape:\n                if not nextchar:      # end of file\n                    if self.debug >= 2:\n                        print \"shlex: I see EOF in escape state\"\n                    # XXX what error should be raised here?\n                    raise ValueError, \"No escaped character\"\n                # In posix shells, only the quote itself or the escape\n                # character may be escaped within quotes.\n                if escapedstate in self.quotes and \\\n                   nextchar != self.state and nextchar != escapedstate:\n                    self.token = self.token + self.state\n                self.token = self.token + nextchar\n                self.state = escapedstate\n            elif self.state == 'a':\n                if not nextchar:\n                    self.state = None   # end of file\n                    break\n                elif nextchar in self.whitespace:\n                    if self.debug >= 2:\n                        print \"shlex: I see whitespace in word state\"\n                    self.state = ' '\n                    if self.token or (self.posix and quoted):\n                        break   # emit current token\n                    else:\n                        continue\n                elif nextchar in self.commenters:\n                    self.instream.readline()\n                    self.lineno = self.lineno + 1\n                    if self.posix:\n                        self.state = ' '\n                        if self.token or (self.posix and quoted):\n                            break   # emit current token\n                        else:\n                            continue\n                elif self.posix and nextchar in self.quotes:\n                    self.state = nextchar\n                elif self.posix and nextchar in self.escape:\n                    escapedstate = 'a'\n                    self.state = nextchar\n                elif nextchar in self.wordchars or nextchar in self.quotes \\\n                    or self.whitespace_split:\n                    self.token = self.token + nextchar\n                else:\n                    self.pushback.appendleft(nextchar)\n                    if self.debug >= 2:\n                        print \"shlex: I see punctuation in word state\"\n                    self.state = ' '\n                    if self.token:\n                        break   # emit current token\n                    else:\n                        continue\n        result = self.token\n        self.token = ''\n        if self.posix and not quoted and result == '':\n            result = None\n        if self.debug > 1:\n            if result:\n                print \"shlex: raw token=\" + repr(result)\n            else:\n                print \"shlex: raw token=EOF\"\n        return result\n\n    def sourcehook(self, newfile):\n        \"Hook called on a filename to be sourced.\"\n        if newfile[0] == '\"':\n            newfile = newfile[1:-1]\n        # This implements cpp-like semantics for relative-path inclusion.\n        if isinstance(self.infile, basestring) and not os.path.isabs(newfile):\n            newfile = os.path.join(os.path.dirname(self.infile), newfile)\n        return (newfile, open(newfile, \"r\"))\n\n    def error_leader(self, infile=None, lineno=None):\n        \"Emit a C-compiler-like, Emacs-friendly error-message leader.\"\n        if infile is None:\n            infile = self.infile\n        if lineno is None:\n            lineno = self.lineno\n        return \"\\\"%s\\\", line %d: \" % (infile, lineno)\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        token = self.get_token()\n        if token == self.eof:\n            raise StopIteration\n        return token\n\ndef split(s, comments=False, posix=True):\n    lex = shlex(s, posix=posix)\n    lex.whitespace_split = True\n    if not comments:\n        lex.commenters = ''\n    return list(lex)\n\nif __name__ == '__main__':\n    if len(sys.argv) == 1:\n        lexer = shlex()\n    else:\n        file = sys.argv[1]\n        lexer = shlex(open(file), file)\n    while 1:\n        tt = lexer.get_token()\n        if tt:\n            print \"Token: \" + repr(tt)\n        else:\n            break\n", 
    "shutil": "\"\"\"Utility functions for copying and archiving files and directory trees.\n\nXXX The functions here don't copy the resource fork or other metadata on Mac.\n\n\"\"\"\n\nimport os\nimport sys\nimport stat\nfrom os.path import abspath\nimport fnmatch\nimport collections\nimport errno\n\ntry:\n    from pwd import getpwnam\nexcept ImportError:\n    getpwnam = None\n\ntry:\n    from grp import getgrnam\nexcept ImportError:\n    getgrnam = None\n\n__all__ = [\"copyfileobj\", \"copyfile\", \"copymode\", \"copystat\", \"copy\", \"copy2\",\n           \"copytree\", \"move\", \"rmtree\", \"Error\", \"SpecialFileError\",\n           \"ExecError\", \"make_archive\", \"get_archive_formats\",\n           \"register_archive_format\", \"unregister_archive_format\",\n           \"ignore_patterns\"]\n\nclass Error(EnvironmentError):\n    pass\n\nclass SpecialFileError(EnvironmentError):\n    \"\"\"Raised when trying to do a kind of operation (e.g. copying) which is\n    not supported on a special file (e.g. a named pipe)\"\"\"\n\nclass ExecError(EnvironmentError):\n    \"\"\"Raised when a command could not be executed\"\"\"\n\ntry:\n    WindowsError\nexcept NameError:\n    WindowsError = None\n\ndef copyfileobj(fsrc, fdst, length=16*1024):\n    \"\"\"copy data from file-like object fsrc to file-like object fdst\"\"\"\n    while 1:\n        buf = fsrc.read(length)\n        if not buf:\n            break\n        fdst.write(buf)\n\ndef _samefile(src, dst):\n    # Macintosh, Unix.\n    if hasattr(os.path, 'samefile'):\n        try:\n            return os.path.samefile(src, dst)\n        except OSError:\n            return False\n\n    # All other platforms: check for same pathname.\n    return (os.path.normcase(os.path.abspath(src)) ==\n            os.path.normcase(os.path.abspath(dst)))\n\ndef copyfile(src, dst):\n    \"\"\"Copy data from src to dst\"\"\"\n    if _samefile(src, dst):\n        raise Error(\"`%s` and `%s` are the same file\" % (src, dst))\n\n    for fn in [src, dst]:\n        try:\n            st = os.stat(fn)\n        except OSError:\n            # File most likely does not exist\n            pass\n        else:\n            # XXX What about other special files? (sockets, devices...)\n            if stat.S_ISFIFO(st.st_mode):\n                raise SpecialFileError(\"`%s` is a named pipe\" % fn)\n\n    with open(src, 'rb') as fsrc:\n        with open(dst, 'wb') as fdst:\n            copyfileobj(fsrc, fdst)\n\ndef copymode(src, dst):\n    \"\"\"Copy mode bits from src to dst\"\"\"\n    if hasattr(os, 'chmod'):\n        st = os.stat(src)\n        mode = stat.S_IMODE(st.st_mode)\n        os.chmod(dst, mode)\n\ndef copystat(src, dst):\n    \"\"\"Copy all stat info (mode bits, atime, mtime, flags) from src to dst\"\"\"\n    st = os.stat(src)\n    mode = stat.S_IMODE(st.st_mode)\n    if hasattr(os, 'utime'):\n        os.utime(dst, (st.st_atime, st.st_mtime))\n    if hasattr(os, 'chmod'):\n        os.chmod(dst, mode)\n    if hasattr(os, 'chflags') and hasattr(st, 'st_flags'):\n        try:\n            os.chflags(dst, st.st_flags)\n        except OSError, why:\n            for err in 'EOPNOTSUPP', 'ENOTSUP':\n                if hasattr(errno, err) and why.errno == getattr(errno, err):\n                    break\n            else:\n                raise\n\ndef copy(src, dst):\n    \"\"\"Copy data and mode bits (\"cp src dst\").\n\n    The destination may be a directory.\n\n    \"\"\"\n    if os.path.isdir(dst):\n        dst = os.path.join(dst, os.path.basename(src))\n    copyfile(src, dst)\n    copymode(src, dst)\n\ndef copy2(src, dst):\n    \"\"\"Copy data and all stat info (\"cp -p src dst\").\n\n    The destination may be a directory.\n\n    \"\"\"\n    if os.path.isdir(dst):\n        dst = os.path.join(dst, os.path.basename(src))\n    copyfile(src, dst)\n    copystat(src, dst)\n\ndef ignore_patterns(*patterns):\n    \"\"\"Function that can be used as copytree() ignore parameter.\n\n    Patterns is a sequence of glob-style patterns\n    that are used to exclude files\"\"\"\n    def _ignore_patterns(path, names):\n        ignored_names = []\n        for pattern in patterns:\n            ignored_names.extend(fnmatch.filter(names, pattern))\n        return set(ignored_names)\n    return _ignore_patterns\n\ndef copytree(src, dst, symlinks=False, ignore=None):\n    \"\"\"Recursively copy a directory tree using copy2().\n\n    The destination directory must not already exist.\n    If exception(s) occur, an Error is raised with a list of reasons.\n\n    If the optional symlinks flag is true, symbolic links in the\n    source tree result in symbolic links in the destination tree; if\n    it is false, the contents of the files pointed to by symbolic\n    links are copied.\n\n    The optional ignore argument is a callable. If given, it\n    is called with the `src` parameter, which is the directory\n    being visited by copytree(), and `names` which is the list of\n    `src` contents, as returned by os.listdir():\n\n        callable(src, names) -> ignored_names\n\n    Since copytree() is called recursively, the callable will be\n    called once for each directory that is copied. It returns a\n    list of names relative to the `src` directory that should\n    not be copied.\n\n    XXX Consider this example code rather than the ultimate tool.\n\n    \"\"\"\n    names = os.listdir(src)\n    if ignore is not None:\n        ignored_names = ignore(src, names)\n    else:\n        ignored_names = set()\n\n    os.makedirs(dst)\n    errors = []\n    for name in names:\n        if name in ignored_names:\n            continue\n        srcname = os.path.join(src, name)\n        dstname = os.path.join(dst, name)\n        try:\n            if symlinks and os.path.islink(srcname):\n                linkto = os.readlink(srcname)\n                os.symlink(linkto, dstname)\n            elif os.path.isdir(srcname):\n                copytree(srcname, dstname, symlinks, ignore)\n            else:\n                # Will raise a SpecialFileError for unsupported file types\n                copy2(srcname, dstname)\n        # catch the Error from the recursive copytree so that we can\n        # continue with other files\n        except Error, err:\n            errors.extend(err.args[0])\n        except EnvironmentError, why:\n            errors.append((srcname, dstname, str(why)))\n    try:\n        copystat(src, dst)\n    except OSError, why:\n        if WindowsError is not None and isinstance(why, WindowsError):\n            # Copying file access times may fail on Windows\n            pass\n        else:\n            errors.append((src, dst, str(why)))\n    if errors:\n        raise Error, errors\n\ndef rmtree(path, ignore_errors=False, onerror=None):\n    \"\"\"Recursively delete a directory tree.\n\n    If ignore_errors is set, errors are ignored; otherwise, if onerror\n    is set, it is called to handle the error with arguments (func,\n    path, exc_info) where func is os.listdir, os.remove, or os.rmdir;\n    path is the argument to that function that caused it to fail; and\n    exc_info is a tuple returned by sys.exc_info().  If ignore_errors\n    is false and onerror is None, an exception is raised.\n\n    \"\"\"\n    if ignore_errors:\n        def onerror(*args):\n            pass\n    elif onerror is None:\n        def onerror(*args):\n            raise\n    try:\n        if os.path.islink(path):\n            # symlinks to directories are forbidden, see bug #1669\n            raise OSError(\"Cannot call rmtree on a symbolic link\")\n    except OSError:\n        onerror(os.path.islink, path, sys.exc_info())\n        # can't continue even if onerror hook returns\n        return\n    names = []\n    try:\n        names = os.listdir(path)\n    except os.error, err:\n        onerror(os.listdir, path, sys.exc_info())\n    for name in names:\n        fullname = os.path.join(path, name)\n        try:\n            mode = os.lstat(fullname).st_mode\n        except os.error:\n            mode = 0\n        if stat.S_ISDIR(mode):\n            rmtree(fullname, ignore_errors, onerror)\n        else:\n            try:\n                os.remove(fullname)\n            except os.error, err:\n                onerror(os.remove, fullname, sys.exc_info())\n    try:\n        os.rmdir(path)\n    except os.error:\n        onerror(os.rmdir, path, sys.exc_info())\n\n\ndef _basename(path):\n    # A basename() variant which first strips the trailing slash, if present.\n    # Thus we always get the last component of the path, even for directories.\n    sep = os.path.sep + (os.path.altsep or '')\n    return os.path.basename(path.rstrip(sep))\n\ndef move(src, dst):\n    \"\"\"Recursively move a file or directory to another location. This is\n    similar to the Unix \"mv\" command.\n\n    If the destination is a directory or a symlink to a directory, the source\n    is moved inside the directory. The destination path must not already\n    exist.\n\n    If the destination already exists but is not a directory, it may be\n    overwritten depending on os.rename() semantics.\n\n    If the destination is on our current filesystem, then rename() is used.\n    Otherwise, src is copied to the destination and then removed.\n    A lot more could be done here...  A look at a mv.c shows a lot of\n    the issues this implementation glosses over.\n\n    \"\"\"\n    real_dst = dst\n    if os.path.isdir(dst):\n        if _samefile(src, dst):\n            # We might be on a case insensitive filesystem,\n            # perform the rename anyway.\n            os.rename(src, dst)\n            return\n\n        real_dst = os.path.join(dst, _basename(src))\n        if os.path.exists(real_dst):\n            raise Error, \"Destination path '%s' already exists\" % real_dst\n    try:\n        os.rename(src, real_dst)\n    except OSError:\n        if os.path.isdir(src):\n            if _destinsrc(src, dst):\n                raise Error, \"Cannot move a directory '%s' into itself '%s'.\" % (src, dst)\n            copytree(src, real_dst, symlinks=True)\n            rmtree(src)\n        else:\n            copy2(src, real_dst)\n            os.unlink(src)\n\ndef _destinsrc(src, dst):\n    src = abspath(src)\n    dst = abspath(dst)\n    if not src.endswith(os.path.sep):\n        src += os.path.sep\n    if not dst.endswith(os.path.sep):\n        dst += os.path.sep\n    return dst.startswith(src)\n\ndef _get_gid(name):\n    \"\"\"Returns a gid, given a group name.\"\"\"\n    if getgrnam is None or name is None:\n        return None\n    try:\n        result = getgrnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None\n\ndef _get_uid(name):\n    \"\"\"Returns an uid, given a user name.\"\"\"\n    if getpwnam is None or name is None:\n        return None\n    try:\n        result = getpwnam(name)\n    except KeyError:\n        result = None\n    if result is not None:\n        return result[2]\n    return None\n\ndef _make_tarball(base_name, base_dir, compress=\"gzip\", verbose=0, dry_run=0,\n                  owner=None, group=None, logger=None):\n    \"\"\"Create a (possibly compressed) tar file from all the files under\n    'base_dir'.\n\n    'compress' must be \"gzip\" (the default), \"bzip2\", or None.\n\n    'owner' and 'group' can be used to define an owner and a group for the\n    archive that is being built. If not provided, the current owner and group\n    will be used.\n\n    The output tar file will be named 'base_name' +  \".tar\", possibly plus\n    the appropriate compression extension (\".gz\", or \".bz2\").\n\n    Returns the output filename.\n    \"\"\"\n    tar_compression = {'gzip': 'gz', 'bzip2': 'bz2', None: ''}\n    compress_ext = {'gzip': '.gz', 'bzip2': '.bz2'}\n\n    # flags for compression program, each element of list will be an argument\n    if compress is not None and compress not in compress_ext.keys():\n        raise ValueError, \\\n              (\"bad value for 'compress': must be None, 'gzip' or 'bzip2'\")\n\n    archive_name = base_name + '.tar' + compress_ext.get(compress, '')\n    archive_dir = os.path.dirname(archive_name)\n\n    if not os.path.exists(archive_dir):\n        if logger is not None:\n            logger.info(\"creating %s\", archive_dir)\n        if not dry_run:\n            os.makedirs(archive_dir)\n\n\n    # creating the tarball\n    import tarfile  # late import so Python build itself doesn't break\n\n    if logger is not None:\n        logger.info('Creating tar archive')\n\n    uid = _get_uid(owner)\n    gid = _get_gid(group)\n\n    def _set_uid_gid(tarinfo):\n        if gid is not None:\n            tarinfo.gid = gid\n            tarinfo.gname = group\n        if uid is not None:\n            tarinfo.uid = uid\n            tarinfo.uname = owner\n        return tarinfo\n\n    if not dry_run:\n        tar = tarfile.open(archive_name, 'w|%s' % tar_compression[compress])\n        try:\n            tar.add(base_dir, filter=_set_uid_gid)\n        finally:\n            tar.close()\n\n    return archive_name\n\ndef _call_external_zip(base_dir, zip_filename, verbose=False, dry_run=False):\n    # XXX see if we want to keep an external call here\n    if verbose:\n        zipoptions = \"-r\"\n    else:\n        zipoptions = \"-rq\"\n    from distutils.errors import DistutilsExecError\n    from distutils.spawn import spawn\n    try:\n        spawn([\"zip\", zipoptions, zip_filename, base_dir], dry_run=dry_run)\n    except DistutilsExecError:\n        # XXX really should distinguish between \"couldn't find\n        # external 'zip' command\" and \"zip failed\".\n        raise ExecError, \\\n            (\"unable to create zip file '%s': \"\n            \"could neither import the 'zipfile' module nor \"\n            \"find a standalone zip utility\") % zip_filename\n\ndef _make_zipfile(base_name, base_dir, verbose=0, dry_run=0, logger=None):\n    \"\"\"Create a zip file from all the files under 'base_dir'.\n\n    The output zip file will be named 'base_name' + \".zip\".  Uses either the\n    \"zipfile\" Python module (if available) or the InfoZIP \"zip\" utility\n    (if installed and found on the default search path).  If neither tool is\n    available, raises ExecError.  Returns the name of the output zip\n    file.\n    \"\"\"\n    zip_filename = base_name + \".zip\"\n    archive_dir = os.path.dirname(base_name)\n\n    if not os.path.exists(archive_dir):\n        if logger is not None:\n            logger.info(\"creating %s\", archive_dir)\n        if not dry_run:\n            os.makedirs(archive_dir)\n\n    # If zipfile module is not available, try spawning an external 'zip'\n    # command.\n    try:\n        import zipfile\n    except ImportError:\n        zipfile = None\n\n    if zipfile is None:\n        _call_external_zip(base_dir, zip_filename, verbose, dry_run)\n    else:\n        if logger is not None:\n            logger.info(\"creating '%s' and adding '%s' to it\",\n                        zip_filename, base_dir)\n\n        if not dry_run:\n            with zipfile.ZipFile(zip_filename, \"w\",\n                                 compression=zipfile.ZIP_DEFLATED) as zf:\n                for dirpath, dirnames, filenames in os.walk(base_dir):\n                    for name in filenames:\n                        path = os.path.normpath(os.path.join(dirpath, name))\n                        if os.path.isfile(path):\n                            zf.write(path, path)\n                            if logger is not None:\n                                logger.info(\"adding '%s'\", path)\n\n    return zip_filename\n\n_ARCHIVE_FORMATS = {\n    'gztar': (_make_tarball, [('compress', 'gzip')], \"gzip'ed tar-file\"),\n    'bztar': (_make_tarball, [('compress', 'bzip2')], \"bzip2'ed tar-file\"),\n    'tar':   (_make_tarball, [('compress', None)], \"uncompressed tar file\"),\n    'zip':   (_make_zipfile, [],\"ZIP file\")\n    }\n\ndef get_archive_formats():\n    \"\"\"Returns a list of supported formats for archiving and unarchiving.\n\n    Each element of the returned sequence is a tuple (name, description)\n    \"\"\"\n    formats = [(name, registry[2]) for name, registry in\n               _ARCHIVE_FORMATS.items()]\n    formats.sort()\n    return formats\n\ndef register_archive_format(name, function, extra_args=None, description=''):\n    \"\"\"Registers an archive format.\n\n    name is the name of the format. function is the callable that will be\n    used to create archives. If provided, extra_args is a sequence of\n    (name, value) tuples that will be passed as arguments to the callable.\n    description can be provided to describe the format, and will be returned\n    by the get_archive_formats() function.\n    \"\"\"\n    if extra_args is None:\n        extra_args = []\n    if not isinstance(function, collections.Callable):\n        raise TypeError('The %s object is not callable' % function)\n    if not isinstance(extra_args, (tuple, list)):\n        raise TypeError('extra_args needs to be a sequence')\n    for element in extra_args:\n        if not isinstance(element, (tuple, list)) or len(element) !=2 :\n            raise TypeError('extra_args elements are : (arg_name, value)')\n\n    _ARCHIVE_FORMATS[name] = (function, extra_args, description)\n\ndef unregister_archive_format(name):\n    del _ARCHIVE_FORMATS[name]\n\ndef make_archive(base_name, format, root_dir=None, base_dir=None, verbose=0,\n                 dry_run=0, owner=None, group=None, logger=None):\n    \"\"\"Create an archive file (eg. zip or tar).\n\n    'base_name' is the name of the file to create, minus any format-specific\n    extension; 'format' is the archive format: one of \"zip\", \"tar\", \"bztar\"\n    or \"gztar\".\n\n    'root_dir' is a directory that will be the root directory of the\n    archive; ie. we typically chdir into 'root_dir' before creating the\n    archive.  'base_dir' is the directory where we start archiving from;\n    ie. 'base_dir' will be the common prefix of all files and\n    directories in the archive.  'root_dir' and 'base_dir' both default\n    to the current directory.  Returns the name of the archive file.\n\n    'owner' and 'group' are used when creating a tar archive. By default,\n    uses the current owner and group.\n    \"\"\"\n    save_cwd = os.getcwd()\n    if root_dir is not None:\n        if logger is not None:\n            logger.debug(\"changing into '%s'\", root_dir)\n        base_name = os.path.abspath(base_name)\n        if not dry_run:\n            os.chdir(root_dir)\n\n    if base_dir is None:\n        base_dir = os.curdir\n\n    kwargs = {'dry_run': dry_run, 'logger': logger}\n\n    try:\n        format_info = _ARCHIVE_FORMATS[format]\n    except KeyError:\n        raise ValueError, \"unknown archive format '%s'\" % format\n\n    func = format_info[0]\n    for arg, val in format_info[1]:\n        kwargs[arg] = val\n\n    if format != 'zip':\n        kwargs['owner'] = owner\n        kwargs['group'] = group\n\n    try:\n        filename = func(base_name, base_dir, **kwargs)\n    finally:\n        if root_dir is not None:\n            if logger is not None:\n                logger.debug(\"changing back to '%s'\", save_cwd)\n            os.chdir(save_cwd)\n\n    return filename\n", 
    "signal": "ITIMER_PROF = ITIMER_REAL = ITIMER_VIRTUAL = ItimerError = NSIG = SIGABRT = SIGALRM = SIGBUS = SIGCHLD = SIGCLD = None\nSIGCONT = SIGFPE = SIGHUP = SIGILL = SIGINT = SIGIO = SIGIOT = SIGKILL = SIGPIPE = SIGPOLL = SIGPROF = SIGPWR = None\nSIGQUIT = SIGRTMAX = SIGRTMIN = SIGSEGV = SIGSTOP = SIGSYS = SIGTERM = SIGTRAP = SIGTSTP = SIGTTIN = SIGTTOU = None\nSIGURG = SIGUSR1 = SIGUSR2 = SIGVTALRM = SIGWINCH = SIGXCPU = SIGXFSZ = SIG_DFL = SIG_IGN = None\n\n\ndef signal(*args):\n    return None\n\ndef siginterrupt(*args):\n    return None", 
    "socket": "# Wrapper module for _socket, providing some additional facilities\n# implemented in Python.\n\n\"\"\"\\\nThis module provides socket operations and some related functions.\nOn Unix, it supports IP (Internet Protocol) and Unix domain sockets.\nOn other systems, it only supports IP. Functions specific for a\nsocket are available as methods of the socket object.\n\nFunctions:\n\nsocket() -- create a new socket object\nsocketpair() -- create a pair of new socket objects [*]\nfromfd() -- create a socket object from an open file descriptor [*]\ngethostname() -- return the current hostname\ngethostbyname() -- map a hostname to its IP number\ngethostbyaddr() -- map an IP number or hostname to DNS info\ngetservbyname() -- map a service name and a protocol name to a port number\ngetprotobyname() -- map a protocol name (e.g. 'tcp') to a number\nntohs(), ntohl() -- convert 16, 32 bit int from network to host byte order\nhtons(), htonl() -- convert 16, 32 bit int from host to network byte order\ninet_aton() -- convert IP addr string (123.45.67.89) to 32-bit packed format\ninet_ntoa() -- convert 32-bit packed format IP to string (123.45.67.89)\nssl() -- secure socket layer support (only available if configured)\nsocket.getdefaulttimeout() -- get the default timeout value\nsocket.setdefaulttimeout() -- set the default timeout value\ncreate_connection() -- connects to an address, with an optional timeout and\n                       optional source address.\n\n [*] not available on all platforms!\n\nSpecial objects:\n\nSocketType -- type object for socket objects\nerror -- exception raised for I/O errors\nhas_ipv6 -- boolean value indicating if IPv6 is supported\n\nInteger constants:\n\nAF_INET, AF_UNIX -- socket domains (first argument to socket() call)\nSOCK_STREAM, SOCK_DGRAM, SOCK_RAW -- socket types (second argument)\n\nMany other constants may be defined; these may be used in calls to\nthe setsockopt() and getsockopt() methods.\n\"\"\"\n\nimport _socket\nfrom _socket import *\n\ntry:\n    import _ssl\nexcept ImportError:\n    # no SSL support\n    pass\nelse:\n    def ssl(sock, keyfile=None, certfile=None):\n        # we do an internal import here because the ssl\n        # module imports the socket module\n        import ssl as _realssl\n        warnings.warn(\"socket.ssl() is deprecated.  Use ssl.wrap_socket() instead.\",\n                      DeprecationWarning, stacklevel=2)\n        return _realssl.sslwrap_simple(sock, keyfile, certfile)\n\n    # we need to import the same constants we used to...\n    from _ssl import SSLError as sslerror\n    from _ssl import \\\n         RAND_add, \\\n         RAND_egd, \\\n         RAND_status, \\\n         SSL_ERROR_ZERO_RETURN, \\\n         SSL_ERROR_WANT_READ, \\\n         SSL_ERROR_WANT_WRITE, \\\n         SSL_ERROR_WANT_X509_LOOKUP, \\\n         SSL_ERROR_SYSCALL, \\\n         SSL_ERROR_SSL, \\\n         SSL_ERROR_WANT_CONNECT, \\\n         SSL_ERROR_EOF, \\\n         SSL_ERROR_INVALID_ERROR_CODE\n\nimport os, sys, warnings\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\ntry:\n    import errno\nexcept ImportError:\n    errno = None\nEBADF = getattr(errno, 'EBADF', 9)\nEINTR = getattr(errno, 'EINTR', 4)\n\n__all__ = [\"getfqdn\", \"create_connection\"]\n__all__.extend(os._get_exports_list(_socket))\n\n\n_realsocket = socket\n_type = type\n\n# WSA error codes\nif sys.platform.lower().startswith(\"win\"):\n    errorTab = {}\n    errorTab[10004] = \"The operation was interrupted.\"\n    errorTab[10009] = \"A bad file handle was passed.\"\n    errorTab[10013] = \"Permission denied.\"\n    errorTab[10014] = \"A fault occurred on the network??\" # WSAEFAULT\n    errorTab[10022] = \"An invalid operation was attempted.\"\n    errorTab[10035] = \"The socket operation would block\"\n    errorTab[10036] = \"A blocking operation is already in progress.\"\n    errorTab[10048] = \"The network address is in use.\"\n    errorTab[10054] = \"The connection has been reset.\"\n    errorTab[10058] = \"The network has been shut down.\"\n    errorTab[10060] = \"The operation timed out.\"\n    errorTab[10061] = \"Connection refused.\"\n    errorTab[10063] = \"The name is too long.\"\n    errorTab[10064] = \"The host is down.\"\n    errorTab[10065] = \"The host is unreachable.\"\n    __all__.append(\"errorTab\")\n\n\n\ndef getfqdn(name=''):\n    \"\"\"Get fully qualified domain name from name.\n\n    An empty argument is interpreted as meaning the local host.\n\n    First the hostname returned by gethostbyaddr() is checked, then\n    possibly existing aliases. In case no FQDN is available, hostname\n    from gethostname() is returned.\n    \"\"\"\n    name = name.strip()\n    if not name or name == '0.0.0.0':\n        name = gethostname()\n    try:\n        hostname, aliases, ipaddrs = gethostbyaddr(name)\n    except error:\n        pass\n    else:\n        aliases.insert(0, hostname)\n        for name in aliases:\n            if '.' in name:\n                break\n        else:\n            name = hostname\n    return name\n\nclass RefCountingWarning(UserWarning):\n    pass\n\ndef _do_reuse_or_drop(socket, methname):\n    try:\n        method = getattr(socket, methname)\n    except (AttributeError, TypeError):\n        warnings.warn(\"\"\"'%s' object has no _reuse/_drop methods\n{{\n    You make use (or a library you are using makes use) of the internal\n    classes '_socketobject' and '_fileobject' in socket.py, initializing\n    them with custom objects.  On PyPy, these custom objects need two\n    extra methods, _reuse() and _drop(), that maintain an explicit\n    reference counter.  When _drop() has been called as many times as\n    _reuse(), then the object should be freed.\n\n    Without these methods, you get the warning here.  This is to\n    prevent the following situation: if your (or the library's) code\n    relies on reference counting for prompt closing, then on PyPy, the\n    __del__ method will be called later than on CPython.  You can\n    easily end up in a situation where you open and close a lot of\n    (high-level) '_socketobject' or '_fileobject', but the (low-level)\n    custom objects will accumulate before their __del__ are called.\n    You quickly risk running out of file descriptors, for example.\n}}\"\"\" % (socket.__class__.__name__,), RefCountingWarning, stacklevel=3)\n    else:\n        method()\n\n\n_socketmethods = (\n    'bind', 'connect', 'connect_ex', 'fileno', 'listen',\n    'getpeername', 'getsockname', 'getsockopt', 'setsockopt',\n    'sendall', 'setblocking',\n    'settimeout', 'gettimeout', 'shutdown')\n\nif os.name == \"nt\":\n    _socketmethods = _socketmethods + ('ioctl',)\n\nif sys.platform == \"riscos\":\n    _socketmethods = _socketmethods + ('sleeptaskw',)\n\nclass _closedsocket(object):\n    __slots__ = []\n    def _dummy(*args):\n        raise error(EBADF, 'Bad file descriptor')\n    # All _delegate_methods must also be initialized here.\n    send = recv = recv_into = sendto = recvfrom = recvfrom_into = _dummy\n    __getattr__ = _dummy\n    def _drop(self):\n        pass\n\n# Wrapper around platform socket objects. This implements\n# a platform-independent dup() functionality. The\n# implementation currently relies on reference counting\n# to close the underlying socket object.\nclass _socketobject(object):\n\n    __doc__ = _realsocket.__doc__\n\n    __slots__ = [\"_sock\", \"__weakref__\"]\n\n    def __init__(self, family=AF_INET, type=SOCK_STREAM, proto=0, _sock=None):\n        if _sock is None:\n            _sock = _realsocket(family, type, proto)\n        else:\n            _do_reuse_or_drop(_sock, '_reuse')\n\n        self._sock = _sock\n\n    def send(self, data, flags=0):\n        return self._sock.send(data, flags)\n    send.__doc__ = _realsocket.send.__doc__\n\n    def recv(self, buffersize, flags=0):\n        return self._sock.recv(buffersize, flags)\n    recv.__doc__ = _realsocket.recv.__doc__\n\n    def recv_into(self, buffer, nbytes=0, flags=0):\n        return self._sock.recv_into(buffer, nbytes, flags)\n    recv_into.__doc__ = _realsocket.recv_into.__doc__\n\n    def recvfrom(self, buffersize, flags=0):\n        return self._sock.recvfrom(buffersize, flags)\n    recvfrom.__doc__ = _realsocket.recvfrom.__doc__\n\n    def recvfrom_into(self, buffer, nbytes=0, flags=0):\n        return self._sock.recvfrom_into(buffer, nbytes, flags)\n    recvfrom_into.__doc__ = _realsocket.recvfrom_into.__doc__\n\n    def sendto(self, data, param2, param3=None):\n        if param3 is None:\n            return self._sock.sendto(data, param2)\n        else:\n            return self._sock.sendto(data, param2, param3)\n    sendto.__doc__ = _realsocket.sendto.__doc__\n\n    def close(self):\n        s = self._sock\n        self._sock = _closedsocket()\n        _do_reuse_or_drop(s, '_drop')\n    close.__doc__ = _realsocket.close.__doc__\n\n    def accept(self):\n        sock, addr = self._sock.accept()\n        sockobj = _socketobject(_sock=sock)\n        _do_reuse_or_drop(sock, '_drop') # already a copy in the _socketobject()\n        return sockobj, addr\n    accept.__doc__ = _realsocket.accept.__doc__\n\n    def dup(self):\n        \"\"\"dup() -> socket object\n\n        Return a new socket object connected to the same system resource.\"\"\"\n        return _socketobject(_sock=self._sock)\n\n    def makefile(self, mode='r', bufsize=-1):\n        \"\"\"makefile([mode[, bufsize]]) -> file object\n\n        Return a regular file object corresponding to the socket.  The mode\n        and bufsize arguments are as for the built-in open() function.\"\"\"\n        return _fileobject(self._sock, mode, bufsize)\n\n    family = property(lambda self: self._sock.family, doc=\"the socket family\")\n    type = property(lambda self: self._sock.type, doc=\"the socket type\")\n    proto = property(lambda self: self._sock.proto, doc=\"the socket protocol\")\n\n    # Delegate many calls to the raw socket object.\n    _s = (\"def %(name)s(self, %(args)s): return self._sock.%(name)s(%(args)s)\\n\\n\"\n          \"%(name)s.__doc__ = _realsocket.%(name)s.__doc__\\n\")\n    for _m in _socketmethods:\n        # yupi! we're on pypy, all code objects have this interface\n        argcount = getattr(_realsocket, _m).im_func.func_code.co_argcount - 1\n        exec _s % {'name': _m, 'args': ', '.join('arg%d' % i for i in range(argcount))}\n    del _m, _s, argcount\n\n    # Delegation methods with default arguments, that the code above\n    # cannot handle correctly\n    def sendall(self, data, flags=0):\n        self._sock.sendall(data, flags)\n    sendall.__doc__ = _realsocket.sendall.__doc__\n\n    def getsockopt(self, level, optname, buflen=None):\n        if buflen is None:\n            return self._sock.getsockopt(level, optname)\n        return self._sock.getsockopt(level, optname, buflen)\n    getsockopt.__doc__ = _realsocket.getsockopt.__doc__\n\nsocket = SocketType = _socketobject\n\nclass _fileobject(object):\n    \"\"\"Faux file object attached to a socket object.\"\"\"\n\n    default_bufsize = 8192\n    name = \"<socket>\"\n\n    __slots__ = [\"mode\", \"bufsize\", \"softspace\",\n                 # \"closed\" is a property, see below\n                 \"_sock\", \"_rbufsize\", \"_wbufsize\", \"_rbuf\", \"_wbuf\", \"_wbuf_len\",\n                 \"_close\"]\n\n    def __init__(self, sock, mode='rb', bufsize=-1, close=False):\n        _do_reuse_or_drop(sock, '_reuse')\n        self._sock = sock\n        self.mode = mode # Not actually used in this version\n        if bufsize < 0:\n            bufsize = self.default_bufsize\n        self.bufsize = bufsize\n        self.softspace = False\n        # _rbufsize is the suggested recv buffer size.  It is *strictly*\n        # obeyed within readline() for recv calls.  If it is larger than\n        # default_bufsize it will be used for recv calls within read().\n        if bufsize == 0:\n            self._rbufsize = 1\n        elif bufsize == 1:\n            self._rbufsize = self.default_bufsize\n        else:\n            self._rbufsize = bufsize\n        self._wbufsize = bufsize\n        # We use StringIO for the read buffer to avoid holding a list\n        # of variously sized string objects which have been known to\n        # fragment the heap due to how they are malloc()ed and often\n        # realloc()ed down much smaller than their original allocation.\n        self._rbuf = StringIO()\n        self._wbuf = [] # A list of strings\n        self._wbuf_len = 0\n        self._close = close\n\n    def _getclosed(self):\n        return self._sock is None\n    closed = property(_getclosed, doc=\"True if the file is closed\")\n\n    def close(self):\n        try:\n            if self._sock:\n                self.flush()\n        finally:\n            s = self._sock\n            self._sock = None\n            if s is not None:\n                if self._close:\n                    s.close()\n                else:\n                    _do_reuse_or_drop(s, '_drop')\n\n    def __del__(self):\n        try:\n            self.close()\n        except:\n            # close() may fail if __init__ didn't complete\n            pass\n\n    def flush(self):\n        if self._wbuf:\n            data = \"\".join(self._wbuf)\n            self._wbuf = []\n            self._wbuf_len = 0\n            buffer_size = max(self._rbufsize, self.default_bufsize)\n            data_size = len(data)\n            write_offset = 0\n            view = memoryview(data)\n            try:\n                while write_offset < data_size:\n                    self._sock.sendall(view[write_offset:write_offset+buffer_size])\n                    write_offset += buffer_size\n            finally:\n                if write_offset < data_size:\n                    remainder = data[write_offset:]\n                    del view, data  # explicit free\n                    self._wbuf.append(remainder)\n                    self._wbuf_len = len(remainder)\n\n    def fileno(self):\n        return self._sock.fileno()\n\n    def write(self, data):\n        data = str(data) # XXX Should really reject non-string non-buffers\n        if not data:\n            return\n        self._wbuf.append(data)\n        self._wbuf_len += len(data)\n        if (self._wbufsize == 0 or\n            (self._wbufsize == 1 and '\\n' in data) or\n            (self._wbufsize > 1 and self._wbuf_len >= self._wbufsize)):\n            self.flush()\n\n    def writelines(self, list):\n        # XXX We could do better here for very long lists\n        # XXX Should really reject non-string non-buffers\n        lines = filter(None, map(str, list))\n        self._wbuf_len += sum(map(len, lines))\n        self._wbuf.extend(lines)\n        if (self._wbufsize <= 1 or\n            self._wbuf_len >= self._wbufsize):\n            self.flush()\n\n    def read(self, size=-1):\n        # Use max, disallow tiny reads in a loop as they are very inefficient.\n        # We never leave read() with any leftover data from a new recv() call\n        # in our internal buffer.\n        rbufsize = max(self._rbufsize, self.default_bufsize)\n        # Our use of StringIO rather than lists of string objects returned by\n        # recv() minimizes memory usage and fragmentation that occurs when\n        # rbufsize is large compared to the typical return value of recv().\n        buf = self._rbuf\n        buf.seek(0, 2)  # seek end\n        if size < 0:\n            # Read until EOF\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                try:\n                    data = self._sock.recv(rbufsize)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                buf.write(data)\n            return buf.getvalue()\n        else:\n            # Read until size bytes or EOF seen, whichever comes first\n            buf_len = buf.tell()\n            if buf_len >= size:\n                # Already have size bytes in our buffer?  Extract and return.\n                buf.seek(0)\n                rv = buf.read(size)\n                self._rbuf = StringIO()\n                self._rbuf.write(buf.read())\n                return rv\n\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                left = size - buf_len\n                # recv() will malloc the amount of memory given as its\n                # parameter even though it often returns much less data\n                # than that.  The returned data string is short lived\n                # as we copy it into a StringIO and free it.  This avoids\n                # fragmentation issues on many platforms.\n                try:\n                    data = self._sock.recv(left)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                n = len(data)\n                if n == size and not buf_len:\n                    # Shortcut.  Avoid buffer data copies when:\n                    # - We have no data in our buffer.\n                    # AND\n                    # - Our call to recv returned exactly the\n                    #   number of bytes we were asked to read.\n                    return data\n                if n == left:\n                    buf.write(data)\n                    del data  # explicit free\n                    break\n                assert n <= left, \"recv(%d) returned %d bytes\" % (left, n)\n                buf.write(data)\n                buf_len += n\n                del data  # explicit free\n                #assert buf_len == buf.tell()\n            return buf.getvalue()\n\n    def readline(self, size=-1):\n        buf = self._rbuf\n        buf.seek(0, 2)  # seek end\n        if buf.tell() > 0:\n            # check if we already have it in our buffer\n            buf.seek(0)\n            bline = buf.readline(size)\n            if bline.endswith('\\n') or len(bline) == size:\n                self._rbuf = StringIO()\n                self._rbuf.write(buf.read())\n                return bline\n            del bline\n        if size < 0:\n            # Read until \\n or EOF, whichever comes first\n            if self._rbufsize <= 1:\n                # Speed up unbuffered case\n                buf.seek(0)\n                buffers = [buf.read()]\n                self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n                data = None\n                recv = self._sock.recv\n                while True:\n                    try:\n                        while data != \"\\n\":\n                            data = recv(1)\n                            if not data:\n                                break\n                            buffers.append(data)\n                    except error, e:\n                        # The try..except to catch EINTR was moved outside the\n                        # recv loop to avoid the per byte overhead.\n                        if e.args[0] == EINTR:\n                            continue\n                        raise\n                    break\n                return \"\".join(buffers)\n\n            buf.seek(0, 2)  # seek end\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                try:\n                    data = self._sock.recv(self._rbufsize)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                nl = data.find('\\n')\n                if nl >= 0:\n                    nl += 1\n                    buf.write(data[:nl])\n                    self._rbuf.write(data[nl:])\n                    del data\n                    break\n                buf.write(data)\n            return buf.getvalue()\n        else:\n            # Read until size bytes or \\n or EOF seen, whichever comes first\n            buf.seek(0, 2)  # seek end\n            buf_len = buf.tell()\n            if buf_len >= size:\n                buf.seek(0)\n                rv = buf.read(size)\n                self._rbuf = StringIO()\n                self._rbuf.write(buf.read())\n                return rv\n            self._rbuf = StringIO()  # reset _rbuf.  we consume it via buf.\n            while True:\n                try:\n                    data = self._sock.recv(self._rbufsize)\n                except error, e:\n                    if e.args[0] == EINTR:\n                        continue\n                    raise\n                if not data:\n                    break\n                left = size - buf_len\n                # did we just receive a newline?\n                nl = data.find('\\n', 0, left)\n                if nl >= 0:\n                    nl += 1\n                    # save the excess data to _rbuf\n                    self._rbuf.write(data[nl:])\n                    if buf_len:\n                        buf.write(data[:nl])\n                        break\n                    else:\n                        # Shortcut.  Avoid data copy through buf when returning\n                        # a substring of our first recv().\n                        return data[:nl]\n                n = len(data)\n                if n == size and not buf_len:\n                    # Shortcut.  Avoid data copy through buf when\n                    # returning exactly all of our first recv().\n                    return data\n                if n >= left:\n                    buf.write(data[:left])\n                    self._rbuf.write(data[left:])\n                    break\n                buf.write(data)\n                buf_len += n\n                #assert buf_len == buf.tell()\n            return buf.getvalue()\n\n    def readlines(self, sizehint=0):\n        total = 0\n        list = []\n        while True:\n            line = self.readline()\n            if not line:\n                break\n            list.append(line)\n            total += len(line)\n            if sizehint and total >= sizehint:\n                break\n        return list\n\n    # Iterator protocols\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        line = self.readline()\n        if not line:\n            raise StopIteration\n        return line\n\n_GLOBAL_DEFAULT_TIMEOUT = object()\n\ndef create_connection(address, timeout=_GLOBAL_DEFAULT_TIMEOUT,\n                      source_address=None):\n    \"\"\"Connect to *address* and return the socket object.\n\n    Convenience function.  Connect to *address* (a 2-tuple ``(host,\n    port)``) and return the socket object.  Passing the optional\n    *timeout* parameter will set the timeout on the socket instance\n    before attempting to connect.  If no *timeout* is supplied, the\n    global default timeout setting returned by :func:`getdefaulttimeout`\n    is used.  If *source_address* is set it must be a tuple of (host, port)\n    for the socket to bind as a source address before making the connection.\n    An host of '' or port 0 tells the OS to use the default.\n    \"\"\"\n\n    host, port = address\n    err = None\n    for res in getaddrinfo(host, port, 0, SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket(af, socktype, proto)\n            if timeout is not _GLOBAL_DEFAULT_TIMEOUT:\n                sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            return sock\n\n        except error as _:\n            err = _\n            if sock is not None:\n                sock.close()\n\n    if err is not None:\n        raise err\n    else:\n        raise error(\"getaddrinfo returns an empty list\")\n", 
    "sre_compile": "# -*- coding: utf-8 -*-\n#\n# Secret Labs' Regular Expression Engine\n#\n# convert template to internal format\n#\n# Copyright (c) 1997-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\nimport _sre, sys\nimport sre_parse\nfrom sre_constants import *\n\nassert _sre.MAGIC == MAGIC, \"SRE module mismatch\"\n\nif _sre.CODESIZE == 2:\n    MAXCODE = 65535\nelse:\n    MAXCODE = 0xFFFFFFFFL\n\n_LITERAL_CODES = set([LITERAL, NOT_LITERAL])\n_REPEATING_CODES = set([REPEAT, MIN_REPEAT, MAX_REPEAT])\n_SUCCESS_CODES = set([SUCCESS, FAILURE])\n_ASSERT_CODES = set([ASSERT, ASSERT_NOT])\n\n# Sets of lowercase characters which have the same uppercase.\n_equivalences = (\n    # LATIN SMALL LETTER I, LATIN SMALL LETTER DOTLESS I\n    (0x69, 0x131), # i\u0131\n    # LATIN SMALL LETTER S, LATIN SMALL LETTER LONG S\n    (0x73, 0x17f), # s\u017f\n    # MICRO SIGN, GREEK SMALL LETTER MU\n    (0xb5, 0x3bc), # \u00b5\u03bc\n    # COMBINING GREEK YPOGEGRAMMENI, GREEK SMALL LETTER IOTA, GREEK PROSGEGRAMMENI\n    (0x345, 0x3b9, 0x1fbe), # \\u0345\u03b9\u1fbe\n    # GREEK SMALL LETTER BETA, GREEK BETA SYMBOL\n    (0x3b2, 0x3d0), # \u03b2\u03d0\n    # GREEK SMALL LETTER EPSILON, GREEK LUNATE EPSILON SYMBOL\n    (0x3b5, 0x3f5), # \u03b5\u03f5\n    # GREEK SMALL LETTER THETA, GREEK THETA SYMBOL\n    (0x3b8, 0x3d1), # \u03b8\u03d1\n    # GREEK SMALL LETTER KAPPA, GREEK KAPPA SYMBOL\n    (0x3ba, 0x3f0), # \u03ba\u03f0\n    # GREEK SMALL LETTER PI, GREEK PI SYMBOL\n    (0x3c0, 0x3d6), # \u03c0\u03d6\n    # GREEK SMALL LETTER RHO, GREEK RHO SYMBOL\n    (0x3c1, 0x3f1), # \u03c1\u03f1\n    # GREEK SMALL LETTER FINAL SIGMA, GREEK SMALL LETTER SIGMA\n    (0x3c2, 0x3c3), # \u03c2\u03c3\n    # GREEK SMALL LETTER PHI, GREEK PHI SYMBOL\n    (0x3c6, 0x3d5), # \u03c6\u03d5\n    # LATIN SMALL LETTER S WITH DOT ABOVE, LATIN SMALL LETTER LONG S WITH DOT ABOVE\n    (0x1e61, 0x1e9b), # \u1e61\u1e9b\n)\n\n# Maps the lowercase code to lowercase codes which have the same uppercase.\n_ignorecase_fixes = {i: tuple(j for j in t if i != j)\n                     for t in _equivalences for i in t}\n\ndef _compile(code, pattern, flags):\n    # internal: compile a (sub)pattern\n    emit = code.append\n    _len = len\n    LITERAL_CODES = _LITERAL_CODES\n    REPEATING_CODES = _REPEATING_CODES\n    SUCCESS_CODES = _SUCCESS_CODES\n    ASSERT_CODES = _ASSERT_CODES\n    if (flags & SRE_FLAG_IGNORECASE and\n            not (flags & SRE_FLAG_LOCALE) and\n            flags & SRE_FLAG_UNICODE):\n        fixes = _ignorecase_fixes\n    else:\n        fixes = None\n    for op, av in pattern:\n        if op in LITERAL_CODES:\n            if flags & SRE_FLAG_IGNORECASE:\n                lo = _sre.getlower(av, flags)\n                if fixes and lo in fixes:\n                    emit(OPCODES[IN_IGNORE])\n                    skip = _len(code); emit(0)\n                    if op is NOT_LITERAL:\n                        emit(OPCODES[NEGATE])\n                    for k in (lo,) + fixes[lo]:\n                        emit(OPCODES[LITERAL])\n                        emit(k)\n                    emit(OPCODES[FAILURE])\n                    code[skip] = _len(code) - skip\n                else:\n                    emit(OPCODES[OP_IGNORE[op]])\n                    emit(lo)\n            else:\n                emit(OPCODES[op])\n                emit(av)\n        elif op is IN:\n            if flags & SRE_FLAG_IGNORECASE:\n                emit(OPCODES[OP_IGNORE[op]])\n                def fixup(literal, flags=flags):\n                    return _sre.getlower(literal, flags)\n            else:\n                emit(OPCODES[op])\n                fixup = None\n            skip = _len(code); emit(0)\n            _compile_charset(av, flags, code, fixup, fixes)\n            code[skip] = _len(code) - skip\n        elif op is ANY:\n            if flags & SRE_FLAG_DOTALL:\n                emit(OPCODES[ANY_ALL])\n            else:\n                emit(OPCODES[ANY])\n        elif op in REPEATING_CODES:\n            if flags & SRE_FLAG_TEMPLATE:\n                raise error, \"internal: unsupported template operator\"\n                emit(OPCODES[REPEAT])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                emit(OPCODES[SUCCESS])\n                code[skip] = _len(code) - skip\n            elif _simple(av) and op is not REPEAT:\n                if op is MAX_REPEAT:\n                    emit(OPCODES[REPEAT_ONE])\n                else:\n                    emit(OPCODES[MIN_REPEAT_ONE])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                emit(OPCODES[SUCCESS])\n                code[skip] = _len(code) - skip\n            else:\n                emit(OPCODES[REPEAT])\n                skip = _len(code); emit(0)\n                emit(av[0])\n                emit(av[1])\n                _compile(code, av[2], flags)\n                code[skip] = _len(code) - skip\n                if op is MAX_REPEAT:\n                    emit(OPCODES[MAX_UNTIL])\n                else:\n                    emit(OPCODES[MIN_UNTIL])\n        elif op is SUBPATTERN:\n            if av[0]:\n                emit(OPCODES[MARK])\n                emit((av[0]-1)*2)\n            # _compile_info(code, av[1], flags)\n            _compile(code, av[1], flags)\n            if av[0]:\n                emit(OPCODES[MARK])\n                emit((av[0]-1)*2+1)\n        elif op in SUCCESS_CODES:\n            emit(OPCODES[op])\n        elif op in ASSERT_CODES:\n            emit(OPCODES[op])\n            skip = _len(code); emit(0)\n            if av[0] >= 0:\n                emit(0) # look ahead\n            else:\n                lo, hi = av[1].getwidth()\n                if lo != hi:\n                    raise error, \"look-behind requires fixed-width pattern\"\n                emit(lo) # look behind\n            _compile(code, av[1], flags)\n            emit(OPCODES[SUCCESS])\n            code[skip] = _len(code) - skip\n        elif op is CALL:\n            emit(OPCODES[op])\n            skip = _len(code); emit(0)\n            _compile(code, av, flags)\n            emit(OPCODES[SUCCESS])\n            code[skip] = _len(code) - skip\n        elif op is AT:\n            emit(OPCODES[op])\n            if flags & SRE_FLAG_MULTILINE:\n                av = AT_MULTILINE.get(av, av)\n            if flags & SRE_FLAG_LOCALE:\n                av = AT_LOCALE.get(av, av)\n            elif flags & SRE_FLAG_UNICODE:\n                av = AT_UNICODE.get(av, av)\n            emit(ATCODES[av])\n        elif op is BRANCH:\n            emit(OPCODES[op])\n            tail = []\n            tailappend = tail.append\n            for av in av[1]:\n                skip = _len(code); emit(0)\n                # _compile_info(code, av, flags)\n                _compile(code, av, flags)\n                emit(OPCODES[JUMP])\n                tailappend(_len(code)); emit(0)\n                code[skip] = _len(code) - skip\n            emit(0) # end of branch\n            for tail in tail:\n                code[tail] = _len(code) - tail\n        elif op is CATEGORY:\n            emit(OPCODES[op])\n            if flags & SRE_FLAG_LOCALE:\n                av = CH_LOCALE[av]\n            elif flags & SRE_FLAG_UNICODE:\n                av = CH_UNICODE[av]\n            emit(CHCODES[av])\n        elif op is GROUPREF:\n            if flags & SRE_FLAG_IGNORECASE:\n                emit(OPCODES[OP_IGNORE[op]])\n            else:\n                emit(OPCODES[op])\n            emit(av-1)\n        elif op is GROUPREF_EXISTS:\n            emit(OPCODES[op])\n            emit(av[0]-1)\n            skipyes = _len(code); emit(0)\n            _compile(code, av[1], flags)\n            if av[2]:\n                emit(OPCODES[JUMP])\n                skipno = _len(code); emit(0)\n                code[skipyes] = _len(code) - skipyes + 1\n                _compile(code, av[2], flags)\n                code[skipno] = _len(code) - skipno\n            else:\n                code[skipyes] = _len(code) - skipyes + 1\n        else:\n            raise ValueError, (\"unsupported operand type\", op)\n\ndef _compile_charset(charset, flags, code, fixup=None, fixes=None):\n    # compile charset subprogram\n    emit = code.append\n    for op, av in _optimize_charset(charset, fixup, fixes,\n                                    flags & SRE_FLAG_UNICODE):\n        emit(OPCODES[op])\n        if op is NEGATE:\n            pass\n        elif op is LITERAL:\n            emit(av)\n        elif op is RANGE:\n            emit(av[0])\n            emit(av[1])\n        elif op is CHARSET:\n            code.extend(av)\n        elif op is BIGCHARSET:\n            code.extend(av)\n        elif op is CATEGORY:\n            if flags & SRE_FLAG_LOCALE:\n                emit(CHCODES[CH_LOCALE[av]])\n            elif flags & SRE_FLAG_UNICODE:\n                emit(CHCODES[CH_UNICODE[av]])\n            else:\n                emit(CHCODES[av])\n        else:\n            raise error, \"internal: unsupported set operator\"\n    emit(OPCODES[FAILURE])\n\ndef _optimize_charset(charset, fixup, fixes, isunicode):\n    # internal: optimize character set\n    out = []\n    tail = []\n    charmap = bytearray(256)\n    for op, av in charset:\n        while True:\n            try:\n                if op is LITERAL:\n                    if fixup:\n                        i = fixup(av)\n                        charmap[i] = 1\n                        if fixes and i in fixes:\n                            for k in fixes[i]:\n                                charmap[k] = 1\n                    else:\n                        charmap[av] = 1\n                elif op is RANGE:\n                    r = range(av[0], av[1]+1)\n                    if fixup:\n                        r = map(fixup, r)\n                    if fixup and fixes:\n                        for i in r:\n                            charmap[i] = 1\n                            if i in fixes:\n                                for k in fixes[i]:\n                                    charmap[k] = 1\n                    else:\n                        for i in r:\n                            charmap[i] = 1\n                elif op is NEGATE:\n                    out.append((op, av))\n                else:\n                    tail.append((op, av))\n            except IndexError:\n                if len(charmap) == 256:\n                    # character set contains non-UCS1 character codes\n                    charmap += b'\\0' * 0xff00\n                    continue\n                # character set contains non-BMP character codes\n                if fixup and isunicode and op is RANGE:\n                    lo, hi = av\n                    ranges = [av]\n                    # There are only two ranges of cased astral characters:\n                    # 10400-1044F (Deseret) and 118A0-118DF (Warang Citi).\n                    _fixup_range(max(0x10000, lo), min(0x11fff, hi),\n                                 ranges, fixup)\n                    for lo, hi in ranges:\n                        if lo == hi:\n                            tail.append((LITERAL, hi))\n                        else:\n                            tail.append((RANGE, (lo, hi)))\n                else:\n                    tail.append((op, av))\n            break\n\n    # compress character map\n    runs = []\n    q = 0\n    while True:\n        p = charmap.find(b'\\1', q)\n        if p < 0:\n            break\n        if len(runs) >= 2:\n            runs = None\n            break\n        q = charmap.find(b'\\0', p)\n        if q < 0:\n            runs.append((p, len(charmap)))\n            break\n        runs.append((p, q))\n    if runs is not None:\n        # use literal/range\n        for p, q in runs:\n            if q - p == 1:\n                out.append((LITERAL, p))\n            else:\n                out.append((RANGE, (p, q - 1)))\n        out += tail\n        # if the case was changed or new representation is more compact\n        if fixup or len(out) < len(charset):\n            return out\n        # else original character set is good enough\n        return charset\n\n    # use bitmap\n    if len(charmap) == 256:\n        data = _mk_bitmap(charmap)\n        out.append((CHARSET, data))\n        out += tail\n        return out\n\n    # To represent a big charset, first a bitmap of all characters in the\n    # set is constructed. Then, this bitmap is sliced into chunks of 256\n    # characters, duplicate chunks are eliminated, and each chunk is\n    # given a number. In the compiled expression, the charset is\n    # represented by a 32-bit word sequence, consisting of one word for\n    # the number of different chunks, a sequence of 256 bytes (64 words)\n    # of chunk numbers indexed by their original chunk position, and a\n    # sequence of 256-bit chunks (8 words each).\n\n    # Compression is normally good: in a typical charset, large ranges of\n    # Unicode will be either completely excluded (e.g. if only cyrillic\n    # letters are to be matched), or completely included (e.g. if large\n    # subranges of Kanji match). These ranges will be represented by\n    # chunks of all one-bits or all zero-bits.\n\n    # Matching can be also done efficiently: the more significant byte of\n    # the Unicode character is an index into the chunk number, and the\n    # less significant byte is a bit index in the chunk (just like the\n    # CHARSET matching).\n\n    # In UCS-4 mode, the BIGCHARSET opcode still supports only subsets\n    # of the basic multilingual plane; an efficient representation\n    # for all of Unicode has not yet been developed.\n\n    charmap = bytes(charmap) # should be hashable\n    comps = {}\n    mapping = bytearray(256)\n    block = 0\n    data = bytearray()\n    for i in range(0, 65536, 256):\n        chunk = charmap[i: i + 256]\n        if chunk in comps:\n            mapping[i // 256] = comps[chunk]\n        else:\n            mapping[i // 256] = comps[chunk] = block\n            block += 1\n            data += chunk\n    data = _mk_bitmap(data)\n    data[0:0] = [block] + _bytes_to_codes(mapping)\n    out.append((BIGCHARSET, data))\n    out += tail\n    return out\n\ndef _fixup_range(lo, hi, ranges, fixup):\n    for i in map(fixup, range(lo, hi+1)):\n        for k, (lo, hi) in enumerate(ranges):\n            if i < lo:\n                if l == lo - 1:\n                    ranges[k] = (i, hi)\n                else:\n                    ranges.insert(k, (i, i))\n                break\n            elif i > hi:\n                if i == hi + 1:\n                    ranges[k] = (lo, i)\n                    break\n            else:\n                break\n        else:\n            ranges.append((i, i))\n\n_CODEBITS = _sre.CODESIZE * 8\n_BITS_TRANS = b'0' + b'1' * 255\ndef _mk_bitmap(bits, _CODEBITS=_CODEBITS, _int=int):\n    s = bytes(bits).translate(_BITS_TRANS)[::-1]\n    return [_int(s[i - _CODEBITS: i], 2)\n            for i in range(len(s), 0, -_CODEBITS)]\n\ndef _bytes_to_codes(b):\n    # Convert block indices to word array\n    import array\n    if _sre.CODESIZE == 2:\n        code = 'H'\n    else:\n        code = 'I'\n    a = array.array(code, bytes(b))\n    assert a.itemsize == _sre.CODESIZE\n    assert len(a) * a.itemsize == len(b)\n    return a.tolist()\n\ndef _simple(av):\n    # check if av is a \"simple\" operator\n    lo, hi = av[2].getwidth()\n    return lo == hi == 1 and av[2][0][0] != SUBPATTERN\n\ndef _compile_info(code, pattern, flags):\n    # internal: compile an info block.  in the current version,\n    # this contains min/max pattern width, and an optional literal\n    # prefix or a character map\n    lo, hi = pattern.getwidth()\n    if lo == 0:\n        return # not worth it\n    # look for a literal prefix\n    prefix = []\n    prefixappend = prefix.append\n    prefix_skip = 0\n    charset = [] # not used\n    charsetappend = charset.append\n    if not (flags & SRE_FLAG_IGNORECASE):\n        # look for literal prefix\n        for op, av in pattern.data:\n            if op is LITERAL:\n                if len(prefix) == prefix_skip:\n                    prefix_skip = prefix_skip + 1\n                prefixappend(av)\n            elif op is SUBPATTERN and len(av[1]) == 1:\n                op, av = av[1][0]\n                if op is LITERAL:\n                    prefixappend(av)\n                else:\n                    break\n            else:\n                break\n        # if no prefix, look for charset prefix\n        if not prefix and pattern.data:\n            op, av = pattern.data[0]\n            if op is SUBPATTERN and av[1]:\n                op, av = av[1][0]\n                if op is LITERAL:\n                    charsetappend((op, av))\n                elif op is BRANCH:\n                    c = []\n                    cappend = c.append\n                    for p in av[1]:\n                        if not p:\n                            break\n                        op, av = p[0]\n                        if op is LITERAL:\n                            cappend((op, av))\n                        else:\n                            break\n                    else:\n                        charset = c\n            elif op is BRANCH:\n                c = []\n                cappend = c.append\n                for p in av[1]:\n                    if not p:\n                        break\n                    op, av = p[0]\n                    if op is LITERAL:\n                        cappend((op, av))\n                    else:\n                        break\n                else:\n                    charset = c\n            elif op is IN:\n                charset = av\n##     if prefix:\n##         print \"*** PREFIX\", prefix, prefix_skip\n##     if charset:\n##         print \"*** CHARSET\", charset\n    # add an info block\n    emit = code.append\n    emit(OPCODES[INFO])\n    skip = len(code); emit(0)\n    # literal flag\n    mask = 0\n    if prefix:\n        mask = SRE_INFO_PREFIX\n        if len(prefix) == prefix_skip == len(pattern.data):\n            mask = mask + SRE_INFO_LITERAL\n    elif charset:\n        mask = mask + SRE_INFO_CHARSET\n    emit(mask)\n    # pattern length\n    if lo < MAXCODE:\n        emit(lo)\n    else:\n        emit(MAXCODE)\n        prefix = prefix[:MAXCODE]\n    if hi < MAXCODE:\n        emit(hi)\n    else:\n        emit(0)\n    # add literal prefix\n    if prefix:\n        emit(len(prefix)) # length\n        emit(prefix_skip) # skip\n        code.extend(prefix)\n        # generate overlap table\n        table = [-1] + ([0]*len(prefix))\n        for i in xrange(len(prefix)):\n            table[i+1] = table[i]+1\n            while table[i+1] > 0 and prefix[i] != prefix[table[i+1]-1]:\n                table[i+1] = table[table[i+1]-1]+1\n        code.extend(table[1:]) # don't store first entry\n    elif charset:\n        _compile_charset(charset, flags, code)\n    code[skip] = len(code) - skip\n\ntry:\n    unicode\nexcept NameError:\n    STRING_TYPES = (type(\"\"),)\nelse:\n    STRING_TYPES = (type(\"\"), type(unicode(\"\")))\n\ndef isstring(obj):\n    for tp in STRING_TYPES:\n        if isinstance(obj, tp):\n            return 1\n    return 0\n\ndef _code(p, flags):\n\n    flags = p.pattern.flags | flags\n    code = []\n\n    # compile info block\n    _compile_info(code, p, flags)\n\n    # compile the pattern\n    _compile(code, p.data, flags)\n\n    code.append(OPCODES[SUCCESS])\n\n    return code\n\ndef compile(p, flags=0):\n    # internal: convert pattern list to internal format\n\n    if isstring(p):\n        pattern = p\n        p = sre_parse.parse(p, flags)\n    else:\n        pattern = None\n\n    code = _code(p, flags)\n\n    # print code\n\n    # XXX: <fl> get rid of this limitation!\n    if p.pattern.groups > 100:\n        raise AssertionError(\n            \"sorry, but this version only supports 100 named groups\"\n            )\n\n    # map in either direction\n    groupindex = p.pattern.groupdict\n    indexgroup = [None] * p.pattern.groups\n    for k, i in groupindex.items():\n        indexgroup[i] = k\n\n    return _sre.compile(\n        pattern, flags | p.pattern.flags, code,\n        p.pattern.groups-1,\n        groupindex, indexgroup\n        )\n", 
    "sre_constants": "#\n# Secret Labs' Regular Expression Engine\n#\n# various symbols used by the regular expression engine.\n# run this script to update the _sre include files!\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\n# update when constants are added or removed\n\nMAGIC = 20031017\n\ntry:\n    from _sre import MAXREPEAT\nexcept ImportError:\n    import _sre\n    MAXREPEAT = _sre.MAXREPEAT = 65535\n\n# SRE standard exception (access as sre.error)\n# should this really be here?\n\nclass error(Exception):\n    pass\n\n# operators\n\nFAILURE = \"failure\"\nSUCCESS = \"success\"\n\nANY = \"any\"\nANY_ALL = \"any_all\"\nASSERT = \"assert\"\nASSERT_NOT = \"assert_not\"\nAT = \"at\"\nBIGCHARSET = \"bigcharset\"\nBRANCH = \"branch\"\nCALL = \"call\"\nCATEGORY = \"category\"\nCHARSET = \"charset\"\nGROUPREF = \"groupref\"\nGROUPREF_IGNORE = \"groupref_ignore\"\nGROUPREF_EXISTS = \"groupref_exists\"\nIN = \"in\"\nIN_IGNORE = \"in_ignore\"\nINFO = \"info\"\nJUMP = \"jump\"\nLITERAL = \"literal\"\nLITERAL_IGNORE = \"literal_ignore\"\nMARK = \"mark\"\nMAX_REPEAT = \"max_repeat\"\nMAX_UNTIL = \"max_until\"\nMIN_REPEAT = \"min_repeat\"\nMIN_UNTIL = \"min_until\"\nNEGATE = \"negate\"\nNOT_LITERAL = \"not_literal\"\nNOT_LITERAL_IGNORE = \"not_literal_ignore\"\nRANGE = \"range\"\nREPEAT = \"repeat\"\nREPEAT_ONE = \"repeat_one\"\nSUBPATTERN = \"subpattern\"\nMIN_REPEAT_ONE = \"min_repeat_one\"\n\n# positions\nAT_BEGINNING = \"at_beginning\"\nAT_BEGINNING_LINE = \"at_beginning_line\"\nAT_BEGINNING_STRING = \"at_beginning_string\"\nAT_BOUNDARY = \"at_boundary\"\nAT_NON_BOUNDARY = \"at_non_boundary\"\nAT_END = \"at_end\"\nAT_END_LINE = \"at_end_line\"\nAT_END_STRING = \"at_end_string\"\nAT_LOC_BOUNDARY = \"at_loc_boundary\"\nAT_LOC_NON_BOUNDARY = \"at_loc_non_boundary\"\nAT_UNI_BOUNDARY = \"at_uni_boundary\"\nAT_UNI_NON_BOUNDARY = \"at_uni_non_boundary\"\n\n# categories\nCATEGORY_DIGIT = \"category_digit\"\nCATEGORY_NOT_DIGIT = \"category_not_digit\"\nCATEGORY_SPACE = \"category_space\"\nCATEGORY_NOT_SPACE = \"category_not_space\"\nCATEGORY_WORD = \"category_word\"\nCATEGORY_NOT_WORD = \"category_not_word\"\nCATEGORY_LINEBREAK = \"category_linebreak\"\nCATEGORY_NOT_LINEBREAK = \"category_not_linebreak\"\nCATEGORY_LOC_WORD = \"category_loc_word\"\nCATEGORY_LOC_NOT_WORD = \"category_loc_not_word\"\nCATEGORY_UNI_DIGIT = \"category_uni_digit\"\nCATEGORY_UNI_NOT_DIGIT = \"category_uni_not_digit\"\nCATEGORY_UNI_SPACE = \"category_uni_space\"\nCATEGORY_UNI_NOT_SPACE = \"category_uni_not_space\"\nCATEGORY_UNI_WORD = \"category_uni_word\"\nCATEGORY_UNI_NOT_WORD = \"category_uni_not_word\"\nCATEGORY_UNI_LINEBREAK = \"category_uni_linebreak\"\nCATEGORY_UNI_NOT_LINEBREAK = \"category_uni_not_linebreak\"\n\nOPCODES = [\n\n    # failure=0 success=1 (just because it looks better that way :-)\n    FAILURE, SUCCESS,\n\n    ANY, ANY_ALL,\n    ASSERT, ASSERT_NOT,\n    AT,\n    BRANCH,\n    CALL,\n    CATEGORY,\n    CHARSET, BIGCHARSET,\n    GROUPREF, GROUPREF_EXISTS, GROUPREF_IGNORE,\n    IN, IN_IGNORE,\n    INFO,\n    JUMP,\n    LITERAL, LITERAL_IGNORE,\n    MARK,\n    MAX_UNTIL,\n    MIN_UNTIL,\n    NOT_LITERAL, NOT_LITERAL_IGNORE,\n    NEGATE,\n    RANGE,\n    REPEAT,\n    REPEAT_ONE,\n    SUBPATTERN,\n    MIN_REPEAT_ONE\n\n]\n\nATCODES = [\n    AT_BEGINNING, AT_BEGINNING_LINE, AT_BEGINNING_STRING, AT_BOUNDARY,\n    AT_NON_BOUNDARY, AT_END, AT_END_LINE, AT_END_STRING,\n    AT_LOC_BOUNDARY, AT_LOC_NON_BOUNDARY, AT_UNI_BOUNDARY,\n    AT_UNI_NON_BOUNDARY\n]\n\nCHCODES = [\n    CATEGORY_DIGIT, CATEGORY_NOT_DIGIT, CATEGORY_SPACE,\n    CATEGORY_NOT_SPACE, CATEGORY_WORD, CATEGORY_NOT_WORD,\n    CATEGORY_LINEBREAK, CATEGORY_NOT_LINEBREAK, CATEGORY_LOC_WORD,\n    CATEGORY_LOC_NOT_WORD, CATEGORY_UNI_DIGIT, CATEGORY_UNI_NOT_DIGIT,\n    CATEGORY_UNI_SPACE, CATEGORY_UNI_NOT_SPACE, CATEGORY_UNI_WORD,\n    CATEGORY_UNI_NOT_WORD, CATEGORY_UNI_LINEBREAK,\n    CATEGORY_UNI_NOT_LINEBREAK\n]\n\ndef makedict(list):\n    d = {}\n    i = 0\n    for item in list:\n        d[item] = i\n        i = i + 1\n    return d\n\nOPCODES = makedict(OPCODES)\nATCODES = makedict(ATCODES)\nCHCODES = makedict(CHCODES)\n\n# replacement operations for \"ignore case\" mode\nOP_IGNORE = {\n    GROUPREF: GROUPREF_IGNORE,\n    IN: IN_IGNORE,\n    LITERAL: LITERAL_IGNORE,\n    NOT_LITERAL: NOT_LITERAL_IGNORE\n}\n\nAT_MULTILINE = {\n    AT_BEGINNING: AT_BEGINNING_LINE,\n    AT_END: AT_END_LINE\n}\n\nAT_LOCALE = {\n    AT_BOUNDARY: AT_LOC_BOUNDARY,\n    AT_NON_BOUNDARY: AT_LOC_NON_BOUNDARY\n}\n\nAT_UNICODE = {\n    AT_BOUNDARY: AT_UNI_BOUNDARY,\n    AT_NON_BOUNDARY: AT_UNI_NON_BOUNDARY\n}\n\nCH_LOCALE = {\n    CATEGORY_DIGIT: CATEGORY_DIGIT,\n    CATEGORY_NOT_DIGIT: CATEGORY_NOT_DIGIT,\n    CATEGORY_SPACE: CATEGORY_SPACE,\n    CATEGORY_NOT_SPACE: CATEGORY_NOT_SPACE,\n    CATEGORY_WORD: CATEGORY_LOC_WORD,\n    CATEGORY_NOT_WORD: CATEGORY_LOC_NOT_WORD,\n    CATEGORY_LINEBREAK: CATEGORY_LINEBREAK,\n    CATEGORY_NOT_LINEBREAK: CATEGORY_NOT_LINEBREAK\n}\n\nCH_UNICODE = {\n    CATEGORY_DIGIT: CATEGORY_UNI_DIGIT,\n    CATEGORY_NOT_DIGIT: CATEGORY_UNI_NOT_DIGIT,\n    CATEGORY_SPACE: CATEGORY_UNI_SPACE,\n    CATEGORY_NOT_SPACE: CATEGORY_UNI_NOT_SPACE,\n    CATEGORY_WORD: CATEGORY_UNI_WORD,\n    CATEGORY_NOT_WORD: CATEGORY_UNI_NOT_WORD,\n    CATEGORY_LINEBREAK: CATEGORY_UNI_LINEBREAK,\n    CATEGORY_NOT_LINEBREAK: CATEGORY_UNI_NOT_LINEBREAK\n}\n\n# flags\nSRE_FLAG_TEMPLATE = 1 # template mode (disable backtracking)\nSRE_FLAG_IGNORECASE = 2 # case insensitive\nSRE_FLAG_LOCALE = 4 # honour system locale\nSRE_FLAG_MULTILINE = 8 # treat target as multiline string\nSRE_FLAG_DOTALL = 16 # treat target as a single string\nSRE_FLAG_UNICODE = 32 # use unicode locale\nSRE_FLAG_VERBOSE = 64 # ignore whitespace and comments\nSRE_FLAG_DEBUG = 128 # debugging\n\n# flags for INFO primitive\nSRE_INFO_PREFIX = 1 # has prefix\nSRE_INFO_LITERAL = 2 # entire pattern is literal (given by prefix)\nSRE_INFO_CHARSET = 4 # pattern starts with character from given set\n\nif __name__ == \"__main__\":\n    def dump(f, d, prefix):\n        items = d.items()\n        items.sort(key=lambda a: a[1])\n        for k, v in items:\n            f.write(\"#define %s_%s %s\\n\" % (prefix, k.upper(), v))\n    f = open(\"sre_constants.h\", \"w\")\n    f.write(\"\"\"\\\n/*\n * Secret Labs' Regular Expression Engine\n *\n * regular expression matching engine\n *\n * NOTE: This file is generated by sre_constants.py.  If you need\n * to change anything in here, edit sre_constants.py and run it.\n *\n * Copyright (c) 1997-2001 by Secret Labs AB.  All rights reserved.\n *\n * See the _sre.c file for information on usage and redistribution.\n */\n\n\"\"\")\n\n    f.write(\"#define SRE_MAGIC %d\\n\" % MAGIC)\n\n    dump(f, OPCODES, \"SRE_OP\")\n    dump(f, ATCODES, \"SRE\")\n    dump(f, CHCODES, \"SRE\")\n\n    f.write(\"#define SRE_FLAG_TEMPLATE %d\\n\" % SRE_FLAG_TEMPLATE)\n    f.write(\"#define SRE_FLAG_IGNORECASE %d\\n\" % SRE_FLAG_IGNORECASE)\n    f.write(\"#define SRE_FLAG_LOCALE %d\\n\" % SRE_FLAG_LOCALE)\n    f.write(\"#define SRE_FLAG_MULTILINE %d\\n\" % SRE_FLAG_MULTILINE)\n    f.write(\"#define SRE_FLAG_DOTALL %d\\n\" % SRE_FLAG_DOTALL)\n    f.write(\"#define SRE_FLAG_UNICODE %d\\n\" % SRE_FLAG_UNICODE)\n    f.write(\"#define SRE_FLAG_VERBOSE %d\\n\" % SRE_FLAG_VERBOSE)\n\n    f.write(\"#define SRE_INFO_PREFIX %d\\n\" % SRE_INFO_PREFIX)\n    f.write(\"#define SRE_INFO_LITERAL %d\\n\" % SRE_INFO_LITERAL)\n    f.write(\"#define SRE_INFO_CHARSET %d\\n\" % SRE_INFO_CHARSET)\n\n    f.close()\n    print \"done\"\n", 
    "sre_parse": "#\n# Secret Labs' Regular Expression Engine\n#\n# convert re-style regular expression to sre pattern\n#\n# Copyright (c) 1998-2001 by Secret Labs AB.  All rights reserved.\n#\n# See the sre.py file for information on usage and redistribution.\n#\n\n\"\"\"Internal support module for sre\"\"\"\n\n# XXX: show string offset and offending character for all errors\n\nimport sys\n\nfrom sre_constants import *\n\ntry:\n    from __pypy__ import newdict\nexcept ImportError:\n    assert '__pypy__' not in sys.builtin_module_names\n    newdict = lambda _ : {}\n\nSPECIAL_CHARS = \".\\\\[{()*+?^$|\"\nREPEAT_CHARS = \"*+?{\"\n\nDIGITS = set(\"0123456789\")\n\nOCTDIGITS = set(\"01234567\")\nHEXDIGITS = set(\"0123456789abcdefABCDEF\")\n\nWHITESPACE = set(\" \\t\\n\\r\\v\\f\")\n\nESCAPES = {\n    r\"\\a\": (LITERAL, ord(\"\\a\")),\n    r\"\\b\": (LITERAL, ord(\"\\b\")),\n    r\"\\f\": (LITERAL, ord(\"\\f\")),\n    r\"\\n\": (LITERAL, ord(\"\\n\")),\n    r\"\\r\": (LITERAL, ord(\"\\r\")),\n    r\"\\t\": (LITERAL, ord(\"\\t\")),\n    r\"\\v\": (LITERAL, ord(\"\\v\")),\n    r\"\\\\\": (LITERAL, ord(\"\\\\\"))\n}\n\nCATEGORIES = {\n    r\"\\A\": (AT, AT_BEGINNING_STRING), # start of string\n    r\"\\b\": (AT, AT_BOUNDARY),\n    r\"\\B\": (AT, AT_NON_BOUNDARY),\n    r\"\\d\": (IN, [(CATEGORY, CATEGORY_DIGIT)]),\n    r\"\\D\": (IN, [(CATEGORY, CATEGORY_NOT_DIGIT)]),\n    r\"\\s\": (IN, [(CATEGORY, CATEGORY_SPACE)]),\n    r\"\\S\": (IN, [(CATEGORY, CATEGORY_NOT_SPACE)]),\n    r\"\\w\": (IN, [(CATEGORY, CATEGORY_WORD)]),\n    r\"\\W\": (IN, [(CATEGORY, CATEGORY_NOT_WORD)]),\n    r\"\\Z\": (AT, AT_END_STRING), # end of string\n}\n\nFLAGS = {\n    # standard flags\n    \"i\": SRE_FLAG_IGNORECASE,\n    \"L\": SRE_FLAG_LOCALE,\n    \"m\": SRE_FLAG_MULTILINE,\n    \"s\": SRE_FLAG_DOTALL,\n    \"x\": SRE_FLAG_VERBOSE,\n    # extensions\n    \"t\": SRE_FLAG_TEMPLATE,\n    \"u\": SRE_FLAG_UNICODE,\n}\n\nclass Pattern:\n    # master pattern object.  keeps track of global attributes\n    def __init__(self):\n        self.flags = 0\n        self.open = []\n        self.groups = 1\n        self.groupdict = newdict(\"module\")\n    def opengroup(self, name=None):\n        gid = self.groups\n        self.groups = gid + 1\n        if name is not None:\n            ogid = self.groupdict.get(name, None)\n            if ogid is not None:\n                raise error, (\"redefinition of group name %s as group %d; \"\n                              \"was group %d\" % (repr(name), gid,  ogid))\n            self.groupdict[name] = gid\n        self.open.append(gid)\n        return gid\n    def closegroup(self, gid):\n        self.open.remove(gid)\n    def checkgroup(self, gid):\n        return gid < self.groups and gid not in self.open\n\nclass SubPattern:\n    # a subpattern, in intermediate form\n    def __init__(self, pattern, data=None):\n        self.pattern = pattern\n        if data is None:\n            data = []\n        self.data = data\n        self.width = None\n    def dump(self, level=0):\n        seqtypes = (tuple, list)\n        for op, av in self.data:\n            print level*\"  \" + op,\n            if op == IN:\n                # member sublanguage\n                print\n                for op, a in av:\n                    print (level+1)*\"  \" + op, a\n            elif op == BRANCH:\n                print\n                for i, a in enumerate(av[1]):\n                    if i:\n                        print level*\"  \" + \"or\"\n                    a.dump(level+1)\n            elif op == GROUPREF_EXISTS:\n                condgroup, item_yes, item_no = av\n                print condgroup\n                item_yes.dump(level+1)\n                if item_no:\n                    print level*\"  \" + \"else\"\n                    item_no.dump(level+1)\n            elif isinstance(av, seqtypes):\n                nl = 0\n                for a in av:\n                    if isinstance(a, SubPattern):\n                        if not nl:\n                            print\n                        a.dump(level+1)\n                        nl = 1\n                    else:\n                        print a,\n                        nl = 0\n                if not nl:\n                    print\n            else:\n                print av\n    def __repr__(self):\n        return repr(self.data)\n    def __len__(self):\n        return len(self.data)\n    def __delitem__(self, index):\n        del self.data[index]\n    def __getitem__(self, index):\n        if isinstance(index, slice):\n            return SubPattern(self.pattern, self.data[index])\n        return self.data[index]\n    def __setitem__(self, index, code):\n        self.data[index] = code\n    def insert(self, index, code):\n        self.data.insert(index, code)\n    def append(self, code):\n        self.data.append(code)\n    def getwidth(self):\n        # determine the width (min, max) for this subpattern\n        if self.width:\n            return self.width\n        lo = hi = 0\n        UNITCODES = (ANY, RANGE, IN, LITERAL, NOT_LITERAL, CATEGORY)\n        REPEATCODES = (MIN_REPEAT, MAX_REPEAT)\n        for op, av in self.data:\n            if op is BRANCH:\n                i = MAXREPEAT - 1\n                j = 0\n                for av in av[1]:\n                    l, h = av.getwidth()\n                    i = min(i, l)\n                    j = max(j, h)\n                lo = lo + i\n                hi = hi + j\n            elif op is CALL:\n                i, j = av.getwidth()\n                lo = lo + i\n                hi = hi + j\n            elif op is SUBPATTERN:\n                i, j = av[1].getwidth()\n                lo = lo + i\n                hi = hi + j\n            elif op in REPEATCODES:\n                i, j = av[2].getwidth()\n                lo = lo + i * av[0]\n                hi = hi + j * av[1]\n            elif op in UNITCODES:\n                lo = lo + 1\n                hi = hi + 1\n            elif op == SUCCESS:\n                break\n        self.width = min(lo, MAXREPEAT - 1), min(hi, MAXREPEAT)\n        return self.width\n\nclass Tokenizer:\n    def __init__(self, string):\n        self.string = string\n        self.index = 0\n        self.__next()\n    def __next(self):\n        if self.index >= len(self.string):\n            self.next = None\n            return\n        char = self.string[self.index]\n        if char[0] == \"\\\\\":\n            try:\n                c = self.string[self.index + 1]\n            except IndexError:\n                raise error, \"bogus escape (end of line)\"\n            char = char + c\n        self.index = self.index + len(char)\n        self.next = char\n    def match(self, char, skip=1):\n        if char == self.next:\n            if skip:\n                self.__next()\n            return 1\n        return 0\n    def get(self):\n        this = self.next\n        self.__next()\n        return this\n    def tell(self):\n        return self.index, self.next\n    def seek(self, index):\n        self.index, self.next = index\n\ndef isident(char):\n    return \"a\" <= char <= \"z\" or \"A\" <= char <= \"Z\" or char == \"_\"\n\ndef isdigit(char):\n    return \"0\" <= char <= \"9\"\n\ndef isname(name):\n    # check that group name is a valid string\n    if not isident(name[0]):\n        return False\n    for char in name[1:]:\n        if not isident(char) and not isdigit(char):\n            return False\n    return True\n\ndef _class_escape(source, escape):\n    # handle escape code inside character class\n    code = ESCAPES.get(escape)\n    if code:\n        return code\n    code = CATEGORIES.get(escape)\n    if code and code[0] == IN:\n        return code\n    try:\n        c = escape[1:2]\n        if c == \"x\":\n            # hexadecimal escape (exactly two digits)\n            while source.next in HEXDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            escape = escape[2:]\n            if len(escape) != 2:\n                raise error, \"bogus escape: %s\" % repr(\"\\\\\" + escape)\n            return LITERAL, int(escape, 16) & 0xff\n        elif c in OCTDIGITS:\n            # octal escape (up to three digits)\n            while source.next in OCTDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            escape = escape[1:]\n            return LITERAL, int(escape, 8) & 0xff\n        elif c in DIGITS:\n            raise error, \"bogus escape: %s\" % repr(escape)\n        if len(escape) == 2:\n            return LITERAL, ord(escape[1])\n    except ValueError:\n        pass\n    raise error, \"bogus escape: %s\" % repr(escape)\n\ndef _escape(source, escape, state):\n    # handle escape code in expression\n    code = CATEGORIES.get(escape)\n    if code:\n        return code\n    code = ESCAPES.get(escape)\n    if code:\n        return code\n    try:\n        c = escape[1:2]\n        if c == \"x\":\n            # hexadecimal escape\n            while source.next in HEXDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            if len(escape) != 4:\n                raise ValueError\n            return LITERAL, int(escape[2:], 16) & 0xff\n        elif c == \"0\":\n            # octal escape\n            while source.next in OCTDIGITS and len(escape) < 4:\n                escape = escape + source.get()\n            return LITERAL, int(escape[1:], 8) & 0xff\n        elif c in DIGITS:\n            # octal escape *or* decimal group reference (sigh)\n            if source.next in DIGITS:\n                escape = escape + source.get()\n                if (escape[1] in OCTDIGITS and escape[2] in OCTDIGITS and\n                    source.next in OCTDIGITS):\n                    # got three octal digits; this is an octal escape\n                    escape = escape + source.get()\n                    return LITERAL, int(escape[1:], 8) & 0xff\n            # not an octal escape, so this is a group reference\n            group = int(escape[1:])\n            if group < state.groups:\n                if not state.checkgroup(group):\n                    raise error, \"cannot refer to open group\"\n                return GROUPREF, group\n            raise ValueError\n        if len(escape) == 2:\n            return LITERAL, ord(escape[1])\n    except ValueError:\n        pass\n    raise error, \"bogus escape: %s\" % repr(escape)\n\ndef _parse_sub(source, state, nested=1):\n    # parse an alternation: a|b|c\n\n    items = []\n    itemsappend = items.append\n    sourcematch = source.match\n    while 1:\n        itemsappend(_parse(source, state))\n        if sourcematch(\"|\"):\n            continue\n        if not nested:\n            break\n        if not source.next or sourcematch(\")\", 0):\n            break\n        else:\n            raise error, \"pattern not properly closed\"\n\n    if len(items) == 1:\n        return items[0]\n\n    subpattern = SubPattern(state)\n    subpatternappend = subpattern.append\n\n    # check if all items share a common prefix\n    while 1:\n        prefix = None\n        for item in items:\n            if not item:\n                break\n            if prefix is None:\n                prefix = item[0]\n            elif item[0] != prefix:\n                break\n        else:\n            # all subitems start with a common \"prefix\".\n            # move it out of the branch\n            for item in items:\n                del item[0]\n            subpatternappend(prefix)\n            continue # check next one\n        break\n\n    # check if the branch can be replaced by a character set\n    for item in items:\n        if len(item) != 1 or item[0][0] != LITERAL:\n            break\n    else:\n        # we can store this as a character set instead of a\n        # branch (the compiler may optimize this even more)\n        set = []\n        setappend = set.append\n        for item in items:\n            setappend(item[0])\n        subpatternappend((IN, set))\n        return subpattern\n\n    subpattern.append((BRANCH, (None, items)))\n    return subpattern\n\ndef _parse_sub_cond(source, state, condgroup):\n    item_yes = _parse(source, state)\n    if source.match(\"|\"):\n        item_no = _parse(source, state)\n        if source.match(\"|\"):\n            raise error, \"conditional backref with more than two branches\"\n    else:\n        item_no = None\n    if source.next and not source.match(\")\", 0):\n        raise error, \"pattern not properly closed\"\n    subpattern = SubPattern(state)\n    subpattern.append((GROUPREF_EXISTS, (condgroup, item_yes, item_no)))\n    return subpattern\n\n_PATTERNENDERS = set(\"|)\")\n_ASSERTCHARS = set(\"=!<\")\n_LOOKBEHINDASSERTCHARS = set(\"=!\")\n_REPEATCODES = set([MIN_REPEAT, MAX_REPEAT])\n\ndef _parse(source, state):\n    # parse a simple pattern\n    subpattern = SubPattern(state)\n\n    # precompute constants into local variables\n    subpatternappend = subpattern.append\n    sourceget = source.get\n    sourcematch = source.match\n    _len = len\n    PATTERNENDERS = _PATTERNENDERS\n    ASSERTCHARS = _ASSERTCHARS\n    LOOKBEHINDASSERTCHARS = _LOOKBEHINDASSERTCHARS\n    REPEATCODES = _REPEATCODES\n\n    while 1:\n\n        if source.next in PATTERNENDERS:\n            break # end of subpattern\n        this = sourceget()\n        if this is None:\n            break # end of pattern\n\n        if state.flags & SRE_FLAG_VERBOSE:\n            # skip whitespace and comments\n            if this in WHITESPACE:\n                continue\n            if this == \"#\":\n                while 1:\n                    this = sourceget()\n                    if this in (None, \"\\n\"):\n                        break\n                continue\n\n        if this and this[0] not in SPECIAL_CHARS:\n            subpatternappend((LITERAL, ord(this)))\n\n        elif this == \"[\":\n            # character set\n            set = []\n            setappend = set.append\n##          if sourcematch(\":\"):\n##              pass # handle character classes\n            if sourcematch(\"^\"):\n                setappend((NEGATE, None))\n            # check remaining characters\n            start = set[:]\n            while 1:\n                this = sourceget()\n                if this == \"]\" and set != start:\n                    break\n                elif this and this[0] == \"\\\\\":\n                    code1 = _class_escape(source, this)\n                elif this:\n                    code1 = LITERAL, ord(this)\n                else:\n                    raise error, \"unexpected end of regular expression\"\n                if sourcematch(\"-\"):\n                    # potential range\n                    this = sourceget()\n                    if this == \"]\":\n                        if code1[0] is IN:\n                            code1 = code1[1][0]\n                        setappend(code1)\n                        setappend((LITERAL, ord(\"-\")))\n                        break\n                    elif this:\n                        if this[0] == \"\\\\\":\n                            code2 = _class_escape(source, this)\n                        else:\n                            code2 = LITERAL, ord(this)\n                        if code1[0] != LITERAL or code2[0] != LITERAL:\n                            raise error, \"bad character range\"\n                        lo = code1[1]\n                        hi = code2[1]\n                        if hi < lo:\n                            raise error, \"bad character range\"\n                        setappend((RANGE, (lo, hi)))\n                    else:\n                        raise error, \"unexpected end of regular expression\"\n                else:\n                    if code1[0] is IN:\n                        code1 = code1[1][0]\n                    setappend(code1)\n\n            # XXX: <fl> should move set optimization to compiler!\n            if _len(set)==1 and set[0][0] is LITERAL:\n                subpatternappend(set[0]) # optimization\n            elif _len(set)==2 and set[0][0] is NEGATE and set[1][0] is LITERAL:\n                subpatternappend((NOT_LITERAL, set[1][1])) # optimization\n            else:\n                # XXX: <fl> should add charmap optimization here\n                subpatternappend((IN, set))\n\n        elif this and this[0] in REPEAT_CHARS:\n            # repeat previous item\n            if this == \"?\":\n                min, max = 0, 1\n            elif this == \"*\":\n                min, max = 0, MAXREPEAT\n\n            elif this == \"+\":\n                min, max = 1, MAXREPEAT\n            elif this == \"{\":\n                if source.next == \"}\":\n                    subpatternappend((LITERAL, ord(this)))\n                    continue\n                here = source.tell()\n                min, max = 0, MAXREPEAT\n                lo = hi = \"\"\n                while source.next in DIGITS:\n                    lo = lo + source.get()\n                if sourcematch(\",\"):\n                    while source.next in DIGITS:\n                        hi = hi + sourceget()\n                else:\n                    hi = lo\n                if not sourcematch(\"}\"):\n                    subpatternappend((LITERAL, ord(this)))\n                    source.seek(here)\n                    continue\n                if lo:\n                    min = int(lo)\n                    if min >= MAXREPEAT:\n                        raise OverflowError(\"the repetition number is too large\")\n                if hi:\n                    max = int(hi)\n                    if max >= MAXREPEAT:\n                        raise OverflowError(\"the repetition number is too large\")\n                    if max < min:\n                        raise error(\"bad repeat interval\")\n            else:\n                raise error, \"not supported\"\n            # figure out which item to repeat\n            if subpattern:\n                item = subpattern[-1:]\n            else:\n                item = None\n            if not item or (_len(item) == 1 and item[0][0] == AT):\n                raise error, \"nothing to repeat\"\n            if item[0][0] in REPEATCODES:\n                raise error, \"multiple repeat\"\n            if sourcematch(\"?\"):\n                subpattern[-1] = (MIN_REPEAT, (min, max, item))\n            else:\n                subpattern[-1] = (MAX_REPEAT, (min, max, item))\n\n        elif this == \".\":\n            subpatternappend((ANY, None))\n\n        elif this == \"(\":\n            group = 1\n            name = None\n            condgroup = None\n            if sourcematch(\"?\"):\n                group = 0\n                # options\n                if sourcematch(\"P\"):\n                    # python extensions\n                    if sourcematch(\"<\"):\n                        # named group: skip forward to end of name\n                        name = \"\"\n                        while 1:\n                            char = sourceget()\n                            if char is None:\n                                raise error, \"unterminated name\"\n                            if char == \">\":\n                                break\n                            name = name + char\n                        group = 1\n                        if not name:\n                            raise error(\"missing group name\")\n                        if not isname(name):\n                            raise error(\"bad character in group name %r\" %\n                                        name)\n                    elif sourcematch(\"=\"):\n                        # named backreference\n                        name = \"\"\n                        while 1:\n                            char = sourceget()\n                            if char is None:\n                                raise error, \"unterminated name\"\n                            if char == \")\":\n                                break\n                            name = name + char\n                        if not name:\n                            raise error(\"missing group name\")\n                        if not isname(name):\n                            raise error(\"bad character in backref group name \"\n                                        \"%r\" % name)\n                        gid = state.groupdict.get(name)\n                        if gid is None:\n                            msg = \"unknown group name: {0!r}\".format(name)\n                            raise error(msg)\n                        subpatternappend((GROUPREF, gid))\n                        continue\n                    else:\n                        char = sourceget()\n                        if char is None:\n                            raise error, \"unexpected end of pattern\"\n                        raise error, \"unknown specifier: ?P%s\" % char\n                elif sourcematch(\":\"):\n                    # non-capturing group\n                    group = 2\n                elif sourcematch(\"#\"):\n                    # comment\n                    while 1:\n                        if source.next is None or source.next == \")\":\n                            break\n                        sourceget()\n                    if not sourcematch(\")\"):\n                        raise error, \"unbalanced parenthesis\"\n                    continue\n                elif source.next in ASSERTCHARS:\n                    # lookahead assertions\n                    char = sourceget()\n                    dir = 1\n                    if char == \"<\":\n                        if source.next not in LOOKBEHINDASSERTCHARS:\n                            raise error, \"syntax error\"\n                        dir = -1 # lookbehind\n                        char = sourceget()\n                    p = _parse_sub(source, state)\n                    if not sourcematch(\")\"):\n                        raise error, \"unbalanced parenthesis\"\n                    if char == \"=\":\n                        subpatternappend((ASSERT, (dir, p)))\n                    else:\n                        subpatternappend((ASSERT_NOT, (dir, p)))\n                    continue\n                elif sourcematch(\"(\"):\n                    # conditional backreference group\n                    condname = \"\"\n                    while 1:\n                        char = sourceget()\n                        if char is None:\n                            raise error, \"unterminated name\"\n                        if char == \")\":\n                            break\n                        condname = condname + char\n                    group = 2\n                    if not condname:\n                        raise error(\"missing group name\")\n                    if isname(condname):\n                        condgroup = state.groupdict.get(condname)\n                        if condgroup is None:\n                            msg = \"unknown group name: {0!r}\".format(condname)\n                            raise error(msg)\n                    else:\n                        try:\n                            condgroup = int(condname)\n                        except ValueError:\n                            raise error, \"bad character in group name\"\n                else:\n                    # flags\n                    if not source.next in FLAGS:\n                        raise error, \"unexpected end of pattern\"\n                    while source.next in FLAGS:\n                        state.flags = state.flags | FLAGS[sourceget()]\n            if group:\n                # parse group contents\n                if group == 2:\n                    # anonymous group\n                    group = None\n                else:\n                    group = state.opengroup(name)\n                if condgroup:\n                    p = _parse_sub_cond(source, state, condgroup)\n                else:\n                    p = _parse_sub(source, state)\n                if not sourcematch(\")\"):\n                    raise error, \"unbalanced parenthesis\"\n                if group is not None:\n                    state.closegroup(group)\n                subpatternappend((SUBPATTERN, (group, p)))\n            else:\n                while 1:\n                    char = sourceget()\n                    if char is None:\n                        raise error, \"unexpected end of pattern\"\n                    if char == \")\":\n                        break\n                    raise error, \"unknown extension\"\n\n        elif this == \"^\":\n            subpatternappend((AT, AT_BEGINNING))\n\n        elif this == \"$\":\n            subpattern.append((AT, AT_END))\n\n        elif this and this[0] == \"\\\\\":\n            code = _escape(source, this, state)\n            subpatternappend(code)\n\n        else:\n            raise error, \"parser error\"\n\n    return subpattern\n\ndef parse(str, flags=0, pattern=None):\n    # parse 're' pattern into list of (opcode, argument) tuples\n\n    source = Tokenizer(str)\n\n    if pattern is None:\n        pattern = Pattern()\n    pattern.flags = flags\n    pattern.str = str\n\n    p = _parse_sub(source, pattern, 0)\n\n    tail = source.get()\n    if tail == \")\":\n        raise error, \"unbalanced parenthesis\"\n    elif tail:\n        raise error, \"bogus characters at end of regular expression\"\n\n    if flags & SRE_FLAG_DEBUG:\n        p.dump()\n\n    if not (flags & SRE_FLAG_VERBOSE) and p.pattern.flags & SRE_FLAG_VERBOSE:\n        # the VERBOSE flag was switched on inside the pattern.  to be\n        # on the safe side, we'll parse the whole thing again...\n        return parse(str, p.pattern.flags)\n\n    return p\n\ndef parse_template(source, pattern):\n    # parse 're' replacement string into list of literals and\n    # group references\n    s = Tokenizer(source)\n    sget = s.get\n    p = []\n    a = p.append\n    def literal(literal, p=p, pappend=a):\n        if p and p[-1][0] is LITERAL:\n            p[-1] = LITERAL, p[-1][1] + literal\n        else:\n            pappend((LITERAL, literal))\n    sep = source[:0]\n    if type(sep) is type(\"\"):\n        makechar = chr\n    else:\n        makechar = unichr\n    while 1:\n        this = sget()\n        if this is None:\n            break # end of replacement string\n        if this and this[0] == \"\\\\\":\n            # group\n            c = this[1:2]\n            if c == \"g\":\n                name = \"\"\n                if s.match(\"<\"):\n                    while 1:\n                        char = sget()\n                        if char is None:\n                            raise error, \"unterminated group name\"\n                        if char == \">\":\n                            break\n                        name = name + char\n                if not name:\n                    raise error, \"missing group name\"\n                try:\n                    index = int(name)\n                    if index < 0:\n                        raise error, \"negative group number\"\n                except ValueError:\n                    if not isname(name):\n                        raise error, \"bad character in group name\"\n                    try:\n                        index = pattern.groupindex[name]\n                    except KeyError:\n                        msg = \"unknown group name: {0!r}\".format(name)\n                        raise IndexError(msg)\n                a((MARK, index))\n            elif c == \"0\":\n                if s.next in OCTDIGITS:\n                    this = this + sget()\n                    if s.next in OCTDIGITS:\n                        this = this + sget()\n                literal(makechar(int(this[1:], 8) & 0xff))\n            elif c in DIGITS:\n                isoctal = False\n                if s.next in DIGITS:\n                    this = this + sget()\n                    if (c in OCTDIGITS and this[2] in OCTDIGITS and\n                        s.next in OCTDIGITS):\n                        this = this + sget()\n                        isoctal = True\n                        literal(makechar(int(this[1:], 8) & 0xff))\n                if not isoctal:\n                    a((MARK, int(this[1:])))\n            else:\n                try:\n                    this = makechar(ESCAPES[this][1])\n                except KeyError:\n                    pass\n                literal(this)\n        else:\n            literal(this)\n    # convert template to groups and literals lists\n    i = 0\n    groups = []\n    groupsappend = groups.append\n    literals = [None] * len(p)\n    for c, s in p:\n        if c is MARK:\n            groupsappend((i, s))\n            # literal[i] is already None\n        else:\n            literals[i] = s\n        i = i + 1\n    return groups, literals\n\ndef expand_template(template, match):\n    g = match.group\n    sep = match.string[:0]\n    groups, literals = template\n    literals = literals[:]\n    try:\n        for index, group in groups:\n            literals[index] = s = g(group)\n            if s is None:\n                raise error, \"unmatched group\"\n    except IndexError:\n        raise error, \"invalid group reference\"\n    return sep.join(literals)\n", 
    "stat": "\"\"\"Constants/functions for interpreting results of os.stat() and os.lstat().\n\nSuggested usage: from stat import *\n\"\"\"\n\n# Indices for stat struct members in the tuple returned by os.stat()\n\nST_MODE  = 0\nST_INO   = 1\nST_DEV   = 2\nST_NLINK = 3\nST_UID   = 4\nST_GID   = 5\nST_SIZE  = 6\nST_ATIME = 7\nST_MTIME = 8\nST_CTIME = 9\n\n# Extract bits from the mode\n\ndef S_IMODE(mode):\n    return mode & 07777\n\ndef S_IFMT(mode):\n    return mode & 0170000\n\n# Constants used as S_IFMT() for various file types\n# (not all are implemented on all systems)\n\nS_IFDIR  = 0040000\nS_IFCHR  = 0020000\nS_IFBLK  = 0060000\nS_IFREG  = 0100000\nS_IFIFO  = 0010000\nS_IFLNK  = 0120000\nS_IFSOCK = 0140000\n\n# Functions to test for each file type\n\ndef S_ISDIR(mode):\n    return S_IFMT(mode) == S_IFDIR\n\ndef S_ISCHR(mode):\n    return S_IFMT(mode) == S_IFCHR\n\ndef S_ISBLK(mode):\n    return S_IFMT(mode) == S_IFBLK\n\ndef S_ISREG(mode):\n    return S_IFMT(mode) == S_IFREG\n\ndef S_ISFIFO(mode):\n    return S_IFMT(mode) == S_IFIFO\n\ndef S_ISLNK(mode):\n    return S_IFMT(mode) == S_IFLNK\n\ndef S_ISSOCK(mode):\n    return S_IFMT(mode) == S_IFSOCK\n\n# Names for permission bits\n\nS_ISUID = 04000\nS_ISGID = 02000\nS_ENFMT = S_ISGID\nS_ISVTX = 01000\nS_IREAD = 00400\nS_IWRITE = 00200\nS_IEXEC = 00100\nS_IRWXU = 00700\nS_IRUSR = 00400\nS_IWUSR = 00200\nS_IXUSR = 00100\nS_IRWXG = 00070\nS_IRGRP = 00040\nS_IWGRP = 00020\nS_IXGRP = 00010\nS_IRWXO = 00007\nS_IROTH = 00004\nS_IWOTH = 00002\nS_IXOTH = 00001\n\n# Names for file flags\n\nUF_NODUMP    = 0x00000001\nUF_IMMUTABLE = 0x00000002\nUF_APPEND    = 0x00000004\nUF_OPAQUE    = 0x00000008\nUF_NOUNLINK  = 0x00000010\nUF_COMPRESSED = 0x00000020  # OS X: file is hfs-compressed\nUF_HIDDEN    = 0x00008000   # OS X: file should not be displayed\nSF_ARCHIVED  = 0x00010000\nSF_IMMUTABLE = 0x00020000\nSF_APPEND    = 0x00040000\nSF_NOUNLINK  = 0x00100000\nSF_SNAPSHOT  = 0x00200000\n", 
    "string": "\"\"\"A collection of string operations (most are no longer used).\n\nWarning: most of the code you see here isn't normally used nowadays.\nBeginning with Python 1.6, many of these functions are implemented as\nmethods on the standard string object. They used to be implemented by\na built-in module called strop, but strop is now obsolete itself.\n\nPublic module variables:\n\nwhitespace -- a string containing all characters considered whitespace\nlowercase -- a string containing all characters considered lowercase letters\nuppercase -- a string containing all characters considered uppercase letters\nletters -- a string containing all characters considered letters\ndigits -- a string containing all characters considered decimal digits\nhexdigits -- a string containing all characters considered hexadecimal digits\noctdigits -- a string containing all characters considered octal digits\npunctuation -- a string containing all characters considered punctuation\nprintable -- a string containing all characters considered printable\n\n\"\"\"\n\n# Some strings for ctype-style character classification\nwhitespace = ' \\t\\n\\r\\v\\f'\nlowercase = 'abcdefghijklmnopqrstuvwxyz'\nuppercase = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\nletters = lowercase + uppercase\nascii_lowercase = lowercase\nascii_uppercase = uppercase\nascii_letters = ascii_lowercase + ascii_uppercase\ndigits = '0123456789'\nhexdigits = digits + 'abcdef' + 'ABCDEF'\noctdigits = '01234567'\npunctuation = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\nprintable = digits + letters + punctuation + whitespace\n\n# Case conversion helpers\n# Use str to convert Unicode literal in case of -U\nl = map(chr, xrange(256))\n_idmap = str('').join(l)\ndel l\n\n# Functions which aren't available as string methods.\n\n# Capitalize the words in a string, e.g. \" aBc  dEf \" -> \"Abc Def\".\ndef capwords(s, sep=None):\n    \"\"\"capwords(s [,sep]) -> string\n\n    Split the argument into words using split, capitalize each\n    word using capitalize, and join the capitalized words using\n    join.  If the optional second argument sep is absent or None,\n    runs of whitespace characters are replaced by a single space\n    and leading and trailing whitespace are removed, otherwise\n    sep is used to split and join the words.\n\n    \"\"\"\n    return (sep or ' ').join(x.capitalize() for x in s.split(sep))\n\n\n# Construct a translation string\n_idmapL = None\ndef maketrans(fromstr, tostr):\n    \"\"\"maketrans(frm, to) -> string\n\n    Return a translation table (a string of 256 bytes long)\n    suitable for use in string.translate.  The strings frm and to\n    must be of the same length.\n\n    \"\"\"\n    n = len(fromstr)\n    if n != len(tostr):\n        raise ValueError, \"maketrans arguments must have same length\"\n    # this function has been rewritten to suit PyPy better; it is\n    # almost 10x faster than the original.\n    buf = bytearray(256)\n    for i in range(256):\n        buf[i] = i\n    for i in range(n):\n        buf[ord(fromstr[i])] = tostr[i]\n    return str(buf)\n\n\n\n####################################################################\nimport re as _re\n\nclass _multimap:\n    \"\"\"Helper class for combining multiple mappings.\n\n    Used by .{safe_,}substitute() to combine the mapping and keyword\n    arguments.\n    \"\"\"\n    def __init__(self, primary, secondary):\n        self._primary = primary\n        self._secondary = secondary\n\n    def __getitem__(self, key):\n        try:\n            return self._primary[key]\n        except KeyError:\n            return self._secondary[key]\n\n\nclass _TemplateMetaclass(type):\n    pattern = r\"\"\"\n    %(delim)s(?:\n      (?P<escaped>%(delim)s) |   # Escape sequence of two delimiters\n      (?P<named>%(id)s)      |   # delimiter and a Python identifier\n      {(?P<braced>%(id)s)}   |   # delimiter and a braced identifier\n      (?P<invalid>)              # Other ill-formed delimiter exprs\n    )\n    \"\"\"\n\n    def __init__(cls, name, bases, dct):\n        super(_TemplateMetaclass, cls).__init__(name, bases, dct)\n        if 'pattern' in dct:\n            pattern = cls.pattern\n        else:\n            pattern = _TemplateMetaclass.pattern % {\n                'delim' : _re.escape(cls.delimiter),\n                'id'    : cls.idpattern,\n                }\n        cls.pattern = _re.compile(pattern, _re.IGNORECASE | _re.VERBOSE)\n\n\nclass Template:\n    \"\"\"A string class for supporting $-substitutions.\"\"\"\n    __metaclass__ = _TemplateMetaclass\n\n    delimiter = '$'\n    idpattern = r'[_a-z][_a-z0-9]*'\n\n    def __init__(self, template):\n        self.template = template\n\n    # Search for $$, $identifier, ${identifier}, and any bare $'s\n\n    def _invalid(self, mo):\n        i = mo.start('invalid')\n        lines = self.template[:i].splitlines(True)\n        if not lines:\n            colno = 1\n            lineno = 1\n        else:\n            colno = i - len(''.join(lines[:-1]))\n            lineno = len(lines)\n        raise ValueError('Invalid placeholder in string: line %d, col %d' %\n                         (lineno, colno))\n\n    def substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            # Check the most common path first.\n            named = mo.group('named') or mo.group('braced')\n            if named is not None:\n                val = mapping[named]\n                # We use this idiom instead of str() because the latter will\n                # fail if val is a Unicode containing non-ASCII characters.\n                return '%s' % (val,)\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                self._invalid(mo)\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n    def safe_substitute(self, *args, **kws):\n        if len(args) > 1:\n            raise TypeError('Too many positional arguments')\n        if not args:\n            mapping = kws\n        elif kws:\n            mapping = _multimap(kws, args[0])\n        else:\n            mapping = args[0]\n        # Helper function for .sub()\n        def convert(mo):\n            named = mo.group('named') or mo.group('braced')\n            if named is not None:\n                try:\n                    # We use this idiom instead of str() because the latter\n                    # will fail if val is a Unicode containing non-ASCII\n                    return '%s' % (mapping[named],)\n                except KeyError:\n                    return mo.group()\n            if mo.group('escaped') is not None:\n                return self.delimiter\n            if mo.group('invalid') is not None:\n                return mo.group()\n            raise ValueError('Unrecognized named group in pattern',\n                             self.pattern)\n        return self.pattern.sub(convert, self.template)\n\n\n\n####################################################################\n# NOTE: Everything below here is deprecated.  Use string methods instead.\n# This stuff will go away in Python 3.0.\n\n# Backward compatible names for exceptions\nindex_error = ValueError\natoi_error = ValueError\natof_error = ValueError\natol_error = ValueError\n\n# convert UPPER CASE letters to lower case\ndef lower(s):\n    \"\"\"lower(s) -> string\n\n    Return a copy of the string s converted to lowercase.\n\n    \"\"\"\n    return s.lower()\n\n# Convert lower case letters to UPPER CASE\ndef upper(s):\n    \"\"\"upper(s) -> string\n\n    Return a copy of the string s converted to uppercase.\n\n    \"\"\"\n    return s.upper()\n\n# Swap lower case letters and UPPER CASE\ndef swapcase(s):\n    \"\"\"swapcase(s) -> string\n\n    Return a copy of the string s with upper case characters\n    converted to lowercase and vice versa.\n\n    \"\"\"\n    return s.swapcase()\n\n# Strip leading and trailing tabs and spaces\ndef strip(s, chars=None):\n    \"\"\"strip(s [,chars]) -> string\n\n    Return a copy of the string s with leading and trailing\n    whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n    If chars is unicode, S will be converted to unicode before stripping.\n\n    \"\"\"\n    return s.strip(chars)\n\n# Strip leading tabs and spaces\ndef lstrip(s, chars=None):\n    \"\"\"lstrip(s [,chars]) -> string\n\n    Return a copy of the string s with leading whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.lstrip(chars)\n\n# Strip trailing tabs and spaces\ndef rstrip(s, chars=None):\n    \"\"\"rstrip(s [,chars]) -> string\n\n    Return a copy of the string s with trailing whitespace removed.\n    If chars is given and not None, remove characters in chars instead.\n\n    \"\"\"\n    return s.rstrip(chars)\n\n\n# Split a string into a list of space/tab-separated words\ndef split(s, sep=None, maxsplit=-1):\n    \"\"\"split(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string.  If maxsplit is given, splits at no more than\n    maxsplit places (resulting in at most maxsplit+1 words).  If sep\n    is not specified or is None, any whitespace string is a separator.\n\n    (split and splitfields are synonymous)\n\n    \"\"\"\n    return s.split(sep, maxsplit)\nsplitfields = split\n\n# Split a string into a list of space/tab-separated words\ndef rsplit(s, sep=None, maxsplit=-1):\n    \"\"\"rsplit(s [,sep [,maxsplit]]) -> list of strings\n\n    Return a list of the words in the string s, using sep as the\n    delimiter string, starting at the end of the string and working\n    to the front.  If maxsplit is given, at most maxsplit splits are\n    done. If sep is not specified or is None, any whitespace string\n    is a separator.\n    \"\"\"\n    return s.rsplit(sep, maxsplit)\n\n# Join fields with optional separator\ndef join(words, sep = ' '):\n    \"\"\"join(list [,sep]) -> string\n\n    Return a string composed of the words in list, with\n    intervening occurrences of sep.  The default separator is a\n    single space.\n\n    (joinfields and join are synonymous)\n\n    \"\"\"\n    return sep.join(words)\njoinfields = join\n\n# Find substring, raise exception if not found\ndef index(s, *args):\n    \"\"\"index(s, sub [,start [,end]]) -> int\n\n    Like find but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.index(*args)\n\n# Find last substring, raise exception if not found\ndef rindex(s, *args):\n    \"\"\"rindex(s, sub [,start [,end]]) -> int\n\n    Like rfind but raises ValueError when the substring is not found.\n\n    \"\"\"\n    return s.rindex(*args)\n\n# Count non-overlapping occurrences of substring\ndef count(s, *args):\n    \"\"\"count(s, sub[, start[,end]]) -> int\n\n    Return the number of occurrences of substring sub in string\n    s[start:end].  Optional arguments start and end are\n    interpreted as in slice notation.\n\n    \"\"\"\n    return s.count(*args)\n\n# Find substring, return -1 if not found\ndef find(s, *args):\n    \"\"\"find(s, sub [,start [,end]]) -> in\n\n    Return the lowest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.find(*args)\n\n# Find last substring, return -1 if not found\ndef rfind(s, *args):\n    \"\"\"rfind(s, sub [,start [,end]]) -> int\n\n    Return the highest index in s where substring sub is found,\n    such that sub is contained within s[start,end].  Optional\n    arguments start and end are interpreted as in slice notation.\n\n    Return -1 on failure.\n\n    \"\"\"\n    return s.rfind(*args)\n\n# for a bit of speed\n_float = float\n_int = int\n_long = long\n\n# Convert string to float\ndef atof(s):\n    \"\"\"atof(s) -> float\n\n    Return the floating point number represented by the string s.\n\n    \"\"\"\n    return _float(s)\n\n\n# Convert string to integer\ndef atoi(s , base=10):\n    \"\"\"atoi(s [,base]) -> int\n\n    Return the integer represented by the string s in the given\n    base, which defaults to 10.  The string s must consist of one\n    or more digits, possibly preceded by a sign.  If base is 0, it\n    is chosen from the leading characters of s, 0 for octal, 0x or\n    0X for hexadecimal.  If base is 16, a preceding 0x or 0X is\n    accepted.\n\n    \"\"\"\n    return _int(s, base)\n\n\n# Convert string to long integer\ndef atol(s, base=10):\n    \"\"\"atol(s [,base]) -> long\n\n    Return the long integer represented by the string s in the\n    given base, which defaults to 10.  The string s must consist\n    of one or more digits, possibly preceded by a sign.  If base\n    is 0, it is chosen from the leading characters of s, 0 for\n    octal, 0x or 0X for hexadecimal.  If base is 16, a preceding\n    0x or 0X is accepted.  A trailing L or l is not accepted,\n    unless base is 0.\n\n    \"\"\"\n    return _long(s, base)\n\n\n# Left-justify a string\ndef ljust(s, width, *args):\n    \"\"\"ljust(s, width[, fillchar]) -> string\n\n    Return a left-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.ljust(width, *args)\n\n# Right-justify a string\ndef rjust(s, width, *args):\n    \"\"\"rjust(s, width[, fillchar]) -> string\n\n    Return a right-justified version of s, in a field of the\n    specified width, padded with spaces as needed.  The string is\n    never truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.rjust(width, *args)\n\n# Center a string\ndef center(s, width, *args):\n    \"\"\"center(s, width[, fillchar]) -> string\n\n    Return a center version of s, in a field of the specified\n    width. padded with spaces as needed.  The string is never\n    truncated.  If specified the fillchar is used instead of spaces.\n\n    \"\"\"\n    return s.center(width, *args)\n\n# Zero-fill a number, e.g., (12, 3) --> '012' and (-3, 3) --> '-03'\n# Decadent feature: the argument may be a string or a number\n# (Use of this is deprecated; it should be a string as with ljust c.s.)\ndef zfill(x, width):\n    \"\"\"zfill(x, width) -> string\n\n    Pad a numeric string x with zeros on the left, to fill a field\n    of the specified width.  The string x is never truncated.\n\n    \"\"\"\n    if not isinstance(x, basestring):\n        x = repr(x)\n    return x.zfill(width)\n\n# Expand tabs in a string.\n# Doesn't take non-printing chars into account, but does understand \\n.\ndef expandtabs(s, tabsize=8):\n    \"\"\"expandtabs(s [,tabsize]) -> string\n\n    Return a copy of the string s with all tab characters replaced\n    by the appropriate number of spaces, depending on the current\n    column, and the tabsize (default 8).\n\n    \"\"\"\n    return s.expandtabs(tabsize)\n\n# Character translation through look-up table.\ndef translate(s, table, deletions=\"\"):\n    \"\"\"translate(s,table [,deletions]) -> string\n\n    Return a copy of the string s, where all characters occurring\n    in the optional argument deletions are removed, and the\n    remaining characters have been mapped through the given\n    translation table, which must be a string of length 256.  The\n    deletions argument is not allowed for Unicode strings.\n\n    \"\"\"\n    if deletions or table is None:\n        return s.translate(table, deletions)\n    else:\n        # Add s[:0] so that if s is Unicode and table is an 8-bit string,\n        # table is converted to Unicode.  This means that table *cannot*\n        # be a dictionary -- for that feature, use u.translate() directly.\n        return s.translate(table + s[:0])\n\n# Capitalize a string, e.g. \"aBc  dEf\" -> \"Abc  def\".\ndef capitalize(s):\n    \"\"\"capitalize(s) -> string\n\n    Return a copy of the string s with only its first character\n    capitalized.\n\n    \"\"\"\n    return s.capitalize()\n\n# Substring replacement (global)\ndef replace(s, old, new, maxreplace=-1):\n    \"\"\"replace (str, old, new[, maxreplace]) -> string\n\n    Return a copy of string str with all occurrences of substring\n    old replaced by new. If the optional argument maxreplace is\n    given, only the first maxreplace occurrences are replaced.\n\n    \"\"\"\n    return s.replace(old, new, maxreplace)\n\n\n# Try importing optional built-in module \"strop\" -- if it exists,\n# it redefines some string operations that are 100-1000 times faster.\n# It also defines values for whitespace, lowercase and uppercase\n# that match <ctype.h>'s definitions.\n\ntry:\n    from strop import maketrans, lowercase, uppercase, whitespace\n    letters = lowercase + uppercase\nexcept ImportError:\n    pass                                          # Use the original versions\n\n########################################################################\n# the Formatter class\n# see PEP 3101 for details and purpose of this class\n\n# The hard parts are reused from the C implementation.  They're exposed as \"_\"\n# prefixed methods of str and unicode.\n\n# The overall parser is implemented in str._formatter_parser.\n# The field name parser is implemented in str._formatter_field_name_split\n\nclass Formatter(object):\n    def format(self, format_string, *args, **kwargs):\n        return self.vformat(format_string, args, kwargs)\n\n    def vformat(self, format_string, args, kwargs):\n        used_args = set()\n        result = self._vformat(format_string, args, kwargs, used_args, 2)\n        self.check_unused_args(used_args, args, kwargs)\n        return result\n\n    def _vformat(self, format_string, args, kwargs, used_args, recursion_depth):\n        if recursion_depth < 0:\n            raise ValueError('Max string recursion exceeded')\n        result = []\n        for literal_text, field_name, format_spec, conversion in \\\n                self.parse(format_string):\n\n            # output the literal text\n            if literal_text:\n                result.append(literal_text)\n\n            # if there's a field, output it\n            if field_name is not None:\n                # this is some markup, find the object and do\n                #  the formatting\n\n                # given the field_name, find the object it references\n                #  and the argument it came from\n                obj, arg_used = self.get_field(field_name, args, kwargs)\n                used_args.add(arg_used)\n\n                # do any conversion on the resulting object\n                obj = self.convert_field(obj, conversion)\n\n                # expand the format spec, if needed\n                format_spec = self._vformat(format_spec, args, kwargs,\n                                            used_args, recursion_depth-1)\n\n                # format the object and append to the result\n                result.append(self.format_field(obj, format_spec))\n\n        return ''.join(result)\n\n\n    def get_value(self, key, args, kwargs):\n        if isinstance(key, (int, long)):\n            return args[key]\n        else:\n            return kwargs[key]\n\n\n    def check_unused_args(self, used_args, args, kwargs):\n        pass\n\n\n    def format_field(self, value, format_spec):\n        return format(value, format_spec)\n\n\n    def convert_field(self, value, conversion):\n        # do any conversion on the resulting object\n        if conversion is None:\n            return value\n        elif conversion == 's':\n            return str(value)\n        elif conversion == 'r':\n            return repr(value)\n        raise ValueError(\"Unknown conversion specifier {0!s}\".format(conversion))\n\n\n    # returns an iterable that contains tuples of the form:\n    # (literal_text, field_name, format_spec, conversion)\n    # literal_text can be zero length\n    # field_name can be None, in which case there's no\n    #  object to format and output\n    # if field_name is not None, it is looked up, formatted\n    #  with format_spec and conversion and then used\n    def parse(self, format_string):\n        return format_string._formatter_parser()\n\n\n    # given a field_name, find the object it references.\n    #  field_name:   the field being looked up, e.g. \"0.name\"\n    #                 or \"lookup[3]\"\n    #  used_args:    a set of which args have been used\n    #  args, kwargs: as passed in to vformat\n    def get_field(self, field_name, args, kwargs):\n        first, rest = field_name._formatter_field_name_split()\n\n        obj = self.get_value(first, args, kwargs)\n\n        # loop through the rest of the field_name, doing\n        #  getattr or getitem as needed\n        for is_attr, i in rest:\n            if is_attr:\n                obj = getattr(obj, i)\n            else:\n                obj = obj[i]\n\n        return obj, first\n", 
    "struct": "from _struct import *\nfrom _struct import _clearcache\nfrom _struct import __doc__\n", 
    "subprocess": "_args_from_interpreter_flags = lambda *x:[]\n", 
    "tarfile": "# -*- coding: utf-8 -*-\n#-------------------------------------------------------------------\n# tarfile.py\n#-------------------------------------------------------------------\n# Copyright (C) 2002 Lars Gust\u00e4bel <lars@gustaebel.de>\n# All rights reserved.\n#\n# Permission  is  hereby granted,  free  of charge,  to  any person\n# obtaining a  copy of  this software  and associated documentation\n# files  (the  \"Software\"),  to   deal  in  the  Software   without\n# restriction,  including  without limitation  the  rights to  use,\n# copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies  of  the  Software,  and to  permit  persons  to  whom the\n# Software  is  furnished  to  do  so,  subject  to  the  following\n# conditions:\n#\n# The above copyright  notice and this  permission notice shall  be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS  IS\", WITHOUT WARRANTY OF ANY  KIND,\n# EXPRESS OR IMPLIED, INCLUDING  BUT NOT LIMITED TO  THE WARRANTIES\n# OF  MERCHANTABILITY,  FITNESS   FOR  A  PARTICULAR   PURPOSE  AND\n# NONINFRINGEMENT.  IN  NO  EVENT SHALL  THE  AUTHORS  OR COPYRIGHT\n# HOLDERS  BE LIABLE  FOR ANY  CLAIM, DAMAGES  OR OTHER  LIABILITY,\n# WHETHER  IN AN  ACTION OF  CONTRACT, TORT  OR OTHERWISE,  ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n# OTHER DEALINGS IN THE SOFTWARE.\n#\n\"\"\"Read from and write to tar format archives.\n\"\"\"\n\n__version__ = \"$Revision: 85213 $\"\n# $Source$\n\nversion     = \"0.9.0\"\n__author__  = \"Lars Gust\u00e4bel (lars@gustaebel.de)\"\n__date__    = \"$Date$\"\n__cvsid__   = \"$Id$\"\n__credits__ = \"Gustavo Niemeyer, Niels Gust\u00e4bel, Richard Townsend.\"\n\n#---------\n# Imports\n#---------\nimport sys\nimport os\nimport shutil\nimport stat\nimport errno\nimport time\nimport struct\nimport copy\nimport re\nimport operator\n\ntry:\n    import grp, pwd\nexcept ImportError:\n    grp = pwd = None\n\n# from tarfile import *\n__all__ = [\"TarFile\", \"TarInfo\", \"is_tarfile\", \"TarError\"]\n\n#---------------------------------------------------------\n# tar constants\n#---------------------------------------------------------\nNUL = \"\\0\"                      # the null character\nBLOCKSIZE = 512                 # length of processing blocks\nRECORDSIZE = BLOCKSIZE * 20     # length of records\nGNU_MAGIC = \"ustar  \\0\"         # magic gnu tar string\nPOSIX_MAGIC = \"ustar\\x0000\"     # magic posix tar string\n\nLENGTH_NAME = 100               # maximum length of a filename\nLENGTH_LINK = 100               # maximum length of a linkname\nLENGTH_PREFIX = 155             # maximum length of the prefix field\n\nREGTYPE = \"0\"                   # regular file\nAREGTYPE = \"\\0\"                 # regular file\nLNKTYPE = \"1\"                   # link (inside tarfile)\nSYMTYPE = \"2\"                   # symbolic link\nCHRTYPE = \"3\"                   # character special device\nBLKTYPE = \"4\"                   # block special device\nDIRTYPE = \"5\"                   # directory\nFIFOTYPE = \"6\"                  # fifo special device\nCONTTYPE = \"7\"                  # contiguous file\n\nGNUTYPE_LONGNAME = \"L\"          # GNU tar longname\nGNUTYPE_LONGLINK = \"K\"          # GNU tar longlink\nGNUTYPE_SPARSE = \"S\"            # GNU tar sparse file\n\nXHDTYPE = \"x\"                   # POSIX.1-2001 extended header\nXGLTYPE = \"g\"                   # POSIX.1-2001 global header\nSOLARIS_XHDTYPE = \"X\"           # Solaris extended header\n\nUSTAR_FORMAT = 0                # POSIX.1-1988 (ustar) format\nGNU_FORMAT = 1                  # GNU tar format\nPAX_FORMAT = 2                  # POSIX.1-2001 (pax) format\nDEFAULT_FORMAT = GNU_FORMAT\n\n#---------------------------------------------------------\n# tarfile constants\n#---------------------------------------------------------\n# File types that tarfile supports:\nSUPPORTED_TYPES = (REGTYPE, AREGTYPE, LNKTYPE,\n                   SYMTYPE, DIRTYPE, FIFOTYPE,\n                   CONTTYPE, CHRTYPE, BLKTYPE,\n                   GNUTYPE_LONGNAME, GNUTYPE_LONGLINK,\n                   GNUTYPE_SPARSE)\n\n# File types that will be treated as a regular file.\nREGULAR_TYPES = (REGTYPE, AREGTYPE,\n                 CONTTYPE, GNUTYPE_SPARSE)\n\n# File types that are part of the GNU tar format.\nGNU_TYPES = (GNUTYPE_LONGNAME, GNUTYPE_LONGLINK,\n             GNUTYPE_SPARSE)\n\n# Fields from a pax header that override a TarInfo attribute.\nPAX_FIELDS = (\"path\", \"linkpath\", \"size\", \"mtime\",\n              \"uid\", \"gid\", \"uname\", \"gname\")\n\n# Fields in a pax header that are numbers, all other fields\n# are treated as strings.\nPAX_NUMBER_FIELDS = {\n    \"atime\": float,\n    \"ctime\": float,\n    \"mtime\": float,\n    \"uid\": int,\n    \"gid\": int,\n    \"size\": int\n}\n\n#---------------------------------------------------------\n# Bits used in the mode field, values in octal.\n#---------------------------------------------------------\nS_IFLNK = 0120000        # symbolic link\nS_IFREG = 0100000        # regular file\nS_IFBLK = 0060000        # block device\nS_IFDIR = 0040000        # directory\nS_IFCHR = 0020000        # character device\nS_IFIFO = 0010000        # fifo\n\nTSUID   = 04000          # set UID on execution\nTSGID   = 02000          # set GID on execution\nTSVTX   = 01000          # reserved\n\nTUREAD  = 0400           # read by owner\nTUWRITE = 0200           # write by owner\nTUEXEC  = 0100           # execute/search by owner\nTGREAD  = 0040           # read by group\nTGWRITE = 0020           # write by group\nTGEXEC  = 0010           # execute/search by group\nTOREAD  = 0004           # read by other\nTOWRITE = 0002           # write by other\nTOEXEC  = 0001           # execute/search by other\n\n#---------------------------------------------------------\n# initialization\n#---------------------------------------------------------\nENCODING = sys.getfilesystemencoding()\nif ENCODING is None:\n    ENCODING = sys.getdefaultencoding()\n\n#---------------------------------------------------------\n# Some useful functions\n#---------------------------------------------------------\n\ndef stn(s, length):\n    \"\"\"Convert a python string to a null-terminated string buffer.\n    \"\"\"\n    return s[:length] + (length - len(s)) * NUL\n\ndef nts(s):\n    \"\"\"Convert a null-terminated string field to a python string.\n    \"\"\"\n    # Use the string up to the first null char.\n    p = s.find(\"\\0\")\n    if p == -1:\n        return s\n    return s[:p]\n\ndef nti(s):\n    \"\"\"Convert a number field to a python number.\n    \"\"\"\n    # There are two possible encodings for a number field, see\n    # itn() below.\n    if s[0] != chr(0200):\n        try:\n            n = int(nts(s) or \"0\", 8)\n        except ValueError:\n            raise InvalidHeaderError(\"invalid header\")\n    else:\n        n = 0L\n        for i in xrange(len(s) - 1):\n            n <<= 8\n            n += ord(s[i + 1])\n    return n\n\ndef itn(n, digits=8, format=DEFAULT_FORMAT):\n    \"\"\"Convert a python number to a number field.\n    \"\"\"\n    # POSIX 1003.1-1988 requires numbers to be encoded as a string of\n    # octal digits followed by a null-byte, this allows values up to\n    # (8**(digits-1))-1. GNU tar allows storing numbers greater than\n    # that if necessary. A leading 0200 byte indicates this particular\n    # encoding, the following digits-1 bytes are a big-endian\n    # representation. This allows values up to (256**(digits-1))-1.\n    if 0 <= n < 8 ** (digits - 1):\n        s = \"%0*o\" % (digits - 1, n) + NUL\n    else:\n        if format != GNU_FORMAT or n >= 256 ** (digits - 1):\n            raise ValueError(\"overflow in number field\")\n\n        if n < 0:\n            # XXX We mimic GNU tar's behaviour with negative numbers,\n            # this could raise OverflowError.\n            n = struct.unpack(\"L\", struct.pack(\"l\", n))[0]\n\n        s = \"\"\n        for i in xrange(digits - 1):\n            s = chr(n & 0377) + s\n            n >>= 8\n        s = chr(0200) + s\n    return s\n\ndef uts(s, encoding, errors):\n    \"\"\"Convert a unicode object to a string.\n    \"\"\"\n    if errors == \"utf-8\":\n        # An extra error handler similar to the -o invalid=UTF-8 option\n        # in POSIX.1-2001. Replace untranslatable characters with their\n        # UTF-8 representation.\n        try:\n            return s.encode(encoding, \"strict\")\n        except UnicodeEncodeError:\n            x = []\n            for c in s:\n                try:\n                    x.append(c.encode(encoding, \"strict\"))\n                except UnicodeEncodeError:\n                    x.append(c.encode(\"utf8\"))\n            return \"\".join(x)\n    else:\n        return s.encode(encoding, errors)\n\ndef calc_chksums(buf):\n    \"\"\"Calculate the checksum for a member's header by summing up all\n       characters except for the chksum field which is treated as if\n       it was filled with spaces. According to the GNU tar sources,\n       some tars (Sun and NeXT) calculate chksum with signed char,\n       which will be different if there are chars in the buffer with\n       the high bit set. So we calculate two checksums, unsigned and\n       signed.\n    \"\"\"\n    unsigned_chksum = 256 + sum(struct.unpack(\"148B\", buf[:148]) + struct.unpack(\"356B\", buf[156:512]))\n    signed_chksum = 256 + sum(struct.unpack(\"148b\", buf[:148]) + struct.unpack(\"356b\", buf[156:512]))\n    return unsigned_chksum, signed_chksum\n\ndef copyfileobj(src, dst, length=None):\n    \"\"\"Copy length bytes from fileobj src to fileobj dst.\n       If length is None, copy the entire content.\n    \"\"\"\n    if length == 0:\n        return\n    if length is None:\n        shutil.copyfileobj(src, dst)\n        return\n\n    BUFSIZE = 16 * 1024\n    blocks, remainder = divmod(length, BUFSIZE)\n    for b in xrange(blocks):\n        buf = src.read(BUFSIZE)\n        if len(buf) < BUFSIZE:\n            raise IOError(\"end of file reached\")\n        dst.write(buf)\n\n    if remainder != 0:\n        buf = src.read(remainder)\n        if len(buf) < remainder:\n            raise IOError(\"end of file reached\")\n        dst.write(buf)\n    return\n\nfilemode_table = (\n    ((S_IFLNK,      \"l\"),\n     (S_IFREG,      \"-\"),\n     (S_IFBLK,      \"b\"),\n     (S_IFDIR,      \"d\"),\n     (S_IFCHR,      \"c\"),\n     (S_IFIFO,      \"p\")),\n\n    ((TUREAD,       \"r\"),),\n    ((TUWRITE,      \"w\"),),\n    ((TUEXEC|TSUID, \"s\"),\n     (TSUID,        \"S\"),\n     (TUEXEC,       \"x\")),\n\n    ((TGREAD,       \"r\"),),\n    ((TGWRITE,      \"w\"),),\n    ((TGEXEC|TSGID, \"s\"),\n     (TSGID,        \"S\"),\n     (TGEXEC,       \"x\")),\n\n    ((TOREAD,       \"r\"),),\n    ((TOWRITE,      \"w\"),),\n    ((TOEXEC|TSVTX, \"t\"),\n     (TSVTX,        \"T\"),\n     (TOEXEC,       \"x\"))\n)\n\ndef filemode(mode):\n    \"\"\"Convert a file's mode to a string of the form\n       -rwxrwxrwx.\n       Used by TarFile.list()\n    \"\"\"\n    perm = []\n    for table in filemode_table:\n        for bit, char in table:\n            if mode & bit == bit:\n                perm.append(char)\n                break\n        else:\n            perm.append(\"-\")\n    return \"\".join(perm)\n\nclass TarError(Exception):\n    \"\"\"Base exception.\"\"\"\n    pass\nclass ExtractError(TarError):\n    \"\"\"General exception for extract errors.\"\"\"\n    pass\nclass ReadError(TarError):\n    \"\"\"Exception for unreadable tar archives.\"\"\"\n    pass\nclass CompressionError(TarError):\n    \"\"\"Exception for unavailable compression methods.\"\"\"\n    pass\nclass StreamError(TarError):\n    \"\"\"Exception for unsupported operations on stream-like TarFiles.\"\"\"\n    pass\nclass HeaderError(TarError):\n    \"\"\"Base exception for header errors.\"\"\"\n    pass\nclass EmptyHeaderError(HeaderError):\n    \"\"\"Exception for empty headers.\"\"\"\n    pass\nclass TruncatedHeaderError(HeaderError):\n    \"\"\"Exception for truncated headers.\"\"\"\n    pass\nclass EOFHeaderError(HeaderError):\n    \"\"\"Exception for end of file headers.\"\"\"\n    pass\nclass InvalidHeaderError(HeaderError):\n    \"\"\"Exception for invalid headers.\"\"\"\n    pass\nclass SubsequentHeaderError(HeaderError):\n    \"\"\"Exception for missing and invalid extended headers.\"\"\"\n    pass\n\n#---------------------------\n# internal stream interface\n#---------------------------\nclass _LowLevelFile:\n    \"\"\"Low-level file object. Supports reading and writing.\n       It is used instead of a regular file object for streaming\n       access.\n    \"\"\"\n\n    def __init__(self, name, mode):\n        mode = {\n            \"r\": os.O_RDONLY,\n            \"w\": os.O_WRONLY | os.O_CREAT | os.O_TRUNC,\n        }[mode]\n        if hasattr(os, \"O_BINARY\"):\n            mode |= os.O_BINARY\n        self.fd = os.open(name, mode, 0666)\n\n    def close(self):\n        os.close(self.fd)\n\n    def read(self, size):\n        return os.read(self.fd, size)\n\n    def write(self, s):\n        os.write(self.fd, s)\n\nclass _Stream:\n    \"\"\"Class that serves as an adapter between TarFile and\n       a stream-like object.  The stream-like object only\n       needs to have a read() or write() method and is accessed\n       blockwise.  Use of gzip or bzip2 compression is possible.\n       A stream-like object could be for example: sys.stdin,\n       sys.stdout, a socket, a tape device etc.\n\n       _Stream is intended to be used only internally.\n    \"\"\"\n\n    def __init__(self, name, mode, comptype, fileobj, bufsize):\n        \"\"\"Construct a _Stream object.\n        \"\"\"\n        self._extfileobj = True\n        if fileobj is None:\n            fileobj = _LowLevelFile(name, mode)\n            self._extfileobj = False\n\n        if comptype == '*':\n            # Enable transparent compression detection for the\n            # stream interface\n            fileobj = _StreamProxy(fileobj)\n            comptype = fileobj.getcomptype()\n\n        self.name     = name or \"\"\n        self.mode     = mode\n        self.comptype = comptype\n        self.fileobj  = fileobj\n        self.bufsize  = bufsize\n        self.buf      = \"\"\n        self.pos      = 0L\n        self.closed   = False\n\n        try:\n            if comptype == \"gz\":\n                try:\n                    import zlib\n                except ImportError:\n                    raise CompressionError(\"zlib module is not available\")\n                self.zlib = zlib\n                self.crc = zlib.crc32(\"\") & 0xffffffffL\n                if mode == \"r\":\n                    self._init_read_gz()\n                else:\n                    self._init_write_gz()\n\n            elif comptype == \"bz2\":\n                try:\n                    import bz2\n                except ImportError:\n                    raise CompressionError(\"bz2 module is not available\")\n                if mode == \"r\":\n                    self.dbuf = \"\"\n                    self.cmp = bz2.BZ2Decompressor()\n                else:\n                    self.cmp = bz2.BZ2Compressor()\n        except:\n            if not self._extfileobj:\n                self.fileobj.close()\n            self.closed = True\n            raise\n\n    def __del__(self):\n        if hasattr(self, \"closed\") and not self.closed:\n            self.close()\n\n    def _init_write_gz(self):\n        \"\"\"Initialize for writing with gzip compression.\n        \"\"\"\n        self.cmp = self.zlib.compressobj(9, self.zlib.DEFLATED,\n                                            -self.zlib.MAX_WBITS,\n                                            self.zlib.DEF_MEM_LEVEL,\n                                            0)\n        timestamp = struct.pack(\"<L\", long(time.time()))\n        self.__write(\"\\037\\213\\010\\010%s\\002\\377\" % timestamp)\n        if type(self.name) is unicode:\n            self.name = self.name.encode(\"iso-8859-1\", \"replace\")\n        if self.name.endswith(\".gz\"):\n            self.name = self.name[:-3]\n        self.__write(self.name + NUL)\n\n    def write(self, s):\n        \"\"\"Write string s to the stream.\n        \"\"\"\n        if self.comptype == \"gz\":\n            self.crc = self.zlib.crc32(s, self.crc) & 0xffffffffL\n        self.pos += len(s)\n        if self.comptype != \"tar\":\n            s = self.cmp.compress(s)\n        self.__write(s)\n\n    def __write(self, s):\n        \"\"\"Write string s to the stream if a whole new block\n           is ready to be written.\n        \"\"\"\n        self.buf += s\n        while len(self.buf) > self.bufsize:\n            self.fileobj.write(self.buf[:self.bufsize])\n            self.buf = self.buf[self.bufsize:]\n\n    def close(self):\n        \"\"\"Close the _Stream object. No operation should be\n           done on it afterwards.\n        \"\"\"\n        if self.closed:\n            return\n\n        if self.mode == \"w\" and self.comptype != \"tar\":\n            self.buf += self.cmp.flush()\n\n        if self.mode == \"w\" and self.buf:\n            self.fileobj.write(self.buf)\n            self.buf = \"\"\n            if self.comptype == \"gz\":\n                # The native zlib crc is an unsigned 32-bit integer, but\n                # the Python wrapper implicitly casts that to a signed C\n                # long.  So, on a 32-bit box self.crc may \"look negative\",\n                # while the same crc on a 64-bit box may \"look positive\".\n                # To avoid irksome warnings from the `struct` module, force\n                # it to look positive on all boxes.\n                self.fileobj.write(struct.pack(\"<L\", self.crc & 0xffffffffL))\n                self.fileobj.write(struct.pack(\"<L\", self.pos & 0xffffFFFFL))\n\n        if not self._extfileobj:\n            self.fileobj.close()\n\n        self.closed = True\n\n    def _init_read_gz(self):\n        \"\"\"Initialize for reading a gzip compressed fileobj.\n        \"\"\"\n        self.cmp = self.zlib.decompressobj(-self.zlib.MAX_WBITS)\n        self.dbuf = \"\"\n\n        # taken from gzip.GzipFile with some alterations\n        if self.__read(2) != \"\\037\\213\":\n            raise ReadError(\"not a gzip file\")\n        if self.__read(1) != \"\\010\":\n            raise CompressionError(\"unsupported compression method\")\n\n        flag = ord(self.__read(1))\n        self.__read(6)\n\n        if flag & 4:\n            xlen = ord(self.__read(1)) + 256 * ord(self.__read(1))\n            self.read(xlen)\n        if flag & 8:\n            while True:\n                s = self.__read(1)\n                if not s or s == NUL:\n                    break\n        if flag & 16:\n            while True:\n                s = self.__read(1)\n                if not s or s == NUL:\n                    break\n        if flag & 2:\n            self.__read(2)\n\n    def tell(self):\n        \"\"\"Return the stream's file pointer position.\n        \"\"\"\n        return self.pos\n\n    def seek(self, pos=0):\n        \"\"\"Set the stream's file pointer to pos. Negative seeking\n           is forbidden.\n        \"\"\"\n        if pos - self.pos >= 0:\n            blocks, remainder = divmod(pos - self.pos, self.bufsize)\n            for i in xrange(blocks):\n                self.read(self.bufsize)\n            self.read(remainder)\n        else:\n            raise StreamError(\"seeking backwards is not allowed\")\n        return self.pos\n\n    def read(self, size=None):\n        \"\"\"Return the next size number of bytes from the stream.\n           If size is not defined, return all bytes of the stream\n           up to EOF.\n        \"\"\"\n        if size is None:\n            t = []\n            while True:\n                buf = self._read(self.bufsize)\n                if not buf:\n                    break\n                t.append(buf)\n            buf = \"\".join(t)\n        else:\n            buf = self._read(size)\n        self.pos += len(buf)\n        return buf\n\n    def _read(self, size):\n        \"\"\"Return size bytes from the stream.\n        \"\"\"\n        if self.comptype == \"tar\":\n            return self.__read(size)\n\n        c = len(self.dbuf)\n        t = [self.dbuf]\n        while c < size:\n            buf = self.__read(self.bufsize)\n            if not buf:\n                break\n            try:\n                buf = self.cmp.decompress(buf)\n            except IOError:\n                raise ReadError(\"invalid compressed data\")\n            t.append(buf)\n            c += len(buf)\n        t = \"\".join(t)\n        self.dbuf = t[size:]\n        return t[:size]\n\n    def __read(self, size):\n        \"\"\"Return size bytes from stream. If internal buffer is empty,\n           read another block from the stream.\n        \"\"\"\n        c = len(self.buf)\n        t = [self.buf]\n        while c < size:\n            buf = self.fileobj.read(self.bufsize)\n            if not buf:\n                break\n            t.append(buf)\n            c += len(buf)\n        t = \"\".join(t)\n        self.buf = t[size:]\n        return t[:size]\n# class _Stream\n\nclass _StreamProxy(object):\n    \"\"\"Small proxy class that enables transparent compression\n       detection for the Stream interface (mode 'r|*').\n    \"\"\"\n\n    def __init__(self, fileobj):\n        self.fileobj = fileobj\n        self.buf = self.fileobj.read(BLOCKSIZE)\n\n    def read(self, size):\n        self.read = self.fileobj.read\n        return self.buf\n\n    def getcomptype(self):\n        if self.buf.startswith(\"\\037\\213\\010\"):\n            return \"gz\"\n        if self.buf[0:3] == \"BZh\" and self.buf[4:10] == \"1AY&SY\":\n            return \"bz2\"\n        return \"tar\"\n\n    def close(self):\n        self.fileobj.close()\n# class StreamProxy\n\nclass _BZ2Proxy(object):\n    \"\"\"Small proxy class that enables external file object\n       support for \"r:bz2\" and \"w:bz2\" modes. This is actually\n       a workaround for a limitation in bz2 module's BZ2File\n       class which (unlike gzip.GzipFile) has no support for\n       a file object argument.\n    \"\"\"\n\n    blocksize = 16 * 1024\n\n    def __init__(self, fileobj, mode):\n        self.fileobj = fileobj\n        self.mode = mode\n        self.name = getattr(self.fileobj, \"name\", None)\n        self.init()\n\n    def init(self):\n        import bz2\n        self.pos = 0\n        if self.mode == \"r\":\n            self.bz2obj = bz2.BZ2Decompressor()\n            self.fileobj.seek(0)\n            self.buf = \"\"\n        else:\n            self.bz2obj = bz2.BZ2Compressor()\n\n    def read(self, size):\n        b = [self.buf]\n        x = len(self.buf)\n        while x < size:\n            raw = self.fileobj.read(self.blocksize)\n            if not raw:\n                break\n            data = self.bz2obj.decompress(raw)\n            b.append(data)\n            x += len(data)\n        self.buf = \"\".join(b)\n\n        buf = self.buf[:size]\n        self.buf = self.buf[size:]\n        self.pos += len(buf)\n        return buf\n\n    def seek(self, pos):\n        if pos < self.pos:\n            self.init()\n        self.read(pos - self.pos)\n\n    def tell(self):\n        return self.pos\n\n    def write(self, data):\n        self.pos += len(data)\n        raw = self.bz2obj.compress(data)\n        self.fileobj.write(raw)\n\n    def close(self):\n        if self.mode == \"w\":\n            raw = self.bz2obj.flush()\n            self.fileobj.write(raw)\n# class _BZ2Proxy\n\n#------------------------\n# Extraction file object\n#------------------------\nclass _FileInFile(object):\n    \"\"\"A thin wrapper around an existing file object that\n       provides a part of its data as an individual file\n       object.\n    \"\"\"\n\n    def __init__(self, fileobj, offset, size, sparse=None):\n        self.fileobj = fileobj\n        self.offset = offset\n        self.size = size\n        self.sparse = sparse\n        self.position = 0\n\n    def tell(self):\n        \"\"\"Return the current file position.\n        \"\"\"\n        return self.position\n\n    def seek(self, position):\n        \"\"\"Seek to a position in the file.\n        \"\"\"\n        self.position = position\n\n    def read(self, size=None):\n        \"\"\"Read data from the file.\n        \"\"\"\n        if size is None:\n            size = self.size - self.position\n        else:\n            size = min(size, self.size - self.position)\n\n        if self.sparse is None:\n            return self.readnormal(size)\n        else:\n            return self.readsparse(size)\n\n    def readnormal(self, size):\n        \"\"\"Read operation for regular files.\n        \"\"\"\n        self.fileobj.seek(self.offset + self.position)\n        self.position += size\n        return self.fileobj.read(size)\n\n    def readsparse(self, size):\n        \"\"\"Read operation for sparse files.\n        \"\"\"\n        data = []\n        while size > 0:\n            buf = self.readsparsesection(size)\n            if not buf:\n                break\n            size -= len(buf)\n            data.append(buf)\n        return \"\".join(data)\n\n    def readsparsesection(self, size):\n        \"\"\"Read a single section of a sparse file.\n        \"\"\"\n        section = self.sparse.find(self.position)\n\n        if section is None:\n            return \"\"\n\n        size = min(size, section.offset + section.size - self.position)\n\n        if isinstance(section, _data):\n            realpos = section.realpos + self.position - section.offset\n            self.fileobj.seek(self.offset + realpos)\n            self.position += size\n            return self.fileobj.read(size)\n        else:\n            self.position += size\n            return NUL * size\n#class _FileInFile\n\n\nclass ExFileObject(object):\n    \"\"\"File-like object for reading an archive member.\n       Is returned by TarFile.extractfile().\n    \"\"\"\n    blocksize = 1024\n\n    def __init__(self, tarfile, tarinfo):\n        self.fileobj = _FileInFile(tarfile.fileobj,\n                                   tarinfo.offset_data,\n                                   tarinfo.size,\n                                   getattr(tarinfo, \"sparse\", None))\n        self.name = tarinfo.name\n        self.mode = \"r\"\n        self.closed = False\n        self.size = tarinfo.size\n\n        self.position = 0\n        self.buffer = \"\"\n\n    def read(self, size=None):\n        \"\"\"Read at most size bytes from the file. If size is not\n           present or None, read all data until EOF is reached.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file\")\n\n        buf = \"\"\n        if self.buffer:\n            if size is None:\n                buf = self.buffer\n                self.buffer = \"\"\n            else:\n                buf = self.buffer[:size]\n                self.buffer = self.buffer[size:]\n\n        if size is None:\n            buf += self.fileobj.read()\n        else:\n            buf += self.fileobj.read(size - len(buf))\n\n        self.position += len(buf)\n        return buf\n\n    def readline(self, size=-1):\n        \"\"\"Read one entire line from the file. If size is present\n           and non-negative, return a string with at most that\n           size, which may be an incomplete line.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file\")\n\n        if \"\\n\" in self.buffer:\n            pos = self.buffer.find(\"\\n\") + 1\n        else:\n            buffers = [self.buffer]\n            while True:\n                buf = self.fileobj.read(self.blocksize)\n                buffers.append(buf)\n                if not buf or \"\\n\" in buf:\n                    self.buffer = \"\".join(buffers)\n                    pos = self.buffer.find(\"\\n\") + 1\n                    if pos == 0:\n                        # no newline found.\n                        pos = len(self.buffer)\n                    break\n\n        if size != -1:\n            pos = min(size, pos)\n\n        buf = self.buffer[:pos]\n        self.buffer = self.buffer[pos:]\n        self.position += len(buf)\n        return buf\n\n    def readlines(self):\n        \"\"\"Return a list with all remaining lines.\n        \"\"\"\n        result = []\n        while True:\n            line = self.readline()\n            if not line: break\n            result.append(line)\n        return result\n\n    def tell(self):\n        \"\"\"Return the current file position.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file\")\n\n        return self.position\n\n    def seek(self, pos, whence=os.SEEK_SET):\n        \"\"\"Seek to a position in the file.\n        \"\"\"\n        if self.closed:\n            raise ValueError(\"I/O operation on closed file\")\n\n        if whence == os.SEEK_SET:\n            self.position = min(max(pos, 0), self.size)\n        elif whence == os.SEEK_CUR:\n            if pos < 0:\n                self.position = max(self.position + pos, 0)\n            else:\n                self.position = min(self.position + pos, self.size)\n        elif whence == os.SEEK_END:\n            self.position = max(min(self.size + pos, self.size), 0)\n        else:\n            raise ValueError(\"Invalid argument\")\n\n        self.buffer = \"\"\n        self.fileobj.seek(self.position)\n\n    def close(self):\n        \"\"\"Close the file object.\n        \"\"\"\n        self.closed = True\n\n    def __iter__(self):\n        \"\"\"Get an iterator over the file's lines.\n        \"\"\"\n        while True:\n            line = self.readline()\n            if not line:\n                break\n            yield line\n#class ExFileObject\n\n#------------------\n# Exported Classes\n#------------------\nclass TarInfo(object):\n    \"\"\"Informational class which holds the details about an\n       archive member given by a tar header block.\n       TarInfo objects are returned by TarFile.getmember(),\n       TarFile.getmembers() and TarFile.gettarinfo() and are\n       usually created internally.\n    \"\"\"\n\n    def __init__(self, name=\"\"):\n        \"\"\"Construct a TarInfo object. name is the optional name\n           of the member.\n        \"\"\"\n        self.name = name        # member name\n        self.mode = 0644        # file permissions\n        self.uid = 0            # user id\n        self.gid = 0            # group id\n        self.size = 0           # file size\n        self.mtime = 0          # modification time\n        self.chksum = 0         # header checksum\n        self.type = REGTYPE     # member type\n        self.linkname = \"\"      # link name\n        self.uname = \"\"         # user name\n        self.gname = \"\"         # group name\n        self.devmajor = 0       # device major number\n        self.devminor = 0       # device minor number\n\n        self.offset = 0         # the tar header starts here\n        self.offset_data = 0    # the file's data starts here\n\n        self.pax_headers = {}   # pax header information\n\n    # In pax headers the \"name\" and \"linkname\" field are called\n    # \"path\" and \"linkpath\".\n    def _getpath(self):\n        return self.name\n    def _setpath(self, name):\n        self.name = name\n    path = property(_getpath, _setpath)\n\n    def _getlinkpath(self):\n        return self.linkname\n    def _setlinkpath(self, linkname):\n        self.linkname = linkname\n    linkpath = property(_getlinkpath, _setlinkpath)\n\n    def __repr__(self):\n        return \"<%s %r at %#x>\" % (self.__class__.__name__,self.name,id(self))\n\n    def get_info(self, encoding, errors):\n        \"\"\"Return the TarInfo's attributes as a dictionary.\n        \"\"\"\n        info = {\n            \"name\":     self.name,\n            \"mode\":     self.mode & 07777,\n            \"uid\":      self.uid,\n            \"gid\":      self.gid,\n            \"size\":     self.size,\n            \"mtime\":    self.mtime,\n            \"chksum\":   self.chksum,\n            \"type\":     self.type,\n            \"linkname\": self.linkname,\n            \"uname\":    self.uname,\n            \"gname\":    self.gname,\n            \"devmajor\": self.devmajor,\n            \"devminor\": self.devminor\n        }\n\n        if info[\"type\"] == DIRTYPE and not info[\"name\"].endswith(\"/\"):\n            info[\"name\"] += \"/\"\n\n        for key in (\"name\", \"linkname\", \"uname\", \"gname\"):\n            if type(info[key]) is unicode:\n                info[key] = info[key].encode(encoding, errors)\n\n        return info\n\n    def tobuf(self, format=DEFAULT_FORMAT, encoding=ENCODING, errors=\"strict\"):\n        \"\"\"Return a tar header as a string of 512 byte blocks.\n        \"\"\"\n        info = self.get_info(encoding, errors)\n\n        if format == USTAR_FORMAT:\n            return self.create_ustar_header(info)\n        elif format == GNU_FORMAT:\n            return self.create_gnu_header(info)\n        elif format == PAX_FORMAT:\n            return self.create_pax_header(info, encoding, errors)\n        else:\n            raise ValueError(\"invalid format\")\n\n    def create_ustar_header(self, info):\n        \"\"\"Return the object as a ustar header block.\n        \"\"\"\n        info[\"magic\"] = POSIX_MAGIC\n\n        if len(info[\"linkname\"]) > LENGTH_LINK:\n            raise ValueError(\"linkname is too long\")\n\n        if len(info[\"name\"]) > LENGTH_NAME:\n            info[\"prefix\"], info[\"name\"] = self._posix_split_name(info[\"name\"])\n\n        return self._create_header(info, USTAR_FORMAT)\n\n    def create_gnu_header(self, info):\n        \"\"\"Return the object as a GNU header block sequence.\n        \"\"\"\n        info[\"magic\"] = GNU_MAGIC\n\n        buf = \"\"\n        if len(info[\"linkname\"]) > LENGTH_LINK:\n            buf += self._create_gnu_long_header(info[\"linkname\"], GNUTYPE_LONGLINK)\n\n        if len(info[\"name\"]) > LENGTH_NAME:\n            buf += self._create_gnu_long_header(info[\"name\"], GNUTYPE_LONGNAME)\n\n        return buf + self._create_header(info, GNU_FORMAT)\n\n    def create_pax_header(self, info, encoding, errors):\n        \"\"\"Return the object as a ustar header block. If it cannot be\n           represented this way, prepend a pax extended header sequence\n           with supplement information.\n        \"\"\"\n        info[\"magic\"] = POSIX_MAGIC\n        pax_headers = self.pax_headers.copy()\n\n        # Test string fields for values that exceed the field length or cannot\n        # be represented in ASCII encoding.\n        for name, hname, length in (\n                (\"name\", \"path\", LENGTH_NAME), (\"linkname\", \"linkpath\", LENGTH_LINK),\n                (\"uname\", \"uname\", 32), (\"gname\", \"gname\", 32)):\n\n            if hname in pax_headers:\n                # The pax header has priority.\n                continue\n\n            val = info[name].decode(encoding, errors)\n\n            # Try to encode the string as ASCII.\n            try:\n                val.encode(\"ascii\")\n            except UnicodeEncodeError:\n                pax_headers[hname] = val\n                continue\n\n            if len(info[name]) > length:\n                pax_headers[hname] = val\n\n        # Test number fields for values that exceed the field limit or values\n        # that like to be stored as float.\n        for name, digits in ((\"uid\", 8), (\"gid\", 8), (\"size\", 12), (\"mtime\", 12)):\n            if name in pax_headers:\n                # The pax header has priority. Avoid overflow.\n                info[name] = 0\n                continue\n\n            val = info[name]\n            if not 0 <= val < 8 ** (digits - 1) or isinstance(val, float):\n                pax_headers[name] = unicode(val)\n                info[name] = 0\n\n        # Create a pax extended header if necessary.\n        if pax_headers:\n            buf = self._create_pax_generic_header(pax_headers)\n        else:\n            buf = \"\"\n\n        return buf + self._create_header(info, USTAR_FORMAT)\n\n    @classmethod\n    def create_pax_global_header(cls, pax_headers):\n        \"\"\"Return the object as a pax global header block sequence.\n        \"\"\"\n        return cls._create_pax_generic_header(pax_headers, type=XGLTYPE)\n\n    def _posix_split_name(self, name):\n        \"\"\"Split a name longer than 100 chars into a prefix\n           and a name part.\n        \"\"\"\n        prefix = name[:LENGTH_PREFIX + 1]\n        while prefix and prefix[-1] != \"/\":\n            prefix = prefix[:-1]\n\n        name = name[len(prefix):]\n        prefix = prefix[:-1]\n\n        if not prefix or len(name) > LENGTH_NAME:\n            raise ValueError(\"name is too long\")\n        return prefix, name\n\n    @staticmethod\n    def _create_header(info, format):\n        \"\"\"Return a header block. info is a dictionary with file\n           information, format must be one of the *_FORMAT constants.\n        \"\"\"\n        parts = [\n            stn(info.get(\"name\", \"\"), 100),\n            itn(info.get(\"mode\", 0) & 07777, 8, format),\n            itn(info.get(\"uid\", 0), 8, format),\n            itn(info.get(\"gid\", 0), 8, format),\n            itn(info.get(\"size\", 0), 12, format),\n            itn(info.get(\"mtime\", 0), 12, format),\n            \"        \", # checksum field\n            info.get(\"type\", REGTYPE),\n            stn(info.get(\"linkname\", \"\"), 100),\n            stn(info.get(\"magic\", POSIX_MAGIC), 8),\n            stn(info.get(\"uname\", \"\"), 32),\n            stn(info.get(\"gname\", \"\"), 32),\n            itn(info.get(\"devmajor\", 0), 8, format),\n            itn(info.get(\"devminor\", 0), 8, format),\n            stn(info.get(\"prefix\", \"\"), 155)\n        ]\n\n        buf = struct.pack(\"%ds\" % BLOCKSIZE, \"\".join(parts))\n        chksum = calc_chksums(buf[-BLOCKSIZE:])[0]\n        buf = buf[:-364] + \"%06o\\0\" % chksum + buf[-357:]\n        return buf\n\n    @staticmethod\n    def _create_payload(payload):\n        \"\"\"Return the string payload filled with zero bytes\n           up to the next 512 byte border.\n        \"\"\"\n        blocks, remainder = divmod(len(payload), BLOCKSIZE)\n        if remainder > 0:\n            payload += (BLOCKSIZE - remainder) * NUL\n        return payload\n\n    @classmethod\n    def _create_gnu_long_header(cls, name, type):\n        \"\"\"Return a GNUTYPE_LONGNAME or GNUTYPE_LONGLINK sequence\n           for name.\n        \"\"\"\n        name += NUL\n\n        info = {}\n        info[\"name\"] = \"././@LongLink\"\n        info[\"type\"] = type\n        info[\"size\"] = len(name)\n        info[\"magic\"] = GNU_MAGIC\n\n        # create extended header + name blocks.\n        return cls._create_header(info, USTAR_FORMAT) + \\\n                cls._create_payload(name)\n\n    @classmethod\n    def _create_pax_generic_header(cls, pax_headers, type=XHDTYPE):\n        \"\"\"Return a POSIX.1-2001 extended or global header sequence\n           that contains a list of keyword, value pairs. The values\n           must be unicode objects.\n        \"\"\"\n        records = []\n        for keyword, value in pax_headers.iteritems():\n            keyword = keyword.encode(\"utf8\")\n            value = value.encode(\"utf8\")\n            l = len(keyword) + len(value) + 3   # ' ' + '=' + '\\n'\n            n = p = 0\n            while True:\n                n = l + len(str(p))\n                if n == p:\n                    break\n                p = n\n            records.append(\"%d %s=%s\\n\" % (p, keyword, value))\n        records = \"\".join(records)\n\n        # We use a hardcoded \"././@PaxHeader\" name like star does\n        # instead of the one that POSIX recommends.\n        info = {}\n        info[\"name\"] = \"././@PaxHeader\"\n        info[\"type\"] = type\n        info[\"size\"] = len(records)\n        info[\"magic\"] = POSIX_MAGIC\n\n        # Create pax header + record blocks.\n        return cls._create_header(info, USTAR_FORMAT) + \\\n                cls._create_payload(records)\n\n    @classmethod\n    def frombuf(cls, buf):\n        \"\"\"Construct a TarInfo object from a 512 byte string buffer.\n        \"\"\"\n        if len(buf) == 0:\n            raise EmptyHeaderError(\"empty header\")\n        if len(buf) != BLOCKSIZE:\n            raise TruncatedHeaderError(\"truncated header\")\n        if buf.count(NUL) == BLOCKSIZE:\n            raise EOFHeaderError(\"end of file header\")\n\n        chksum = nti(buf[148:156])\n        if chksum not in calc_chksums(buf):\n            raise InvalidHeaderError(\"bad checksum\")\n\n        obj = cls()\n        obj.buf = buf\n        obj.name = nts(buf[0:100])\n        obj.mode = nti(buf[100:108])\n        obj.uid = nti(buf[108:116])\n        obj.gid = nti(buf[116:124])\n        obj.size = nti(buf[124:136])\n        obj.mtime = nti(buf[136:148])\n        obj.chksum = chksum\n        obj.type = buf[156:157]\n        obj.linkname = nts(buf[157:257])\n        obj.uname = nts(buf[265:297])\n        obj.gname = nts(buf[297:329])\n        obj.devmajor = nti(buf[329:337])\n        obj.devminor = nti(buf[337:345])\n        prefix = nts(buf[345:500])\n\n        # Old V7 tar format represents a directory as a regular\n        # file with a trailing slash.\n        if obj.type == AREGTYPE and obj.name.endswith(\"/\"):\n            obj.type = DIRTYPE\n\n        # Remove redundant slashes from directories.\n        if obj.isdir():\n            obj.name = obj.name.rstrip(\"/\")\n\n        # Reconstruct a ustar longname.\n        if prefix and obj.type not in GNU_TYPES:\n            obj.name = prefix + \"/\" + obj.name\n        return obj\n\n    @classmethod\n    def fromtarfile(cls, tarfile):\n        \"\"\"Return the next TarInfo object from TarFile object\n           tarfile.\n        \"\"\"\n        buf = tarfile.fileobj.read(BLOCKSIZE)\n        obj = cls.frombuf(buf)\n        obj.offset = tarfile.fileobj.tell() - BLOCKSIZE\n        return obj._proc_member(tarfile)\n\n    #--------------------------------------------------------------------------\n    # The following are methods that are called depending on the type of a\n    # member. The entry point is _proc_member() which can be overridden in a\n    # subclass to add custom _proc_*() methods. A _proc_*() method MUST\n    # implement the following\n    # operations:\n    # 1. Set self.offset_data to the position where the data blocks begin,\n    #    if there is data that follows.\n    # 2. Set tarfile.offset to the position where the next member's header will\n    #    begin.\n    # 3. Return self or another valid TarInfo object.\n    def _proc_member(self, tarfile):\n        \"\"\"Choose the right processing method depending on\n           the type and call it.\n        \"\"\"\n        if self.type in (GNUTYPE_LONGNAME, GNUTYPE_LONGLINK):\n            return self._proc_gnulong(tarfile)\n        elif self.type == GNUTYPE_SPARSE:\n            return self._proc_sparse(tarfile)\n        elif self.type in (XHDTYPE, XGLTYPE, SOLARIS_XHDTYPE):\n            return self._proc_pax(tarfile)\n        else:\n            return self._proc_builtin(tarfile)\n\n    def _proc_builtin(self, tarfile):\n        \"\"\"Process a builtin type or an unknown type which\n           will be treated as a regular file.\n        \"\"\"\n        self.offset_data = tarfile.fileobj.tell()\n        offset = self.offset_data\n        if self.isreg() or self.type not in SUPPORTED_TYPES:\n            # Skip the following data blocks.\n            offset += self._block(self.size)\n        tarfile.offset = offset\n\n        # Patch the TarInfo object with saved global\n        # header information.\n        self._apply_pax_info(tarfile.pax_headers, tarfile.encoding, tarfile.errors)\n\n        return self\n\n    def _proc_gnulong(self, tarfile):\n        \"\"\"Process the blocks that hold a GNU longname\n           or longlink member.\n        \"\"\"\n        buf = tarfile.fileobj.read(self._block(self.size))\n\n        # Fetch the next header and process it.\n        try:\n            next = self.fromtarfile(tarfile)\n        except HeaderError:\n            raise SubsequentHeaderError(\"missing or bad subsequent header\")\n\n        # Patch the TarInfo object from the next header with\n        # the longname information.\n        next.offset = self.offset\n        if self.type == GNUTYPE_LONGNAME:\n            next.name = nts(buf)\n        elif self.type == GNUTYPE_LONGLINK:\n            next.linkname = nts(buf)\n\n        return next\n\n    def _proc_sparse(self, tarfile):\n        \"\"\"Process a GNU sparse header plus extra headers.\n        \"\"\"\n        buf = self.buf\n        sp = _ringbuffer()\n        pos = 386\n        lastpos = 0L\n        realpos = 0L\n        # There are 4 possible sparse structs in the\n        # first header.\n        for i in xrange(4):\n            try:\n                offset = nti(buf[pos:pos + 12])\n                numbytes = nti(buf[pos + 12:pos + 24])\n            except ValueError:\n                break\n            if offset > lastpos:\n                sp.append(_hole(lastpos, offset - lastpos))\n            sp.append(_data(offset, numbytes, realpos))\n            realpos += numbytes\n            lastpos = offset + numbytes\n            pos += 24\n\n        isextended = ord(buf[482])\n        origsize = nti(buf[483:495])\n\n        # If the isextended flag is given,\n        # there are extra headers to process.\n        while isextended == 1:\n            buf = tarfile.fileobj.read(BLOCKSIZE)\n            pos = 0\n            for i in xrange(21):\n                try:\n                    offset = nti(buf[pos:pos + 12])\n                    numbytes = nti(buf[pos + 12:pos + 24])\n                except ValueError:\n                    break\n                if offset > lastpos:\n                    sp.append(_hole(lastpos, offset - lastpos))\n                sp.append(_data(offset, numbytes, realpos))\n                realpos += numbytes\n                lastpos = offset + numbytes\n                pos += 24\n            isextended = ord(buf[504])\n\n        if lastpos < origsize:\n            sp.append(_hole(lastpos, origsize - lastpos))\n\n        self.sparse = sp\n\n        self.offset_data = tarfile.fileobj.tell()\n        tarfile.offset = self.offset_data + self._block(self.size)\n        self.size = origsize\n\n        return self\n\n    def _proc_pax(self, tarfile):\n        \"\"\"Process an extended or global header as described in\n           POSIX.1-2001.\n        \"\"\"\n        # Read the header information.\n        buf = tarfile.fileobj.read(self._block(self.size))\n\n        # A pax header stores supplemental information for either\n        # the following file (extended) or all following files\n        # (global).\n        if self.type == XGLTYPE:\n            pax_headers = tarfile.pax_headers\n        else:\n            pax_headers = tarfile.pax_headers.copy()\n\n        # Parse pax header information. A record looks like that:\n        # \"%d %s=%s\\n\" % (length, keyword, value). length is the size\n        # of the complete record including the length field itself and\n        # the newline. keyword and value are both UTF-8 encoded strings.\n        regex = re.compile(r\"(\\d+) ([^=]+)=\", re.U)\n        pos = 0\n        while True:\n            match = regex.match(buf, pos)\n            if not match:\n                break\n\n            length, keyword = match.groups()\n            length = int(length)\n            value = buf[match.end(2) + 1:match.start(1) + length - 1]\n\n            keyword = keyword.decode(\"utf8\")\n            value = value.decode(\"utf8\")\n\n            pax_headers[keyword] = value\n            pos += length\n\n        # Fetch the next header.\n        try:\n            next = self.fromtarfile(tarfile)\n        except HeaderError:\n            raise SubsequentHeaderError(\"missing or bad subsequent header\")\n\n        if self.type in (XHDTYPE, SOLARIS_XHDTYPE):\n            # Patch the TarInfo object with the extended header info.\n            next._apply_pax_info(pax_headers, tarfile.encoding, tarfile.errors)\n            next.offset = self.offset\n\n            if \"size\" in pax_headers:\n                # If the extended header replaces the size field,\n                # we need to recalculate the offset where the next\n                # header starts.\n                offset = next.offset_data\n                if next.isreg() or next.type not in SUPPORTED_TYPES:\n                    offset += next._block(next.size)\n                tarfile.offset = offset\n\n        return next\n\n    def _apply_pax_info(self, pax_headers, encoding, errors):\n        \"\"\"Replace fields with supplemental information from a previous\n           pax extended or global header.\n        \"\"\"\n        for keyword, value in pax_headers.iteritems():\n            if keyword not in PAX_FIELDS:\n                continue\n\n            if keyword == \"path\":\n                value = value.rstrip(\"/\")\n\n            if keyword in PAX_NUMBER_FIELDS:\n                try:\n                    value = PAX_NUMBER_FIELDS[keyword](value)\n                except ValueError:\n                    value = 0\n            else:\n                value = uts(value, encoding, errors)\n\n            setattr(self, keyword, value)\n\n        self.pax_headers = pax_headers.copy()\n\n    def _block(self, count):\n        \"\"\"Round up a byte count by BLOCKSIZE and return it,\n           e.g. _block(834) => 1024.\n        \"\"\"\n        blocks, remainder = divmod(count, BLOCKSIZE)\n        if remainder:\n            blocks += 1\n        return blocks * BLOCKSIZE\n\n    def isreg(self):\n        return self.type in REGULAR_TYPES\n    def isfile(self):\n        return self.isreg()\n    def isdir(self):\n        return self.type == DIRTYPE\n    def issym(self):\n        return self.type == SYMTYPE\n    def islnk(self):\n        return self.type == LNKTYPE\n    def ischr(self):\n        return self.type == CHRTYPE\n    def isblk(self):\n        return self.type == BLKTYPE\n    def isfifo(self):\n        return self.type == FIFOTYPE\n    def issparse(self):\n        return self.type == GNUTYPE_SPARSE\n    def isdev(self):\n        return self.type in (CHRTYPE, BLKTYPE, FIFOTYPE)\n# class TarInfo\n\nclass TarFile(object):\n    \"\"\"The TarFile Class provides an interface to tar archives.\n    \"\"\"\n\n    debug = 0                   # May be set from 0 (no msgs) to 3 (all msgs)\n\n    dereference = False         # If true, add content of linked file to the\n                                # tar file, else the link.\n\n    ignore_zeros = False        # If true, skips empty or invalid blocks and\n                                # continues processing.\n\n    errorlevel = 1              # If 0, fatal errors only appear in debug\n                                # messages (if debug >= 0). If > 0, errors\n                                # are passed to the caller as exceptions.\n\n    format = DEFAULT_FORMAT     # The format to use when creating an archive.\n\n    encoding = ENCODING         # Encoding for 8-bit character strings.\n\n    errors = None               # Error handler for unicode conversion.\n\n    tarinfo = TarInfo           # The default TarInfo class to use.\n\n    fileobject = ExFileObject   # The default ExFileObject class to use.\n\n    def __init__(self, name=None, mode=\"r\", fileobj=None, format=None,\n            tarinfo=None, dereference=None, ignore_zeros=None, encoding=None,\n            errors=None, pax_headers=None, debug=None, errorlevel=None):\n        \"\"\"Open an (uncompressed) tar archive `name'. `mode' is either 'r' to\n           read from an existing archive, 'a' to append data to an existing\n           file or 'w' to create a new file overwriting an existing one. `mode'\n           defaults to 'r'.\n           If `fileobj' is given, it is used for reading or writing data. If it\n           can be determined, `mode' is overridden by `fileobj's mode.\n           `fileobj' is not closed, when TarFile is closed.\n        \"\"\"\n        modes = {\"r\": \"rb\", \"a\": \"r+b\", \"w\": \"wb\"}\n        if mode not in modes:\n            raise ValueError(\"mode must be 'r', 'a' or 'w'\")\n        self.mode = mode\n        self._mode = modes[mode]\n\n        if not fileobj:\n            if self.mode == \"a\" and not os.path.exists(name):\n                # Create nonexistent files in append mode.\n                self.mode = \"w\"\n                self._mode = \"wb\"\n            fileobj = bltn_open(name, self._mode)\n            self._extfileobj = False\n        else:\n            if name is None and hasattr(fileobj, \"name\"):\n                name = fileobj.name\n            if hasattr(fileobj, \"mode\"):\n                self._mode = fileobj.mode\n            self._extfileobj = True\n        self.name = os.path.abspath(name) if name else None\n        self.fileobj = fileobj\n\n        # Init attributes.\n        if format is not None:\n            self.format = format\n        if tarinfo is not None:\n            self.tarinfo = tarinfo\n        if dereference is not None:\n            self.dereference = dereference\n        if ignore_zeros is not None:\n            self.ignore_zeros = ignore_zeros\n        if encoding is not None:\n            self.encoding = encoding\n\n        if errors is not None:\n            self.errors = errors\n        elif mode == \"r\":\n            self.errors = \"utf-8\"\n        else:\n            self.errors = \"strict\"\n\n        if pax_headers is not None and self.format == PAX_FORMAT:\n            self.pax_headers = pax_headers\n        else:\n            self.pax_headers = {}\n\n        if debug is not None:\n            self.debug = debug\n        if errorlevel is not None:\n            self.errorlevel = errorlevel\n\n        # Init datastructures.\n        self.closed = False\n        self.members = []       # list of members as TarInfo objects\n        self._loaded = False    # flag if all members have been read\n        self.offset = self.fileobj.tell()\n                                # current position in the archive file\n        self.inodes = {}        # dictionary caching the inodes of\n                                # archive members already added\n\n        try:\n            if self.mode == \"r\":\n                self.firstmember = None\n                self.firstmember = self.next()\n\n            if self.mode == \"a\":\n                # Move to the end of the archive,\n                # before the first empty block.\n                while True:\n                    self.fileobj.seek(self.offset)\n                    try:\n                        tarinfo = self.tarinfo.fromtarfile(self)\n                        self.members.append(tarinfo)\n                    except EOFHeaderError:\n                        self.fileobj.seek(self.offset)\n                        break\n                    except HeaderError, e:\n                        raise ReadError(str(e))\n\n            if self.mode in \"aw\":\n                self._loaded = True\n\n                if self.pax_headers:\n                    buf = self.tarinfo.create_pax_global_header(self.pax_headers.copy())\n                    self.fileobj.write(buf)\n                    self.offset += len(buf)\n        except:\n            if not self._extfileobj:\n                self.fileobj.close()\n            self.closed = True\n            raise\n\n    def _getposix(self):\n        return self.format == USTAR_FORMAT\n    def _setposix(self, value):\n        import warnings\n        warnings.warn(\"use the format attribute instead\", DeprecationWarning,\n                      2)\n        if value:\n            self.format = USTAR_FORMAT\n        else:\n            self.format = GNU_FORMAT\n    posix = property(_getposix, _setposix)\n\n    #--------------------------------------------------------------------------\n    # Below are the classmethods which act as alternate constructors to the\n    # TarFile class. The open() method is the only one that is needed for\n    # public use; it is the \"super\"-constructor and is able to select an\n    # adequate \"sub\"-constructor for a particular compression using the mapping\n    # from OPEN_METH.\n    #\n    # This concept allows one to subclass TarFile without losing the comfort of\n    # the super-constructor. A sub-constructor is registered and made available\n    # by adding it to the mapping in OPEN_METH.\n\n    @classmethod\n    def open(cls, name=None, mode=\"r\", fileobj=None, bufsize=RECORDSIZE, **kwargs):\n        \"\"\"Open a tar archive for reading, writing or appending. Return\n           an appropriate TarFile class.\n\n           mode:\n           'r' or 'r:*' open for reading with transparent compression\n           'r:'         open for reading exclusively uncompressed\n           'r:gz'       open for reading with gzip compression\n           'r:bz2'      open for reading with bzip2 compression\n           'a' or 'a:'  open for appending, creating the file if necessary\n           'w' or 'w:'  open for writing without compression\n           'w:gz'       open for writing with gzip compression\n           'w:bz2'      open for writing with bzip2 compression\n\n           'r|*'        open a stream of tar blocks with transparent compression\n           'r|'         open an uncompressed stream of tar blocks for reading\n           'r|gz'       open a gzip compressed stream of tar blocks\n           'r|bz2'      open a bzip2 compressed stream of tar blocks\n           'w|'         open an uncompressed stream for writing\n           'w|gz'       open a gzip compressed stream for writing\n           'w|bz2'      open a bzip2 compressed stream for writing\n        \"\"\"\n\n        if not name and not fileobj:\n            raise ValueError(\"nothing to open\")\n\n        if mode in (\"r\", \"r:*\"):\n            # Find out which *open() is appropriate for opening the file.\n            for comptype in cls.OPEN_METH:\n                func = getattr(cls, cls.OPEN_METH[comptype])\n                if fileobj is not None:\n                    saved_pos = fileobj.tell()\n                try:\n                    return func(name, \"r\", fileobj, **kwargs)\n                except (ReadError, CompressionError), e:\n                    if fileobj is not None:\n                        fileobj.seek(saved_pos)\n                    continue\n            raise ReadError(\"file could not be opened successfully\")\n\n        elif \":\" in mode:\n            filemode, comptype = mode.split(\":\", 1)\n            filemode = filemode or \"r\"\n            comptype = comptype or \"tar\"\n\n            # Select the *open() function according to\n            # given compression.\n            if comptype in cls.OPEN_METH:\n                func = getattr(cls, cls.OPEN_METH[comptype])\n            else:\n                raise CompressionError(\"unknown compression type %r\" % comptype)\n            return func(name, filemode, fileobj, **kwargs)\n\n        elif \"|\" in mode:\n            filemode, comptype = mode.split(\"|\", 1)\n            filemode = filemode or \"r\"\n            comptype = comptype or \"tar\"\n\n            if filemode not in (\"r\", \"w\"):\n                raise ValueError(\"mode must be 'r' or 'w'\")\n\n            stream = _Stream(name, filemode, comptype, fileobj, bufsize)\n            try:\n                t = cls(name, filemode, stream, **kwargs)\n            except:\n                stream.close()\n                raise\n            t._extfileobj = False\n            return t\n\n        elif mode in (\"a\", \"w\"):\n            return cls.taropen(name, mode, fileobj, **kwargs)\n\n        raise ValueError(\"undiscernible mode\")\n\n    @classmethod\n    def taropen(cls, name, mode=\"r\", fileobj=None, **kwargs):\n        \"\"\"Open uncompressed tar archive name for reading or writing.\n        \"\"\"\n        if mode not in (\"r\", \"a\", \"w\"):\n            raise ValueError(\"mode must be 'r', 'a' or 'w'\")\n        return cls(name, mode, fileobj, **kwargs)\n\n    @classmethod\n    def gzopen(cls, name, mode=\"r\", fileobj=None, compresslevel=9, **kwargs):\n        \"\"\"Open gzip compressed tar archive name for reading or writing.\n           Appending is not allowed.\n        \"\"\"\n        if mode not in (\"r\", \"w\"):\n            raise ValueError(\"mode must be 'r' or 'w'\")\n\n        try:\n            import gzip\n            gzip.GzipFile\n        except (ImportError, AttributeError):\n            raise CompressionError(\"gzip module is not available\")\n\n        try:\n            fileobj = gzip.GzipFile(name, mode, compresslevel, fileobj)\n        except OSError:\n            if fileobj is not None and mode == 'r':\n                raise ReadError(\"not a gzip file\")\n            raise\n\n        try:\n            t = cls.taropen(name, mode, fileobj, **kwargs)\n        except IOError:\n            fileobj.close()\n            if mode == 'r':\n                raise ReadError(\"not a gzip file\")\n            raise\n        except:\n            fileobj.close()\n            raise\n        t._extfileobj = False\n        return t\n\n    @classmethod\n    def bz2open(cls, name, mode=\"r\", fileobj=None, compresslevel=9, **kwargs):\n        \"\"\"Open bzip2 compressed tar archive name for reading or writing.\n           Appending is not allowed.\n        \"\"\"\n        if mode not in (\"r\", \"w\"):\n            raise ValueError(\"mode must be 'r' or 'w'.\")\n\n        try:\n            import bz2\n        except ImportError:\n            raise CompressionError(\"bz2 module is not available\")\n\n        if fileobj is not None:\n            fileobj = _BZ2Proxy(fileobj, mode)\n        else:\n            fileobj = bz2.BZ2File(name, mode, compresslevel=compresslevel)\n\n        try:\n            t = cls.taropen(name, mode, fileobj, **kwargs)\n        except (IOError, EOFError):\n            fileobj.close()\n            if mode == 'r':\n                raise ReadError(\"not a bzip2 file\")\n            raise\n        except:\n            fileobj.close()\n            raise\n        t._extfileobj = False\n        return t\n\n    # All *open() methods are registered here.\n    OPEN_METH = {\n        \"tar\": \"taropen\",   # uncompressed tar\n        \"gz\":  \"gzopen\",    # gzip compressed tar\n        \"bz2\": \"bz2open\"    # bzip2 compressed tar\n    }\n\n    #--------------------------------------------------------------------------\n    # The public methods which TarFile provides:\n\n    def close(self):\n        \"\"\"Close the TarFile. In write-mode, two finishing zero blocks are\n           appended to the archive.\n        \"\"\"\n        if self.closed:\n            return\n\n        if self.mode in \"aw\":\n            self.fileobj.write(NUL * (BLOCKSIZE * 2))\n            self.offset += (BLOCKSIZE * 2)\n            # fill up the end with zero-blocks\n            # (like option -b20 for tar does)\n            blocks, remainder = divmod(self.offset, RECORDSIZE)\n            if remainder > 0:\n                self.fileobj.write(NUL * (RECORDSIZE - remainder))\n\n        if not self._extfileobj:\n            self.fileobj.close()\n        self.closed = True\n\n    def getmember(self, name):\n        \"\"\"Return a TarInfo object for member `name'. If `name' can not be\n           found in the archive, KeyError is raised. If a member occurs more\n           than once in the archive, its last occurrence is assumed to be the\n           most up-to-date version.\n        \"\"\"\n        tarinfo = self._getmember(name)\n        if tarinfo is None:\n            raise KeyError(\"filename %r not found\" % name)\n        return tarinfo\n\n    def getmembers(self):\n        \"\"\"Return the members of the archive as a list of TarInfo objects. The\n           list has the same order as the members in the archive.\n        \"\"\"\n        self._check()\n        if not self._loaded:    # if we want to obtain a list of\n            self._load()        # all members, we first have to\n                                # scan the whole archive.\n        return self.members\n\n    def getnames(self):\n        \"\"\"Return the members of the archive as a list of their names. It has\n           the same order as the list returned by getmembers().\n        \"\"\"\n        return [tarinfo.name for tarinfo in self.getmembers()]\n\n    def gettarinfo(self, name=None, arcname=None, fileobj=None):\n        \"\"\"Create a TarInfo object for either the file `name' or the file\n           object `fileobj' (using os.fstat on its file descriptor). You can\n           modify some of the TarInfo's attributes before you add it using\n           addfile(). If given, `arcname' specifies an alternative name for the\n           file in the archive.\n        \"\"\"\n        self._check(\"aw\")\n\n        # When fileobj is given, replace name by\n        # fileobj's real name.\n        if fileobj is not None:\n            name = fileobj.name\n\n        # Building the name of the member in the archive.\n        # Backward slashes are converted to forward slashes,\n        # Absolute paths are turned to relative paths.\n        if arcname is None:\n            arcname = name\n        drv, arcname = os.path.splitdrive(arcname)\n        arcname = arcname.replace(os.sep, \"/\")\n        arcname = arcname.lstrip(\"/\")\n\n        # Now, fill the TarInfo object with\n        # information specific for the file.\n        tarinfo = self.tarinfo()\n        tarinfo.tarfile = self\n\n        # Use os.stat or os.lstat, depending on platform\n        # and if symlinks shall be resolved.\n        if fileobj is None:\n            if hasattr(os, \"lstat\") and not self.dereference:\n                statres = os.lstat(name)\n            else:\n                statres = os.stat(name)\n        else:\n            statres = os.fstat(fileobj.fileno())\n        linkname = \"\"\n\n        stmd = statres.st_mode\n        if stat.S_ISREG(stmd):\n            inode = (statres.st_ino, statres.st_dev)\n            if not self.dereference and statres.st_nlink > 1 and \\\n                    inode in self.inodes and arcname != self.inodes[inode]:\n                # Is it a hardlink to an already\n                # archived file?\n                type = LNKTYPE\n                linkname = self.inodes[inode]\n            else:\n                # The inode is added only if its valid.\n                # For win32 it is always 0.\n                type = REGTYPE\n                if inode[0]:\n                    self.inodes[inode] = arcname\n        elif stat.S_ISDIR(stmd):\n            type = DIRTYPE\n        elif stat.S_ISFIFO(stmd):\n            type = FIFOTYPE\n        elif stat.S_ISLNK(stmd):\n            type = SYMTYPE\n            linkname = os.readlink(name)\n        elif stat.S_ISCHR(stmd):\n            type = CHRTYPE\n        elif stat.S_ISBLK(stmd):\n            type = BLKTYPE\n        else:\n            return None\n\n        # Fill the TarInfo object with all\n        # information we can get.\n        tarinfo.name = arcname\n        tarinfo.mode = stmd\n        tarinfo.uid = statres.st_uid\n        tarinfo.gid = statres.st_gid\n        if type == REGTYPE:\n            tarinfo.size = statres.st_size\n        else:\n            tarinfo.size = 0L\n        tarinfo.mtime = statres.st_mtime\n        tarinfo.type = type\n        tarinfo.linkname = linkname\n        if pwd:\n            try:\n                tarinfo.uname = pwd.getpwuid(tarinfo.uid)[0]\n            except KeyError:\n                pass\n        if grp:\n            try:\n                tarinfo.gname = grp.getgrgid(tarinfo.gid)[0]\n            except KeyError:\n                pass\n\n        if type in (CHRTYPE, BLKTYPE):\n            if hasattr(os, \"major\") and hasattr(os, \"minor\"):\n                tarinfo.devmajor = os.major(statres.st_rdev)\n                tarinfo.devminor = os.minor(statres.st_rdev)\n        return tarinfo\n\n    def list(self, verbose=True):\n        \"\"\"Print a table of contents to sys.stdout. If `verbose' is False, only\n           the names of the members are printed. If it is True, an `ls -l'-like\n           output is produced.\n        \"\"\"\n        self._check()\n\n        for tarinfo in self:\n            if verbose:\n                print filemode(tarinfo.mode),\n                print \"%s/%s\" % (tarinfo.uname or tarinfo.uid,\n                                 tarinfo.gname or tarinfo.gid),\n                if tarinfo.ischr() or tarinfo.isblk():\n                    print \"%10s\" % (\"%d,%d\" \\\n                                    % (tarinfo.devmajor, tarinfo.devminor)),\n                else:\n                    print \"%10d\" % tarinfo.size,\n                print \"%d-%02d-%02d %02d:%02d:%02d\" \\\n                      % time.localtime(tarinfo.mtime)[:6],\n\n            print tarinfo.name + (\"/\" if tarinfo.isdir() else \"\"),\n\n            if verbose:\n                if tarinfo.issym():\n                    print \"->\", tarinfo.linkname,\n                if tarinfo.islnk():\n                    print \"link to\", tarinfo.linkname,\n            print\n\n    def add(self, name, arcname=None, recursive=True, exclude=None, filter=None):\n        \"\"\"Add the file `name' to the archive. `name' may be any type of file\n           (directory, fifo, symbolic link, etc.). If given, `arcname'\n           specifies an alternative name for the file in the archive.\n           Directories are added recursively by default. This can be avoided by\n           setting `recursive' to False. `exclude' is a function that should\n           return True for each filename to be excluded. `filter' is a function\n           that expects a TarInfo object argument and returns the changed\n           TarInfo object, if it returns None the TarInfo object will be\n           excluded from the archive.\n        \"\"\"\n        self._check(\"aw\")\n\n        if arcname is None:\n            arcname = name\n\n        # Exclude pathnames.\n        if exclude is not None:\n            import warnings\n            warnings.warn(\"use the filter argument instead\",\n                    DeprecationWarning, 2)\n            if exclude(name):\n                self._dbg(2, \"tarfile: Excluded %r\" % name)\n                return\n\n        # Skip if somebody tries to archive the archive...\n        if self.name is not None and os.path.abspath(name) == self.name:\n            self._dbg(2, \"tarfile: Skipped %r\" % name)\n            return\n\n        self._dbg(1, name)\n\n        # Create a TarInfo object from the file.\n        tarinfo = self.gettarinfo(name, arcname)\n\n        if tarinfo is None:\n            self._dbg(1, \"tarfile: Unsupported type %r\" % name)\n            return\n\n        # Change or exclude the TarInfo object.\n        if filter is not None:\n            tarinfo = filter(tarinfo)\n            if tarinfo is None:\n                self._dbg(2, \"tarfile: Excluded %r\" % name)\n                return\n\n        # Append the tar header and data to the archive.\n        if tarinfo.isreg():\n            with bltn_open(name, \"rb\") as f:\n                self.addfile(tarinfo, f)\n\n        elif tarinfo.isdir():\n            self.addfile(tarinfo)\n            if recursive:\n                for f in os.listdir(name):\n                    self.add(os.path.join(name, f), os.path.join(arcname, f),\n                            recursive, exclude, filter)\n\n        else:\n            self.addfile(tarinfo)\n\n    def addfile(self, tarinfo, fileobj=None):\n        \"\"\"Add the TarInfo object `tarinfo' to the archive. If `fileobj' is\n           given, tarinfo.size bytes are read from it and added to the archive.\n           You can create TarInfo objects using gettarinfo().\n           On Windows platforms, `fileobj' should always be opened with mode\n           'rb' to avoid irritation about the file size.\n        \"\"\"\n        self._check(\"aw\")\n\n        tarinfo = copy.copy(tarinfo)\n\n        buf = tarinfo.tobuf(self.format, self.encoding, self.errors)\n        self.fileobj.write(buf)\n        self.offset += len(buf)\n\n        # If there's data to follow, append it.\n        if fileobj is not None:\n            copyfileobj(fileobj, self.fileobj, tarinfo.size)\n            blocks, remainder = divmod(tarinfo.size, BLOCKSIZE)\n            if remainder > 0:\n                self.fileobj.write(NUL * (BLOCKSIZE - remainder))\n                blocks += 1\n            self.offset += blocks * BLOCKSIZE\n\n        self.members.append(tarinfo)\n\n    def extractall(self, path=\".\", members=None):\n        \"\"\"Extract all members from the archive to the current working\n           directory and set owner, modification time and permissions on\n           directories afterwards. `path' specifies a different directory\n           to extract to. `members' is optional and must be a subset of the\n           list returned by getmembers().\n        \"\"\"\n        directories = []\n\n        if members is None:\n            members = self\n\n        for tarinfo in members:\n            if tarinfo.isdir():\n                # Extract directories with a safe mode.\n                directories.append(tarinfo)\n                tarinfo = copy.copy(tarinfo)\n                tarinfo.mode = 0700\n            self.extract(tarinfo, path)\n\n        # Reverse sort directories.\n        directories.sort(key=operator.attrgetter('name'))\n        directories.reverse()\n\n        # Set correct owner, mtime and filemode on directories.\n        for tarinfo in directories:\n            dirpath = os.path.join(path, tarinfo.name)\n            try:\n                self.chown(tarinfo, dirpath)\n                self.utime(tarinfo, dirpath)\n                self.chmod(tarinfo, dirpath)\n            except ExtractError, e:\n                if self.errorlevel > 1:\n                    raise\n                else:\n                    self._dbg(1, \"tarfile: %s\" % e)\n\n    def extract(self, member, path=\"\"):\n        \"\"\"Extract a member from the archive to the current working directory,\n           using its full name. Its file information is extracted as accurately\n           as possible. `member' may be a filename or a TarInfo object. You can\n           specify a different directory using `path'.\n        \"\"\"\n        self._check(\"r\")\n\n        if isinstance(member, basestring):\n            tarinfo = self.getmember(member)\n        else:\n            tarinfo = member\n\n        # Prepare the link target for makelink().\n        if tarinfo.islnk():\n            tarinfo._link_target = os.path.join(path, tarinfo.linkname)\n\n        try:\n            self._extract_member(tarinfo, os.path.join(path, tarinfo.name))\n        except EnvironmentError, e:\n            if self.errorlevel > 0:\n                raise\n            else:\n                if e.filename is None:\n                    self._dbg(1, \"tarfile: %s\" % e.strerror)\n                else:\n                    self._dbg(1, \"tarfile: %s %r\" % (e.strerror, e.filename))\n        except ExtractError, e:\n            if self.errorlevel > 1:\n                raise\n            else:\n                self._dbg(1, \"tarfile: %s\" % e)\n\n    def extractfile(self, member):\n        \"\"\"Extract a member from the archive as a file object. `member' may be\n           a filename or a TarInfo object. If `member' is a regular file, a\n           file-like object is returned. If `member' is a link, a file-like\n           object is constructed from the link's target. If `member' is none of\n           the above, None is returned.\n           The file-like object is read-only and provides the following\n           methods: read(), readline(), readlines(), seek() and tell()\n        \"\"\"\n        self._check(\"r\")\n\n        if isinstance(member, basestring):\n            tarinfo = self.getmember(member)\n        else:\n            tarinfo = member\n\n        if tarinfo.isreg():\n            return self.fileobject(self, tarinfo)\n\n        elif tarinfo.type not in SUPPORTED_TYPES:\n            # If a member's type is unknown, it is treated as a\n            # regular file.\n            return self.fileobject(self, tarinfo)\n\n        elif tarinfo.islnk() or tarinfo.issym():\n            if isinstance(self.fileobj, _Stream):\n                # A small but ugly workaround for the case that someone tries\n                # to extract a (sym)link as a file-object from a non-seekable\n                # stream of tar blocks.\n                raise StreamError(\"cannot extract (sym)link as file object\")\n            else:\n                # A (sym)link's file object is its target's file object.\n                return self.extractfile(self._find_link_target(tarinfo))\n        else:\n            # If there's no data associated with the member (directory, chrdev,\n            # blkdev, etc.), return None instead of a file object.\n            return None\n\n    def _extract_member(self, tarinfo, targetpath):\n        \"\"\"Extract the TarInfo object tarinfo to a physical\n           file called targetpath.\n        \"\"\"\n        # Fetch the TarInfo object for the given name\n        # and build the destination pathname, replacing\n        # forward slashes to platform specific separators.\n        targetpath = targetpath.rstrip(\"/\")\n        targetpath = targetpath.replace(\"/\", os.sep)\n\n        # Create all upper directories.\n        upperdirs = os.path.dirname(targetpath)\n        if upperdirs and not os.path.exists(upperdirs):\n            # Create directories that are not part of the archive with\n            # default permissions.\n            os.makedirs(upperdirs)\n\n        if tarinfo.islnk() or tarinfo.issym():\n            self._dbg(1, \"%s -> %s\" % (tarinfo.name, tarinfo.linkname))\n        else:\n            self._dbg(1, tarinfo.name)\n\n        if tarinfo.isreg():\n            self.makefile(tarinfo, targetpath)\n        elif tarinfo.isdir():\n            self.makedir(tarinfo, targetpath)\n        elif tarinfo.isfifo():\n            self.makefifo(tarinfo, targetpath)\n        elif tarinfo.ischr() or tarinfo.isblk():\n            self.makedev(tarinfo, targetpath)\n        elif tarinfo.islnk() or tarinfo.issym():\n            self.makelink(tarinfo, targetpath)\n        elif tarinfo.type not in SUPPORTED_TYPES:\n            self.makeunknown(tarinfo, targetpath)\n        else:\n            self.makefile(tarinfo, targetpath)\n\n        self.chown(tarinfo, targetpath)\n        if not tarinfo.issym():\n            self.chmod(tarinfo, targetpath)\n            self.utime(tarinfo, targetpath)\n\n    #--------------------------------------------------------------------------\n    # Below are the different file methods. They are called via\n    # _extract_member() when extract() is called. They can be replaced in a\n    # subclass to implement other functionality.\n\n    def makedir(self, tarinfo, targetpath):\n        \"\"\"Make a directory called targetpath.\n        \"\"\"\n        try:\n            # Use a safe mode for the directory, the real mode is set\n            # later in _extract_member().\n            os.mkdir(targetpath, 0700)\n        except EnvironmentError, e:\n            if e.errno != errno.EEXIST:\n                raise\n\n    def makefile(self, tarinfo, targetpath):\n        \"\"\"Make a file called targetpath.\n        \"\"\"\n        source = self.extractfile(tarinfo)\n        try:\n            with bltn_open(targetpath, \"wb\") as target:\n                copyfileobj(source, target)\n        finally:\n            source.close()\n\n    def makeunknown(self, tarinfo, targetpath):\n        \"\"\"Make a file from a TarInfo object with an unknown type\n           at targetpath.\n        \"\"\"\n        self.makefile(tarinfo, targetpath)\n        self._dbg(1, \"tarfile: Unknown file type %r, \" \\\n                     \"extracted as regular file.\" % tarinfo.type)\n\n    def makefifo(self, tarinfo, targetpath):\n        \"\"\"Make a fifo called targetpath.\n        \"\"\"\n        if hasattr(os, \"mkfifo\"):\n            os.mkfifo(targetpath)\n        else:\n            raise ExtractError(\"fifo not supported by system\")\n\n    def makedev(self, tarinfo, targetpath):\n        \"\"\"Make a character or block device called targetpath.\n        \"\"\"\n        if not hasattr(os, \"mknod\") or not hasattr(os, \"makedev\"):\n            raise ExtractError(\"special devices not supported by system\")\n\n        mode = tarinfo.mode\n        if tarinfo.isblk():\n            mode |= stat.S_IFBLK\n        else:\n            mode |= stat.S_IFCHR\n\n        os.mknod(targetpath, mode,\n                 os.makedev(tarinfo.devmajor, tarinfo.devminor))\n\n    def makelink(self, tarinfo, targetpath):\n        \"\"\"Make a (symbolic) link called targetpath. If it cannot be created\n          (platform limitation), we try to make a copy of the referenced file\n          instead of a link.\n        \"\"\"\n        if hasattr(os, \"symlink\") and hasattr(os, \"link\"):\n            # For systems that support symbolic and hard links.\n            if tarinfo.issym():\n                if os.path.lexists(targetpath):\n                    os.unlink(targetpath)\n                os.symlink(tarinfo.linkname, targetpath)\n            else:\n                # See extract().\n                if os.path.exists(tarinfo._link_target):\n                    if os.path.lexists(targetpath):\n                        os.unlink(targetpath)\n                    os.link(tarinfo._link_target, targetpath)\n                else:\n                    self._extract_member(self._find_link_target(tarinfo), targetpath)\n        else:\n            try:\n                self._extract_member(self._find_link_target(tarinfo), targetpath)\n            except KeyError:\n                raise ExtractError(\"unable to resolve link inside archive\")\n\n    def chown(self, tarinfo, targetpath):\n        \"\"\"Set owner of targetpath according to tarinfo.\n        \"\"\"\n        if pwd and hasattr(os, \"geteuid\") and os.geteuid() == 0:\n            # We have to be root to do so.\n            try:\n                g = grp.getgrnam(tarinfo.gname)[2]\n            except KeyError:\n                g = tarinfo.gid\n            try:\n                u = pwd.getpwnam(tarinfo.uname)[2]\n            except KeyError:\n                u = tarinfo.uid\n            try:\n                if tarinfo.issym() and hasattr(os, \"lchown\"):\n                    os.lchown(targetpath, u, g)\n                else:\n                    if sys.platform != \"os2emx\":\n                        os.chown(targetpath, u, g)\n            except EnvironmentError, e:\n                raise ExtractError(\"could not change owner\")\n\n    def chmod(self, tarinfo, targetpath):\n        \"\"\"Set file permissions of targetpath according to tarinfo.\n        \"\"\"\n        if hasattr(os, 'chmod'):\n            try:\n                os.chmod(targetpath, tarinfo.mode)\n            except EnvironmentError, e:\n                raise ExtractError(\"could not change mode\")\n\n    def utime(self, tarinfo, targetpath):\n        \"\"\"Set modification time of targetpath according to tarinfo.\n        \"\"\"\n        if not hasattr(os, 'utime'):\n            return\n        try:\n            os.utime(targetpath, (tarinfo.mtime, tarinfo.mtime))\n        except EnvironmentError, e:\n            raise ExtractError(\"could not change modification time\")\n\n    #--------------------------------------------------------------------------\n    def next(self):\n        \"\"\"Return the next member of the archive as a TarInfo object, when\n           TarFile is opened for reading. Return None if there is no more\n           available.\n        \"\"\"\n        self._check(\"ra\")\n        if self.firstmember is not None:\n            m = self.firstmember\n            self.firstmember = None\n            return m\n\n        # Read the next block.\n        self.fileobj.seek(self.offset)\n        tarinfo = None\n        while True:\n            try:\n                tarinfo = self.tarinfo.fromtarfile(self)\n            except EOFHeaderError, e:\n                if self.ignore_zeros:\n                    self._dbg(2, \"0x%X: %s\" % (self.offset, e))\n                    self.offset += BLOCKSIZE\n                    continue\n            except InvalidHeaderError, e:\n                if self.ignore_zeros:\n                    self._dbg(2, \"0x%X: %s\" % (self.offset, e))\n                    self.offset += BLOCKSIZE\n                    continue\n                elif self.offset == 0:\n                    raise ReadError(str(e))\n            except EmptyHeaderError:\n                if self.offset == 0:\n                    raise ReadError(\"empty file\")\n            except TruncatedHeaderError, e:\n                if self.offset == 0:\n                    raise ReadError(str(e))\n            except SubsequentHeaderError, e:\n                raise ReadError(str(e))\n            break\n\n        if tarinfo is not None:\n            self.members.append(tarinfo)\n        else:\n            self._loaded = True\n\n        return tarinfo\n\n    #--------------------------------------------------------------------------\n    # Little helper methods:\n\n    def _getmember(self, name, tarinfo=None, normalize=False):\n        \"\"\"Find an archive member by name from bottom to top.\n           If tarinfo is given, it is used as the starting point.\n        \"\"\"\n        # Ensure that all members have been loaded.\n        members = self.getmembers()\n\n        # Limit the member search list up to tarinfo.\n        if tarinfo is not None:\n            members = members[:members.index(tarinfo)]\n\n        if normalize:\n            name = os.path.normpath(name)\n\n        for member in reversed(members):\n            if normalize:\n                member_name = os.path.normpath(member.name)\n            else:\n                member_name = member.name\n\n            if name == member_name:\n                return member\n\n    def _load(self):\n        \"\"\"Read through the entire archive file and look for readable\n           members.\n        \"\"\"\n        while True:\n            tarinfo = self.next()\n            if tarinfo is None:\n                break\n        self._loaded = True\n\n    def _check(self, mode=None):\n        \"\"\"Check if TarFile is still open, and if the operation's mode\n           corresponds to TarFile's mode.\n        \"\"\"\n        if self.closed:\n            raise IOError(\"%s is closed\" % self.__class__.__name__)\n        if mode is not None and self.mode not in mode:\n            raise IOError(\"bad operation for mode %r\" % self.mode)\n\n    def _find_link_target(self, tarinfo):\n        \"\"\"Find the target member of a symlink or hardlink member in the\n           archive.\n        \"\"\"\n        if tarinfo.issym():\n            # Always search the entire archive.\n            linkname = \"/\".join(filter(None, (os.path.dirname(tarinfo.name), tarinfo.linkname)))\n            limit = None\n        else:\n            # Search the archive before the link, because a hard link is\n            # just a reference to an already archived file.\n            linkname = tarinfo.linkname\n            limit = tarinfo\n\n        member = self._getmember(linkname, tarinfo=limit, normalize=True)\n        if member is None:\n            raise KeyError(\"linkname %r not found\" % linkname)\n        return member\n\n    def __iter__(self):\n        \"\"\"Provide an iterator object.\n        \"\"\"\n        if self._loaded:\n            return iter(self.members)\n        else:\n            return TarIter(self)\n\n    def _dbg(self, level, msg):\n        \"\"\"Write debugging output to sys.stderr.\n        \"\"\"\n        if level <= self.debug:\n            print >> sys.stderr, msg\n\n    def __enter__(self):\n        self._check()\n        return self\n\n    def __exit__(self, type, value, traceback):\n        if type is None:\n            self.close()\n        else:\n            # An exception occurred. We must not call close() because\n            # it would try to write end-of-archive blocks and padding.\n            if not self._extfileobj:\n                self.fileobj.close()\n            self.closed = True\n# class TarFile\n\nclass TarIter:\n    \"\"\"Iterator Class.\n\n       for tarinfo in TarFile(...):\n           suite...\n    \"\"\"\n\n    def __init__(self, tarfile):\n        \"\"\"Construct a TarIter object.\n        \"\"\"\n        self.tarfile = tarfile\n        self.index = 0\n    def __iter__(self):\n        \"\"\"Return iterator object.\n        \"\"\"\n        return self\n    def next(self):\n        \"\"\"Return the next item using TarFile's next() method.\n           When all members have been read, set TarFile as _loaded.\n        \"\"\"\n        # Fix for SF #1100429: Under rare circumstances it can\n        # happen that getmembers() is called during iteration,\n        # which will cause TarIter to stop prematurely.\n\n        if self.index == 0 and self.tarfile.firstmember is not None:\n            tarinfo = self.tarfile.next()\n        elif self.index < len(self.tarfile.members):\n            tarinfo = self.tarfile.members[self.index]\n        elif not self.tarfile._loaded:\n            tarinfo = self.tarfile.next()\n            if not tarinfo:\n                self.tarfile._loaded = True\n                raise StopIteration\n        else:\n            raise StopIteration\n        self.index += 1\n        return tarinfo\n\n# Helper classes for sparse file support\nclass _section:\n    \"\"\"Base class for _data and _hole.\n    \"\"\"\n    def __init__(self, offset, size):\n        self.offset = offset\n        self.size = size\n    def __contains__(self, offset):\n        return self.offset <= offset < self.offset + self.size\n\nclass _data(_section):\n    \"\"\"Represent a data section in a sparse file.\n    \"\"\"\n    def __init__(self, offset, size, realpos):\n        _section.__init__(self, offset, size)\n        self.realpos = realpos\n\nclass _hole(_section):\n    \"\"\"Represent a hole section in a sparse file.\n    \"\"\"\n    pass\n\nclass _ringbuffer(list):\n    \"\"\"Ringbuffer class which increases performance\n       over a regular list.\n    \"\"\"\n    def __init__(self):\n        self.idx = 0\n    def find(self, offset):\n        idx = self.idx\n        while True:\n            item = self[idx]\n            if offset in item:\n                break\n            idx += 1\n            if idx == len(self):\n                idx = 0\n            if idx == self.idx:\n                # End of File\n                return None\n        self.idx = idx\n        return item\n\n#---------------------------------------------\n# zipfile compatible TarFile class\n#---------------------------------------------\nTAR_PLAIN = 0           # zipfile.ZIP_STORED\nTAR_GZIPPED = 8         # zipfile.ZIP_DEFLATED\nclass TarFileCompat:\n    \"\"\"TarFile class compatible with standard module zipfile's\n       ZipFile class.\n    \"\"\"\n    def __init__(self, file, mode=\"r\", compression=TAR_PLAIN):\n        from warnings import warnpy3k\n        warnpy3k(\"the TarFileCompat class has been removed in Python 3.0\",\n                stacklevel=2)\n        if compression == TAR_PLAIN:\n            self.tarfile = TarFile.taropen(file, mode)\n        elif compression == TAR_GZIPPED:\n            self.tarfile = TarFile.gzopen(file, mode)\n        else:\n            raise ValueError(\"unknown compression constant\")\n        if mode[0:1] == \"r\":\n            members = self.tarfile.getmembers()\n            for m in members:\n                m.filename = m.name\n                m.file_size = m.size\n                m.date_time = time.gmtime(m.mtime)[:6]\n    def namelist(self):\n        return map(lambda m: m.name, self.infolist())\n    def infolist(self):\n        return filter(lambda m: m.type in REGULAR_TYPES,\n                      self.tarfile.getmembers())\n    def printdir(self):\n        self.tarfile.list()\n    def testzip(self):\n        return\n    def getinfo(self, name):\n        return self.tarfile.getmember(name)\n    def read(self, name):\n        return self.tarfile.extractfile(self.tarfile.getmember(name)).read()\n    def write(self, filename, arcname=None, compress_type=None):\n        self.tarfile.add(filename, arcname)\n    def writestr(self, zinfo, bytes):\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        import calendar\n        tinfo = TarInfo(zinfo.filename)\n        tinfo.size = len(bytes)\n        tinfo.mtime = calendar.timegm(zinfo.date_time)\n        self.tarfile.addfile(tinfo, StringIO(bytes))\n    def close(self):\n        self.tarfile.close()\n#class TarFileCompat\n\n#--------------------\n# exported functions\n#--------------------\ndef is_tarfile(name):\n    \"\"\"Return True if name points to a tar archive that we\n       are able to handle, else return False.\n    \"\"\"\n    try:\n        t = open(name)\n        t.close()\n        return True\n    except TarError:\n        return False\n\nbltn_open = open\nopen = TarFile.open\n", 
    "tempfile": "\"\"\"Temporary files.\n\nThis module provides generic, low- and high-level interfaces for\ncreating temporary files and directories.  All of the interfaces\nprovided by this module can be used without fear of race conditions\nexcept for 'mktemp'.  'mktemp' is subject to race conditions and\nshould not be used; it is provided for backward compatibility only.\n\nThis module also provides some data items to the user:\n\n  TMP_MAX  - maximum number of names that will be tried before\n             giving up.\n  template - the default prefix for all temporary names.\n             You may change this to control the default prefix.\n  tempdir  - If this is set to a string before the first use of\n             any routine from this module, it will be considered as\n             another candidate location to store temporary files.\n\"\"\"\n\n__all__ = [\n    \"NamedTemporaryFile\", \"TemporaryFile\", # high level safe interfaces\n    \"SpooledTemporaryFile\",\n    \"mkstemp\", \"mkdtemp\",                  # low level safe interfaces\n    \"mktemp\",                              # deprecated unsafe interface\n    \"TMP_MAX\", \"gettempprefix\",            # constants\n    \"tempdir\", \"gettempdir\"\n   ]\n\n\n# Imports.\n\nimport io as _io\nimport os as _os\nimport errno as _errno\nfrom random import Random as _Random\n\ntry:\n    from cStringIO import StringIO as _StringIO\nexcept ImportError:\n    from StringIO import StringIO as _StringIO\n\ntry:\n    import fcntl as _fcntl\nexcept ImportError:\n    def _set_cloexec(fd):\n        pass\nelse:\n    def _set_cloexec(fd):\n        try:\n            flags = _fcntl.fcntl(fd, _fcntl.F_GETFD, 0)\n        except IOError:\n            pass\n        else:\n            # flags read successfully, modify\n            flags |= _fcntl.FD_CLOEXEC\n            _fcntl.fcntl(fd, _fcntl.F_SETFD, flags)\n\n\ntry:\n    import thread as _thread\nexcept ImportError:\n    import dummy_thread as _thread\n_allocate_lock = _thread.allocate_lock\n\n_text_openflags = _os.O_RDWR | _os.O_CREAT | _os.O_EXCL\nif hasattr(_os, 'O_NOINHERIT'):\n    _text_openflags |= _os.O_NOINHERIT\nif hasattr(_os, 'O_NOFOLLOW'):\n    _text_openflags |= _os.O_NOFOLLOW\n\n_bin_openflags = _text_openflags\nif hasattr(_os, 'O_BINARY'):\n    _bin_openflags |= _os.O_BINARY\n\nif hasattr(_os, 'TMP_MAX'):\n    TMP_MAX = _os.TMP_MAX\nelse:\n    TMP_MAX = 10000\n\ntemplate = \"tmp\"\n\n# Internal routines.\n\n_once_lock = _allocate_lock()\n\nif hasattr(_os, \"lstat\"):\n    _stat = _os.lstat\nelif hasattr(_os, \"stat\"):\n    _stat = _os.stat\nelse:\n    # Fallback.  All we need is something that raises os.error if the\n    # file doesn't exist.\n    def _stat(fn):\n        try:\n            f = open(fn)\n        except IOError:\n            raise _os.error\n        f.close()\n\ndef _exists(fn):\n    try:\n        _stat(fn)\n    except _os.error:\n        return False\n    else:\n        return True\n\nclass _RandomNameSequence:\n    \"\"\"An instance of _RandomNameSequence generates an endless\n    sequence of unpredictable strings which can safely be incorporated\n    into file names.  Each string is six characters long.  Multiple\n    threads can safely use the same instance at the same time.\n\n    _RandomNameSequence is an iterator.\"\"\"\n\n    characters = (\"abcdefghijklmnopqrstuvwxyz\" +\n                  \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" +\n                  \"0123456789_\")\n\n    def __init__(self):\n        self.mutex = _allocate_lock()\n        self.normcase = _os.path.normcase\n\n    @property\n    def rng(self):\n        cur_pid = _os.getpid()\n        if cur_pid != getattr(self, '_rng_pid', None):\n            self._rng = _Random()\n            self._rng_pid = cur_pid\n        return self._rng\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        m = self.mutex\n        c = self.characters\n        choose = self.rng.choice\n\n        m.acquire()\n        try:\n            letters = [choose(c) for dummy in \"123456\"]\n        finally:\n            m.release()\n\n        return self.normcase(''.join(letters))\n\ndef _candidate_tempdir_list():\n    \"\"\"Generate a list of candidate temporary directories which\n    _get_default_tempdir will try.\"\"\"\n\n    dirlist = []\n\n    # First, try the environment.\n    for envname in 'TMPDIR', 'TEMP', 'TMP':\n        dirname = _os.getenv(envname)\n        if dirname: dirlist.append(dirname)\n\n    # Failing that, try OS-specific locations.\n    if _os.name == 'riscos':\n        dirname = _os.getenv('Wimp$ScrapDir')\n        if dirname: dirlist.append(dirname)\n    elif _os.name == 'nt':\n        dirlist.extend([ r'c:\\temp', r'c:\\tmp', r'\\temp', r'\\tmp' ])\n    else:\n        dirlist.extend([ '/tmp', '/var/tmp', '/usr/tmp' ])\n\n    # As a last resort, the current directory.\n    try:\n        dirlist.append(_os.getcwd())\n    except (AttributeError, _os.error):\n        dirlist.append(_os.curdir)\n\n    return dirlist\n\ndef _get_default_tempdir():\n    \"\"\"Calculate the default directory to use for temporary files.\n    This routine should be called exactly once.\n\n    We determine whether or not a candidate temp dir is usable by\n    trying to create and write to a file in that directory.  If this\n    is successful, the test file is deleted.  To prevent denial of\n    service, the name of the test file must be randomized.\"\"\"\n\n    namer = _RandomNameSequence()\n    dirlist = _candidate_tempdir_list()\n    flags = _text_openflags\n\n    for dir in dirlist:\n        if dir != _os.curdir:\n            dir = _os.path.normcase(_os.path.abspath(dir))\n        # Try only a few names per directory.\n        for seq in xrange(100):\n            name = namer.next()\n            filename = _os.path.join(dir, name)\n            try:\n                fd = _os.open(filename, flags, 0o600)\n                try:\n                    try:\n                        with _io.open(fd, 'wb', closefd=False) as fp:\n                            fp.write(b'blat')\n                    finally:\n                        _os.close(fd)\n                finally:\n                    _os.unlink(filename)\n                return dir\n            except (OSError, IOError) as e:\n                if e.args[0] != _errno.EEXIST:\n                    break # no point trying more names in this directory\n                pass\n    raise IOError, (_errno.ENOENT,\n                    (\"No usable temporary directory found in %s\" % dirlist))\n\n_name_sequence = None\n\ndef _get_candidate_names():\n    \"\"\"Common setup sequence for all user-callable interfaces.\"\"\"\n\n    global _name_sequence\n    if _name_sequence is None:\n        _once_lock.acquire()\n        try:\n            if _name_sequence is None:\n                _name_sequence = _RandomNameSequence()\n        finally:\n            _once_lock.release()\n    return _name_sequence\n\n\ndef _mkstemp_inner(dir, pre, suf, flags):\n    \"\"\"Code common to mkstemp, TemporaryFile, and NamedTemporaryFile.\"\"\"\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, pre + name + suf)\n        try:\n            fd = _os.open(file, flags, 0600)\n            _set_cloexec(fd)\n            return (fd, _os.path.abspath(file))\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            if _os.name == 'nt' and e.errno == _errno.EACCES:\n                # On windows, when a directory with the chosen name already\n                # exists, EACCES error code is returned instead of EEXIST.\n                continue\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary file name found\")\n\n\n# User visible interfaces.\n\ndef gettempprefix():\n    \"\"\"Accessor for tempdir.template.\"\"\"\n    return template\n\ntempdir = None\n\ndef gettempdir():\n    \"\"\"Accessor for tempfile.tempdir.\"\"\"\n    global tempdir\n    if tempdir is None:\n        _once_lock.acquire()\n        try:\n            if tempdir is None:\n                tempdir = _get_default_tempdir()\n        finally:\n            _once_lock.release()\n    return tempdir\n\ndef mkstemp(suffix=\"\", prefix=template, dir=None, text=False):\n    \"\"\"User-callable function to create and return a unique temporary\n    file.  The return value is a pair (fd, name) where fd is the\n    file descriptor returned by os.open, and name is the filename.\n\n    If 'suffix' is specified, the file name will end with that suffix,\n    otherwise there will be no suffix.\n\n    If 'prefix' is specified, the file name will begin with that prefix,\n    otherwise a default prefix is used.\n\n    If 'dir' is specified, the file will be created in that directory,\n    otherwise a default directory is used.\n\n    If 'text' is specified and true, the file is opened in text\n    mode.  Else (the default) the file is opened in binary mode.  On\n    some operating systems, this makes no difference.\n\n    The file is readable and writable only by the creating user ID.\n    If the operating system uses permission bits to indicate whether a\n    file is executable, the file is executable by no one. The file\n    descriptor is not inherited by children of this process.\n\n    Caller is responsible for deleting the file when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if text:\n        flags = _text_openflags\n    else:\n        flags = _bin_openflags\n\n    return _mkstemp_inner(dir, prefix, suffix, flags)\n\n\ndef mkdtemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to create and return a unique temporary\n    directory.  The return value is the pathname of the directory.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    The directory is readable, writable, and searchable only by the\n    creating user.\n\n    Caller is responsible for deleting the directory when done with it.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        try:\n            _os.mkdir(file, 0700)\n            return file\n        except OSError, e:\n            if e.errno == _errno.EEXIST:\n                continue # try again\n            raise\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary directory name found\")\n\ndef mktemp(suffix=\"\", prefix=template, dir=None):\n    \"\"\"User-callable function to return a unique temporary file name.  The\n    file is not created.\n\n    Arguments are as for mkstemp, except that the 'text' argument is\n    not accepted.\n\n    This function is unsafe and should not be used.  The file name\n    refers to a file that did not exist at some point, but by the time\n    you get around to creating it, someone else may have beaten you to\n    the punch.\n    \"\"\"\n\n##    from warnings import warn as _warn\n##    _warn(\"mktemp is a potential security risk to your program\",\n##          RuntimeWarning, stacklevel=2)\n\n    if dir is None:\n        dir = gettempdir()\n\n    names = _get_candidate_names()\n    for seq in xrange(TMP_MAX):\n        name = names.next()\n        file = _os.path.join(dir, prefix + name + suffix)\n        if not _exists(file):\n            return file\n\n    raise IOError, (_errno.EEXIST, \"No usable temporary filename found\")\n\n\nclass _TemporaryFileWrapper:\n    \"\"\"Temporary file wrapper\n\n    This class provides a wrapper around files opened for\n    temporary use.  In particular, it seeks to automatically\n    remove the file when it is no longer needed.\n    \"\"\"\n\n    def __init__(self, file, name, delete=True):\n        self.file = file\n        self.name = name\n        self.close_called = False\n        self.delete = delete\n\n    def __getattr__(self, name):\n        # Attribute lookups are delegated to the underlying file\n        # and cached for non-numeric results\n        # (i.e. methods are cached, closed and friends are not)\n        file = self.__dict__['file']\n        a = getattr(file, name)\n        if not issubclass(type(a), type(0)):\n            setattr(self, name, a)\n        return a\n\n    # The underlying __enter__ method returns the wrong object\n    # (self.file) so override it to return the wrapper\n    def __enter__(self):\n        self.file.__enter__()\n        return self\n\n    # NT provides delete-on-close as a primitive, so we don't need\n    # the wrapper to do anything special.  We still use it so that\n    # file.name is useful (i.e. not \"(fdopen)\") with NamedTemporaryFile.\n    if _os.name != 'nt':\n        # Cache the unlinker so we don't get spurious errors at\n        # shutdown when the module-level \"os\" is None'd out.  Note\n        # that this must be referenced as self.unlink, because the\n        # name TemporaryFileWrapper may also get None'd out before\n        # __del__ is called.\n        unlink = _os.unlink\n\n        def close(self):\n            if not self.close_called:\n                self.close_called = True\n                self.file.close()\n                if self.delete:\n                    self.unlink(self.name)\n\n        def __del__(self):\n            self.close()\n\n        # Need to trap __exit__ as well to ensure the file gets\n        # deleted when used in a with statement\n        def __exit__(self, exc, value, tb):\n            result = self.file.__exit__(exc, value, tb)\n            self.close()\n            return result\n    else:\n        def __exit__(self, exc, value, tb):\n            self.file.__exit__(exc, value, tb)\n\n\ndef NamedTemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                       prefix=template, dir=None, delete=True):\n    \"\"\"Create and return a temporary file.\n    Arguments:\n    'prefix', 'suffix', 'dir' -- as for mkstemp.\n    'mode' -- the mode argument to os.fdopen (default \"w+b\").\n    'bufsize' -- the buffer size argument to os.fdopen (default -1).\n    'delete' -- whether the file is deleted on close (default True).\n    The file is created as mkstemp() would do it.\n\n    Returns an object with a file-like interface; the name of the file\n    is accessible as file.name.  The file will be automatically deleted\n    when it is closed unless the 'delete' argument is set to False.\n    \"\"\"\n\n    if dir is None:\n        dir = gettempdir()\n\n    if 'b' in mode:\n        flags = _bin_openflags\n    else:\n        flags = _text_openflags\n\n    # Setting O_TEMPORARY in the flags causes the OS to delete\n    # the file when it is closed.  This is only supported by Windows.\n    if _os.name == 'nt' and delete:\n        flags |= _os.O_TEMPORARY\n\n    (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n    try:\n        file = _os.fdopen(fd, mode, bufsize)\n        return _TemporaryFileWrapper(file, name, delete)\n    except:\n        _os.close(fd)\n        raise\n\nif _os.name != 'posix' or _os.sys.platform == 'cygwin':\n    # On non-POSIX and Cygwin systems, assume that we cannot unlink a file\n    # while it is open.\n    TemporaryFile = NamedTemporaryFile\n\nelse:\n    def TemporaryFile(mode='w+b', bufsize=-1, suffix=\"\",\n                      prefix=template, dir=None):\n        \"\"\"Create and return a temporary file.\n        Arguments:\n        'prefix', 'suffix', 'dir' -- as for mkstemp.\n        'mode' -- the mode argument to os.fdopen (default \"w+b\").\n        'bufsize' -- the buffer size argument to os.fdopen (default -1).\n        The file is created as mkstemp() would do it.\n\n        Returns an object with a file-like interface.  The file has no\n        name, and will cease to exist when it is closed.\n        \"\"\"\n\n        if dir is None:\n            dir = gettempdir()\n\n        if 'b' in mode:\n            flags = _bin_openflags\n        else:\n            flags = _text_openflags\n        (fd, name) = _mkstemp_inner(dir, prefix, suffix, flags)\n        try:\n            rv = _os.fdopen(fd, mode, bufsize)\n            _os.unlink(name)\n            return rv\n        except:\n            _os.close(fd)\n            raise\n\nclass SpooledTemporaryFile:\n    \"\"\"Temporary file wrapper, specialized to switch from\n    StringIO to a real file when it exceeds a certain size or\n    when a fileno is needed.\n    \"\"\"\n    _rolled = False\n\n    def __init__(self, max_size=0, mode='w+b', bufsize=-1,\n                 suffix=\"\", prefix=template, dir=None):\n        self._file = _StringIO()\n        self._max_size = max_size\n        self._rolled = False\n        self._TemporaryFileArgs = (mode, bufsize, suffix, prefix, dir)\n\n    def _check(self, file):\n        if self._rolled: return\n        max_size = self._max_size\n        if max_size and file.tell() > max_size:\n            self.rollover()\n\n    def rollover(self):\n        if self._rolled: return\n        file = self._file\n        newfile = self._file = TemporaryFile(*self._TemporaryFileArgs)\n        del self._TemporaryFileArgs\n\n        newfile.write(file.getvalue())\n        newfile.seek(file.tell(), 0)\n\n        self._rolled = True\n\n    # The method caching trick from NamedTemporaryFile\n    # won't work here, because _file may change from a\n    # _StringIO instance to a real file. So we list\n    # all the methods directly.\n\n    # Context management protocol\n    def __enter__(self):\n        if self._file.closed:\n            raise ValueError(\"Cannot enter context with closed file\")\n        return self\n\n    def __exit__(self, exc, value, tb):\n        self._file.close()\n\n    # file protocol\n    def __iter__(self):\n        return self._file.__iter__()\n\n    def close(self):\n        self._file.close()\n\n    @property\n    def closed(self):\n        return self._file.closed\n\n    def fileno(self):\n        self.rollover()\n        return self._file.fileno()\n\n    def flush(self):\n        self._file.flush()\n\n    def isatty(self):\n        return self._file.isatty()\n\n    @property\n    def mode(self):\n        try:\n            return self._file.mode\n        except AttributeError:\n            return self._TemporaryFileArgs[0]\n\n    @property\n    def name(self):\n        try:\n            return self._file.name\n        except AttributeError:\n            return None\n\n    def next(self):\n        return self._file.next\n\n    def read(self, *args):\n        return self._file.read(*args)\n\n    def readline(self, *args):\n        return self._file.readline(*args)\n\n    def readlines(self, *args):\n        return self._file.readlines(*args)\n\n    def seek(self, *args):\n        self._file.seek(*args)\n\n    @property\n    def softspace(self):\n        return self._file.softspace\n\n    def tell(self):\n        return self._file.tell()\n\n    def truncate(self):\n        self._file.truncate()\n\n    def write(self, s):\n        file = self._file\n        rv = file.write(s)\n        self._check(file)\n        return rv\n\n    def writelines(self, iterable):\n        file = self._file\n        rv = file.writelines(iterable)\n        self._check(file)\n        return rv\n\n    def xreadlines(self, *args):\n        if hasattr(self._file, 'xreadlines'):  # real file\n            return iter(self._file)\n        else:  # StringIO()\n            return iter(self._file.readlines(*args))\n", 
    "texttable": "#!/usr/bin/env python\n#\n# texttable - module for creating simple ASCII tables\n# Copyright (C) 2003-2015 Gerome Fournier <jef(at)foutaise.org>\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301, USA\n\n\"\"\"module for creating simple ASCII tables\n\n\nExample:\n\n    table = Texttable()\n    table.set_cols_align([\"l\", \"r\", \"c\"])\n    table.set_cols_valign([\"t\", \"m\", \"b\"])\n    table.add_rows([[\"Name\", \"Age\", \"Nickname\"], \n                    [\"Mr\\\\nXavier\\\\nHuon\", 32, \"Xav'\"],\n                    [\"Mr\\\\nBaptiste\\\\nClement\", 1, \"Baby\"]])\n    print table.draw() + \"\\\\n\"\n\n    table = Texttable()\n    table.set_deco(Texttable.HEADER)\n    table.set_cols_dtype(['t',  # text \n                          'f',  # float (decimal)\n                          'e',  # float (exponent)\n                          'i',  # integer\n                          'a']) # automatic\n    table.set_cols_align([\"l\", \"r\", \"r\", \"r\", \"l\"])\n    table.add_rows([[\"text\",    \"float\", \"exp\", \"int\", \"auto\"],\n                    [\"abcd\",    \"67\",    654,   89,    128.001],\n                    [\"efghijk\", 67.5434, .654,  89.6,  12800000000000000000000.00023],\n                    [\"lmn\",     5e-78,   5e-78, 89.4,  .000000000000128],\n                    [\"opqrstu\", .023,    5e+78, 92.,   12800000000000000000000]])\n    print table.draw()\n\nResult:\n\n    +----------+-----+----------+\n    |   Name   | Age | Nickname |\n    +==========+=====+==========+\n    | Mr       |     |          |\n    | Xavier   |  32 |          |\n    | Huon     |     |   Xav'   |\n    +----------+-----+----------+\n    | Mr       |     |          |\n    | Baptiste |   1 |          |\n    | Clement  |     |   Baby   |\n    +----------+-----+----------+\n\n    text   float       exp      int     auto\n    ===========================================\n    abcd   67.000   6.540e+02   89    128.001\n    efgh   67.543   6.540e-01   90    1.280e+22\n    ijkl   0.000    5.000e-78   89    0.000\n    mnop   0.023    5.000e+78   92    1.280e+22\n\"\"\"\n\n__all__ = [\"Texttable\", \"ArraySizeError\"]\n\n__author__ = 'Gerome Fournier <jef(at)foutaise.org>'\n__license__ = 'LGPL'\n__version__ = '0.8.3'\n__credits__ = \"\"\"\\\nJeff Kowalczyk:\n    - textwrap improved import\n    - comment concerning header output\n\nAnonymous:\n    - add_rows method, for adding rows in one go\n\nSergey Simonenko:\n    - redefined len() function to deal with non-ASCII characters\n\nRoger Lew:\n    - columns datatype specifications\n\nBrian Peterson:\n    - better handling of unicode errors\n\nFrank Sachsenheim:\n    - add Python 2/3-compatibility\n\nMaximilian Hils:\n    - fix minor bug for Python 3 compatibility\n\"\"\"\n\nimport sys\nimport string\n\ntry:\n    if sys.version >= '2.3':\n        import textwrap\n    elif sys.version >= '2.2':\n        from optparse import textwrap\n    else:\n        from optik import textwrap\nexcept ImportError:\n    sys.stderr.write(\"Can't import textwrap module!\\n\")\n    raise\n\nif sys.version >= '2.7':\n    from functools import reduce\n\ndef len(iterable):\n    \"\"\"Redefining len here so it will be able to work with non-ASCII characters\n    \"\"\"\n    if not isinstance(iterable, str):\n        return iterable.__len__()\n    \n    try:\n        if sys.version >= '3.0':\n            return len(str)\n        else:\n            return len(unicode(iterable, 'utf'))\n    except:\n        return iterable.__len__()\n\n\nclass ArraySizeError(Exception):\n    \"\"\"Exception raised when specified rows don't fit the required size\n    \"\"\"\n\n    def __init__(self, msg):\n        self.msg = msg\n        Exception.__init__(self, msg, '')\n\n    def __str__(self):\n        return self.msg\n\n\nclass Texttable:\n\n    BORDER = 1\n    HEADER = 1 << 1\n    HLINES = 1 << 2\n    VLINES = 1 << 3\n\n    def __init__(self, max_width=80):\n        \"\"\"Constructor\n\n        - max_width is an integer, specifying the maximum width of the table\n        - if set to 0, size is unlimited, therefore cells won't be wrapped\n        \"\"\"\n\n        if max_width <= 0:\n            max_width = False\n        self._max_width = max_width\n        self._precision = 3\n\n        self._deco = Texttable.VLINES | Texttable.HLINES | Texttable.BORDER | \\\n            Texttable.HEADER\n        self.set_chars(['-', '|', '+', '='])\n        self.reset()\n\n    def reset(self):\n        \"\"\"Reset the instance\n\n        - reset rows and header\n        \"\"\"\n\n        self._hline_string = None\n        self._row_size = None\n        self._header = []\n        self._rows = []\n\n    def set_chars(self, array):\n        \"\"\"Set the characters used to draw lines between rows and columns\n\n        - the array should contain 4 fields:\n\n            [horizontal, vertical, corner, header]\n\n        - default is set to:\n\n            ['-', '|', '+', '=']\n        \"\"\"\n\n        if len(array) != 4:\n            raise ArraySizeError(\"array should contain 4 characters\")\n        array = [ x[:1] for x in [ str(s) for s in array ] ]\n        (self._char_horiz, self._char_vert,\n            self._char_corner, self._char_header) = array\n\n    def set_deco(self, deco):\n        \"\"\"Set the table decoration\n\n        - 'deco' can be a combinaison of:\n\n            Texttable.BORDER: Border around the table\n            Texttable.HEADER: Horizontal line below the header\n            Texttable.HLINES: Horizontal lines between rows\n            Texttable.VLINES: Vertical lines between columns\n\n           All of them are enabled by default\n\n        - example:\n\n            Texttable.BORDER | Texttable.HEADER\n        \"\"\"\n\n        self._deco = deco\n\n    def set_cols_align(self, array):\n        \"\"\"Set the desired columns alignment\n\n        - the elements of the array should be either \"l\", \"c\" or \"r\":\n\n            * \"l\": column flushed left\n            * \"c\": column centered\n            * \"r\": column flushed right\n        \"\"\"\n\n        self._check_row_size(array)\n        self._align = array\n\n    def set_cols_valign(self, array):\n        \"\"\"Set the desired columns vertical alignment\n\n        - the elements of the array should be either \"t\", \"m\" or \"b\":\n\n            * \"t\": column aligned on the top of the cell\n            * \"m\": column aligned on the middle of the cell\n            * \"b\": column aligned on the bottom of the cell\n        \"\"\"\n\n        self._check_row_size(array)\n        self._valign = array\n\n    def set_cols_dtype(self, array):\n        \"\"\"Set the desired columns datatype for the cols.\n\n        - the elements of the array should be either \"a\", \"t\", \"f\", \"e\" or \"i\":\n\n            * \"a\": automatic (try to use the most appropriate datatype)\n            * \"t\": treat as text\n            * \"f\": treat as float in decimal format\n            * \"e\": treat as float in exponential format\n            * \"i\": treat as int\n\n        - by default, automatic datatyping is used for each column\n        \"\"\"\n\n        self._check_row_size(array)\n        self._dtype = array\n\n    def set_cols_width(self, array):\n        \"\"\"Set the desired columns width\n\n        - the elements of the array should be integers, specifying the\n          width of each column. For example:\n\n                [10, 20, 5]\n        \"\"\"\n\n        self._check_row_size(array)\n        try:\n            array = list(map(int, array))\n            if reduce(min, array) <= 0:\n                raise ValueError\n        except ValueError:\n            sys.stderr.write(\"Wrong argument in column width specification\\n\")\n            raise\n        self._width = array\n\n    def set_precision(self, width):\n        \"\"\"Set the desired precision for float/exponential formats\n\n        - width must be an integer >= 0\n\n        - default value is set to 3\n        \"\"\"\n\n        if not type(width) is int or width < 0:\n            raise ValueError('width must be an integer greater then 0')\n        self._precision = width\n\n    def header(self, array):\n        \"\"\"Specify the header of the table\n        \"\"\"\n\n        self._check_row_size(array)\n        self._header = list(map(str, array))\n\n    def add_row(self, array):\n        \"\"\"Add a row in the rows stack\n\n        - cells can contain newlines and tabs\n        \"\"\"\n\n        self._check_row_size(array)\n\n        if not hasattr(self, \"_dtype\"):\n            self._dtype = [\"a\"] * self._row_size\n            \n        cells = []\n        for i, x in enumerate(array):\n            cells.append(self._str(i, x))\n        self._rows.append(cells)\n\n    def add_rows(self, rows, header=True):\n        \"\"\"Add several rows in the rows stack\n\n        - The 'rows' argument can be either an iterator returning arrays,\n          or a by-dimensional array\n        - 'header' specifies if the first row should be used as the header\n          of the table\n        \"\"\"\n\n        # nb: don't use 'iter' on by-dimensional arrays, to get a \n        #     usable code for python 2.1\n        if header:\n            if hasattr(rows, '__iter__') and hasattr(rows, 'next'):\n                self.header(rows.next())\n            else:\n                self.header(rows[0])\n                rows = rows[1:]\n        for row in rows:\n            self.add_row(row)\n\n    def draw(self):\n        \"\"\"Draw the table\n\n        - the table is returned as a whole string\n        \"\"\"\n\n        if not self._header and not self._rows:\n            return\n        self._compute_cols_width()\n        self._check_align()\n        out = \"\"\n        if self._has_border():\n            out += self._hline()\n        if self._header:\n            out += self._draw_line(self._header, isheader=True)\n            if self._has_header():\n                out += self._hline_header()\n        length = 0\n        for row in self._rows:\n            length += 1\n            out += self._draw_line(row)\n            if self._has_hlines() and length < len(self._rows):\n                out += self._hline()\n        if self._has_border():\n            out += self._hline()\n        return out[:-1]\n\n    def _str(self, i, x):\n        \"\"\"Handles string formatting of cell data\n\n            i - index of the cell datatype in self._dtype \n            x - cell data to format\n        \"\"\"\n        try:\n            f = float(x)\n        except:\n            return str(x)\n\n        n = self._precision\n        dtype = self._dtype[i]\n\n        if dtype == 'i':\n            return str(int(round(f)))\n        elif dtype == 'f':\n            return '%.*f' % (n, f)\n        elif dtype == 'e':\n            return '%.*e' % (n, f)\n        elif dtype == 't':\n            return str(x)\n        else:\n            if f - round(f) == 0:\n                if abs(f) > 1e8:\n                    return '%.*e' % (n, f)\n                else:\n                    return str(int(round(f)))\n            else:\n                if abs(f) > 1e8:\n                    return '%.*e' % (n, f)\n                else:\n                    return '%.*f' % (n, f)\n\n    def _check_row_size(self, array):\n        \"\"\"Check that the specified array fits the previous rows size\n        \"\"\"\n\n        if not self._row_size:\n            self._row_size = len(array)\n        elif self._row_size != len(array):\n            raise ArraySizeError(\"array should contain %d elements\" \\\n                % self._row_size)\n\n    def _has_vlines(self):\n        \"\"\"Return a boolean, if vlines are required or not\n        \"\"\"\n\n        return self._deco & Texttable.VLINES > 0\n\n    def _has_hlines(self):\n        \"\"\"Return a boolean, if hlines are required or not\n        \"\"\"\n\n        return self._deco & Texttable.HLINES > 0\n\n    def _has_border(self):\n        \"\"\"Return a boolean, if border is required or not\n        \"\"\"\n\n        return self._deco & Texttable.BORDER > 0\n\n    def _has_header(self):\n        \"\"\"Return a boolean, if header line is required or not\n        \"\"\"\n\n        return self._deco & Texttable.HEADER > 0\n\n    def _hline_header(self):\n        \"\"\"Print header's horizontal line\n        \"\"\"\n\n        return self._build_hline(True)\n\n    def _hline(self):\n        \"\"\"Print an horizontal line\n        \"\"\"\n\n        if not self._hline_string:\n            self._hline_string = self._build_hline()\n        return self._hline_string\n\n    def _build_hline(self, is_header=False):\n        \"\"\"Return a string used to separated rows or separate header from\n        rows\n        \"\"\"\n        horiz = self._char_horiz\n        if (is_header):\n            horiz = self._char_header\n        # compute cell separator\n        s = \"%s%s%s\" % (horiz, [horiz, self._char_corner][self._has_vlines()],\n            horiz)\n        # build the line\n        l = s.join([horiz * n for n in self._width])\n        # add border if needed\n        if self._has_border():\n            l = \"%s%s%s%s%s\\n\" % (self._char_corner, horiz, l, horiz,\n                self._char_corner)\n        else:\n            l += \"\\n\"\n        return l\n\n    def _len_cell(self, cell):\n        \"\"\"Return the width of the cell\n\n        Special characters are taken into account to return the width of the\n        cell, such like newlines and tabs\n        \"\"\"\n\n        cell_lines = cell.split('\\n')\n        maxi = 0\n        for line in cell_lines:\n            length = 0\n            parts = line.split('\\t')\n            for part, i in zip(parts, list(range(1, len(parts) + 1))):\n                length = length + len(part)\n                if i < len(parts):\n                    length = (length//8 + 1) * 8\n            maxi = max(maxi, length)\n        return maxi\n\n    def _compute_cols_width(self):\n        \"\"\"Return an array with the width of each column\n\n        If a specific width has been specified, exit. If the total of the\n        columns width exceed the table desired width, another width will be\n        computed to fit, and cells will be wrapped.\n        \"\"\"\n\n        if hasattr(self, \"_width\"):\n            return\n        maxi = []\n        if self._header:\n            maxi = [ self._len_cell(x) for x in self._header ]\n        for row in self._rows:\n            for cell,i in zip(row, list(range(len(row)))):\n                try:\n                    maxi[i] = max(maxi[i], self._len_cell(cell))\n                except (TypeError, IndexError):\n                    maxi.append(self._len_cell(cell))\n        items = len(maxi)\n        length = reduce(lambda x, y: x+y, maxi)\n        if self._max_width and length + items * 3 + 1 > self._max_width:\n            maxi = [(self._max_width - items * 3 -1) // items \\\n                    for n in range(items)]\n        self._width = maxi\n\n    def _check_align(self):\n        \"\"\"Check if alignment has been specified, set default one if not\n        \"\"\"\n\n        if not hasattr(self, \"_align\"):\n            self._align = [\"l\"] * self._row_size\n        if not hasattr(self, \"_valign\"):\n            self._valign = [\"t\"] * self._row_size\n\n    def _draw_line(self, line, isheader=False):\n        \"\"\"Draw a line\n\n        Loop over a single cell length, over all the cells\n        \"\"\"\n\n        line = self._splitit(line, isheader)\n        space = \" \"\n        out = \"\"\n        for i in range(len(line[0])):\n            if self._has_border():\n                out += \"%s \" % self._char_vert\n            length = 0\n            for cell, width, align in zip(line, self._width, self._align):\n                length += 1\n                cell_line = cell[i]\n                fill = width - len(cell_line)\n                if isheader:\n                    align = \"c\"\n                if align == \"r\":\n                    out += \"%s \" % (fill * space + cell_line)\n                elif align == \"c\":\n                    out += \"%s \" % (int(fill/2) * space + cell_line \\\n                            + int(fill/2 + fill%2) * space)\n                else:\n                    out += \"%s \" % (cell_line + fill * space)\n                if length < len(line):\n                    out += \"%s \" % [space, self._char_vert][self._has_vlines()]\n            out += \"%s\\n\" % ['', self._char_vert][self._has_border()]\n        return out\n\n    def _splitit(self, line, isheader):\n        \"\"\"Split each element of line to fit the column width\n\n        Each element is turned into a list, result of the wrapping of the\n        string to the desired width\n        \"\"\"\n\n        line_wrapped = []\n        for cell, width in zip(line, self._width):\n            array = []\n            for c in cell.split('\\n'):\n                try:\n                    if sys.version >= '3.0':\n                        c = str(c)\n                    else:\n                        c = unicode(c, 'utf')\n                except UnicodeDecodeError as strerror:\n                    sys.stderr.write(\"UnicodeDecodeError exception for string '%s': %s\\n\" % (c, strerror))\n                    if sys.version >= '3.0':\n                        c = str(c, 'utf', 'replace')\n                    else:\n                        c = unicode(c, 'utf', 'replace')\n                array.extend(textwrap.wrap(c, width))\n            line_wrapped.append(array)\n        max_cell_lines = reduce(max, list(map(len, line_wrapped)))\n        for cell, valign in zip(line_wrapped, self._valign):\n            if isheader:\n                valign = \"t\"\n            if valign == \"m\":\n                missing = max_cell_lines - len(cell)\n                cell[:0] = [\"\"] * int(missing / 2)\n                cell.extend([\"\"] * int(missing / 2 + missing % 2))\n            elif valign == \"b\":\n                cell[:0] = [\"\"] * (max_cell_lines - len(cell))\n            else:\n                cell.extend([\"\"] * (max_cell_lines - len(cell)))\n        return line_wrapped\n\n\nif __name__ == '__main__':\n    table = Texttable()\n    table.set_cols_align([\"l\", \"r\", \"c\"])\n    table.set_cols_valign([\"t\", \"m\", \"b\"])\n    table.add_rows([[\"Name\", \"Age\", \"Nickname\"],\n                    [\"Mr\\nXavier\\nHuon\", 32, \"Xav'\"],\n                    [\"Mr\\nBaptiste\\nClement\", 1, \"Baby\"]])\n    print(table.draw() + \"\\n\")\n\n    table = Texttable()\n    table.set_deco(Texttable.HEADER)\n    table.set_cols_dtype(['t',  # text \n                          'f',  # float (decimal)\n                          'e',  # float (exponent)\n                          'i',  # integer\n                          'a']) # automatic\n    table.set_cols_align([\"l\", \"r\", \"r\", \"r\", \"l\"])\n    table.add_rows([[\"text\",    \"float\", \"exp\", \"int\", \"auto\"],\n                    [\"abcd\",    \"67\",    654,   89,    128.001],\n                    [\"efghijk\", 67.5434, .654,  89.6,  12800000000000000000000.00023],\n                    [\"lmn\",     5e-78,   5e-78, 89.4,  .000000000000128],\n                    [\"opqrstu\", .023,    5e+78, 92.,   12800000000000000000000]])\n    print(table.draw())\n", 
    "textwrap": "\"\"\"Text wrapping and filling.\n\"\"\"\n\n# Copyright (C) 1999-2001 Gregory P. Ward.\n# Copyright (C) 2002, 2003 Python Software Foundation.\n# Written by Greg Ward <gward@python.net>\n\n__revision__ = \"$Id$\"\n\nimport string, re\n\ntry:\n    _unicode = unicode\nexcept NameError:\n    # If Python is built without Unicode support, the unicode type\n    # will not exist. Fake one.\n    class _unicode(object):\n        pass\n\n# Do the right thing with boolean values for all known Python versions\n# (so this module can be copied to projects that don't depend on Python\n# 2.3, e.g. Optik and Docutils) by uncommenting the block of code below.\n#try:\n#    True, False\n#except NameError:\n#    (True, False) = (1, 0)\n\n__all__ = ['TextWrapper', 'wrap', 'fill', 'dedent']\n\n# Hardcode the recognized whitespace characters to the US-ASCII\n# whitespace characters.  The main reason for doing this is that in\n# ISO-8859-1, 0xa0 is non-breaking whitespace, so in certain locales\n# that character winds up in string.whitespace.  Respecting\n# string.whitespace in those cases would 1) make textwrap treat 0xa0 the\n# same as any other whitespace char, which is clearly wrong (it's a\n# *non-breaking* space), 2) possibly cause problems with Unicode,\n# since 0xa0 is not in range(128).\n_whitespace = '\\t\\n\\x0b\\x0c\\r '\n\nclass TextWrapper:\n    \"\"\"\n    Object for wrapping/filling text.  The public interface consists of\n    the wrap() and fill() methods; the other methods are just there for\n    subclasses to override in order to tweak the default behaviour.\n    If you want to completely replace the main wrapping algorithm,\n    you'll probably have to override _wrap_chunks().\n\n    Several instance attributes control various aspects of wrapping:\n      width (default: 70)\n        the maximum width of wrapped lines (unless break_long_words\n        is false)\n      initial_indent (default: \"\")\n        string that will be prepended to the first line of wrapped\n        output.  Counts towards the line's width.\n      subsequent_indent (default: \"\")\n        string that will be prepended to all lines save the first\n        of wrapped output; also counts towards each line's width.\n      expand_tabs (default: true)\n        Expand tabs in input text to spaces before further processing.\n        Each tab will become 1 .. 8 spaces, depending on its position in\n        its line.  If false, each tab is treated as a single character.\n      replace_whitespace (default: true)\n        Replace all whitespace characters in the input text by spaces\n        after tab expansion.  Note that if expand_tabs is false and\n        replace_whitespace is true, every tab will be converted to a\n        single space!\n      fix_sentence_endings (default: false)\n        Ensure that sentence-ending punctuation is always followed\n        by two spaces.  Off by default because the algorithm is\n        (unavoidably) imperfect.\n      break_long_words (default: true)\n        Break words longer than 'width'.  If false, those words will not\n        be broken, and some lines might be longer than 'width'.\n      break_on_hyphens (default: true)\n        Allow breaking hyphenated words. If true, wrapping will occur\n        preferably on whitespaces and right after hyphens part of\n        compound words.\n      drop_whitespace (default: true)\n        Drop leading and trailing whitespace from lines.\n    \"\"\"\n\n    whitespace_trans = string.maketrans(_whitespace, ' ' * len(_whitespace))\n\n    unicode_whitespace_trans = {}\n    uspace = ord(u' ')\n    for x in map(ord, _whitespace):\n        unicode_whitespace_trans[x] = uspace\n\n    # This funky little regex is just the trick for splitting\n    # text up into word-wrappable chunks.  E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-/ball,/ /use/ /the/ /-b/ /option!\n    # (after stripping out empty strings).\n    wordsep_re = re.compile(\n        r'(\\s+|'                                  # any whitespace\n        r'[^\\s\\w]*\\w+[^0-9\\W]-(?=\\w+[^0-9\\W])|'   # hyphenated words\n        r'(?<=[\\w\\!\\\"\\'\\&\\.\\,\\?])-{2,}(?=\\w))')   # em-dash\n\n    # This less funky little regex just split on recognized spaces. E.g.\n    #   \"Hello there -- you goof-ball, use the -b option!\"\n    # splits into\n    #   Hello/ /there/ /--/ /you/ /goof-ball,/ /use/ /the/ /-b/ /option!/\n    wordsep_simple_re = re.compile(r'(\\s+)')\n\n    # XXX this is not locale- or charset-aware -- string.lowercase\n    # is US-ASCII only (and therefore English-only)\n    sentence_end_re = re.compile(r'[%s]'              # lowercase letter\n                                 r'[\\.\\!\\?]'          # sentence-ending punct.\n                                 r'[\\\"\\']?'           # optional end-of-quote\n                                 r'\\Z'                # end of chunk\n                                 % string.lowercase)\n\n\n    def __init__(self,\n                 width=70,\n                 initial_indent=\"\",\n                 subsequent_indent=\"\",\n                 expand_tabs=True,\n                 replace_whitespace=True,\n                 fix_sentence_endings=False,\n                 break_long_words=True,\n                 drop_whitespace=True,\n                 break_on_hyphens=True):\n        self.width = width\n        self.initial_indent = initial_indent\n        self.subsequent_indent = subsequent_indent\n        self.expand_tabs = expand_tabs\n        self.replace_whitespace = replace_whitespace\n        self.fix_sentence_endings = fix_sentence_endings\n        self.break_long_words = break_long_words\n        self.drop_whitespace = drop_whitespace\n        self.break_on_hyphens = break_on_hyphens\n\n        # recompile the regexes for Unicode mode -- done in this clumsy way for\n        # backwards compatibility because it's rather common to monkey-patch\n        # the TextWrapper class' wordsep_re attribute.\n        self.wordsep_re_uni = re.compile(self.wordsep_re.pattern, re.U)\n        self.wordsep_simple_re_uni = re.compile(\n            self.wordsep_simple_re.pattern, re.U)\n\n\n    # -- Private methods -----------------------------------------------\n    # (possibly useful for subclasses to override)\n\n    def _munge_whitespace(self, text):\n        \"\"\"_munge_whitespace(text : string) -> string\n\n        Munge whitespace in text: expand tabs and convert all other\n        whitespace characters to spaces.  Eg. \" foo\\tbar\\n\\nbaz\"\n        becomes \" foo    bar  baz\".\n        \"\"\"\n        if self.expand_tabs:\n            text = text.expandtabs()\n        if self.replace_whitespace:\n            if isinstance(text, str):\n                text = text.translate(self.whitespace_trans)\n            elif isinstance(text, _unicode):\n                text = text.translate(self.unicode_whitespace_trans)\n        return text\n\n\n    def _split(self, text):\n        \"\"\"_split(text : string) -> [string]\n\n        Split the text to wrap into indivisible chunks.  Chunks are\n        not quite the same as words; see _wrap_chunks() for full\n        details.  As an example, the text\n          Look, goof-ball -- use the -b option!\n        breaks into the following chunks:\n          'Look,', ' ', 'goof-', 'ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', 'option!'\n        if break_on_hyphens is True, or in:\n          'Look,', ' ', 'goof-ball', ' ', '--', ' ',\n          'use', ' ', 'the', ' ', '-b', ' ', option!'\n        otherwise.\n        \"\"\"\n        if isinstance(text, _unicode):\n            if self.break_on_hyphens:\n                pat = self.wordsep_re_uni\n            else:\n                pat = self.wordsep_simple_re_uni\n        else:\n            if self.break_on_hyphens:\n                pat = self.wordsep_re\n            else:\n                pat = self.wordsep_simple_re\n        chunks = pat.split(text)\n        chunks = filter(None, chunks)  # remove empty chunks\n        return chunks\n\n    def _fix_sentence_endings(self, chunks):\n        \"\"\"_fix_sentence_endings(chunks : [string])\n\n        Correct for sentence endings buried in 'chunks'.  Eg. when the\n        original text contains \"... foo.\\nBar ...\", munge_whitespace()\n        and split() will convert that to [..., \"foo.\", \" \", \"Bar\", ...]\n        which has one too few spaces; this method simply changes the one\n        space to two.\n        \"\"\"\n        i = 0\n        patsearch = self.sentence_end_re.search\n        while i < len(chunks)-1:\n            if chunks[i+1] == \" \" and patsearch(chunks[i]):\n                chunks[i+1] = \"  \"\n                i += 2\n            else:\n                i += 1\n\n    def _handle_long_word(self, reversed_chunks, cur_line, cur_len, width):\n        \"\"\"_handle_long_word(chunks : [string],\n                             cur_line : [string],\n                             cur_len : int, width : int)\n\n        Handle a chunk of text (most likely a word, not whitespace) that\n        is too long to fit in any line.\n        \"\"\"\n        # Figure out when indent is larger than the specified width, and make\n        # sure at least one character is stripped off on every pass\n        if width < 1:\n            space_left = 1\n        else:\n            space_left = width - cur_len\n\n        # If we're allowed to break long words, then do so: put as much\n        # of the next chunk onto the current line as will fit.\n        if self.break_long_words:\n            cur_line.append(reversed_chunks[-1][:space_left])\n            reversed_chunks[-1] = reversed_chunks[-1][space_left:]\n\n        # Otherwise, we have to preserve the long word intact.  Only add\n        # it to the current line if there's nothing already there --\n        # that minimizes how much we violate the width constraint.\n        elif not cur_line:\n            cur_line.append(reversed_chunks.pop())\n\n        # If we're not allowed to break long words, and there's already\n        # text on the current line, do nothing.  Next time through the\n        # main loop of _wrap_chunks(), we'll wind up here again, but\n        # cur_len will be zero, so the next line will be entirely\n        # devoted to the long word that we can't handle right now.\n\n    def _wrap_chunks(self, chunks):\n        \"\"\"_wrap_chunks(chunks : [string]) -> [string]\n\n        Wrap a sequence of text chunks and return a list of lines of\n        length 'self.width' or less.  (If 'break_long_words' is false,\n        some lines may be longer than this.)  Chunks correspond roughly\n        to words and the whitespace between them: each chunk is\n        indivisible (modulo 'break_long_words'), but a line break can\n        come between any two chunks.  Chunks should not have internal\n        whitespace; ie. a chunk is either all whitespace or a \"word\".\n        Whitespace chunks will be removed from the beginning and end of\n        lines, but apart from that whitespace is preserved.\n        \"\"\"\n        lines = []\n        if self.width <= 0:\n            raise ValueError(\"invalid width %r (must be > 0)\" % self.width)\n\n        # Arrange in reverse order so items can be efficiently popped\n        # from a stack of chucks.\n        chunks.reverse()\n\n        while chunks:\n\n            # Start the list of chunks that will make up the current line.\n            # cur_len is just the length of all the chunks in cur_line.\n            cur_line = []\n            cur_len = 0\n\n            # Figure out which static string will prefix this line.\n            if lines:\n                indent = self.subsequent_indent\n            else:\n                indent = self.initial_indent\n\n            # Maximum width for this line.\n            width = self.width - len(indent)\n\n            # First chunk on line is whitespace -- drop it, unless this\n            # is the very beginning of the text (ie. no lines started yet).\n            if self.drop_whitespace and chunks[-1].strip() == '' and lines:\n                del chunks[-1]\n\n            while chunks:\n                l = len(chunks[-1])\n\n                # Can at least squeeze this chunk onto the current line.\n                if cur_len + l <= width:\n                    cur_line.append(chunks.pop())\n                    cur_len += l\n\n                # Nope, this line is full.\n                else:\n                    break\n\n            # The current line is full, and the next chunk is too big to\n            # fit on *any* line (not just this one).\n            if chunks and len(chunks[-1]) > width:\n                self._handle_long_word(chunks, cur_line, cur_len, width)\n\n            # If the last chunk on this line is all whitespace, drop it.\n            if self.drop_whitespace and cur_line and cur_line[-1].strip() == '':\n                del cur_line[-1]\n\n            # Convert current line back to a string and store it in list\n            # of all lines (return value).\n            if cur_line:\n                lines.append(indent + ''.join(cur_line))\n\n        return lines\n\n\n    # -- Public interface ----------------------------------------------\n\n    def wrap(self, text):\n        \"\"\"wrap(text : string) -> [string]\n\n        Reformat the single paragraph in 'text' so it fits in lines of\n        no more than 'self.width' columns, and return a list of wrapped\n        lines.  Tabs in 'text' are expanded with string.expandtabs(),\n        and all other whitespace characters (including newline) are\n        converted to space.\n        \"\"\"\n        text = self._munge_whitespace(text)\n        chunks = self._split(text)\n        if self.fix_sentence_endings:\n            self._fix_sentence_endings(chunks)\n        return self._wrap_chunks(chunks)\n\n    def fill(self, text):\n        \"\"\"fill(text : string) -> string\n\n        Reformat the single paragraph in 'text' to fit in lines of no\n        more than 'self.width' columns, and return a new string\n        containing the entire wrapped paragraph.\n        \"\"\"\n        return \"\\n\".join(self.wrap(text))\n\n\n# -- Convenience interface ---------------------------------------------\n\ndef wrap(text, width=70, **kwargs):\n    \"\"\"Wrap a single paragraph of text, returning a list of wrapped lines.\n\n    Reformat the single paragraph in 'text' so it fits in lines of no\n    more than 'width' columns, and return a list of wrapped lines.  By\n    default, tabs in 'text' are expanded with string.expandtabs(), and\n    all other whitespace characters (including newline) are converted to\n    space.  See TextWrapper class for available keyword args to customize\n    wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.wrap(text)\n\ndef fill(text, width=70, **kwargs):\n    \"\"\"Fill a single paragraph of text, returning a new string.\n\n    Reformat the single paragraph in 'text' to fit in lines of no more\n    than 'width' columns, and return a new string containing the entire\n    wrapped paragraph.  As with wrap(), tabs are expanded and other\n    whitespace characters converted to space.  See TextWrapper class for\n    available keyword args to customize wrapping behaviour.\n    \"\"\"\n    w = TextWrapper(width=width, **kwargs)\n    return w.fill(text)\n\n\n# -- Loosely related functionality -------------------------------------\n\n_whitespace_only_re = re.compile('^[ \\t]+$', re.MULTILINE)\n_leading_whitespace_re = re.compile('(^[ \\t]*)(?:[^ \\t\\n])', re.MULTILINE)\n\ndef dedent(text):\n    \"\"\"Remove any common leading whitespace from every line in `text`.\n\n    This can be used to make triple-quoted strings line up with the left\n    edge of the display, while still presenting them in the source code\n    in indented form.\n\n    Note that tabs and spaces are both treated as whitespace, but they\n    are not equal: the lines \"  hello\" and \"\\thello\" are\n    considered to have no common leading whitespace.  (This behaviour is\n    new in Python 2.5; older versions of this module incorrectly\n    expanded tabs before searching for common leading whitespace.)\n    \"\"\"\n    # Look for the longest leading string of spaces and tabs common to\n    # all lines.\n    margin = None\n    text = _whitespace_only_re.sub('', text)\n    indents = _leading_whitespace_re.findall(text)\n    for indent in indents:\n        if margin is None:\n            margin = indent\n\n        # Current line more deeply indented than previous winner:\n        # no change (previous winner is still on top).\n        elif indent.startswith(margin):\n            pass\n\n        # Current line consistent with and no deeper than previous winner:\n        # it's the new winner.\n        elif margin.startswith(indent):\n            margin = indent\n\n        # Current line and previous winner have no common whitespace:\n        # there is no margin.\n        else:\n            margin = \"\"\n            break\n\n    # sanity check (testing/debugging only)\n    if 0 and margin:\n        for line in text.split(\"\\n\"):\n            assert not line or line.startswith(margin), \\\n                   \"line = %r, margin = %r\" % (line, margin)\n\n    if margin:\n        text = re.sub(r'(?m)^' + margin, '', text)\n    return text\n\nif __name__ == \"__main__\":\n    #print dedent(\"\\tfoo\\n\\tbar\")\n    #print dedent(\"  \\thello there\\n  \\t  how are you?\")\n    print dedent(\"Hello there.\\n  This is indented.\")\n", 
    "threading": "\"\"\"Thread module emulating a subset of Java's threading model.\"\"\"\n\nimport sys as _sys\n\ntry:\n    import thread\nexcept ImportError:\n    import dummy_thread as thread\n\n\nimport warnings\n\nfrom collections import deque as _deque\nfrom itertools import count as _count\nfrom time import time as _time, sleep as _sleep\nfrom traceback import format_exc as _format_exc\n\n# Note regarding PEP 8 compliant aliases\n#  This threading model was originally inspired by Java, and inherited\n# the convention of camelCase function and method names from that\n# language. While those names are not in any imminent danger of being\n# deprecated, starting with Python 2.6, the module now provides a\n# PEP 8 compliant alias for any such method name.\n# Using the new PEP 8 compliant names also facilitates substitution\n# with the multiprocessing module, which doesn't provide the old\n# Java inspired names.\n\n\n# Rename some stuff so \"from threading import *\" is safe\n__all__ = ['activeCount', 'active_count', 'Condition', 'currentThread',\n           'current_thread', 'enumerate', 'Event',\n           'Lock', 'RLock', 'Semaphore', 'BoundedSemaphore', 'Thread',\n           'Timer', 'setprofile', 'settrace', 'local', 'stack_size']\n\n_start_new_thread = thread.start_new_thread\n_allocate_lock = thread.allocate_lock\n_get_ident = thread.get_ident\nThreadError = thread.error\ndel thread\n\n\n# sys.exc_clear is used to work around the fact that except blocks\n# don't fully clear the exception until 3.0.\nwarnings.filterwarnings('ignore', category=DeprecationWarning,\n                        module='threading', message='sys.exc_clear')\n\n# Debug support (adapted from ihooks.py).\n# All the major classes here derive from _Verbose.  We force that to\n# be a new-style class so that all the major classes here are new-style.\n# This helps debugging (type(instance) is more revealing for instances\n# of new-style classes).\n\n_VERBOSE = False\n\nif __debug__:\n\n    class _Verbose(object):\n\n        def __init__(self, verbose=None):\n            if verbose is None:\n                verbose = _VERBOSE\n            self.__verbose = verbose\n\n        def _note(self, format, *args):\n            if self.__verbose:\n                format = format % args\n                # Issue #4188: calling current_thread() can incur an infinite\n                # recursion if it has to create a DummyThread on the fly.\n                ident = _get_ident()\n                try:\n                    name = _active[ident].name\n                except KeyError:\n                    name = \"<OS thread %d>\" % ident\n                format = \"%s: %s\\n\" % (name, format)\n                _sys.stderr.write(format)\n\nelse:\n    # Disable this when using \"python -O\"\n    class _Verbose(object):\n        def __init__(self, verbose=None):\n            pass\n        def _note(self, *args):\n            pass\n\n# Support for profile and trace hooks\n\n_profile_hook = None\n_trace_hook = None\n\ndef setprofile(func):\n    \"\"\"Set a profile function for all threads started from the threading module.\n\n    The func will be passed to sys.setprofile() for each thread, before its\n    run() method is called.\n\n    \"\"\"\n    global _profile_hook\n    _profile_hook = func\n\ndef settrace(func):\n    \"\"\"Set a trace function for all threads started from the threading module.\n\n    The func will be passed to sys.settrace() for each thread, before its run()\n    method is called.\n\n    \"\"\"\n    global _trace_hook\n    _trace_hook = func\n\n# Synchronization classes\n\ndef Lock():\n    import js\n    return js.eval('new Semaphore(1)');\n\ndef RLock(*args, **kwargs):\n    \"\"\"Factory function that returns a new reentrant lock.\n\n    A reentrant lock must be released by the thread that acquired it. Once a\n    thread has acquired a reentrant lock, the same thread may acquire it again\n    without blocking; the thread must release it once for each time it has\n    acquired it.\n\n    \"\"\"\n    return _RLock(*args, **kwargs)\n\nclass _RLock(_Verbose):\n    \"\"\"A reentrant lock must be released by the thread that acquired it. Once a\n       thread has acquired a reentrant lock, the same thread may acquire it\n       again without blocking; the thread must release it once for each time it\n       has acquired it.\n    \"\"\"\n\n    def __init__(self, verbose=None):\n        _Verbose.__init__(self, verbose)\n        self.__block = _allocate_lock()\n        self.__owner = None\n        self.__count = 0\n\n    def __repr__(self):\n        owner = self.__owner\n        try:\n            owner = _active[owner].name\n        except KeyError:\n            pass\n        return \"<%s owner=%r count=%d>\" % (\n                self.__class__.__name__, owner, self.__count)\n\n    def acquire(self, blocking=1):\n        \"\"\"Acquire a lock, blocking or non-blocking.\n\n        When invoked without arguments: if this thread already owns the lock,\n        increment the recursion level by one, and return immediately. Otherwise,\n        if another thread owns the lock, block until the lock is unlocked. Once\n        the lock is unlocked (not owned by any thread), then grab ownership, set\n        the recursion level to one, and return. If more than one thread is\n        blocked waiting until the lock is unlocked, only one at a time will be\n        able to grab ownership of the lock. There is no return value in this\n        case.\n\n        When invoked with the blocking argument set to true, do the same thing\n        as when called without arguments, and return true.\n\n        When invoked with the blocking argument set to false, do not block. If a\n        call without an argument would block, return false immediately;\n        otherwise, do the same thing as when called without arguments, and\n        return true.\n\n        \"\"\"\n        me = _get_ident()\n        if self.__owner == me:\n            self.__count = self.__count + 1\n            if __debug__:\n                self._note(\"%s.acquire(%s): recursive success\", self, blocking)\n            return 1\n        rc = self.__block.acquire(blocking)\n        if rc:\n            self.__owner = me\n            self.__count = 1\n            if __debug__:\n                self._note(\"%s.acquire(%s): initial success\", self, blocking)\n        else:\n            if __debug__:\n                self._note(\"%s.acquire(%s): failure\", self, blocking)\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a lock, decrementing the recursion level.\n\n        If after the decrement it is zero, reset the lock to unlocked (not owned\n        by any thread), and if any other threads are blocked waiting for the\n        lock to become unlocked, allow exactly one of them to proceed. If after\n        the decrement the recursion level is still nonzero, the lock remains\n        locked and owned by the calling thread.\n\n        Only call this method when the calling thread owns the lock. A\n        RuntimeError is raised if this method is called when the lock is\n        unlocked.\n\n        There is no return value.\n\n        \"\"\"\n        if self.__owner != _get_ident():\n            raise RuntimeError(\"cannot release un-acquired lock\")\n        self.__count = count = self.__count - 1\n        if not count:\n            self.__owner = None\n            self.__block.release()\n            if __debug__:\n                self._note(\"%s.release(): final release\", self)\n        else:\n            if __debug__:\n                self._note(\"%s.release(): non-final release\", self)\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n    # Internal methods used by condition variables\n\n    def _acquire_restore(self, count_owner):\n        count, owner = count_owner\n        self.__block.acquire()\n        self.__count = count\n        self.__owner = owner\n        if __debug__:\n            self._note(\"%s._acquire_restore()\", self)\n\n    def _release_save(self):\n        if __debug__:\n            self._note(\"%s._release_save()\", self)\n        count = self.__count\n        self.__count = 0\n        owner = self.__owner\n        self.__owner = None\n        self.__block.release()\n        return (count, owner)\n\n    def _is_owned(self):\n        return self.__owner == _get_ident()\n\n\ndef Condition(*args, **kwargs):\n    \"\"\"Factory function that returns a new condition variable object.\n\n    A condition variable allows one or more threads to wait until they are\n    notified by another thread.\n\n    If the lock argument is given and not None, it must be a Lock or RLock\n    object, and it is used as the underlying lock. Otherwise, a new RLock object\n    is created and used as the underlying lock.\n\n    \"\"\"\n    return _Condition(*args, **kwargs)\n\nclass _Condition(_Verbose):\n    \"\"\"Condition variables allow one or more threads to wait until they are\n       notified by another thread.\n    \"\"\"\n\n    def __init__(self, lock=None, verbose=None):\n        _Verbose.__init__(self, verbose)\n        if lock is None:\n            lock = RLock()\n        self.__lock = lock\n        # Export the lock's acquire() and release() methods\n        self.acquire = lock.acquire\n        self.release = lock.release\n        # If the lock defines _release_save() and/or _acquire_restore(),\n        # these override the default implementations (which just call\n        # release() and acquire() on the lock).  Ditto for _is_owned().\n        try:\n            self._release_save = lock._release_save\n        except AttributeError:\n            pass\n        try:\n            self._acquire_restore = lock._acquire_restore\n        except AttributeError:\n            pass\n        try:\n            self._is_owned = lock._is_owned\n        except AttributeError:\n            pass\n        self.__waiters = []\n\n    def __enter__(self):\n        return self.__lock.__enter__()\n\n    def __exit__(self, *args):\n        return self.__lock.__exit__(*args)\n\n    def __repr__(self):\n        return \"<Condition(%s, %d)>\" % (self.__lock, len(self.__waiters))\n\n    def _release_save(self):\n        self.__lock.release()           # No state to save\n\n    def _acquire_restore(self, x):\n        self.__lock.acquire()           # Ignore saved state\n\n    def _is_owned(self):\n        # Return True if lock is owned by current_thread.\n        # This method is called only if __lock doesn't have _is_owned().\n        if self.__lock.acquire(0):\n            self.__lock.release()\n            return False\n        else:\n            return True\n\n    def wait(self, timeout=None):\n        \"\"\"Wait until notified or until a timeout occurs.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method releases the underlying lock, and then blocks until it is\n        awakened by a notify() or notifyAll() call for the same condition\n        variable in another thread, or until the optional timeout occurs. Once\n        awakened or timed out, it re-acquires the lock and returns.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        When the underlying lock is an RLock, it is not released using its\n        release() method, since this may not actually unlock the lock when it\n        was acquired multiple times recursively. Instead, an internal interface\n        of the RLock class is used, which really unlocks it even when it has\n        been recursively acquired several times. Another internal interface is\n        then used to restore the recursion level when the lock is reacquired.\n\n        \"\"\"\n        if not self._is_owned():\n            raise RuntimeError(\"cannot wait on un-acquired lock\")\n        waiter = _allocate_lock()\n        waiter.acquire()\n        self.__waiters.append(waiter)\n        saved_state = self._release_save()\n        try:    # restore state no matter what (e.g., KeyboardInterrupt)\n            if timeout is None:\n                waiter.acquire()\n                if __debug__:\n                    self._note(\"%s.wait(): got it\", self)\n            else:\n                # Balancing act:  We can't afford a pure busy loop, so we\n                # have to sleep; but if we sleep the whole timeout time,\n                # we'll be unresponsive.  The scheme here sleeps very\n                # little at first, longer as time goes on, but never longer\n                # than 20 times per second (or the timeout time remaining).\n                endtime = _time() + timeout\n                delay = 0.0005 # 500 us -> initial delay of 1 ms\n                while True:\n                    gotit = waiter.acquire(0)\n                    if gotit:\n                        break\n                    remaining = endtime - _time()\n                    if remaining <= 0:\n                        break\n                    delay = min(delay * 2, remaining, .05)\n                    _sleep(delay)\n                if not gotit:\n                    if __debug__:\n                        self._note(\"%s.wait(%s): timed out\", self, timeout)\n                    try:\n                        self.__waiters.remove(waiter)\n                    except ValueError:\n                        pass\n                else:\n                    if __debug__:\n                        self._note(\"%s.wait(%s): got it\", self, timeout)\n        finally:\n            self._acquire_restore(saved_state)\n\n    def notify(self, n=1):\n        \"\"\"Wake up one or more threads waiting on this condition, if any.\n\n        If the calling thread has not acquired the lock when this method is\n        called, a RuntimeError is raised.\n\n        This method wakes up at most n of the threads waiting for the condition\n        variable; it is a no-op if no threads are waiting.\n\n        \"\"\"\n#        if not self._is_owned():\n#            raise RuntimeError(\"cannot notify on un-acquired lock\")\n        __waiters = self.__waiters\n        waiters = __waiters[:n]\n        if not waiters:\n            if __debug__:\n                self._note(\"%s.notify(): no waiters\", self)\n            return\n        self._note(\"%s.notify(): notifying %d waiter%s\", self, n,\n                   n!=1 and \"s\" or \"\")\n        for waiter in waiters:\n            waiter.release()\n            try:\n                __waiters.remove(waiter)\n            except ValueError:\n                pass\n\n    def notifyAll(self):\n        \"\"\"Wake up all threads waiting on this condition.\n\n        If the calling thread has not acquired the lock when this method\n        is called, a RuntimeError is raised.\n\n        \"\"\"\n        self.notify(len(self.__waiters))\n\n    notify_all = notifyAll\n\n\ndef Semaphore(*args, **kwargs):\n    \"\"\"A factory function that returns a new semaphore.\n\n    Semaphores manage a counter representing the number of release() calls minus\n    the number of acquire() calls, plus an initial value. The acquire() method\n    blocks if necessary until it can return without making the counter\n    negative. If not given, value defaults to 1.\n\n    \"\"\"\n    return _Semaphore(*args, **kwargs)\n\nclass _Semaphore(_Verbose):\n    \"\"\"Semaphores manage a counter representing the number of release() calls\n       minus the number of acquire() calls, plus an initial value. The acquire()\n       method blocks if necessary until it can return without making the counter\n       negative. If not given, value defaults to 1.\n\n    \"\"\"\n\n    # After Tim Peters' semaphore class, but not quite the same (no maximum)\n\n    def __init__(self, value=1, verbose=None):\n        if value < 0:\n            raise ValueError(\"semaphore initial value must be >= 0\")\n        _Verbose.__init__(self, verbose)\n        self.__cond = Condition(Lock())\n        self.__value = value\n\n    def acquire(self, blocking=1):\n        \"\"\"Acquire a semaphore, decrementing the internal counter by one.\n\n        When invoked without arguments: if the internal counter is larger than\n        zero on entry, decrement it by one and return immediately. If it is zero\n        on entry, block, waiting until some other thread has called release() to\n        make it larger than zero. This is done with proper interlocking so that\n        if multiple acquire() calls are blocked, release() will wake exactly one\n        of them up. The implementation may pick one at random, so the order in\n        which blocked threads are awakened should not be relied on. There is no\n        return value in this case.\n\n        When invoked with blocking set to true, do the same thing as when called\n        without arguments, and return true.\n\n        When invoked with blocking set to false, do not block. If a call without\n        an argument would block, return false immediately; otherwise, do the\n        same thing as when called without arguments, and return true.\n\n        \"\"\"\n        rc = False\n        with self.__cond:\n            while self.__value == 0:\n                if not blocking:\n                    break\n                if __debug__:\n                    self._note(\"%s.acquire(%s): blocked waiting, value=%s\",\n                            self, blocking, self.__value)\n                self.__cond.wait()\n            else:\n                self.__value = self.__value - 1\n                if __debug__:\n                    self._note(\"%s.acquire: success, value=%s\",\n                            self, self.__value)\n                rc = True\n        return rc\n\n    __enter__ = acquire\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        \"\"\"\n        with self.__cond:\n            self.__value = self.__value + 1\n            if __debug__:\n                self._note(\"%s.release: success, value=%s\",\n                        self, self.__value)\n            self.__cond.notify()\n\n    def __exit__(self, t, v, tb):\n        self.release()\n\n\ndef BoundedSemaphore(*args, **kwargs):\n    \"\"\"A factory function that returns a new bounded semaphore.\n\n    A bounded semaphore checks to make sure its current value doesn't exceed its\n    initial value. If it does, ValueError is raised. In most situations\n    semaphores are used to guard resources with limited capacity.\n\n    If the semaphore is released too many times it's a sign of a bug. If not\n    given, value defaults to 1.\n\n    Like regular semaphores, bounded semaphores manage a counter representing\n    the number of release() calls minus the number of acquire() calls, plus an\n    initial value. The acquire() method blocks if necessary until it can return\n    without making the counter negative. If not given, value defaults to 1.\n\n    \"\"\"\n    return _BoundedSemaphore(*args, **kwargs)\n\nclass _BoundedSemaphore(_Semaphore):\n    \"\"\"A bounded semaphore checks to make sure its current value doesn't exceed\n       its initial value. If it does, ValueError is raised. In most situations\n       semaphores are used to guard resources with limited capacity.\n    \"\"\"\n\n    def __init__(self, value=1, verbose=None):\n        _Semaphore.__init__(self, value, verbose)\n        self._initial_value = value\n\n    def release(self):\n        \"\"\"Release a semaphore, incrementing the internal counter by one.\n\n        When the counter is zero on entry and another thread is waiting for it\n        to become larger than zero again, wake up that thread.\n\n        If the number of releases exceeds the number of acquires,\n        raise a ValueError.\n\n        \"\"\"\n        with self._Semaphore__cond:\n            if self._Semaphore__value >= self._initial_value:\n                raise ValueError(\"Semaphore released too many times\")\n            self._Semaphore__value += 1\n            self._Semaphore__cond.notify()\n\n\ndef Event(*args, **kwargs):\n    \"\"\"A factory function that returns a new event.\n\n    Events manage a flag that can be set to true with the set() method and reset\n    to false with the clear() method. The wait() method blocks until the flag is\n    true.\n\n    \"\"\"\n    return _Event(*args, **kwargs)\n\nclass _Event(_Verbose):\n    \"\"\"A factory function that returns a new event object. An event manages a\n       flag that can be set to true with the set() method and reset to false\n       with the clear() method. The wait() method blocks until the flag is true.\n\n    \"\"\"\n\n    # After Tim Peters' event class (without is_posted())\n\n    def __init__(self, verbose=None):\n        _Verbose.__init__(self, verbose)\n        self.__cond = Condition(Lock())\n        self.__flag = False\n\n    def _reset_internal_locks(self):\n        # private!  called by Thread._reset_internal_locks by _after_fork()\n        self.__cond.__init__()\n\n    def isSet(self):\n        'Return true if and only if the internal flag is true.'\n        return self.__flag\n\n    is_set = isSet\n\n    def set(self):\n        \"\"\"Set the internal flag to true.\n\n        All threads waiting for the flag to become true are awakened. Threads\n        that call wait() once the flag is true will not block at all.\n\n        \"\"\"\n        self.__cond.acquire()\n        try:\n            self.__flag = True\n            self.__cond.notify_all()\n        finally:\n            self.__cond.release()\n\n    def clear(self):\n        \"\"\"Reset the internal flag to false.\n\n        Subsequently, threads calling wait() will block until set() is called to\n        set the internal flag to true again.\n\n        \"\"\"\n        self.__cond.acquire()\n        try:\n            self.__flag = False\n        finally:\n            self.__cond.release()\n\n    def wait(self, timeout=None):\n        \"\"\"Block until the internal flag is true.\n\n        If the internal flag is true on entry, return immediately. Otherwise,\n        block until another thread calls set() to set the flag to true, or until\n        the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof).\n\n        This method returns the internal flag on exit, so it will always return\n        True except if a timeout is given and the operation times out.\n\n        \"\"\"\n        self.__cond.acquire()\n        try:\n            if not self.__flag:\n                self.__cond.wait(timeout)\n            return self.__flag\n        finally:\n            self.__cond.release()\n\n# Helper to generate new thread names\n_counter = _count().next\n_counter() # Consume 0 so first non-main thread has id 1.\ndef _newname(template=\"Thread-%d\"):\n    return template % _counter()\n\n# Active thread administration\n_active_limbo_lock = _allocate_lock()\n_active = {}    # maps thread id to Thread object\n_limbo = {}\n\n\n# Main class for threads\n\nclass Thread(_Verbose):\n    \"\"\"A class that represents a thread of control.\n\n    This class can be safely subclassed in a limited fashion.\n\n    \"\"\"\n    __initialized = False\n    # Need to store a reference to sys.exc_info for printing\n    # out exceptions when a thread tries to use a global var. during interp.\n    # shutdown and thus raises an exception about trying to perform some\n    # operation on/with a NoneType\n    __exc_info = _sys.exc_info\n    # Keep sys.exc_clear too to clear the exception just before\n    # allowing .join() to return.\n    __exc_clear = _sys.exc_clear\n\n    def __init__(self, group=None, target=None, name=None,\n                 args=(), kwargs=None, verbose=None):\n        \"\"\"This constructor should always be called with keyword arguments. Arguments are:\n\n        *group* should be None; reserved for future extension when a ThreadGroup\n        class is implemented.\n\n        *target* is the callable object to be invoked by the run()\n        method. Defaults to None, meaning nothing is called.\n\n        *name* is the thread name. By default, a unique name is constructed of\n        the form \"Thread-N\" where N is a small decimal number.\n\n        *args* is the argument tuple for the target invocation. Defaults to ().\n\n        *kwargs* is a dictionary of keyword arguments for the target\n        invocation. Defaults to {}.\n\n        If a subclass overrides the constructor, it must make sure to invoke\n        the base class constructor (Thread.__init__()) before doing anything\n        else to the thread.\n\n\"\"\"\n        assert group is None, \"group argument must be None for now\"\n        _Verbose.__init__(self, verbose)\n        if kwargs is None:\n            kwargs = {}\n        self.__target = target\n        self.__name = str(name or _newname())\n        self.__args = args\n        self.__kwargs = kwargs\n        self.__daemonic = self._set_daemon()\n        self.__ident = None\n        self.__started = Event()\n        self.__stopped = False\n        self.__block = Condition(Lock())\n        self.__initialized = True\n        # sys.stderr is not stored in the class like\n        # sys.exc_info since it can be changed between instances\n        self.__stderr = _sys.stderr\n\n    def _reset_internal_locks(self):\n        # private!  Called by _after_fork() to reset our internal locks as\n        # they may be in an invalid state leading to a deadlock or crash.\n        if hasattr(self, '_Thread__block'):  # DummyThread deletes self.__block\n            self.__block.__init__()\n        self.__started._reset_internal_locks()\n\n    @property\n    def _block(self):\n        # used by a unittest\n        return self.__block\n\n    def _set_daemon(self):\n        # Overridden in _MainThread and _DummyThread\n        return current_thread().daemon\n\n    def __repr__(self):\n        assert self.__initialized, \"Thread.__init__() was not called\"\n        status = \"initial\"\n        if self.__started.is_set():\n            status = \"started\"\n        if self.__stopped:\n            status = \"stopped\"\n        if self.__daemonic:\n            status += \" daemon\"\n        if self.__ident is not None:\n            status += \" %s\" % self.__ident\n        return \"<%s(%s, %s)>\" % (self.__class__.__name__, self.__name, status)\n\n    def start(self):\n        \"\"\"Start the thread's activity.\n\n        It must be called at most once per thread object. It arranges for the\n        object's run() method to be invoked in a separate thread of control.\n\n        This method will raise a RuntimeError if called more than once on the\n        same thread object.\n\n        \"\"\"\n        if not self.__initialized:\n            raise RuntimeError(\"thread.__init__() not called\")\n        if self.__started.is_set():\n            raise RuntimeError(\"threads can only be started once\")\n        if __debug__:\n            self._note(\"%s.start(): starting thread\", self)\n        with _active_limbo_lock:\n            _limbo[self] = self\n        try:\n            _start_new_thread(self.__bootstrap, ())\n        except Exception:\n            with _active_limbo_lock:\n                del _limbo[self]\n            raise\n        self.__started.wait()\n\n    def run(self):\n        \"\"\"Method representing the thread's activity.\n\n        You may override this method in a subclass. The standard run() method\n        invokes the callable object passed to the object's constructor as the\n        target argument, if any, with sequential and keyword arguments taken\n        from the args and kwargs arguments, respectively.\n\n        \"\"\"\n        try:\n            if self.__target:\n                self.__target(*self.__args, **self.__kwargs)\n        finally:\n            # Avoid a refcycle if the thread is running a function with\n            # an argument that has a member that points to the thread.\n            del self.__target, self.__args, self.__kwargs\n\n    def __bootstrap(self):\n        # Wrapper around the real bootstrap code that ignores\n        # exceptions during interpreter cleanup.  Those typically\n        # happen when a daemon thread wakes up at an unfortunate\n        # moment, finds the world around it destroyed, and raises some\n        # random exception *** while trying to report the exception in\n        # __bootstrap_inner() below ***.  Those random exceptions\n        # don't help anybody, and they confuse users, so we suppress\n        # them.  We suppress them only when it appears that the world\n        # indeed has already been destroyed, so that exceptions in\n        # __bootstrap_inner() during normal business hours are properly\n        # reported.  Also, we only suppress them for daemonic threads;\n        # if a non-daemonic encounters this, something else is wrong.\n        try:\n            self.__bootstrap_inner()\n        except:\n            if self.__daemonic and _sys is None:\n                return\n            raise\n\n    def _set_ident(self):\n        self.__ident = _get_ident()\n\n    def __bootstrap_inner(self):\n        try:\n            self._set_ident()\n            self.__started.set()\n            with _active_limbo_lock:\n                _active[self.__ident] = self\n                del _limbo[self]\n            if __debug__:\n                self._note(\"%s.__bootstrap(): thread started\", self)\n\n            if _trace_hook:\n                self._note(\"%s.__bootstrap(): registering trace hook\", self)\n                _sys.settrace(_trace_hook)\n            if _profile_hook:\n                self._note(\"%s.__bootstrap(): registering profile hook\", self)\n                _sys.setprofile(_profile_hook)\n\n            try:\n                self.run()\n            except SystemExit:\n                if __debug__:\n                    self._note(\"%s.__bootstrap(): raised SystemExit\", self)\n            except:\n                if __debug__:\n                    self._note(\"%s.__bootstrap(): unhandled exception\", self)\n                # If sys.stderr is no more (most likely from interpreter\n                # shutdown) use self.__stderr.  Otherwise still use sys (as in\n                # _sys) in case sys.stderr was redefined since the creation of\n                # self.\n                if _sys and _sys.stderr is not None:\n                    print>>_sys.stderr, (\"Exception in thread %s:\\n%s\" %\n                                         (self.name, _format_exc()))\n                elif self.__stderr is not None:\n                    # Do the best job possible w/o a huge amt. of code to\n                    # approximate a traceback (code ideas from\n                    # Lib/traceback.py)\n                    exc_type, exc_value, exc_tb = self.__exc_info()\n                    try:\n                        print>>self.__stderr, (\n                            \"Exception in thread \" + self.name +\n                            \" (most likely raised during interpreter shutdown):\")\n                        print>>self.__stderr, (\n                            \"Traceback (most recent call last):\")\n                        while exc_tb:\n                            print>>self.__stderr, (\n                                '  File \"%s\", line %s, in %s' %\n                                (exc_tb.tb_frame.f_code.co_filename,\n                                    exc_tb.tb_lineno,\n                                    exc_tb.tb_frame.f_code.co_name))\n                            exc_tb = exc_tb.tb_next\n                        print>>self.__stderr, (\"%s: %s\" % (exc_type, exc_value))\n                    # Make sure that exc_tb gets deleted since it is a memory\n                    # hog; deleting everything else is just for thoroughness\n                    finally:\n                        del exc_type, exc_value, exc_tb\n            else:\n                if __debug__:\n                    self._note(\"%s.__bootstrap(): normal return\", self)\n            finally:\n                # Prevent a race in\n                # test_threading.test_no_refcycle_through_target when\n                # the exception keeps the target alive past when we\n                # assert that it's dead.\n                self.__exc_clear()\n        finally:\n            with _active_limbo_lock:\n                self.__stop()\n                try:\n                    # We don't call self.__delete() because it also\n                    # grabs _active_limbo_lock.\n                    del _active[_get_ident()]\n                except:\n                    pass\n\n    def __stop(self):\n        # DummyThreads delete self.__block, but they have no waiters to\n        # notify anyway (join() is forbidden on them).\n        if not hasattr(self, '_Thread__block'):\n            return\n        self.__block.acquire()\n        self.__stopped = True\n        self.__block.notify_all()\n        self.__block.release()\n\n    def __delete(self):\n        \"Remove current thread from the dict of currently running threads.\"\n\n        # Notes about running with dummy_thread:\n        #\n        # Must take care to not raise an exception if dummy_thread is being\n        # used (and thus this module is being used as an instance of\n        # dummy_threading).  dummy_thread.get_ident() always returns -1 since\n        # there is only one thread if dummy_thread is being used.  Thus\n        # len(_active) is always <= 1 here, and any Thread instance created\n        # overwrites the (if any) thread currently registered in _active.\n        #\n        # An instance of _MainThread is always created by 'threading'.  This\n        # gets overwritten the instant an instance of Thread is created; both\n        # threads return -1 from dummy_thread.get_ident() and thus have the\n        # same key in the dict.  So when the _MainThread instance created by\n        # 'threading' tries to clean itself up when atexit calls this method\n        # it gets a KeyError if another Thread instance was created.\n        #\n        # This all means that KeyError from trying to delete something from\n        # _active if dummy_threading is being used is a red herring.  But\n        # since it isn't if dummy_threading is *not* being used then don't\n        # hide the exception.\n\n        try:\n            with _active_limbo_lock:\n                del _active[_get_ident()]\n                # There must not be any python code between the previous line\n                # and after the lock is released.  Otherwise a tracing function\n                # could try to acquire the lock again in the same thread, (in\n                # current_thread()), and would block.\n        except KeyError:\n            if 'dummy_threading' not in _sys.modules:\n                raise\n\n    def join(self, timeout=None):\n        \"\"\"Wait until the thread terminates.\n\n        This blocks the calling thread until the thread whose join() method is\n        called terminates -- either normally or through an unhandled exception\n        or until the optional timeout occurs.\n\n        When the timeout argument is present and not None, it should be a\n        floating point number specifying a timeout for the operation in seconds\n        (or fractions thereof). As join() always returns None, you must call\n        isAlive() after join() to decide whether a timeout happened -- if the\n        thread is still alive, the join() call timed out.\n\n        When the timeout argument is not present or None, the operation will\n        block until the thread terminates.\n\n        A thread can be join()ed many times.\n\n        join() raises a RuntimeError if an attempt is made to join the current\n        thread as that would cause a deadlock. It is also an error to join() a\n        thread before it has been started and attempts to do so raises the same\n        exception.\n\n        \"\"\"\n        if not self.__initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if not self.__started.is_set():\n            raise RuntimeError(\"cannot join thread before it is started\")\n        if self is current_thread():\n            raise RuntimeError(\"cannot join current thread\")\n\n        if __debug__:\n            if not self.__stopped:\n                self._note(\"%s.join(): waiting until thread stops\", self)\n        self.__block.acquire()\n        try:\n            if timeout is None:\n                while not self.__stopped:\n                    self.__block.wait()\n                if __debug__:\n                    self._note(\"%s.join(): thread stopped\", self)\n            else:\n                deadline = _time() + timeout\n                while not self.__stopped:\n                    delay = deadline - _time()\n                    if delay <= 0:\n                        if __debug__:\n                            self._note(\"%s.join(): timed out\", self)\n                        break\n                    self.__block.wait(delay)\n                else:\n                    if __debug__:\n                        self._note(\"%s.join(): thread stopped\", self)\n        finally:\n            self.__block.release()\n\n    @property\n    def name(self):\n        \"\"\"A string used for identification purposes only.\n\n        It has no semantics. Multiple threads may be given the same name. The\n        initial name is set by the constructor.\n\n        \"\"\"\n        assert self.__initialized, \"Thread.__init__() not called\"\n        return self.__name\n\n    @name.setter\n    def name(self, name):\n        assert self.__initialized, \"Thread.__init__() not called\"\n        self.__name = str(name)\n\n    @property\n    def ident(self):\n        \"\"\"Thread identifier of this thread or None if it has not been started.\n\n        This is a nonzero integer. See the thread.get_ident() function. Thread\n        identifiers may be recycled when a thread exits and another thread is\n        created. The identifier is available even after the thread has exited.\n\n        \"\"\"\n        assert self.__initialized, \"Thread.__init__() not called\"\n        return self.__ident\n\n    def isAlive(self):\n        \"\"\"Return whether the thread is alive.\n\n        This method returns True just before the run() method starts until just\n        after the run() method terminates. The module function enumerate()\n        returns a list of all alive threads.\n\n        \"\"\"\n        assert self.__initialized, \"Thread.__init__() not called\"\n        return self.__started.is_set() and not self.__stopped\n\n    is_alive = isAlive\n\n    @property\n    def daemon(self):\n        \"\"\"A boolean value indicating whether this thread is a daemon thread (True) or not (False).\n\n        This must be set before start() is called, otherwise RuntimeError is\n        raised. Its initial value is inherited from the creating thread; the\n        main thread is not a daemon thread and therefore all threads created in\n        the main thread default to daemon = False.\n\n        The entire Python program exits when no alive non-daemon threads are\n        left.\n\n        \"\"\"\n        assert self.__initialized, \"Thread.__init__() not called\"\n        return self.__daemonic\n\n    @daemon.setter\n    def daemon(self, daemonic):\n        if not self.__initialized:\n            raise RuntimeError(\"Thread.__init__() not called\")\n        if self.__started.is_set():\n            raise RuntimeError(\"cannot set daemon status of active thread\");\n        self.__daemonic = daemonic\n\n    def isDaemon(self):\n        return self.daemon\n\n    def setDaemon(self, daemonic):\n        self.daemon = daemonic\n\n    def getName(self):\n        return self.name\n\n    def setName(self, name):\n        self.name = name\n\n# The timer class was contributed by Itamar Shtull-Trauring\n\ndef Timer(*args, **kwargs):\n    \"\"\"Factory function to create a Timer object.\n\n    Timers call a function after a specified number of seconds:\n\n        t = Timer(30.0, f, args=[], kwargs={})\n        t.start()\n        t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n    return _Timer(*args, **kwargs)\n\nclass _Timer(Thread):\n    \"\"\"Call a function after a specified number of seconds:\n\n            t = Timer(30.0, f, args=[], kwargs={})\n            t.start()\n            t.cancel()     # stop the timer's action if it's still waiting\n\n    \"\"\"\n\n    def __init__(self, interval, function, args=[], kwargs={}):\n        Thread.__init__(self)\n        self.interval = interval\n        self.function = function\n        self.args = args\n        self.kwargs = kwargs\n        self.finished = Event()\n\n    def cancel(self):\n        \"\"\"Stop the timer if it hasn't finished yet\"\"\"\n        self.finished.set()\n\n    def run(self):\n        self.finished.wait(self.interval)\n        if not self.finished.is_set():\n            self.function(*self.args, **self.kwargs)\n        self.finished.set()\n\n# Special thread class to represent the main thread\n# This is garbage collected through an exit handler\n\nclass _MainThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=\"MainThread\")\n        self._Thread__started.set()\n        self._set_ident()\n        with _active_limbo_lock:\n            _active[_get_ident()] = self\n\n    def _set_daemon(self):\n        return False\n\n    def _exitfunc(self):\n        self._Thread__stop()\n        t = _pickSomeNonDaemonThread()\n        if t:\n            if __debug__:\n                self._note(\"%s: waiting for other threads\", self)\n        while t:\n            t.join()\n            t = _pickSomeNonDaemonThread()\n        if __debug__:\n            self._note(\"%s: exiting\", self)\n        self._Thread__delete()\n\ndef _pickSomeNonDaemonThread():\n    for t in enumerate():\n        if not t.daemon and t.is_alive():\n            return t\n    return None\n\n\n# Dummy thread class to represent threads not started here.\n# These aren't garbage collected when they die, nor can they be waited for.\n# If they invoke anything in threading.py that calls current_thread(), they\n# leave an entry in the _active dict forever after.\n# Their purpose is to return *something* from current_thread().\n# They are marked as daemon threads so we won't wait for them\n# when we exit (conform previous semantics).\n\nclass _DummyThread(Thread):\n\n    def __init__(self):\n        Thread.__init__(self, name=_newname(\"Dummy-%d\"))\n\n        # Thread.__block consumes an OS-level locking primitive, which\n        # can never be used by a _DummyThread.  Since a _DummyThread\n        # instance is immortal, that's bad, so release this resource.\n        del self._Thread__block\n\n        self._Thread__started.set()\n        self._set_ident()\n        with _active_limbo_lock:\n            _active[_get_ident()] = self\n\n    def _set_daemon(self):\n        return True\n\n    def join(self, timeout=None):\n        assert False, \"cannot join a dummy thread\"\n\n\n# Global API functions\n\ndef currentThread():\n    \"\"\"Return the current Thread object, corresponding to the caller's thread of control.\n\n    If the caller's thread of control was not created through the threading\n    module, a dummy thread object with limited functionality is returned.\n\n    \"\"\"\n    try:\n        return _active[_get_ident()]\n    except KeyError:\n        ##print \"current_thread(): no current thread for\", _get_ident()\n        return _DummyThread()\n\ncurrent_thread = currentThread\n\ndef activeCount():\n    \"\"\"Return the number of Thread objects currently alive.\n\n    The returned count is equal to the length of the list returned by\n    enumerate().\n\n    \"\"\"\n    with _active_limbo_lock:\n        return len(_active) + len(_limbo)\n\nactive_count = activeCount\n\ndef _enumerate():\n    # Same as enumerate(), but without the lock. Internal use only.\n    return _active.values() + _limbo.values()\n\ndef enumerate():\n    \"\"\"Return a list of all Thread objects currently alive.\n\n    The list includes daemonic threads, dummy thread objects created by\n    current_thread(), and the main thread. It excludes terminated threads and\n    threads that have not yet been started.\n\n    \"\"\"\n    with _active_limbo_lock:\n        return _active.values() + _limbo.values()\n\n\n# Create the main thread object,\n# and make it available for the interpreter\n# (Py_Main) as threading._shutdown.\n\n_shutdown = _MainThread()._exitfunc\n\n# get thread-local implementation, either from the thread\n# module, or from the python fallback\n\nfrom _threading_local import local\n\n\ndef _after_fork():\n    # This function is called by Python/ceval.c:PyEval_ReInitThreads which\n    # is called from PyOS_AfterFork.  Here we cleanup threading module state\n    # that should not exist after a fork.\n\n    # Reset _active_limbo_lock, in case we forked while the lock was held\n    # by another (non-forked) thread.  http://bugs.python.org/issue874900\n    global _active_limbo_lock\n    _active_limbo_lock = _allocate_lock()\n\n    # fork() only copied the current thread; clear references to others.\n    new_active = {}\n    current = current_thread()\n    with _active_limbo_lock:\n        for thread in _enumerate():\n            # Any lock/condition variable may be currently locked or in an\n            # invalid state, so we reinitialize them.\n            if hasattr(thread, '_reset_internal_locks'):\n                thread._reset_internal_locks()\n            if thread is current:\n                # There is only one active thread. We reset the ident to\n                # its new value since it can have changed.\n                ident = _get_ident()\n                thread._Thread__ident = ident\n                new_active[ident] = thread\n            else:\n                # All the others are already stopped.\n                thread._Thread__stop()\n\n        _limbo.clear()\n        _active.clear()\n        _active.update(new_active)\n        assert len(_active) == 1\n\n\n# Self-test code\n", 
    "token": "\"\"\"Token constants (from \"token.h\").\"\"\"\n\n#  This file is automatically generated; please don't muck it up!\n#\n#  To update the symbols in this file, 'cd' to the top directory of\n#  the python source tree after building the interpreter and run:\n#\n#    ./python Lib/token.py\n\n#--start constants--\nENDMARKER = 0\nNAME = 1\nNUMBER = 2\nSTRING = 3\nNEWLINE = 4\nINDENT = 5\nDEDENT = 6\nLPAR = 7\nRPAR = 8\nLSQB = 9\nRSQB = 10\nCOLON = 11\nCOMMA = 12\nSEMI = 13\nPLUS = 14\nMINUS = 15\nSTAR = 16\nSLASH = 17\nVBAR = 18\nAMPER = 19\nLESS = 20\nGREATER = 21\nEQUAL = 22\nDOT = 23\nPERCENT = 24\nBACKQUOTE = 25\nLBRACE = 26\nRBRACE = 27\nEQEQUAL = 28\nNOTEQUAL = 29\nLESSEQUAL = 30\nGREATEREQUAL = 31\nTILDE = 32\nCIRCUMFLEX = 33\nLEFTSHIFT = 34\nRIGHTSHIFT = 35\nDOUBLESTAR = 36\nPLUSEQUAL = 37\nMINEQUAL = 38\nSTAREQUAL = 39\nSLASHEQUAL = 40\nPERCENTEQUAL = 41\nAMPEREQUAL = 42\nVBAREQUAL = 43\nCIRCUMFLEXEQUAL = 44\nLEFTSHIFTEQUAL = 45\nRIGHTSHIFTEQUAL = 46\nDOUBLESTAREQUAL = 47\nDOUBLESLASH = 48\nDOUBLESLASHEQUAL = 49\nAT = 50\nOP = 51\nERRORTOKEN = 52\nN_TOKENS = 53\nNT_OFFSET = 256\n#--end constants--\n\ntok_name = {}\nfor _name, _value in globals().items():\n    if type(_value) is type(0):\n        tok_name[_value] = _name\ndel _name, _value\n\n\ndef ISTERMINAL(x):\n    return x < NT_OFFSET\n\ndef ISNONTERMINAL(x):\n    return x >= NT_OFFSET\n\ndef ISEOF(x):\n    return x == ENDMARKER\n\n\ndef main():\n    import re\n    import sys\n    args = sys.argv[1:]\n    inFileName = args and args[0] or \"Include/token.h\"\n    outFileName = \"Lib/token.py\"\n    if len(args) > 1:\n        outFileName = args[1]\n    try:\n        fp = open(inFileName)\n    except IOError, err:\n        sys.stdout.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(1)\n    lines = fp.read().split(\"\\n\")\n    fp.close()\n    prog = re.compile(\n        \"#define[ \\t][ \\t]*([A-Z0-9][A-Z0-9_]*)[ \\t][ \\t]*([0-9][0-9]*)\",\n        re.IGNORECASE)\n    tokens = {}\n    for line in lines:\n        match = prog.match(line)\n        if match:\n            name, val = match.group(1, 2)\n            val = int(val)\n            tokens[val] = name          # reverse so we can sort them...\n    keys = tokens.keys()\n    keys.sort()\n    # load the output skeleton from the target:\n    try:\n        fp = open(outFileName)\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(2)\n    format = fp.read().split(\"\\n\")\n    fp.close()\n    try:\n        start = format.index(\"#--start constants--\") + 1\n        end = format.index(\"#--end constants--\")\n    except ValueError:\n        sys.stderr.write(\"target does not contain format markers\")\n        sys.exit(3)\n    lines = []\n    for val in keys:\n        lines.append(\"%s = %d\" % (tokens[val], val))\n    format[start:end] = lines\n    try:\n        fp = open(outFileName, 'w')\n    except IOError, err:\n        sys.stderr.write(\"I/O error: %s\\n\" % str(err))\n        sys.exit(4)\n    fp.write(\"\\n\".join(format))\n    fp.close()\n\n\nif __name__ == \"__main__\":\n    main()\n", 
    "tokenize": "\"\"\"Tokenization help for Python programs.\n\ngenerate_tokens(readline) is a generator that breaks a stream of\ntext into Python tokens.  It accepts a readline-like method which is called\nrepeatedly to get the next line of input (or \"\" for EOF).  It generates\n5-tuples with these members:\n\n    the token type (see token.py)\n    the token (a string)\n    the starting (row, column) indices of the token (a 2-tuple of ints)\n    the ending (row, column) indices of the token (a 2-tuple of ints)\n    the original line (string)\n\nIt is designed to match the working of the Python tokenizer exactly, except\nthat it produces COMMENT tokens for comments and gives type OP for all\noperators\n\nOlder entry points\n    tokenize_loop(readline, tokeneater)\n    tokenize(readline, tokeneater=printtoken)\nare the same, except instead of generating tokens, tokeneater is a callback\nfunction to which the 5 fields described above are passed as 5 arguments,\neach time a new token is found.\"\"\"\n\n__author__ = 'Ka-Ping Yee <ping@lfw.org>'\n__credits__ = ('GvR, ESR, Tim Peters, Thomas Wouters, Fred Drake, '\n               'Skip Montanaro, Raymond Hettinger')\n\nfrom itertools import chain\nimport string, re\nfrom token import *\n\nimport token\n__all__ = [x for x in dir(token) if not x.startswith(\"_\")]\n__all__ += [\"COMMENT\", \"tokenize\", \"generate_tokens\", \"NL\", \"untokenize\"]\ndel x\ndel token\n\nCOMMENT = N_TOKENS\ntok_name[COMMENT] = 'COMMENT'\nNL = N_TOKENS + 1\ntok_name[NL] = 'NL'\nN_TOKENS += 2\n\ndef group(*choices): return '(' + '|'.join(choices) + ')'\ndef any(*choices): return group(*choices) + '*'\ndef maybe(*choices): return group(*choices) + '?'\n\nWhitespace = r'[ \\f\\t]*'\nComment = r'#[^\\r\\n]*'\nIgnore = Whitespace + any(r'\\\\\\r?\\n' + Whitespace) + maybe(Comment)\nName = r'[a-zA-Z_]\\w*'\n\nHexnumber = r'0[xX][\\da-fA-F]+[lL]?'\nOctnumber = r'(0[oO][0-7]+)|(0[0-7]*)[lL]?'\nBinnumber = r'0[bB][01]+[lL]?'\nDecnumber = r'[1-9]\\d*[lL]?'\nIntnumber = group(Hexnumber, Binnumber, Octnumber, Decnumber)\nExponent = r'[eE][-+]?\\d+'\nPointfloat = group(r'\\d+\\.\\d*', r'\\.\\d+') + maybe(Exponent)\nExpfloat = r'\\d+' + Exponent\nFloatnumber = group(Pointfloat, Expfloat)\nImagnumber = group(r'\\d+[jJ]', Floatnumber + r'[jJ]')\nNumber = group(Imagnumber, Floatnumber, Intnumber)\n\n# Tail end of ' string.\nSingle = r\"[^'\\\\]*(?:\\\\.[^'\\\\]*)*'\"\n# Tail end of \" string.\nDouble = r'[^\"\\\\]*(?:\\\\.[^\"\\\\]*)*\"'\n# Tail end of ''' string.\nSingle3 = r\"[^'\\\\]*(?:(?:\\\\.|'(?!''))[^'\\\\]*)*'''\"\n# Tail end of \"\"\" string.\nDouble3 = r'[^\"\\\\]*(?:(?:\\\\.|\"(?!\"\"))[^\"\\\\]*)*\"\"\"'\nTriple = group(\"[uUbB]?[rR]?'''\", '[uUbB]?[rR]?\"\"\"')\n# Single-line ' or \" string.\nString = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*'\",\n               r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*\"')\n\n# Because of leftmost-then-longest match semantics, be sure to put the\n# longest operators first (e.g., if = came before ==, == would get\n# recognized as two instances of =).\nOperator = group(r\"\\*\\*=?\", r\">>=?\", r\"<<=?\", r\"<>\", r\"!=\",\n                 r\"//=?\",\n                 r\"[+\\-*/%&|^=<>]=?\",\n                 r\"~\")\n\nBracket = '[][(){}]'\nSpecial = group(r'\\r?\\n', r'[:;.,`@]')\nFunny = group(Operator, Bracket, Special)\n\nPlainToken = group(Number, Funny, String, Name)\nToken = Ignore + PlainToken\n\n# First (or only) line of ' or \" string.\nContStr = group(r\"[uUbB]?[rR]?'[^\\n'\\\\]*(?:\\\\.[^\\n'\\\\]*)*\" +\n                group(\"'\", r'\\\\\\r?\\n'),\n                r'[uUbB]?[rR]?\"[^\\n\"\\\\]*(?:\\\\.[^\\n\"\\\\]*)*' +\n                group('\"', r'\\\\\\r?\\n'))\nPseudoExtras = group(r'\\\\\\r?\\n|\\Z', Comment, Triple)\nPseudoToken = Whitespace + group(PseudoExtras, Number, Funny, ContStr, Name)\n\ntokenprog, pseudoprog, single3prog, double3prog = map(\n    re.compile, (Token, PseudoToken, Single3, Double3))\nendprogs = {\"'\": re.compile(Single), '\"': re.compile(Double),\n            \"'''\": single3prog, '\"\"\"': double3prog,\n            \"r'''\": single3prog, 'r\"\"\"': double3prog,\n            \"u'''\": single3prog, 'u\"\"\"': double3prog,\n            \"ur'''\": single3prog, 'ur\"\"\"': double3prog,\n            \"R'''\": single3prog, 'R\"\"\"': double3prog,\n            \"U'''\": single3prog, 'U\"\"\"': double3prog,\n            \"uR'''\": single3prog, 'uR\"\"\"': double3prog,\n            \"Ur'''\": single3prog, 'Ur\"\"\"': double3prog,\n            \"UR'''\": single3prog, 'UR\"\"\"': double3prog,\n            \"b'''\": single3prog, 'b\"\"\"': double3prog,\n            \"br'''\": single3prog, 'br\"\"\"': double3prog,\n            \"B'''\": single3prog, 'B\"\"\"': double3prog,\n            \"bR'''\": single3prog, 'bR\"\"\"': double3prog,\n            \"Br'''\": single3prog, 'Br\"\"\"': double3prog,\n            \"BR'''\": single3prog, 'BR\"\"\"': double3prog,\n            'r': None, 'R': None, 'u': None, 'U': None,\n            'b': None, 'B': None}\n\ntriple_quoted = {}\nfor t in (\"'''\", '\"\"\"',\n          \"r'''\", 'r\"\"\"', \"R'''\", 'R\"\"\"',\n          \"u'''\", 'u\"\"\"', \"U'''\", 'U\"\"\"',\n          \"ur'''\", 'ur\"\"\"', \"Ur'''\", 'Ur\"\"\"',\n          \"uR'''\", 'uR\"\"\"', \"UR'''\", 'UR\"\"\"',\n          \"b'''\", 'b\"\"\"', \"B'''\", 'B\"\"\"',\n          \"br'''\", 'br\"\"\"', \"Br'''\", 'Br\"\"\"',\n          \"bR'''\", 'bR\"\"\"', \"BR'''\", 'BR\"\"\"'):\n    triple_quoted[t] = t\nsingle_quoted = {}\nfor t in (\"'\", '\"',\n          \"r'\", 'r\"', \"R'\", 'R\"',\n          \"u'\", 'u\"', \"U'\", 'U\"',\n          \"ur'\", 'ur\"', \"Ur'\", 'Ur\"',\n          \"uR'\", 'uR\"', \"UR'\", 'UR\"',\n          \"b'\", 'b\"', \"B'\", 'B\"',\n          \"br'\", 'br\"', \"Br'\", 'Br\"',\n          \"bR'\", 'bR\"', \"BR'\", 'BR\"' ):\n    single_quoted[t] = t\n\ntabsize = 8\n\nclass TokenError(Exception): pass\n\nclass StopTokenizing(Exception): pass\n\ndef printtoken(type, token, srow_scol, erow_ecol, line): # for testing\n    srow, scol = srow_scol\n    erow, ecol = erow_ecol\n    print \"%d,%d-%d,%d:\\t%s\\t%s\" % \\\n        (srow, scol, erow, ecol, tok_name[type], repr(token))\n\ndef tokenize(readline, tokeneater=printtoken):\n    \"\"\"\n    The tokenize() function accepts two parameters: one representing the\n    input stream, and one providing an output mechanism for tokenize().\n\n    The first parameter, readline, must be a callable object which provides\n    the same interface as the readline() method of built-in file objects.\n    Each call to the function should return one line of input as a string.\n\n    The second parameter, tokeneater, must also be a callable object. It is\n    called once for each token, with five arguments, corresponding to the\n    tuples generated by generate_tokens().\n    \"\"\"\n    try:\n        tokenize_loop(readline, tokeneater)\n    except StopTokenizing:\n        pass\n\n# backwards compatible interface\ndef tokenize_loop(readline, tokeneater):\n    for token_info in generate_tokens(readline):\n        tokeneater(*token_info)\n\nclass Untokenizer:\n\n    def __init__(self):\n        self.tokens = []\n        self.prev_row = 1\n        self.prev_col = 0\n\n    def add_whitespace(self, start):\n        row, col = start\n        if row < self.prev_row or row == self.prev_row and col < self.prev_col:\n            raise ValueError(\"start ({},{}) precedes previous end ({},{})\"\n                             .format(row, col, self.prev_row, self.prev_col))\n        row_offset = row - self.prev_row\n        if row_offset:\n            self.tokens.append(\"\\\\\\n\" * row_offset)\n            self.prev_col = 0\n        col_offset = col - self.prev_col\n        if col_offset:\n            self.tokens.append(\" \" * col_offset)\n\n    def untokenize(self, iterable):\n        it = iter(iterable)\n        for t in it:\n            if len(t) == 2:\n                self.compat(t, it)\n                break\n            tok_type, token, start, end, line = t\n            if tok_type == ENDMARKER:\n                break\n            self.add_whitespace(start)\n            self.tokens.append(token)\n            self.prev_row, self.prev_col = end\n            if tok_type in (NEWLINE, NL):\n                self.prev_row += 1\n                self.prev_col = 0\n        return \"\".join(self.tokens)\n\n    def compat(self, token, iterable):\n        indents = []\n        toks_append = self.tokens.append\n        startline = token[0] in (NEWLINE, NL)\n        prevstring = False\n\n        for tok in chain([token], iterable):\n            toknum, tokval = tok[:2]\n\n            if toknum in (NAME, NUMBER):\n                tokval += ' '\n\n            # Insert a space between two consecutive strings\n            if toknum == STRING:\n                if prevstring:\n                    tokval = ' ' + tokval\n                prevstring = True\n            else:\n                prevstring = False\n\n            if toknum == INDENT:\n                indents.append(tokval)\n                continue\n            elif toknum == DEDENT:\n                indents.pop()\n                continue\n            elif toknum in (NEWLINE, NL):\n                startline = True\n            elif startline and indents:\n                toks_append(indents[-1])\n                startline = False\n            toks_append(tokval)\n\ndef untokenize(iterable):\n    \"\"\"Transform tokens back into Python source code.\n\n    Each element returned by the iterable must be a token sequence\n    with at least two elements, a token number and token value.  If\n    only two tokens are passed, the resulting output is poor.\n\n    Round-trip invariant for full input:\n        Untokenized source will match input source exactly\n\n    Round-trip invariant for limited intput:\n        # Output text will tokenize the back to the input\n        t1 = [tok[:2] for tok in generate_tokens(f.readline)]\n        newcode = untokenize(t1)\n        readline = iter(newcode.splitlines(1)).next\n        t2 = [tok[:2] for tok in generate_tokens(readline)]\n        assert t1 == t2\n    \"\"\"\n    ut = Untokenizer()\n    return ut.untokenize(iterable)\n\ndef generate_tokens(readline):\n    \"\"\"\n    The generate_tokens() generator requires one argument, readline, which\n    must be a callable object which provides the same interface as the\n    readline() method of built-in file objects. Each call to the function\n    should return one line of input as a string.  Alternately, readline\n    can be a callable function terminating with StopIteration:\n        readline = open(myfile).next    # Example of alternate readline\n\n    The generator produces 5-tuples with these members: the token type; the\n    token string; a 2-tuple (srow, scol) of ints specifying the row and\n    column where the token begins in the source; a 2-tuple (erow, ecol) of\n    ints specifying the row and column where the token ends in the source;\n    and the line on which the token was found. The line passed is the\n    logical line; continuation lines are included.\n    \"\"\"\n    lnum = parenlev = continued = 0\n    namechars, numchars = string.ascii_letters + '_', '0123456789'\n    contstr, needcont = '', 0\n    contline = None\n    indents = [0]\n\n    while 1:                                   # loop over lines in stream\n        try:\n            line = readline()\n        except StopIteration:\n            line = ''\n        lnum += 1\n        pos, max = 0, len(line)\n\n        if contstr:                            # continued string\n            if not line:\n                raise TokenError, (\"EOF in multi-line string\", strstart)\n            endmatch = endprog.match(line)\n            if endmatch:\n                pos = end = endmatch.end(0)\n                yield (STRING, contstr + line[:end],\n                       strstart, (lnum, end), contline + line)\n                contstr, needcont = '', 0\n                contline = None\n            elif needcont and line[-2:] != '\\\\\\n' and line[-3:] != '\\\\\\r\\n':\n                yield (ERRORTOKEN, contstr + line,\n                           strstart, (lnum, len(line)), contline)\n                contstr = ''\n                contline = None\n                continue\n            else:\n                contstr = contstr + line\n                contline = contline + line\n                continue\n\n        elif parenlev == 0 and not continued:  # new statement\n            if not line: break\n            column = 0\n            while pos < max:                   # measure leading whitespace\n                if line[pos] == ' ':\n                    column += 1\n                elif line[pos] == '\\t':\n                    column = (column//tabsize + 1)*tabsize\n                elif line[pos] == '\\f':\n                    column = 0\n                else:\n                    break\n                pos += 1\n            if pos == max:\n                break\n\n            if line[pos] in '#\\r\\n':           # skip comments or blank lines\n                if line[pos] == '#':\n                    comment_token = line[pos:].rstrip('\\r\\n')\n                    nl_pos = pos + len(comment_token)\n                    yield (COMMENT, comment_token,\n                           (lnum, pos), (lnum, pos + len(comment_token)), line)\n                    yield (NL, line[nl_pos:],\n                           (lnum, nl_pos), (lnum, len(line)), line)\n                else:\n                    yield ((NL, COMMENT)[line[pos] == '#'], line[pos:],\n                           (lnum, pos), (lnum, len(line)), line)\n                continue\n\n            if column > indents[-1]:           # count indents or dedents\n                indents.append(column)\n                yield (INDENT, line[:pos], (lnum, 0), (lnum, pos), line)\n            while column < indents[-1]:\n                if column not in indents:\n                    raise IndentationError(\n                        \"unindent does not match any outer indentation level\",\n                        (\"<tokenize>\", lnum, pos, line))\n                indents = indents[:-1]\n                yield (DEDENT, '', (lnum, pos), (lnum, pos), line)\n\n        else:                                  # continued statement\n            if not line:\n                raise TokenError, (\"EOF in multi-line statement\", (lnum, 0))\n            continued = 0\n\n        while pos < max:\n            pseudomatch = pseudoprog.match(line, pos)\n            if pseudomatch:                                # scan for tokens\n                start, end = pseudomatch.span(1)\n                spos, epos, pos = (lnum, start), (lnum, end), end\n                if start == end:\n                    continue\n                token, initial = line[start:end], line[start]\n\n                if initial in numchars or \\\n                   (initial == '.' and token != '.'):      # ordinary number\n                    yield (NUMBER, token, spos, epos, line)\n                elif initial in '\\r\\n':\n                    yield (NL if parenlev > 0 else NEWLINE,\n                           token, spos, epos, line)\n                elif initial == '#':\n                    assert not token.endswith(\"\\n\")\n                    yield (COMMENT, token, spos, epos, line)\n                elif token in triple_quoted:\n                    endprog = endprogs[token]\n                    endmatch = endprog.match(line, pos)\n                    if endmatch:                           # all on one line\n                        pos = endmatch.end(0)\n                        token = line[start:pos]\n                        yield (STRING, token, spos, (lnum, pos), line)\n                    else:\n                        strstart = (lnum, start)           # multiple lines\n                        contstr = line[start:]\n                        contline = line\n                        break\n                elif initial in single_quoted or \\\n                    token[:2] in single_quoted or \\\n                    token[:3] in single_quoted:\n                    if token[-1] == '\\n':                  # continued string\n                        strstart = (lnum, start)\n                        endprog = (endprogs[initial] or endprogs[token[1]] or\n                                   endprogs[token[2]])\n                        contstr, needcont = line[start:], 1\n                        contline = line\n                        break\n                    else:                                  # ordinary string\n                        yield (STRING, token, spos, epos, line)\n                elif initial in namechars:                 # ordinary name\n                    yield (NAME, token, spos, epos, line)\n                elif initial == '\\\\':                      # continued stmt\n                    continued = 1\n                else:\n                    if initial in '([{':\n                        parenlev += 1\n                    elif initial in ')]}':\n                        parenlev -= 1\n                    yield (OP, token, spos, epos, line)\n            else:\n                yield (ERRORTOKEN, line[pos],\n                           (lnum, pos), (lnum, pos+1), line)\n                pos += 1\n\n    for indent in indents[1:]:                 # pop remaining indent levels\n        yield (DEDENT, '', (lnum, 0), (lnum, 0), '')\n    yield (ENDMARKER, '', (lnum, 0), (lnum, 0), '')\n\nif __name__ == '__main__':                     # testing\n    import sys\n    if len(sys.argv) > 1:\n        tokenize(open(sys.argv[1]).readline)\n    else:\n        tokenize(sys.stdin.readline)\n", 
    "traceback": "\"\"\"Extract, format and print information about Python stack traces.\"\"\"\n\nimport linecache\nimport sys\nimport types\n\n__all__ = ['extract_stack', 'extract_tb', 'format_exception',\n           'format_exception_only', 'format_list', 'format_stack',\n           'format_tb', 'print_exc', 'format_exc', 'print_exception',\n           'print_last', 'print_stack', 'print_tb', 'tb_lineno']\n\ndef _print(file, str='', terminator='\\n'):\n    file.write(str+terminator)\n\n\ndef print_list(extracted_list, file=None):\n    \"\"\"Print the list of tuples as returned by extract_tb() or\n    extract_stack() as a formatted stack trace to the given file.\"\"\"\n    if file is None:\n        file = sys.stderr\n    for filename, lineno, name, line in extracted_list:\n        _print(file,\n               '  File \"%s\", line %d, in %s' % (filename,lineno,name))\n        if line:\n            _print(file, '    %s' % line.strip())\n\ndef format_list(extracted_list):\n    \"\"\"Format a list of traceback entry tuples for printing.\n\n    Given a list of tuples as returned by extract_tb() or\n    extract_stack(), return a list of strings ready for printing.\n    Each string in the resulting list corresponds to the item with the\n    same index in the argument list.  Each string ends in a newline;\n    the strings may contain internal newlines as well, for those items\n    whose source text line is not None.\n    \"\"\"\n    list = []\n    for filename, lineno, name, line in extracted_list:\n        item = '  File \"%s\", line %d, in %s\\n' % (filename,lineno,name)\n        if line:\n            item = item + '    %s\\n' % line.strip()\n        list.append(item)\n    return list\n\n\ndef print_tb(tb, limit=None, file=None):\n    \"\"\"Print up to 'limit' stack trace entries from the traceback 'tb'.\n\n    If 'limit' is omitted or None, all entries are printed.  If 'file'\n    is omitted or None, the output goes to sys.stderr; otherwise\n    'file' should be an open file or file-like object with a write()\n    method.\n    \"\"\"\n    if file is None:\n        file = sys.stderr\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    n = 0\n    while tb is not None and (limit is None or n < limit):\n        f = tb.tb_frame\n        lineno = tb.tb_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        _print(file,\n               '  File \"%s\", line %d, in %s' % (filename, lineno, name))\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: _print(file, '    ' + line.strip())\n        tb = tb.tb_next\n        n = n+1\n\ndef format_tb(tb, limit = None):\n    \"\"\"A shorthand for 'format_list(extract_tb(tb, limit))'.\"\"\"\n    return format_list(extract_tb(tb, limit))\n\ndef extract_tb(tb, limit = None):\n    \"\"\"Return list of up to limit pre-processed entries from traceback.\n\n    This is useful for alternate formatting of stack traces.  If\n    'limit' is omitted or None, all entries are extracted.  A\n    pre-processed stack trace entry is a quadruple (filename, line\n    number, function name, text) representing the information that is\n    usually printed for a stack trace.  The text is a string with\n    leading and trailing whitespace stripped; if the source is not\n    available it is None.\n    \"\"\"\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    list = []\n    n = 0\n    while tb is not None and (limit is None or n < limit):\n        f = tb.tb_frame\n        lineno = tb.tb_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: line = line.strip()\n        else: line = None\n        list.append((filename, lineno, name, line))\n        tb = tb.tb_next\n        n = n+1\n    return list\n\n\ndef print_exception(etype, value, tb, limit=None, file=None, _encoding=None):\n    \"\"\"Print exception up to 'limit' stack trace entries from 'tb' to 'file'.\n\n    This differs from print_tb() in the following ways: (1) if\n    traceback is not None, it prints a header \"Traceback (most recent\n    call last):\"; (2) it prints the exception type and value after the\n    stack trace; (3) if type is SyntaxError and value has the\n    appropriate format, it prints the line where the syntax error\n    occurred with a caret on the next line indicating the approximate\n    position of the error.\n    \"\"\"\n    if file is None:\n        file = sys.stderr\n    if tb:\n        _print(file, 'Traceback (most recent call last):')\n        print_tb(tb, limit, file)\n    lines = format_exception_only(etype, value, _encoding)\n    for line in lines:\n        _print(file, line, '')\n\ndef format_exception(etype, value, tb, limit = None):\n    \"\"\"Format a stack trace and the exception information.\n\n    The arguments have the same meaning as the corresponding arguments\n    to print_exception().  The return value is a list of strings, each\n    ending in a newline and some containing internal newlines.  When\n    these lines are concatenated and printed, exactly the same text is\n    printed as does print_exception().\n    \"\"\"\n    if tb:\n        list = ['Traceback (most recent call last):\\n']\n        list = list + format_tb(tb, limit)\n    else:\n        list = []\n    list = list + format_exception_only(etype, value)\n    return list\n\ndef format_exception_only(etype, value, _encoding=None):\n    \"\"\"Format the exception part of a traceback.\n\n    The arguments are the exception type and value such as given by\n    sys.last_type and sys.last_value. The return value is a list of\n    strings, each ending in a newline.\n\n    Normally, the list contains a single string; however, for\n    SyntaxError exceptions, it contains several lines that (when\n    printed) display detailed information about where the syntax\n    error occurred.\n\n    The message indicating which exception occurred is always the last\n    string in the list.\n\n    \"\"\"\n\n    # An instance should not have a meaningful value parameter, but\n    # sometimes does, particularly for string exceptions, such as\n    # >>> raise string1, string2  # deprecated\n    #\n    # Clear these out first because issubtype(string1, SyntaxError)\n    # would raise another exception and mask the original problem.\n    if (isinstance(etype, BaseException) or\n        isinstance(etype, types.InstanceType) or\n        etype is None or type(etype) is str):\n        return [_format_final_exc_line(etype, value, _encoding)]\n\n    stype = etype.__name__\n\n    if not issubclass(etype, SyntaxError):\n        return [_format_final_exc_line(stype, value, _encoding)]\n\n    # It was a syntax error; show exactly where the problem was found.\n    lines = []\n    try:\n        msg, (filename, lineno, offset, badline) = value.args\n    except Exception:\n        pass\n    else:\n        filename = filename or \"<string>\"\n        lines.append('  File \"%s\", line %d\\n' % (filename, lineno))\n        if badline is not None:\n            lines.append('    %s\\n' % badline.strip())\n            if offset is not None:\n                caretspace = badline.rstrip('\\n')\n                offset = min(len(caretspace), offset) - 1\n                caretspace = caretspace[:offset].lstrip()\n                # non-space whitespace (likes tabs) must be kept for alignment\n                caretspace = ((c.isspace() and c or ' ') for c in caretspace)\n                lines.append('    %s^\\n' % ''.join(caretspace))\n        value = msg\n\n    lines.append(_format_final_exc_line(stype, value, _encoding))\n    return lines\n\ndef _format_final_exc_line(etype, value, _encoding=None):\n    \"\"\"Return a list of a single line -- normal case for format_exception_only\"\"\"\n    valuestr = _some_str(value, _encoding)\n    if value is None or not valuestr:\n        line = \"%s\\n\" % etype\n    else:\n        line = \"%s: %s\\n\" % (etype, valuestr)\n    return line\n\ndef _some_str(value, _encoding=None):\n    try:\n        return str(value)\n    except Exception:\n        pass\n    try:\n        value = unicode(value)\n        return value.encode(_encoding or \"ascii\", \"backslashreplace\")\n    except Exception:\n        pass\n    return '<unprintable %s object>' % type(value).__name__\n\n\ndef print_exc(limit=None, file=None):\n    \"\"\"Shorthand for 'print_exception(sys.exc_type, sys.exc_value, sys.exc_traceback, limit, file)'.\n    (In fact, it uses sys.exc_info() to retrieve the same information\n    in a thread-safe way.)\"\"\"\n    if file is None:\n        file = sys.stderr\n    try:\n        etype, value, tb = sys.exc_info()\n        print_exception(etype, value, tb, limit, file)\n    finally:\n        etype = value = tb = None\n\n\ndef format_exc(limit=None):\n    \"\"\"Like print_exc() but return a string.\"\"\"\n    try:\n        etype, value, tb = sys.exc_info()\n        return ''.join(format_exception(etype, value, tb, limit))\n    finally:\n        etype = value = tb = None\n\n\ndef print_last(limit=None, file=None):\n    \"\"\"This is a shorthand for 'print_exception(sys.last_type,\n    sys.last_value, sys.last_traceback, limit, file)'.\"\"\"\n    if not hasattr(sys, \"last_type\"):\n        raise ValueError(\"no last exception\")\n    if file is None:\n        file = sys.stderr\n    print_exception(sys.last_type, sys.last_value, sys.last_traceback,\n                    limit, file)\n\n\ndef print_stack(f=None, limit=None, file=None):\n    \"\"\"Print a stack trace from its invocation point.\n\n    The optional 'f' argument can be used to specify an alternate\n    stack frame at which to start. The optional 'limit' and 'file'\n    arguments have the same meaning as for print_exception().\n    \"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    print_list(extract_stack(f, limit), file)\n\ndef format_stack(f=None, limit=None):\n    \"\"\"Shorthand for 'format_list(extract_stack(f, limit))'.\"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    return format_list(extract_stack(f, limit))\n\ndef extract_stack(f=None, limit = None):\n    \"\"\"Extract the raw traceback from the current stack frame.\n\n    The return value has the same format as for extract_tb().  The\n    optional 'f' and 'limit' arguments have the same meaning as for\n    print_stack().  Each item in the list is a quadruple (filename,\n    line number, function name, text), and the entries are in order\n    from oldest to newest stack frame.\n    \"\"\"\n    if f is None:\n        try:\n            raise ZeroDivisionError\n        except ZeroDivisionError:\n            f = sys.exc_info()[2].tb_frame.f_back\n    if limit is None:\n        if hasattr(sys, 'tracebacklimit'):\n            limit = sys.tracebacklimit\n    list = []\n    n = 0\n    while f is not None and (limit is None or n < limit):\n        lineno = f.f_lineno\n        co = f.f_code\n        filename = co.co_filename\n        name = co.co_name\n        linecache.checkcache(filename)\n        line = linecache.getline(filename, lineno, f.f_globals)\n        if line: line = line.strip()\n        else: line = None\n        list.append((filename, lineno, name, line))\n        f = f.f_back\n        n = n+1\n    list.reverse()\n    return list\n\ndef tb_lineno(tb):\n    \"\"\"Calculate correct line number of traceback given in tb.\n\n    Obsolete in 2.3.\n    \"\"\"\n    return tb.tb_lineno\n", 
    "types": "\"\"\"Define names for all type symbols known in the standard interpreter.\n\nTypes that are part of optional modules (e.g. array) are not listed.\n\"\"\"\nimport sys\n\n# Iterators in Python aren't a matter of type but of protocol.  A large\n# and changing number of builtin types implement *some* flavor of\n# iterator.  Don't check the type!  Use hasattr to check for both\n# \"__iter__\" and \"next\" attributes instead.\n\nNoneType = type(None)\nTypeType = type\nObjectType = object\n\nIntType = int\nLongType = long\nFloatType = float\nBooleanType = bool\ntry:\n    ComplexType = complex\nexcept NameError:\n    pass\n\nStringType = str\n\n# StringTypes is already outdated.  Instead of writing \"type(x) in\n# types.StringTypes\", you should use \"isinstance(x, basestring)\".  But\n# we keep around for compatibility with Python 2.2.\ntry:\n    UnicodeType = unicode\n    StringTypes = (StringType, UnicodeType)\nexcept NameError:\n    StringTypes = (StringType,)\n\nBufferType = buffer\n\nTupleType = tuple\nListType = list\nDictType = DictionaryType = dict\n\ndef _f(): pass\nFunctionType = type(_f)\nLambdaType = type(lambda: None)         # Same as FunctionType\nCodeType = type(_f.func_code)\n\ndef _g():\n    yield 1\nGeneratorType = type(_g())\n\nclass _C:\n    def _m(self): pass\nClassType = type(_C)\nUnboundMethodType = type(_C._m)         # Same as MethodType\n_x = _C()\nInstanceType = type(_x)\nMethodType = type(_x._m)\n\nBuiltinFunctionType = type(len)\nBuiltinMethodType = type([].append)     # Same as BuiltinFunctionType\n\nModuleType = type(sys)\nFileType = file\nXRangeType = xrange\n\ntry:\n    raise TypeError\nexcept TypeError:\n    tb = sys.exc_info()[2]\n    TracebackType = type(tb)\n    FrameType = type(tb.tb_frame)\n    del tb\n\nSliceType = slice\nEllipsisType = type(Ellipsis)\n\nDictProxyType = type(TypeType.__dict__)\nNotImplementedType = type(NotImplemented)\n\n# For Jython, the following two types are identical\nGetSetDescriptorType = type(FunctionType.func_code)\nMemberDescriptorType = type(FunctionType.func_globals)\n\ndel sys, _f, _g, _C, _x                           # Not for export\n", 
    "unittest.__init__": "\"\"\"\nPython unit testing framework, based on Erich Gamma's JUnit and Kent Beck's\nSmalltalk testing framework.\n\nThis module contains the core framework classes that form the basis of\nspecific test cases and suites (TestCase, TestSuite etc.), and also a\ntext-based utility class for running the tests and reporting the results\n (TextTestRunner).\n\nSimple usage:\n\n    import unittest\n\n    class IntegerArithmeticTestCase(unittest.TestCase):\n        def testAdd(self):  ## test method names begin 'test*'\n            self.assertEqual((1 + 2), 3)\n            self.assertEqual(0 + 1, 1)\n        def testMultiply(self):\n            self.assertEqual((0 * 10), 0)\n            self.assertEqual((5 * 8), 40)\n\n    if __name__ == '__main__':\n        unittest.main()\n\nFurther information is available in the bundled documentation, and from\n\n  http://docs.python.org/library/unittest.html\n\nCopyright (c) 1999-2003 Steve Purcell\nCopyright (c) 2003-2010 Python Software Foundation\nThis module is free software, and you may redistribute it and/or modify\nit under the same terms as Python itself, so long as this copyright message\nand disclaimer are retained in their original form.\n\nIN NO EVENT SHALL THE AUTHOR BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT,\nSPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OF\nTHIS CODE, EVEN IF THE AUTHOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\nTHE AUTHOR SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE.  THE CODE PROVIDED HEREUNDER IS ON AN \"AS IS\" BASIS,\nAND THERE IS NO OBLIGATION WHATSOEVER TO PROVIDE MAINTENANCE,\nSUPPORT, UPDATES, ENHANCEMENTS, OR MODIFICATIONS.\n\"\"\"\n\n__all__ = ['TestResult', 'TestCase', 'TestSuite',\n           'TextTestRunner', 'TestLoader', 'FunctionTestCase', 'main',\n           'defaultTestLoader', 'SkipTest', 'skip', 'skipIf', 'skipUnless',\n           'expectedFailure', 'TextTestResult', 'installHandler',\n           'registerResult', 'removeResult', 'removeHandler']\n\n# Expose obsolete functions for backwards compatibility\n__all__.extend(['getTestCaseNames', 'makeSuite', 'findTestCases'])\n\n__unittest = True\n\nfrom .result import TestResult\nfrom .case import (TestCase, FunctionTestCase, SkipTest, skip, skipIf,\n                   skipUnless, expectedFailure)\nfrom .suite import BaseTestSuite, TestSuite\nfrom .loader import (TestLoader, defaultTestLoader, makeSuite, getTestCaseNames,\n                     findTestCases)\nfrom .main import TestProgram, main\nfrom .runner import TextTestRunner, TextTestResult\nfrom .signals import installHandler, registerResult, removeResult, removeHandler\n\n# deprecated\n_TextTestResult = TextTestResult\n", 
    "unittest.case": "\"\"\"Test case implementation\"\"\"\n\nimport collections\nimport sys\nimport functools\nimport difflib\nimport pprint\nimport re\nimport types\nimport warnings\n\nfrom . import result\nfrom .util import (\n    strclass, safe_repr, unorderable_list_difference,\n    _count_diff_all_purpose, _count_diff_hashable\n)\n\n\n__unittest = True\n\n\nDIFF_OMITTED = ('\\nDiff is %s characters long. '\n                 'Set self.maxDiff to None to see it.')\n\nclass SkipTest(Exception):\n    \"\"\"\n    Raise this exception in a test to skip it.\n\n    Usually you can use TestCase.skipTest() or one of the skipping decorators\n    instead of raising this directly.\n    \"\"\"\n    pass\n\nclass _ExpectedFailure(Exception):\n    \"\"\"\n    Raise this when a test is expected to fail.\n\n    This is an implementation detail.\n    \"\"\"\n\n    def __init__(self, exc_info):\n        super(_ExpectedFailure, self).__init__()\n        self.exc_info = exc_info\n\nclass _UnexpectedSuccess(Exception):\n    \"\"\"\n    The test was supposed to fail, but it didn't!\n    \"\"\"\n    pass\n\ndef _id(obj):\n    return obj\n\ndef skip(reason):\n    \"\"\"\n    Unconditionally skip a test.\n    \"\"\"\n    def decorator(test_item):\n        if not isinstance(test_item, (type, types.ClassType)):\n            @functools.wraps(test_item)\n            def skip_wrapper(*args, **kwargs):\n                raise SkipTest(reason)\n            test_item = skip_wrapper\n\n        test_item.__unittest_skip__ = True\n        test_item.__unittest_skip_why__ = reason\n        return test_item\n    return decorator\n\ndef skipIf(condition, reason):\n    \"\"\"\n    Skip a test if the condition is true.\n    \"\"\"\n    if condition:\n        return skip(reason)\n    return _id\n\ndef skipUnless(condition, reason):\n    \"\"\"\n    Skip a test unless the condition is true.\n    \"\"\"\n    if not condition:\n        return skip(reason)\n    return _id\n\n\ndef expectedFailure(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            func(*args, **kwargs)\n        except Exception:\n            raise _ExpectedFailure(sys.exc_info())\n        raise _UnexpectedSuccess\n    return wrapper\n\n\nclass _AssertRaisesContext(object):\n    \"\"\"A context manager used to implement TestCase.assertRaises* methods.\"\"\"\n\n    def __init__(self, expected, test_case, expected_regexp=None):\n        self.expected = expected\n        self.failureException = test_case.failureException\n        self.expected_regexp = expected_regexp\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, tb):\n        if exc_type is None:\n            try:\n                exc_name = self.expected.__name__\n            except AttributeError:\n                exc_name = str(self.expected)\n            raise self.failureException(\n                \"{0} not raised\".format(exc_name))\n        if not issubclass(exc_type, self.expected):\n            # let unexpected exceptions pass through\n            return False\n        self.exception = exc_value # store for later retrieval\n        if self.expected_regexp is None:\n            return True\n\n        expected_regexp = self.expected_regexp\n        if not expected_regexp.search(str(exc_value)):\n            raise self.failureException('\"%s\" does not match \"%s\"' %\n                     (expected_regexp.pattern, str(exc_value)))\n        return True\n\n\nclass TestCase(object):\n    \"\"\"A class whose instances are single test cases.\n\n    By default, the test code itself should be placed in a method named\n    'runTest'.\n\n    If the fixture may be used for many test cases, create as\n    many test methods as are needed. When instantiating such a TestCase\n    subclass, specify in the constructor arguments the name of the test method\n    that the instance is to execute.\n\n    Test authors should subclass TestCase for their own tests. Construction\n    and deconstruction of the test's environment ('fixture') can be\n    implemented by overriding the 'setUp' and 'tearDown' methods respectively.\n\n    If it is necessary to override the __init__ method, the base class\n    __init__ method must always be called. It is important that subclasses\n    should not change the signature of their __init__ method, since instances\n    of the classes are instantiated automatically by parts of the framework\n    in order to be run.\n\n    When subclassing TestCase, you can set these attributes:\n    * failureException: determines which exception will be raised when\n        the instance's assertion methods fail; test methods raising this\n        exception will be deemed to have 'failed' rather than 'errored'.\n    * longMessage: determines whether long messages (including repr of\n        objects used in assert methods) will be printed on failure in *addition*\n        to any explicit message passed.\n    * maxDiff: sets the maximum length of a diff in failure messages\n        by assert methods using difflib. It is looked up as an instance\n        attribute so can be configured by individual tests if required.\n    \"\"\"\n\n    failureException = AssertionError\n\n    longMessage = False\n\n    maxDiff = 80*8\n\n    # If a string is longer than _diffThreshold, use normal comparison instead\n    # of difflib.  See #11763.\n    _diffThreshold = 2**16\n\n    # Attribute used by TestSuite for classSetUp\n\n    _classSetupFailed = False\n\n    def __init__(self, methodName='runTest'):\n        \"\"\"Create an instance of the class that will use the named test\n           method when executed. Raises a ValueError if the instance does\n           not have a method with the specified name.\n        \"\"\"\n        self._testMethodName = methodName\n        self._resultForDoCleanups = None\n        try:\n            testMethod = getattr(self, methodName)\n        except AttributeError:\n            raise ValueError(\"no such test method in %s: %s\" %\n                  (self.__class__, methodName))\n        self._testMethodDoc = testMethod.__doc__\n        self._cleanups = []\n\n        # Map types to custom assertEqual functions that will compare\n        # instances of said type in more detail to generate a more useful\n        # error message.\n        self._type_equality_funcs = {}\n        self.addTypeEqualityFunc(dict, 'assertDictEqual')\n        self.addTypeEqualityFunc(list, 'assertListEqual')\n        self.addTypeEqualityFunc(tuple, 'assertTupleEqual')\n        self.addTypeEqualityFunc(set, 'assertSetEqual')\n        self.addTypeEqualityFunc(frozenset, 'assertSetEqual')\n        try:\n            self.addTypeEqualityFunc(unicode, 'assertMultiLineEqual')\n        except NameError:\n            # No unicode support in this build\n            pass\n\n    def addTypeEqualityFunc(self, typeobj, function):\n        \"\"\"Add a type specific assertEqual style function to compare a type.\n\n        This method is for use by TestCase subclasses that need to register\n        their own type equality functions to provide nicer error messages.\n\n        Args:\n            typeobj: The data type to call this function on when both values\n                    are of the same type in assertEqual().\n            function: The callable taking two arguments and an optional\n                    msg= argument that raises self.failureException with a\n                    useful error message when the two arguments are not equal.\n        \"\"\"\n        self._type_equality_funcs[typeobj] = function\n\n    def addCleanup(self, function, *args, **kwargs):\n        \"\"\"Add a function, with arguments, to be called when the test is\n        completed. Functions added are called on a LIFO basis and are\n        called after tearDown on test failure or success.\n\n        Cleanup items are called even if setUp fails (unlike tearDown).\"\"\"\n        self._cleanups.append((function, args, kwargs))\n\n    def setUp(self):\n        \"Hook method for setting up the test fixture before exercising it.\"\n        pass\n\n    def tearDown(self):\n        \"Hook method for deconstructing the test fixture after testing it.\"\n        pass\n\n    @classmethod\n    def setUpClass(cls):\n        \"Hook method for setting up class fixture before running tests in the class.\"\n\n    @classmethod\n    def tearDownClass(cls):\n        \"Hook method for deconstructing the class fixture after running all tests in the class.\"\n\n    def countTestCases(self):\n        return 1\n\n    def defaultTestResult(self):\n        return result.TestResult()\n\n    def shortDescription(self):\n        \"\"\"Returns a one-line description of the test, or None if no\n        description has been provided.\n\n        The default implementation of this method returns the first line of\n        the specified test method's docstring.\n        \"\"\"\n        doc = self._testMethodDoc\n        return doc and doc.split(\"\\n\")[0].strip() or None\n\n\n    def id(self):\n        return \"%s.%s\" % (strclass(self.__class__), self._testMethodName)\n\n    def __eq__(self, other):\n        if type(self) is not type(other):\n            return NotImplemented\n\n        return self._testMethodName == other._testMethodName\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((type(self), self._testMethodName))\n\n    def __str__(self):\n        return \"%s (%s)\" % (self._testMethodName, strclass(self.__class__))\n\n    def __repr__(self):\n        return \"<%s testMethod=%s>\" % \\\n               (strclass(self.__class__), self._testMethodName)\n\n    def _addSkip(self, result, reason):\n        addSkip = getattr(result, 'addSkip', None)\n        if addSkip is not None:\n            addSkip(self, reason)\n        else:\n            warnings.warn(\"TestResult has no addSkip method, skips not reported\",\n                          RuntimeWarning, 2)\n            result.addSuccess(self)\n\n    def run(self, result=None):\n        orig_result = result\n        if result is None:\n            result = self.defaultTestResult()\n            startTestRun = getattr(result, 'startTestRun', None)\n            if startTestRun is not None:\n                startTestRun()\n\n        self._resultForDoCleanups = result\n        result.startTest(self)\n\n        testMethod = getattr(self, self._testMethodName)\n        if (getattr(self.__class__, \"__unittest_skip__\", False) or\n            getattr(testMethod, \"__unittest_skip__\", False)):\n            # If the class or method was skipped.\n            try:\n                skip_why = (getattr(self.__class__, '__unittest_skip_why__', '')\n                            or getattr(testMethod, '__unittest_skip_why__', ''))\n                self._addSkip(result, skip_why)\n            finally:\n                result.stopTest(self)\n            return\n        try:\n            success = False\n            try:\n                self.setUp()\n            except SkipTest as e:\n                self._addSkip(result, str(e))\n            except KeyboardInterrupt:\n                raise\n            except:\n                result.addError(self, sys.exc_info())\n            else:\n                try:\n                    testMethod()\n                except KeyboardInterrupt:\n                    raise\n                except self.failureException:\n                    result.addFailure(self, sys.exc_info())\n                except _ExpectedFailure as e:\n                    addExpectedFailure = getattr(result, 'addExpectedFailure', None)\n                    if addExpectedFailure is not None:\n                        addExpectedFailure(self, e.exc_info)\n                    else:\n                        warnings.warn(\"TestResult has no addExpectedFailure method, reporting as passes\",\n                                      RuntimeWarning)\n                        result.addSuccess(self)\n                except _UnexpectedSuccess:\n                    addUnexpectedSuccess = getattr(result, 'addUnexpectedSuccess', None)\n                    if addUnexpectedSuccess is not None:\n                        addUnexpectedSuccess(self)\n                    else:\n                        warnings.warn(\"TestResult has no addUnexpectedSuccess method, reporting as failures\",\n                                      RuntimeWarning)\n                        result.addFailure(self, sys.exc_info())\n                except SkipTest as e:\n                    self._addSkip(result, str(e))\n                except:\n                    result.addError(self, sys.exc_info())\n                else:\n                    success = True\n\n                try:\n                    self.tearDown()\n                except KeyboardInterrupt:\n                    raise\n                except:\n                    result.addError(self, sys.exc_info())\n                    success = False\n\n            cleanUpSuccess = self.doCleanups()\n            success = success and cleanUpSuccess\n            if success:\n                result.addSuccess(self)\n        finally:\n            result.stopTest(self)\n            if orig_result is None:\n                stopTestRun = getattr(result, 'stopTestRun', None)\n                if stopTestRun is not None:\n                    stopTestRun()\n\n    def doCleanups(self):\n        \"\"\"Execute all cleanup functions. Normally called for you after\n        tearDown.\"\"\"\n        result = self._resultForDoCleanups\n        ok = True\n        while self._cleanups:\n            function, args, kwargs = self._cleanups.pop(-1)\n            try:\n                function(*args, **kwargs)\n            except KeyboardInterrupt:\n                raise\n            except:\n                ok = False\n                result.addError(self, sys.exc_info())\n        return ok\n\n    def __call__(self, *args, **kwds):\n        return self.run(*args, **kwds)\n\n    def debug(self):\n        \"\"\"Run the test without collecting errors in a TestResult\"\"\"\n        self.setUp()\n        getattr(self, self._testMethodName)()\n        self.tearDown()\n        while self._cleanups:\n            function, args, kwargs = self._cleanups.pop(-1)\n            function(*args, **kwargs)\n\n    def skipTest(self, reason):\n        \"\"\"Skip this test.\"\"\"\n        raise SkipTest(reason)\n\n    def fail(self, msg=None):\n        \"\"\"Fail immediately, with the given message.\"\"\"\n        raise self.failureException(msg)\n\n    def assertFalse(self, expr, msg=None):\n        \"\"\"Check that the expression is false.\"\"\"\n        if expr:\n            msg = self._formatMessage(msg, \"%s is not false\" % safe_repr(expr))\n            raise self.failureException(msg)\n\n    def assertTrue(self, expr, msg=None):\n        \"\"\"Check that the expression is true.\"\"\"\n        if not expr:\n            msg = self._formatMessage(msg, \"%s is not true\" % safe_repr(expr))\n            raise self.failureException(msg)\n\n    def _formatMessage(self, msg, standardMsg):\n        \"\"\"Honour the longMessage attribute when generating failure messages.\n        If longMessage is False this means:\n        * Use only an explicit message if it is provided\n        * Otherwise use the standard message for the assert\n\n        If longMessage is True:\n        * Use the standard message\n        * If an explicit message is provided, plus ' : ' and the explicit message\n        \"\"\"\n        if not self.longMessage:\n            return msg or standardMsg\n        if msg is None:\n            return standardMsg\n        try:\n            # don't switch to '{}' formatting in Python 2.X\n            # it changes the way unicode input is handled\n            return '%s : %s' % (standardMsg, msg)\n        except UnicodeDecodeError:\n            return  '%s : %s' % (safe_repr(standardMsg), safe_repr(msg))\n\n\n    def assertRaises(self, excClass, callableObj=None, *args, **kwargs):\n        \"\"\"Fail unless an exception of class excClass is raised\n           by callableObj when invoked with arguments args and keyword\n           arguments kwargs. If a different type of exception is\n           raised, it will not be caught, and the test case will be\n           deemed to have suffered an error, exactly as for an\n           unexpected exception.\n\n           If called with callableObj omitted or None, will return a\n           context object used like this::\n\n                with self.assertRaises(SomeException):\n                    do_something()\n\n           The context manager keeps a reference to the exception as\n           the 'exception' attribute. This allows you to inspect the\n           exception after the assertion::\n\n               with self.assertRaises(SomeException) as cm:\n                   do_something()\n               the_exception = cm.exception\n               self.assertEqual(the_exception.error_code, 3)\n        \"\"\"\n        context = _AssertRaisesContext(excClass, self)\n        if callableObj is None:\n            return context\n        with context:\n            callableObj(*args, **kwargs)\n\n    def _getAssertEqualityFunc(self, first, second):\n        \"\"\"Get a detailed comparison function for the types of the two args.\n\n        Returns: A callable accepting (first, second, msg=None) that will\n        raise a failure exception if first != second with a useful human\n        readable error message for those types.\n        \"\"\"\n        #\n        # NOTE(gregory.p.smith): I considered isinstance(first, type(second))\n        # and vice versa.  I opted for the conservative approach in case\n        # subclasses are not intended to be compared in detail to their super\n        # class instances using a type equality func.  This means testing\n        # subtypes won't automagically use the detailed comparison.  Callers\n        # should use their type specific assertSpamEqual method to compare\n        # subclasses if the detailed comparison is desired and appropriate.\n        # See the discussion in http://bugs.python.org/issue2578.\n        #\n        if type(first) is type(second):\n            asserter = self._type_equality_funcs.get(type(first))\n            if asserter is not None:\n                if isinstance(asserter, basestring):\n                    asserter = getattr(self, asserter)\n                return asserter\n\n        return self._baseAssertEqual\n\n    def _baseAssertEqual(self, first, second, msg=None):\n        \"\"\"The default assertEqual implementation, not type specific.\"\"\"\n        if not first == second:\n            standardMsg = '%s != %s' % (safe_repr(first), safe_repr(second))\n            msg = self._formatMessage(msg, standardMsg)\n            raise self.failureException(msg)\n\n    def assertEqual(self, first, second, msg=None):\n        \"\"\"Fail if the two objects are unequal as determined by the '=='\n           operator.\n        \"\"\"\n        assertion_func = self._getAssertEqualityFunc(first, second)\n        assertion_func(first, second, msg=msg)\n\n    def assertNotEqual(self, first, second, msg=None):\n        \"\"\"Fail if the two objects are equal as determined by the '!='\n           operator.\n        \"\"\"\n        if not first != second:\n            msg = self._formatMessage(msg, '%s == %s' % (safe_repr(first),\n                                                          safe_repr(second)))\n            raise self.failureException(msg)\n\n\n    def assertAlmostEqual(self, first, second, places=None, msg=None, delta=None):\n        \"\"\"Fail if the two objects are unequal as determined by their\n           difference rounded to the given number of decimal places\n           (default 7) and comparing to zero, or by comparing that the\n           between the two objects is more than the given delta.\n\n           Note that decimal places (from zero) are usually not the same\n           as significant digits (measured from the most signficant digit).\n\n           If the two objects compare equal then they will automatically\n           compare almost equal.\n        \"\"\"\n        if first == second:\n            # shortcut\n            return\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n\n        if delta is not None:\n            if abs(first - second) <= delta:\n                return\n\n            standardMsg = '%s != %s within %s delta' % (safe_repr(first),\n                                                        safe_repr(second),\n                                                        safe_repr(delta))\n        else:\n            if places is None:\n                places = 7\n\n            if round(abs(second-first), places) == 0:\n                return\n\n            standardMsg = '%s != %s within %r places' % (safe_repr(first),\n                                                          safe_repr(second),\n                                                          places)\n        msg = self._formatMessage(msg, standardMsg)\n        raise self.failureException(msg)\n\n    def assertNotAlmostEqual(self, first, second, places=None, msg=None, delta=None):\n        \"\"\"Fail if the two objects are equal as determined by their\n           difference rounded to the given number of decimal places\n           (default 7) and comparing to zero, or by comparing that the\n           between the two objects is less than the given delta.\n\n           Note that decimal places (from zero) are usually not the same\n           as significant digits (measured from the most signficant digit).\n\n           Objects that are equal automatically fail.\n        \"\"\"\n        if delta is not None and places is not None:\n            raise TypeError(\"specify delta or places not both\")\n        if delta is not None:\n            if not (first == second) and abs(first - second) > delta:\n                return\n            standardMsg = '%s == %s within %s delta' % (safe_repr(first),\n                                                        safe_repr(second),\n                                                        safe_repr(delta))\n        else:\n            if places is None:\n                places = 7\n            if not (first == second) and round(abs(second-first), places) != 0:\n                return\n            standardMsg = '%s == %s within %r places' % (safe_repr(first),\n                                                         safe_repr(second),\n                                                         places)\n\n        msg = self._formatMessage(msg, standardMsg)\n        raise self.failureException(msg)\n\n    # Synonyms for assertion methods\n\n    # The plurals are undocumented.  Keep them that way to discourage use.\n    # Do not add more.  Do not remove.\n    # Going through a deprecation cycle on these would annoy many people.\n    assertEquals = assertEqual\n    assertNotEquals = assertNotEqual\n    assertAlmostEquals = assertAlmostEqual\n    assertNotAlmostEquals = assertNotAlmostEqual\n    assert_ = assertTrue\n\n    # These fail* assertion method names are pending deprecation and will\n    # be a DeprecationWarning in 3.2; http://bugs.python.org/issue2578\n    def _deprecate(original_func):\n        def deprecated_func(*args, **kwargs):\n            warnings.warn(\n                'Please use {0} instead.'.format(original_func.__name__),\n                PendingDeprecationWarning, 2)\n            return original_func(*args, **kwargs)\n        return deprecated_func\n\n    failUnlessEqual = _deprecate(assertEqual)\n    failIfEqual = _deprecate(assertNotEqual)\n    failUnlessAlmostEqual = _deprecate(assertAlmostEqual)\n    failIfAlmostEqual = _deprecate(assertNotAlmostEqual)\n    failUnless = _deprecate(assertTrue)\n    failUnlessRaises = _deprecate(assertRaises)\n    failIf = _deprecate(assertFalse)\n\n    def assertSequenceEqual(self, seq1, seq2, msg=None, seq_type=None):\n        \"\"\"An equality assertion for ordered sequences (like lists and tuples).\n\n        For the purposes of this function, a valid ordered sequence type is one\n        which can be indexed, has a length, and has an equality operator.\n\n        Args:\n            seq1: The first sequence to compare.\n            seq2: The second sequence to compare.\n            seq_type: The expected datatype of the sequences, or None if no\n                    datatype should be enforced.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n        \"\"\"\n        if seq_type is not None:\n            seq_type_name = seq_type.__name__\n            if not isinstance(seq1, seq_type):\n                raise self.failureException('First sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq1)))\n            if not isinstance(seq2, seq_type):\n                raise self.failureException('Second sequence is not a %s: %s'\n                                        % (seq_type_name, safe_repr(seq2)))\n        else:\n            seq_type_name = \"sequence\"\n\n        differing = None\n        try:\n            len1 = len(seq1)\n        except (TypeError, NotImplementedError):\n            differing = 'First %s has no length.    Non-sequence?' % (\n                    seq_type_name)\n\n        if differing is None:\n            try:\n                len2 = len(seq2)\n            except (TypeError, NotImplementedError):\n                differing = 'Second %s has no length.    Non-sequence?' % (\n                        seq_type_name)\n\n        if differing is None:\n            if seq1 == seq2:\n                return\n\n            seq1_repr = safe_repr(seq1)\n            seq2_repr = safe_repr(seq2)\n            if len(seq1_repr) > 30:\n                seq1_repr = seq1_repr[:30] + '...'\n            if len(seq2_repr) > 30:\n                seq2_repr = seq2_repr[:30] + '...'\n            elements = (seq_type_name.capitalize(), seq1_repr, seq2_repr)\n            differing = '%ss differ: %s != %s\\n' % elements\n\n            for i in xrange(min(len1, len2)):\n                try:\n                    item1 = seq1[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of first %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                try:\n                    item2 = seq2[i]\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('\\nUnable to index element %d of second %s\\n' %\n                                 (i, seq_type_name))\n                    break\n\n                if item1 != item2:\n                    differing += ('\\nFirst differing element %d:\\n%s\\n%s\\n' %\n                                 (i, item1, item2))\n                    break\n            else:\n                if (len1 == len2 and seq_type is None and\n                    type(seq1) != type(seq2)):\n                    # The sequences are the same, but have differing types.\n                    return\n\n            if len1 > len2:\n                differing += ('\\nFirst %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len1 - len2))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len2, seq1[len2]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of first %s\\n' % (len2, seq_type_name))\n            elif len1 < len2:\n                differing += ('\\nSecond %s contains %d additional '\n                             'elements.\\n' % (seq_type_name, len2 - len1))\n                try:\n                    differing += ('First extra element %d:\\n%s\\n' %\n                                  (len1, seq2[len1]))\n                except (TypeError, IndexError, NotImplementedError):\n                    differing += ('Unable to index element %d '\n                                  'of second %s\\n' % (len1, seq_type_name))\n        standardMsg = differing\n        diffMsg = '\\n' + '\\n'.join(\n            difflib.ndiff(pprint.pformat(seq1).splitlines(),\n                          pprint.pformat(seq2).splitlines()))\n        standardMsg = self._truncateMessage(standardMsg, diffMsg)\n        msg = self._formatMessage(msg, standardMsg)\n        self.fail(msg)\n\n    def _truncateMessage(self, message, diff):\n        max_diff = self.maxDiff\n        if max_diff is None or len(diff) <= max_diff:\n            return message + diff\n        return message + (DIFF_OMITTED % len(diff))\n\n    def assertListEqual(self, list1, list2, msg=None):\n        \"\"\"A list-specific equality assertion.\n\n        Args:\n            list1: The first list to compare.\n            list2: The second list to compare.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n\n        \"\"\"\n        self.assertSequenceEqual(list1, list2, msg, seq_type=list)\n\n    def assertTupleEqual(self, tuple1, tuple2, msg=None):\n        \"\"\"A tuple-specific equality assertion.\n\n        Args:\n            tuple1: The first tuple to compare.\n            tuple2: The second tuple to compare.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n        \"\"\"\n        self.assertSequenceEqual(tuple1, tuple2, msg, seq_type=tuple)\n\n    def assertSetEqual(self, set1, set2, msg=None):\n        \"\"\"A set-specific equality assertion.\n\n        Args:\n            set1: The first set to compare.\n            set2: The second set to compare.\n            msg: Optional message to use on failure instead of a list of\n                    differences.\n\n        assertSetEqual uses ducktyping to support different types of sets, and\n        is optimized for sets specifically (parameters must support a\n        difference method).\n        \"\"\"\n        try:\n            difference1 = set1.difference(set2)\n        except TypeError, e:\n            self.fail('invalid type when attempting set difference: %s' % e)\n        except AttributeError, e:\n            self.fail('first argument does not support set difference: %s' % e)\n\n        try:\n            difference2 = set2.difference(set1)\n        except TypeError, e:\n            self.fail('invalid type when attempting set difference: %s' % e)\n        except AttributeError, e:\n            self.fail('second argument does not support set difference: %s' % e)\n\n        if not (difference1 or difference2):\n            return\n\n        lines = []\n        if difference1:\n            lines.append('Items in the first set but not the second:')\n            for item in difference1:\n                lines.append(repr(item))\n        if difference2:\n            lines.append('Items in the second set but not the first:')\n            for item in difference2:\n                lines.append(repr(item))\n\n        standardMsg = '\\n'.join(lines)\n        self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIn(self, member, container, msg=None):\n        \"\"\"Just like self.assertTrue(a in b), but with a nicer default message.\"\"\"\n        if member not in container:\n            standardMsg = '%s not found in %s' % (safe_repr(member),\n                                                  safe_repr(container))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertNotIn(self, member, container, msg=None):\n        \"\"\"Just like self.assertTrue(a not in b), but with a nicer default message.\"\"\"\n        if member in container:\n            standardMsg = '%s unexpectedly found in %s' % (safe_repr(member),\n                                                        safe_repr(container))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIs(self, expr1, expr2, msg=None):\n        \"\"\"Just like self.assertTrue(a is b), but with a nicer default message.\"\"\"\n        if expr1 is not expr2:\n            standardMsg = '%s is not %s' % (safe_repr(expr1),\n                                             safe_repr(expr2))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsNot(self, expr1, expr2, msg=None):\n        \"\"\"Just like self.assertTrue(a is not b), but with a nicer default message.\"\"\"\n        if expr1 is expr2:\n            standardMsg = 'unexpectedly identical: %s' % (safe_repr(expr1),)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertDictEqual(self, d1, d2, msg=None):\n        self.assertIsInstance(d1, dict, 'First argument is not a dictionary')\n        self.assertIsInstance(d2, dict, 'Second argument is not a dictionary')\n\n        if d1 != d2:\n            standardMsg = '%s != %s' % (safe_repr(d1, True), safe_repr(d2, True))\n            diff = ('\\n' + '\\n'.join(difflib.ndiff(\n                           pprint.pformat(d1).splitlines(),\n                           pprint.pformat(d2).splitlines())))\n            standardMsg = self._truncateMessage(standardMsg, diff)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertDictContainsSubset(self, expected, actual, msg=None):\n        \"\"\"Checks whether actual is a superset of expected.\"\"\"\n        missing = []\n        mismatched = []\n        for key, value in expected.iteritems():\n            if key not in actual:\n                missing.append(key)\n            elif value != actual[key]:\n                mismatched.append('%s, expected: %s, actual: %s' %\n                                  (safe_repr(key), safe_repr(value),\n                                   safe_repr(actual[key])))\n\n        if not (missing or mismatched):\n            return\n\n        standardMsg = ''\n        if missing:\n            standardMsg = 'Missing: %s' % ','.join(safe_repr(m) for m in\n                                                    missing)\n        if mismatched:\n            if standardMsg:\n                standardMsg += '; '\n            standardMsg += 'Mismatched values: %s' % ','.join(mismatched)\n\n        self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertItemsEqual(self, expected_seq, actual_seq, msg=None):\n        \"\"\"An unordered sequence specific comparison. It asserts that\n        actual_seq and expected_seq have the same element counts.\n        Equivalent to::\n\n            self.assertEqual(Counter(iter(actual_seq)),\n                             Counter(iter(expected_seq)))\n\n        Asserts that each element has the same count in both sequences.\n        Example:\n            - [0, 1, 1] and [1, 0, 1] compare equal.\n            - [0, 0, 1] and [0, 1] compare unequal.\n        \"\"\"\n        first_seq, second_seq = list(expected_seq), list(actual_seq)\n        with warnings.catch_warnings():\n            if sys.py3kwarning:\n                # Silence Py3k warning raised during the sorting\n                for _msg in [\"(code|dict|type) inequality comparisons\",\n                             \"builtin_function_or_method order comparisons\",\n                             \"comparing unequal types\"]:\n                    warnings.filterwarnings(\"ignore\", _msg, DeprecationWarning)\n            try:\n                first = collections.Counter(first_seq)\n                second = collections.Counter(second_seq)\n            except TypeError:\n                # Handle case with unhashable elements\n                differences = _count_diff_all_purpose(first_seq, second_seq)\n            else:\n                if first == second:\n                    return\n                differences = _count_diff_hashable(first_seq, second_seq)\n\n        if differences:\n            standardMsg = 'Element counts were not equal:\\n'\n            lines = ['First has %d, Second has %d:  %r' % diff for diff in differences]\n            diffMsg = '\\n'.join(lines)\n            standardMsg = self._truncateMessage(standardMsg, diffMsg)\n            msg = self._formatMessage(msg, standardMsg)\n            self.fail(msg)\n\n    def assertMultiLineEqual(self, first, second, msg=None):\n        \"\"\"Assert that two multi-line strings are equal.\"\"\"\n        self.assertIsInstance(first, basestring,\n                'First argument is not a string')\n        self.assertIsInstance(second, basestring,\n                'Second argument is not a string')\n\n        if first != second:\n            # don't use difflib if the strings are too long\n            if (len(first) > self._diffThreshold or\n                len(second) > self._diffThreshold):\n                self._baseAssertEqual(first, second, msg)\n            firstlines = first.splitlines(True)\n            secondlines = second.splitlines(True)\n            if len(firstlines) == 1 and first.strip('\\r\\n') == first:\n                firstlines = [first + '\\n']\n                secondlines = [second + '\\n']\n            standardMsg = '%s != %s' % (safe_repr(first, True),\n                                        safe_repr(second, True))\n            diff = '\\n' + ''.join(difflib.ndiff(firstlines, secondlines))\n            standardMsg = self._truncateMessage(standardMsg, diff)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertLess(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a < b), but with a nicer default message.\"\"\"\n        if not a < b:\n            standardMsg = '%s not less than %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertLessEqual(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a <= b), but with a nicer default message.\"\"\"\n        if not a <= b:\n            standardMsg = '%s not less than or equal to %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertGreater(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a > b), but with a nicer default message.\"\"\"\n        if not a > b:\n            standardMsg = '%s not greater than %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertGreaterEqual(self, a, b, msg=None):\n        \"\"\"Just like self.assertTrue(a >= b), but with a nicer default message.\"\"\"\n        if not a >= b:\n            standardMsg = '%s not greater than or equal to %s' % (safe_repr(a), safe_repr(b))\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsNone(self, obj, msg=None):\n        \"\"\"Same as self.assertTrue(obj is None), with a nicer default message.\"\"\"\n        if obj is not None:\n            standardMsg = '%s is not None' % (safe_repr(obj),)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsNotNone(self, obj, msg=None):\n        \"\"\"Included for symmetry with assertIsNone.\"\"\"\n        if obj is None:\n            standardMsg = 'unexpectedly None'\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertIsInstance(self, obj, cls, msg=None):\n        \"\"\"Same as self.assertTrue(isinstance(obj, cls)), with a nicer\n        default message.\"\"\"\n        if not isinstance(obj, cls):\n            standardMsg = '%s is not an instance of %r' % (safe_repr(obj), cls)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertNotIsInstance(self, obj, cls, msg=None):\n        \"\"\"Included for symmetry with assertIsInstance.\"\"\"\n        if isinstance(obj, cls):\n            standardMsg = '%s is an instance of %r' % (safe_repr(obj), cls)\n            self.fail(self._formatMessage(msg, standardMsg))\n\n    def assertRaisesRegexp(self, expected_exception, expected_regexp,\n                           callable_obj=None, *args, **kwargs):\n        \"\"\"Asserts that the message in a raised exception matches a regexp.\n\n        Args:\n            expected_exception: Exception class expected to be raised.\n            expected_regexp: Regexp (re pattern object or string) expected\n                    to be found in error message.\n            callable_obj: Function to be called.\n            args: Extra args.\n            kwargs: Extra kwargs.\n        \"\"\"\n        if expected_regexp is not None:\n            expected_regexp = re.compile(expected_regexp)\n        context = _AssertRaisesContext(expected_exception, self, expected_regexp)\n        if callable_obj is None:\n            return context\n        with context:\n            callable_obj(*args, **kwargs)\n\n    def assertRegexpMatches(self, text, expected_regexp, msg=None):\n        \"\"\"Fail the test unless the text matches the regular expression.\"\"\"\n        if isinstance(expected_regexp, basestring):\n            expected_regexp = re.compile(expected_regexp)\n        if not expected_regexp.search(text):\n            msg = msg or \"Regexp didn't match\"\n            msg = '%s: %r not found in %r' % (msg, expected_regexp.pattern, text)\n            raise self.failureException(msg)\n\n    def assertNotRegexpMatches(self, text, unexpected_regexp, msg=None):\n        \"\"\"Fail the test if the text matches the regular expression.\"\"\"\n        if isinstance(unexpected_regexp, basestring):\n            unexpected_regexp = re.compile(unexpected_regexp)\n        match = unexpected_regexp.search(text)\n        if match:\n            msg = msg or \"Regexp matched\"\n            msg = '%s: %r matches %r in %r' % (msg,\n                                               text[match.start():match.end()],\n                                               unexpected_regexp.pattern,\n                                               text)\n            raise self.failureException(msg)\n\n\nclass FunctionTestCase(TestCase):\n    \"\"\"A test case that wraps a test function.\n\n    This is useful for slipping pre-existing test functions into the\n    unittest framework. Optionally, set-up and tidy-up functions can be\n    supplied. As with TestCase, the tidy-up ('tearDown') function will\n    always be called if the set-up ('setUp') function ran successfully.\n    \"\"\"\n\n    def __init__(self, testFunc, setUp=None, tearDown=None, description=None):\n        super(FunctionTestCase, self).__init__()\n        self._setUpFunc = setUp\n        self._tearDownFunc = tearDown\n        self._testFunc = testFunc\n        self._description = description\n\n    def setUp(self):\n        if self._setUpFunc is not None:\n            self._setUpFunc()\n\n    def tearDown(self):\n        if self._tearDownFunc is not None:\n            self._tearDownFunc()\n\n    def runTest(self):\n        self._testFunc()\n\n    def id(self):\n        return self._testFunc.__name__\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n\n        return self._setUpFunc == other._setUpFunc and \\\n               self._tearDownFunc == other._tearDownFunc and \\\n               self._testFunc == other._testFunc and \\\n               self._description == other._description\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __hash__(self):\n        return hash((type(self), self._setUpFunc, self._tearDownFunc,\n                     self._testFunc, self._description))\n\n    def __str__(self):\n        return \"%s (%s)\" % (strclass(self.__class__),\n                            self._testFunc.__name__)\n\n    def __repr__(self):\n        return \"<%s tec=%s>\" % (strclass(self.__class__),\n                                     self._testFunc)\n\n    def shortDescription(self):\n        if self._description is not None:\n            return self._description\n        doc = self._testFunc.__doc__\n        return doc and doc.split(\"\\n\")[0].strip() or None\n", 
    "unittest.loader": "\"\"\"Loading unittests.\"\"\"\n\nimport os\nimport re\nimport sys\nimport traceback\nimport types\n\nfrom functools import cmp_to_key as _CmpToKey\nfrom fnmatch import fnmatch\n\nfrom . import case, suite\n\n__unittest = True\n\n# what about .pyc or .pyo (etc)\n# we would need to avoid loading the same tests multiple times\n# from '.py', '.pyc' *and* '.pyo'\nVALID_MODULE_NAME = re.compile(r'[_a-z]\\w*\\.py$', re.IGNORECASE)\n\n\ndef _make_failed_import_test(name, suiteClass):\n    message = 'Failed to import test module: %s\\n%s' % (name, traceback.format_exc())\n    return _make_failed_test('ModuleImportFailure', name, ImportError(message),\n                             suiteClass)\n\ndef _make_failed_load_tests(name, exception, suiteClass):\n    return _make_failed_test('LoadTestsFailure', name, exception, suiteClass)\n\ndef _make_failed_test(classname, methodname, exception, suiteClass):\n    def testFailure(self):\n        raise exception\n    attrs = {methodname: testFailure}\n    TestClass = type(classname, (case.TestCase,), attrs)\n    return suiteClass((TestClass(methodname),))\n\n\nclass TestLoader(object):\n    \"\"\"\n    This class is responsible for loading tests according to various criteria\n    and returning them wrapped in a TestSuite\n    \"\"\"\n    testMethodPrefix = 'test'\n    sortTestMethodsUsing = cmp\n    suiteClass = suite.TestSuite\n    _top_level_dir = None\n\n    def loadTestsFromTestCase(self, testCaseClass):\n        \"\"\"Return a suite of all tests cases contained in testCaseClass\"\"\"\n        if issubclass(testCaseClass, suite.TestSuite):\n            raise TypeError(\"Test cases should not be derived from TestSuite.\" \\\n                                \" Maybe you meant to derive from TestCase?\")\n        testCaseNames = self.getTestCaseNames(testCaseClass)\n        if not testCaseNames and hasattr(testCaseClass, 'runTest'):\n            testCaseNames = ['runTest']\n        loaded_suite = self.suiteClass(map(testCaseClass, testCaseNames))\n        return loaded_suite\n\n    def loadTestsFromModule(self, module, use_load_tests=True):\n        \"\"\"Return a suite of all tests cases contained in the given module\"\"\"\n        tests = []\n        for name in dir(module):\n            obj = getattr(module, name)\n            if isinstance(obj, type) and issubclass(obj, case.TestCase):\n                tests.append(self.loadTestsFromTestCase(obj))\n\n        load_tests = getattr(module, 'load_tests', None)\n        tests = self.suiteClass(tests)\n        if use_load_tests and load_tests is not None:\n            try:\n                return load_tests(self, tests, None)\n            except Exception, e:\n                return _make_failed_load_tests(module.__name__, e,\n                                               self.suiteClass)\n        return tests\n\n    def loadTestsFromName(self, name, module=None):\n        \"\"\"Return a suite of all tests cases given a string specifier.\n\n        The name may resolve either to a module, a test case class, a\n        test method within a test case class, or a callable object which\n        returns a TestCase or TestSuite instance.\n\n        The method optionally resolves the names relative to a given module.\n        \"\"\"\n        parts = name.split('.')\n        if module is None:\n            parts_copy = parts[:]\n            while parts_copy:\n                try:\n                    module = __import__('.'.join(parts_copy))\n                    break\n                except ImportError:\n                    del parts_copy[-1]\n                    if not parts_copy:\n                        raise\n            parts = parts[1:]\n        obj = module\n        for part in parts:\n            parent, obj = obj, getattr(obj, part)\n\n        if isinstance(obj, types.ModuleType):\n            return self.loadTestsFromModule(obj)\n        elif isinstance(obj, type) and issubclass(obj, case.TestCase):\n            return self.loadTestsFromTestCase(obj)\n        elif (isinstance(obj, types.UnboundMethodType) and\n              isinstance(parent, type) and\n              issubclass(parent, case.TestCase)):\n            name = parts[-1]\n            inst = parent(name)\n            return self.suiteClass([inst])\n        elif isinstance(obj, suite.TestSuite):\n            return obj\n        elif hasattr(obj, '__call__'):\n            test = obj()\n            if isinstance(test, suite.TestSuite):\n                return test\n            elif isinstance(test, case.TestCase):\n                return self.suiteClass([test])\n            else:\n                raise TypeError(\"calling %s returned %s, not a test\" %\n                                (obj, test))\n        else:\n            raise TypeError(\"don't know how to make test from: %s\" % obj)\n\n    def loadTestsFromNames(self, names, module=None):\n        \"\"\"Return a suite of all tests cases found using the given sequence\n        of string specifiers. See 'loadTestsFromName()'.\n        \"\"\"\n        suites = [self.loadTestsFromName(name, module) for name in names]\n        return self.suiteClass(suites)\n\n    def getTestCaseNames(self, testCaseClass):\n        \"\"\"Return a sorted sequence of method names found within testCaseClass\n        \"\"\"\n        def isTestMethod(attrname, testCaseClass=testCaseClass,\n                         prefix=self.testMethodPrefix):\n            return attrname.startswith(prefix) and \\\n                hasattr(getattr(testCaseClass, attrname), '__call__')\n        testFnNames = filter(isTestMethod, dir(testCaseClass))\n        if self.sortTestMethodsUsing:\n            testFnNames.sort(key=_CmpToKey(self.sortTestMethodsUsing))\n        return testFnNames\n\n    def discover(self, start_dir, pattern='test*.py', top_level_dir=None):\n        \"\"\"Find and return all test modules from the specified start\n        directory, recursing into subdirectories to find them. Only test files\n        that match the pattern will be loaded. (Using shell style pattern\n        matching.)\n\n        All test modules must be importable from the top level of the project.\n        If the start directory is not the top level directory then the top\n        level directory must be specified separately.\n\n        If a test package name (directory with '__init__.py') matches the\n        pattern then the package will be checked for a 'load_tests' function. If\n        this exists then it will be called with loader, tests, pattern.\n\n        If load_tests exists then discovery does  *not* recurse into the package,\n        load_tests is responsible for loading all tests in the package.\n\n        The pattern is deliberately not stored as a loader attribute so that\n        packages can continue discovery themselves. top_level_dir is stored so\n        load_tests does not need to pass this argument in to loader.discover().\n        \"\"\"\n        set_implicit_top = False\n        if top_level_dir is None and self._top_level_dir is not None:\n            # make top_level_dir optional if called from load_tests in a package\n            top_level_dir = self._top_level_dir\n        elif top_level_dir is None:\n            set_implicit_top = True\n            top_level_dir = start_dir\n\n        top_level_dir = os.path.abspath(top_level_dir)\n\n        if not top_level_dir in sys.path:\n            # all test modules must be importable from the top level directory\n            # should we *unconditionally* put the start directory in first\n            # in sys.path to minimise likelihood of conflicts between installed\n            # modules and development versions?\n            sys.path.insert(0, top_level_dir)\n        self._top_level_dir = top_level_dir\n\n        is_not_importable = False\n        if os.path.isdir(os.path.abspath(start_dir)):\n            start_dir = os.path.abspath(start_dir)\n            if start_dir != top_level_dir:\n                is_not_importable = not os.path.isfile(os.path.join(start_dir, '__init__.py'))\n        else:\n            # support for discovery from dotted module names\n            try:\n                __import__(start_dir)\n            except ImportError:\n                is_not_importable = True\n            else:\n                the_module = sys.modules[start_dir]\n                top_part = start_dir.split('.')[0]\n                start_dir = os.path.abspath(os.path.dirname((the_module.__file__)))\n                if set_implicit_top:\n                    self._top_level_dir = self._get_directory_containing_module(top_part)\n                    sys.path.remove(top_level_dir)\n\n        if is_not_importable:\n            raise ImportError('Start directory is not importable: %r' % start_dir)\n\n        tests = list(self._find_tests(start_dir, pattern))\n        return self.suiteClass(tests)\n\n    def _get_directory_containing_module(self, module_name):\n        module = sys.modules[module_name]\n        full_path = os.path.abspath(module.__file__)\n\n        if os.path.basename(full_path).lower().startswith('__init__.py'):\n            return os.path.dirname(os.path.dirname(full_path))\n        else:\n            # here we have been given a module rather than a package - so\n            # all we can do is search the *same* directory the module is in\n            # should an exception be raised instead\n            return os.path.dirname(full_path)\n\n    def _get_name_from_path(self, path):\n        path = os.path.splitext(os.path.normpath(path))[0]\n\n        _relpath = os.path.relpath(path, self._top_level_dir)\n        assert not os.path.isabs(_relpath), \"Path must be within the project\"\n        assert not _relpath.startswith('..'), \"Path must be within the project\"\n\n        name = _relpath.replace(os.path.sep, '.')\n        return name\n\n    def _get_module_from_name(self, name):\n        __import__(name)\n        return sys.modules[name]\n\n    def _match_path(self, path, full_path, pattern):\n        # override this method to use alternative matching strategy\n        return fnmatch(path, pattern)\n\n    def _find_tests(self, start_dir, pattern):\n        \"\"\"Used by discovery. Yields test suites it loads.\"\"\"\n        paths = os.listdir(start_dir)\n\n        for path in paths:\n            full_path = os.path.join(start_dir, path)\n            if os.path.isfile(full_path):\n                if not VALID_MODULE_NAME.match(path):\n                    # valid Python identifiers only\n                    continue\n                if not self._match_path(path, full_path, pattern):\n                    continue\n                # if the test file matches, load it\n                name = self._get_name_from_path(full_path)\n                try:\n                    module = self._get_module_from_name(name)\n                except:\n                    yield _make_failed_import_test(name, self.suiteClass)\n                else:\n                    mod_file = os.path.abspath(getattr(module, '__file__', full_path))\n                    realpath = os.path.splitext(os.path.realpath(mod_file))[0]\n                    fullpath_noext = os.path.splitext(os.path.realpath(full_path))[0]\n                    if realpath.lower() != fullpath_noext.lower():\n                        module_dir = os.path.dirname(realpath)\n                        mod_name = os.path.splitext(os.path.basename(full_path))[0]\n                        expected_dir = os.path.dirname(full_path)\n                        msg = (\"%r module incorrectly imported from %r. Expected %r. \"\n                               \"Is this module globally installed?\")\n                        raise ImportError(msg % (mod_name, module_dir, expected_dir))\n                    yield self.loadTestsFromModule(module)\n            elif os.path.isdir(full_path):\n                if not os.path.isfile(os.path.join(full_path, '__init__.py')):\n                    continue\n\n                load_tests = None\n                tests = None\n                if fnmatch(path, pattern):\n                    # only check load_tests if the package directory itself matches the filter\n                    name = self._get_name_from_path(full_path)\n                    package = self._get_module_from_name(name)\n                    load_tests = getattr(package, 'load_tests', None)\n                    tests = self.loadTestsFromModule(package, use_load_tests=False)\n\n                if load_tests is None:\n                    if tests is not None:\n                        # tests loaded from package file\n                        yield tests\n                    # recurse into the package\n                    for test in self._find_tests(full_path, pattern):\n                        yield test\n                else:\n                    try:\n                        yield load_tests(self, tests, pattern)\n                    except Exception, e:\n                        yield _make_failed_load_tests(package.__name__, e,\n                                                      self.suiteClass)\n\ndefaultTestLoader = TestLoader()\n\n\ndef _makeLoader(prefix, sortUsing, suiteClass=None):\n    loader = TestLoader()\n    loader.sortTestMethodsUsing = sortUsing\n    loader.testMethodPrefix = prefix\n    if suiteClass:\n        loader.suiteClass = suiteClass\n    return loader\n\ndef getTestCaseNames(testCaseClass, prefix, sortUsing=cmp):\n    return _makeLoader(prefix, sortUsing).getTestCaseNames(testCaseClass)\n\ndef makeSuite(testCaseClass, prefix='test', sortUsing=cmp,\n              suiteClass=suite.TestSuite):\n    return _makeLoader(prefix, sortUsing, suiteClass).loadTestsFromTestCase(testCaseClass)\n\ndef findTestCases(module, prefix='test', sortUsing=cmp,\n                  suiteClass=suite.TestSuite):\n    return _makeLoader(prefix, sortUsing, suiteClass).loadTestsFromModule(module)\n", 
    "unittest.main": "\"\"\"Unittest main program\"\"\"\n\nimport sys\nimport os\nimport types\n\nfrom . import loader, runner\nfrom .signals import installHandler\n\n__unittest = True\n\nFAILFAST     = \"  -f, --failfast   Stop on first failure\\n\"\nCATCHBREAK   = \"  -c, --catch      Catch control-C and display results\\n\"\nBUFFEROUTPUT = \"  -b, --buffer     Buffer stdout and stderr during test runs\\n\"\n\nUSAGE_AS_MAIN = \"\"\"\\\nUsage: %(progName)s [options] [tests]\n\nOptions:\n  -h, --help       Show this message\n  -v, --verbose    Verbose output\n  -q, --quiet      Minimal output\n%(failfast)s%(catchbreak)s%(buffer)s\nExamples:\n  %(progName)s test_module               - run tests from test_module\n  %(progName)s module.TestClass          - run tests from module.TestClass\n  %(progName)s module.Class.test_method  - run specified test method\n\n[tests] can be a list of any number of test modules, classes and test\nmethods.\n\nAlternative Usage: %(progName)s discover [options]\n\nOptions:\n  -v, --verbose    Verbose output\n%(failfast)s%(catchbreak)s%(buffer)s  -s directory     Directory to start discovery ('.' default)\n  -p pattern       Pattern to match test files ('test*.py' default)\n  -t directory     Top level directory of project (default to\n                   start directory)\n\nFor test discovery all test modules must be importable from the top\nlevel directory of the project.\n\"\"\"\n\nUSAGE_FROM_MODULE = \"\"\"\\\nUsage: %(progName)s [options] [test] [...]\n\nOptions:\n  -h, --help       Show this message\n  -v, --verbose    Verbose output\n  -q, --quiet      Minimal output\n%(failfast)s%(catchbreak)s%(buffer)s\nExamples:\n  %(progName)s                               - run default set of tests\n  %(progName)s MyTestSuite                   - run suite 'MyTestSuite'\n  %(progName)s MyTestCase.testSomething      - run MyTestCase.testSomething\n  %(progName)s MyTestCase                    - run all 'test*' test methods\n                                               in MyTestCase\n\"\"\"\n\n\n\nclass TestProgram(object):\n    \"\"\"A command-line program that runs a set of tests; this is primarily\n       for making test modules conveniently executable.\n    \"\"\"\n    USAGE = USAGE_FROM_MODULE\n\n    # defaults for testing\n    failfast = catchbreak = buffer = progName = None\n\n    def __init__(self, module='__main__', defaultTest=None, argv=None,\n                    testRunner=None, testLoader=loader.defaultTestLoader,\n                    exit=True, verbosity=1, failfast=None, catchbreak=None,\n                    buffer=None):\n        if isinstance(module, basestring):\n            self.module = __import__(module)\n            for part in module.split('.')[1:]:\n                self.module = getattr(self.module, part)\n        else:\n            self.module = module\n        if argv is None:\n            argv = sys.argv\n\n        self.exit = exit\n        self.failfast = failfast\n        self.catchbreak = catchbreak\n        self.verbosity = verbosity\n        self.buffer = buffer\n        self.defaultTest = defaultTest\n        self.testRunner = testRunner\n        self.testLoader = testLoader\n        self.progName = os.path.basename(argv[0])\n        self.parseArgs(argv)\n        self.runTests()\n\n    def usageExit(self, msg=None):\n        if msg:\n            print msg\n        usage = {'progName': self.progName, 'catchbreak': '', 'failfast': '',\n                 'buffer': ''}\n        if self.failfast != False:\n            usage['failfast'] = FAILFAST\n        if self.catchbreak != False:\n            usage['catchbreak'] = CATCHBREAK\n        if self.buffer != False:\n            usage['buffer'] = BUFFEROUTPUT\n        print self.USAGE % usage\n        sys.exit(2)\n\n    def parseArgs(self, argv):\n        if len(argv) > 1 and argv[1].lower() == 'discover':\n            self._do_discovery(argv[2:])\n            return\n\n        import getopt\n        long_opts = ['help', 'verbose', 'quiet', 'failfast', 'catch', 'buffer']\n        try:\n            options, args = getopt.getopt(argv[1:], 'hHvqfcb', long_opts)\n            for opt, value in options:\n                if opt in ('-h','-H','--help'):\n                    self.usageExit()\n                if opt in ('-q','--quiet'):\n                    self.verbosity = 0\n                if opt in ('-v','--verbose'):\n                    self.verbosity = 2\n                if opt in ('-f','--failfast'):\n                    if self.failfast is None:\n                        self.failfast = True\n                    # Should this raise an exception if -f is not valid?\n                if opt in ('-c','--catch'):\n                    if self.catchbreak is None:\n                        self.catchbreak = True\n                    # Should this raise an exception if -c is not valid?\n                if opt in ('-b','--buffer'):\n                    if self.buffer is None:\n                        self.buffer = True\n                    # Should this raise an exception if -b is not valid?\n            if len(args) == 0 and self.defaultTest is None:\n                # createTests will load tests from self.module\n                self.testNames = None\n            elif len(args) > 0:\n                self.testNames = args\n                if __name__ == '__main__':\n                    # to support python -m unittest ...\n                    self.module = None\n            else:\n                self.testNames = (self.defaultTest,)\n            self.createTests()\n        except getopt.error, msg:\n            self.usageExit(msg)\n\n    def createTests(self):\n        if self.testNames is None:\n            self.test = self.testLoader.loadTestsFromModule(self.module)\n        else:\n            self.test = self.testLoader.loadTestsFromNames(self.testNames,\n                                                           self.module)\n\n    def _do_discovery(self, argv, Loader=None):\n        if Loader is None:\n            Loader = lambda: self.testLoader\n\n        # handle command line args for test discovery\n        self.progName = '%s discover' % self.progName\n        import optparse\n        parser = optparse.OptionParser()\n        parser.prog = self.progName\n        parser.add_option('-v', '--verbose', dest='verbose', default=False,\n                          help='Verbose output', action='store_true')\n        if self.failfast != False:\n            parser.add_option('-f', '--failfast', dest='failfast', default=False,\n                              help='Stop on first fail or error',\n                              action='store_true')\n        if self.catchbreak != False:\n            parser.add_option('-c', '--catch', dest='catchbreak', default=False,\n                              help='Catch ctrl-C and display results so far',\n                              action='store_true')\n        if self.buffer != False:\n            parser.add_option('-b', '--buffer', dest='buffer', default=False,\n                              help='Buffer stdout and stderr during tests',\n                              action='store_true')\n        parser.add_option('-s', '--start-directory', dest='start', default='.',\n                          help=\"Directory to start discovery ('.' default)\")\n        parser.add_option('-p', '--pattern', dest='pattern', default='test*.py',\n                          help=\"Pattern to match tests ('test*.py' default)\")\n        parser.add_option('-t', '--top-level-directory', dest='top', default=None,\n                          help='Top level directory of project (defaults to start directory)')\n\n        options, args = parser.parse_args(argv)\n        if len(args) > 3:\n            self.usageExit()\n\n        for name, value in zip(('start', 'pattern', 'top'), args):\n            setattr(options, name, value)\n\n        # only set options from the parsing here\n        # if they weren't set explicitly in the constructor\n        if self.failfast is None:\n            self.failfast = options.failfast\n        if self.catchbreak is None:\n            self.catchbreak = options.catchbreak\n        if self.buffer is None:\n            self.buffer = options.buffer\n\n        if options.verbose:\n            self.verbosity = 2\n\n        start_dir = options.start\n        pattern = options.pattern\n        top_level_dir = options.top\n\n        loader = Loader()\n        self.test = loader.discover(start_dir, pattern, top_level_dir)\n\n    def runTests(self):\n        if self.catchbreak:\n            installHandler()\n        if self.testRunner is None:\n            self.testRunner = runner.TextTestRunner\n        if isinstance(self.testRunner, (type, types.ClassType)):\n            try:\n                testRunner = self.testRunner(verbosity=self.verbosity,\n                                             failfast=self.failfast,\n                                             buffer=self.buffer)\n            except TypeError:\n                # didn't accept the verbosity, buffer or failfast arguments\n                testRunner = self.testRunner()\n        else:\n            # it is assumed to be a TestRunner instance\n            testRunner = self.testRunner\n        self.result = testRunner.run(self.test)\n        if self.exit:\n            sys.exit(not self.result.wasSuccessful())\n\nmain = TestProgram\n", 
    "unittest.result": "\"\"\"Test result object\"\"\"\n\nimport os\nimport sys\nimport traceback\n\nfrom StringIO import StringIO\n\nfrom . import util\nfrom functools import wraps\n\n__unittest = True\n\ndef failfast(method):\n    @wraps(method)\n    def inner(self, *args, **kw):\n        if getattr(self, 'failfast', False):\n            self.stop()\n        return method(self, *args, **kw)\n    return inner\n\nSTDOUT_LINE = '\\nStdout:\\n%s'\nSTDERR_LINE = '\\nStderr:\\n%s'\n\n\nclass TestResult(object):\n    \"\"\"Holder for test result information.\n\n    Test results are automatically managed by the TestCase and TestSuite\n    classes, and do not need to be explicitly manipulated by writers of tests.\n\n    Each instance holds the total number of tests run, and collections of\n    failures and errors that occurred among those test runs. The collections\n    contain tuples of (testcase, exceptioninfo), where exceptioninfo is the\n    formatted traceback of the error that occurred.\n    \"\"\"\n    _previousTestClass = None\n    _testRunEntered = False\n    _moduleSetUpFailed = False\n    def __init__(self, stream=None, descriptions=None, verbosity=None):\n        self.failfast = False\n        self.failures = []\n        self.errors = []\n        self.testsRun = 0\n        self.skipped = []\n        self.expectedFailures = []\n        self.unexpectedSuccesses = []\n        self.shouldStop = False\n        self.buffer = False\n        self._stdout_buffer = None\n        self._stderr_buffer = None\n        self._original_stdout = sys.stdout\n        self._original_stderr = sys.stderr\n        self._mirrorOutput = False\n\n    def printErrors(self):\n        \"Called by TestRunner after test run\"\n\n    def startTest(self, test):\n        \"Called when the given test is about to be run\"\n        self.testsRun += 1\n        self._mirrorOutput = False\n        self._setupStdout()\n\n    def _setupStdout(self):\n        if self.buffer:\n            if self._stderr_buffer is None:\n                self._stderr_buffer = StringIO()\n                self._stdout_buffer = StringIO()\n            sys.stdout = self._stdout_buffer\n            sys.stderr = self._stderr_buffer\n\n    def startTestRun(self):\n        \"\"\"Called once before any tests are executed.\n\n        See startTest for a method called before each test.\n        \"\"\"\n\n    def stopTest(self, test):\n        \"\"\"Called when the given test has been run\"\"\"\n        self._restoreStdout()\n        self._mirrorOutput = False\n\n    def _restoreStdout(self):\n        if self.buffer:\n            if self._mirrorOutput:\n                output = sys.stdout.getvalue()\n                error = sys.stderr.getvalue()\n                if output:\n                    if not output.endswith('\\n'):\n                        output += '\\n'\n                    self._original_stdout.write(STDOUT_LINE % output)\n                if error:\n                    if not error.endswith('\\n'):\n                        error += '\\n'\n                    self._original_stderr.write(STDERR_LINE % error)\n\n            sys.stdout = self._original_stdout\n            sys.stderr = self._original_stderr\n            self._stdout_buffer.seek(0)\n            self._stdout_buffer.truncate()\n            self._stderr_buffer.seek(0)\n            self._stderr_buffer.truncate()\n\n    def stopTestRun(self):\n        \"\"\"Called once after all tests are executed.\n\n        See stopTest for a method called after each test.\n        \"\"\"\n\n    @failfast\n    def addError(self, test, err):\n        \"\"\"Called when an error has occurred. 'err' is a tuple of values as\n        returned by sys.exc_info().\n        \"\"\"\n        self.errors.append((test, self._exc_info_to_string(err, test)))\n        self._mirrorOutput = True\n\n    @failfast\n    def addFailure(self, test, err):\n        \"\"\"Called when an error has occurred. 'err' is a tuple of values as\n        returned by sys.exc_info().\"\"\"\n        self.failures.append((test, self._exc_info_to_string(err, test)))\n        self._mirrorOutput = True\n\n    def addSuccess(self, test):\n        \"Called when a test has completed successfully\"\n        pass\n\n    def addSkip(self, test, reason):\n        \"\"\"Called when a test is skipped.\"\"\"\n        self.skipped.append((test, reason))\n\n    def addExpectedFailure(self, test, err):\n        \"\"\"Called when an expected failure/error occured.\"\"\"\n        self.expectedFailures.append(\n            (test, self._exc_info_to_string(err, test)))\n\n    @failfast\n    def addUnexpectedSuccess(self, test):\n        \"\"\"Called when a test was expected to fail, but succeed.\"\"\"\n        self.unexpectedSuccesses.append(test)\n\n    def wasSuccessful(self):\n        \"Tells whether or not this result was a success\"\n        return len(self.failures) == len(self.errors) == 0\n\n    def stop(self):\n        \"Indicates that the tests should be aborted\"\n        self.shouldStop = True\n\n    def _exc_info_to_string(self, err, test):\n        \"\"\"Converts a sys.exc_info()-style tuple of values into a string.\"\"\"\n        exctype, value, tb = err\n        # Skip test runner traceback levels\n        while tb and self._is_relevant_tb_level(tb):\n            tb = tb.tb_next\n\n        if exctype is test.failureException:\n            # Skip assert*() traceback levels\n            length = self._count_relevant_tb_levels(tb)\n            msgLines = traceback.format_exception(exctype, value, tb, length)\n        else:\n            msgLines = traceback.format_exception(exctype, value, tb)\n\n        if self.buffer:\n            output = sys.stdout.getvalue()\n            error = sys.stderr.getvalue()\n            if output:\n                if not output.endswith('\\n'):\n                    output += '\\n'\n                msgLines.append(STDOUT_LINE % output)\n            if error:\n                if not error.endswith('\\n'):\n                    error += '\\n'\n                msgLines.append(STDERR_LINE % error)\n        return ''.join(msgLines)\n\n\n    def _is_relevant_tb_level(self, tb):\n        return '__unittest' in tb.tb_frame.f_globals\n\n    def _count_relevant_tb_levels(self, tb):\n        length = 0\n        while tb and not self._is_relevant_tb_level(tb):\n            length += 1\n            tb = tb.tb_next\n        return length\n\n    def __repr__(self):\n        return (\"<%s run=%i errors=%i failures=%i>\" %\n               (util.strclass(self.__class__), self.testsRun, len(self.errors),\n                len(self.failures)))\n", 
    "unittest.runner": "\"\"\"Running tests\"\"\"\n\nimport sys\nimport time\n\nfrom . import result\nfrom .signals import registerResult\n\n__unittest = True\n\n\nclass _WritelnDecorator(object):\n    \"\"\"Used to decorate file-like objects with a handy 'writeln' method\"\"\"\n    def __init__(self,stream):\n        self.stream = stream\n\n    def __getattr__(self, attr):\n        if attr in ('stream', '__getstate__'):\n            raise AttributeError(attr)\n        return getattr(self.stream,attr)\n\n    def writeln(self, arg=None):\n        if arg:\n            self.write(arg)\n        self.write('\\n') # text-mode streams translate to \\r\\n if needed\n\n\nclass TextTestResult(result.TestResult):\n    \"\"\"A test result class that can print formatted text results to a stream.\n\n    Used by TextTestRunner.\n    \"\"\"\n    separator1 = '=' * 70\n    separator2 = '-' * 70\n\n    def __init__(self, stream, descriptions, verbosity):\n        super(TextTestResult, self).__init__(stream, descriptions, verbosity)\n        self.stream = stream\n        self.showAll = verbosity > 1\n        self.dots = verbosity == 1\n        self.descriptions = descriptions\n\n    def getDescription(self, test):\n        doc_first_line = test.shortDescription()\n        if self.descriptions and doc_first_line:\n            return '\\n'.join((str(test), doc_first_line))\n        else:\n            return str(test)\n\n    def startTest(self, test):\n        super(TextTestResult, self).startTest(test)\n        if self.showAll:\n            self.stream.write(self.getDescription(test))\n            self.stream.write(\" ... \")\n            self.stream.flush()\n\n    def addSuccess(self, test):\n        super(TextTestResult, self).addSuccess(test)\n        if self.showAll:\n            self.stream.writeln(\"ok\")\n        elif self.dots:\n            self.stream.write('.')\n            self.stream.flush()\n\n    def addError(self, test, err):\n        super(TextTestResult, self).addError(test, err)\n        if self.showAll:\n            self.stream.writeln(\"ERROR\")\n        elif self.dots:\n            self.stream.write('E')\n            self.stream.flush()\n\n    def addFailure(self, test, err):\n        super(TextTestResult, self).addFailure(test, err)\n        if self.showAll:\n            self.stream.writeln(\"FAIL\")\n        elif self.dots:\n            self.stream.write('F')\n            self.stream.flush()\n\n    def addSkip(self, test, reason):\n        super(TextTestResult, self).addSkip(test, reason)\n        if self.showAll:\n            self.stream.writeln(\"skipped {0!r}\".format(reason))\n        elif self.dots:\n            self.stream.write(\"s\")\n            self.stream.flush()\n\n    def addExpectedFailure(self, test, err):\n        super(TextTestResult, self).addExpectedFailure(test, err)\n        if self.showAll:\n            self.stream.writeln(\"expected failure\")\n        elif self.dots:\n            self.stream.write(\"x\")\n            self.stream.flush()\n\n    def addUnexpectedSuccess(self, test):\n        super(TextTestResult, self).addUnexpectedSuccess(test)\n        if self.showAll:\n            self.stream.writeln(\"unexpected success\")\n        elif self.dots:\n            self.stream.write(\"u\")\n            self.stream.flush()\n\n    def printErrors(self):\n        if self.dots or self.showAll:\n            self.stream.writeln()\n        self.printErrorList('ERROR', self.errors)\n        self.printErrorList('FAIL', self.failures)\n\n    def printErrorList(self, flavour, errors):\n        for test, err in errors:\n            self.stream.writeln(self.separator1)\n            self.stream.writeln(\"%s: %s\" % (flavour,self.getDescription(test)))\n            self.stream.writeln(self.separator2)\n            self.stream.writeln(\"%s\" % err)\n\n\nclass TextTestRunner(object):\n    \"\"\"A test runner class that displays results in textual form.\n\n    It prints out the names of tests as they are run, errors as they\n    occur, and a summary of the results at the end of the test run.\n    \"\"\"\n    resultclass = TextTestResult\n\n    def __init__(self, stream=sys.stderr, descriptions=True, verbosity=1,\n                 failfast=False, buffer=False, resultclass=None):\n        self.stream = _WritelnDecorator(stream)\n        self.descriptions = descriptions\n        self.verbosity = verbosity\n        self.failfast = failfast\n        self.buffer = buffer\n        if resultclass is not None:\n            self.resultclass = resultclass\n\n    def _makeResult(self):\n        return self.resultclass(self.stream, self.descriptions, self.verbosity)\n\n    def run(self, test):\n        \"Run the given test case or test suite.\"\n        result = self._makeResult()\n        registerResult(result)\n        result.failfast = self.failfast\n        result.buffer = self.buffer\n        startTime = time.time()\n        startTestRun = getattr(result, 'startTestRun', None)\n        if startTestRun is not None:\n            startTestRun()\n        try:\n            test(result)\n        finally:\n            stopTestRun = getattr(result, 'stopTestRun', None)\n            if stopTestRun is not None:\n                stopTestRun()\n        stopTime = time.time()\n        timeTaken = stopTime - startTime\n        result.printErrors()\n        if hasattr(result, 'separator2'):\n            self.stream.writeln(result.separator2)\n        run = result.testsRun\n        self.stream.writeln(\"Ran %d test%s in %.3fs\" %\n                            (run, run != 1 and \"s\" or \"\", timeTaken))\n        self.stream.writeln()\n\n        expectedFails = unexpectedSuccesses = skipped = 0\n        try:\n            results = map(len, (result.expectedFailures,\n                                result.unexpectedSuccesses,\n                                result.skipped))\n        except AttributeError:\n            pass\n        else:\n            expectedFails, unexpectedSuccesses, skipped = results\n\n        infos = []\n        if not result.wasSuccessful():\n            self.stream.write(\"FAILED\")\n            failed, errored = map(len, (result.failures, result.errors))\n            if failed:\n                infos.append(\"failures=%d\" % failed)\n            if errored:\n                infos.append(\"errors=%d\" % errored)\n        else:\n            self.stream.write(\"OK\")\n        if skipped:\n            infos.append(\"skipped=%d\" % skipped)\n        if expectedFails:\n            infos.append(\"expected failures=%d\" % expectedFails)\n        if unexpectedSuccesses:\n            infos.append(\"unexpected successes=%d\" % unexpectedSuccesses)\n        if infos:\n            self.stream.writeln(\" (%s)\" % (\", \".join(infos),))\n        else:\n            self.stream.write(\"\\n\")\n        return result\n", 
    "unittest.signals": "import signal\nimport weakref\n\nfrom functools import wraps\n\n__unittest = True\n\n\nclass _InterruptHandler(object):\n    def __init__(self, default_handler):\n        self.called = False\n        self.original_handler = default_handler\n        if isinstance(default_handler, int):\n            if default_handler == signal.SIG_DFL:\n                # Pretend it's signal.default_int_handler instead.\n                default_handler = signal.default_int_handler\n            elif default_handler == signal.SIG_IGN:\n                # Not quite the same thing as SIG_IGN, but the closest we\n                # can make it: do nothing.\n                def default_handler(unused_signum, unused_frame):\n                    pass\n            else:\n                raise TypeError(\"expected SIGINT signal handler to be \"\n                                \"signal.SIG_IGN, signal.SIG_DFL, or a \"\n                                \"callable object\")\n        self.default_handler = default_handler\n\n    def __call__(self, signum, frame):\n        installed_handler = signal.getsignal(signal.SIGINT)\n        if installed_handler is not self:\n            # if we aren't the installed handler, then delegate immediately\n            # to the default handler\n            self.default_handler(signum, frame)\n\n        if self.called:\n            self.default_handler(signum, frame)\n        self.called = True\n        for result in _results.keys():\n            result.stop()\n\n_results = weakref.WeakKeyDictionary()\ndef registerResult(result):\n    _results[result] = 1\n\ndef removeResult(result):\n    return bool(_results.pop(result, None))\n\n_interrupt_handler = None\ndef installHandler():\n    global _interrupt_handler\n    if _interrupt_handler is None:\n        default_handler = signal.getsignal(signal.SIGINT)\n        _interrupt_handler = _InterruptHandler(default_handler)\n        signal.signal(signal.SIGINT, _interrupt_handler)\n\n\ndef removeHandler(method=None):\n    if method is not None:\n        @wraps(method)\n        def inner(*args, **kwargs):\n            initial = signal.getsignal(signal.SIGINT)\n            removeHandler()\n            try:\n                return method(*args, **kwargs)\n            finally:\n                signal.signal(signal.SIGINT, initial)\n        return inner\n\n    global _interrupt_handler\n    if _interrupt_handler is not None:\n        signal.signal(signal.SIGINT, _interrupt_handler.original_handler)\n", 
    "unittest.suite": "\"\"\"TestSuite\"\"\"\n\nimport sys\n\nfrom . import case\nfrom . import util\n\n__unittest = True\n\n\ndef _call_if_exists(parent, attr):\n    func = getattr(parent, attr, lambda: None)\n    func()\n\n\nclass BaseTestSuite(object):\n    \"\"\"A simple test suite that doesn't provide class or module shared fixtures.\n    \"\"\"\n    def __init__(self, tests=()):\n        self._tests = []\n        self.addTests(tests)\n\n    def __repr__(self):\n        return \"<%s tests=%s>\" % (util.strclass(self.__class__), list(self))\n\n    def __eq__(self, other):\n        if not isinstance(other, self.__class__):\n            return NotImplemented\n        return list(self) == list(other)\n\n    def __ne__(self, other):\n        return not self == other\n\n    # Can't guarantee hash invariant, so flag as unhashable\n    __hash__ = None\n\n    def __iter__(self):\n        return iter(self._tests)\n\n    def countTestCases(self):\n        cases = 0\n        for test in self:\n            cases += test.countTestCases()\n        return cases\n\n    def addTest(self, test):\n        # sanity checks\n        if not hasattr(test, '__call__'):\n            raise TypeError(\"{} is not callable\".format(repr(test)))\n        if isinstance(test, type) and issubclass(test,\n                                                 (case.TestCase, TestSuite)):\n            raise TypeError(\"TestCases and TestSuites must be instantiated \"\n                            \"before passing them to addTest()\")\n        self._tests.append(test)\n\n    def addTests(self, tests):\n        if isinstance(tests, basestring):\n            raise TypeError(\"tests must be an iterable of tests, not a string\")\n        for test in tests:\n            self.addTest(test)\n\n    def run(self, result):\n        for test in self:\n            if result.shouldStop:\n                break\n            test(result)\n        return result\n\n    def __call__(self, *args, **kwds):\n        return self.run(*args, **kwds)\n\n    def debug(self):\n        \"\"\"Run the tests without collecting errors in a TestResult\"\"\"\n        for test in self:\n            test.debug()\n\n\nclass TestSuite(BaseTestSuite):\n    \"\"\"A test suite is a composite test consisting of a number of TestCases.\n\n    For use, create an instance of TestSuite, then add test case instances.\n    When all tests have been added, the suite can be passed to a test\n    runner, such as TextTestRunner. It will run the individual test cases\n    in the order in which they were added, aggregating the results. When\n    subclassing, do not forget to call the base class constructor.\n    \"\"\"\n\n    def run(self, result, debug=False):\n        topLevel = False\n        if getattr(result, '_testRunEntered', False) is False:\n            result._testRunEntered = topLevel = True\n\n        for test in self:\n            if result.shouldStop:\n                break\n\n            if _isnotsuite(test):\n                self._tearDownPreviousClass(test, result)\n                self._handleModuleFixture(test, result)\n                self._handleClassSetUp(test, result)\n                result._previousTestClass = test.__class__\n\n                if (getattr(test.__class__, '_classSetupFailed', False) or\n                    getattr(result, '_moduleSetUpFailed', False)):\n                    continue\n\n            if not debug:\n                test(result)\n            else:\n                test.debug()\n\n        if topLevel:\n            self._tearDownPreviousClass(None, result)\n            self._handleModuleTearDown(result)\n            result._testRunEntered = False\n        return result\n\n    def debug(self):\n        \"\"\"Run the tests without collecting errors in a TestResult\"\"\"\n        debug = _DebugResult()\n        self.run(debug, True)\n\n    ################################\n\n    def _handleClassSetUp(self, test, result):\n        previousClass = getattr(result, '_previousTestClass', None)\n        currentClass = test.__class__\n        if currentClass == previousClass:\n            return\n        if result._moduleSetUpFailed:\n            return\n        if getattr(currentClass, \"__unittest_skip__\", False):\n            return\n\n        try:\n            currentClass._classSetupFailed = False\n        except TypeError:\n            # test may actually be a function\n            # so its class will be a builtin-type\n            pass\n\n        setUpClass = getattr(currentClass, 'setUpClass', None)\n        if setUpClass is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                setUpClass()\n            except Exception as e:\n                if isinstance(result, _DebugResult):\n                    raise\n                currentClass._classSetupFailed = True\n                className = util.strclass(currentClass)\n                errorName = 'setUpClass (%s)' % className\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n    def _get_previous_module(self, result):\n        previousModule = None\n        previousClass = getattr(result, '_previousTestClass', None)\n        if previousClass is not None:\n            previousModule = previousClass.__module__\n        return previousModule\n\n\n    def _handleModuleFixture(self, test, result):\n        previousModule = self._get_previous_module(result)\n        currentModule = test.__class__.__module__\n        if currentModule == previousModule:\n            return\n\n        self._handleModuleTearDown(result)\n\n        result._moduleSetUpFailed = False\n        try:\n            module = sys.modules[currentModule]\n        except KeyError:\n            return\n        setUpModule = getattr(module, 'setUpModule', None)\n        if setUpModule is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                setUpModule()\n            except Exception, e:\n                if isinstance(result, _DebugResult):\n                    raise\n                result._moduleSetUpFailed = True\n                errorName = 'setUpModule (%s)' % currentModule\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n    def _addClassOrModuleLevelException(self, result, exception, errorName):\n        error = _ErrorHolder(errorName)\n        addSkip = getattr(result, 'addSkip', None)\n        if addSkip is not None and isinstance(exception, case.SkipTest):\n            addSkip(error, str(exception))\n        else:\n            result.addError(error, sys.exc_info())\n\n    def _handleModuleTearDown(self, result):\n        previousModule = self._get_previous_module(result)\n        if previousModule is None:\n            return\n        if result._moduleSetUpFailed:\n            return\n\n        try:\n            module = sys.modules[previousModule]\n        except KeyError:\n            return\n\n        tearDownModule = getattr(module, 'tearDownModule', None)\n        if tearDownModule is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                tearDownModule()\n            except Exception as e:\n                if isinstance(result, _DebugResult):\n                    raise\n                errorName = 'tearDownModule (%s)' % previousModule\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n    def _tearDownPreviousClass(self, test, result):\n        previousClass = getattr(result, '_previousTestClass', None)\n        currentClass = test.__class__\n        if currentClass == previousClass:\n            return\n        if getattr(previousClass, '_classSetupFailed', False):\n            return\n        if getattr(result, '_moduleSetUpFailed', False):\n            return\n        if getattr(previousClass, \"__unittest_skip__\", False):\n            return\n\n        tearDownClass = getattr(previousClass, 'tearDownClass', None)\n        if tearDownClass is not None:\n            _call_if_exists(result, '_setupStdout')\n            try:\n                tearDownClass()\n            except Exception, e:\n                if isinstance(result, _DebugResult):\n                    raise\n                className = util.strclass(previousClass)\n                errorName = 'tearDownClass (%s)' % className\n                self._addClassOrModuleLevelException(result, e, errorName)\n            finally:\n                _call_if_exists(result, '_restoreStdout')\n\n\nclass _ErrorHolder(object):\n    \"\"\"\n    Placeholder for a TestCase inside a result. As far as a TestResult\n    is concerned, this looks exactly like a unit test. Used to insert\n    arbitrary errors into a test suite run.\n    \"\"\"\n    # Inspired by the ErrorHolder from Twisted:\n    # http://twistedmatrix.com/trac/browser/trunk/twisted/trial/runner.py\n\n    # attribute used by TestResult._exc_info_to_string\n    failureException = None\n\n    def __init__(self, description):\n        self.description = description\n\n    def id(self):\n        return self.description\n\n    def shortDescription(self):\n        return None\n\n    def __repr__(self):\n        return \"<ErrorHolder description=%r>\" % (self.description,)\n\n    def __str__(self):\n        return self.id()\n\n    def run(self, result):\n        # could call result.addError(...) - but this test-like object\n        # shouldn't be run anyway\n        pass\n\n    def __call__(self, result):\n        return self.run(result)\n\n    def countTestCases(self):\n        return 0\n\ndef _isnotsuite(test):\n    \"A crude way to tell apart testcases and suites with duck-typing\"\n    try:\n        iter(test)\n    except TypeError:\n        return True\n    return False\n\n\nclass _DebugResult(object):\n    \"Used by the TestSuite to hold previous class when running in debug.\"\n    _previousTestClass = None\n    _moduleSetUpFailed = False\n    shouldStop = False\n", 
    "unittest.util": "\"\"\"Various utility functions.\"\"\"\nfrom collections import namedtuple, OrderedDict\n\n\n__unittest = True\n\n_MAX_LENGTH = 80\ndef safe_repr(obj, short=False):\n    try:\n        result = repr(obj)\n    except Exception:\n        result = object.__repr__(obj)\n    if not short or len(result) < _MAX_LENGTH:\n        return result\n    return result[:_MAX_LENGTH] + ' [truncated]...'\n\n\ndef strclass(cls):\n    return \"%s.%s\" % (cls.__module__, cls.__name__)\n\ndef sorted_list_difference(expected, actual):\n    \"\"\"Finds elements in only one or the other of two, sorted input lists.\n\n    Returns a two-element tuple of lists.    The first list contains those\n    elements in the \"expected\" list but not in the \"actual\" list, and the\n    second contains those elements in the \"actual\" list but not in the\n    \"expected\" list.    Duplicate elements in either input list are ignored.\n    \"\"\"\n    i = j = 0\n    missing = []\n    unexpected = []\n    while True:\n        try:\n            e = expected[i]\n            a = actual[j]\n            if e < a:\n                missing.append(e)\n                i += 1\n                while expected[i] == e:\n                    i += 1\n            elif e > a:\n                unexpected.append(a)\n                j += 1\n                while actual[j] == a:\n                    j += 1\n            else:\n                i += 1\n                try:\n                    while expected[i] == e:\n                        i += 1\n                finally:\n                    j += 1\n                    while actual[j] == a:\n                        j += 1\n        except IndexError:\n            missing.extend(expected[i:])\n            unexpected.extend(actual[j:])\n            break\n    return missing, unexpected\n\n\ndef unorderable_list_difference(expected, actual, ignore_duplicate=False):\n    \"\"\"Same behavior as sorted_list_difference but\n    for lists of unorderable items (like dicts).\n\n    As it does a linear search per item (remove) it\n    has O(n*n) performance.\n    \"\"\"\n    missing = []\n    unexpected = []\n    while expected:\n        item = expected.pop()\n        try:\n            actual.remove(item)\n        except ValueError:\n            missing.append(item)\n        if ignore_duplicate:\n            for lst in expected, actual:\n                try:\n                    while True:\n                        lst.remove(item)\n                except ValueError:\n                    pass\n    if ignore_duplicate:\n        while actual:\n            item = actual.pop()\n            unexpected.append(item)\n            try:\n                while True:\n                    actual.remove(item)\n            except ValueError:\n                pass\n        return missing, unexpected\n\n    # anything left in actual is unexpected\n    return missing, actual\n\n_Mismatch = namedtuple('Mismatch', 'actual expected value')\n\ndef _count_diff_all_purpose(actual, expected):\n    'Returns list of (cnt_act, cnt_exp, elem) triples where the counts differ'\n    # elements need not be hashable\n    s, t = list(actual), list(expected)\n    m, n = len(s), len(t)\n    NULL = object()\n    result = []\n    for i, elem in enumerate(s):\n        if elem is NULL:\n            continue\n        cnt_s = cnt_t = 0\n        for j in range(i, m):\n            if s[j] == elem:\n                cnt_s += 1\n                s[j] = NULL\n        for j, other_elem in enumerate(t):\n            if other_elem == elem:\n                cnt_t += 1\n                t[j] = NULL\n        if cnt_s != cnt_t:\n            diff = _Mismatch(cnt_s, cnt_t, elem)\n            result.append(diff)\n\n    for i, elem in enumerate(t):\n        if elem is NULL:\n            continue\n        cnt_t = 0\n        for j in range(i, n):\n            if t[j] == elem:\n                cnt_t += 1\n                t[j] = NULL\n        diff = _Mismatch(0, cnt_t, elem)\n        result.append(diff)\n    return result\n\ndef _ordered_count(iterable):\n    'Return dict of element counts, in the order they were first seen'\n    c = OrderedDict()\n    for elem in iterable:\n        c[elem] = c.get(elem, 0) + 1\n    return c\n\ndef _count_diff_hashable(actual, expected):\n    'Returns list of (cnt_act, cnt_exp, elem) triples where the counts differ'\n    # elements must be hashable\n    s, t = _ordered_count(actual), _ordered_count(expected)\n    result = []\n    for elem, cnt_s in s.items():\n        cnt_t = t.get(elem, 0)\n        if cnt_s != cnt_t:\n            diff = _Mismatch(cnt_s, cnt_t, elem)\n            result.append(diff)\n    for elem, cnt_t in t.items():\n        if elem not in s:\n            diff = _Mismatch(0, cnt_t, elem)\n            result.append(diff)\n    return result\n", 
    "urllib": "\"\"\"Open an arbitrary URL.\n\nSee the following document for more info on URLs:\n\"Names and Addresses, URIs, URLs, URNs, URCs\", at\nhttp://www.w3.org/pub/WWW/Addressing/Overview.html\n\nSee also the HTTP spec (from which the error codes are derived):\n\"HTTP - Hypertext Transfer Protocol\", at\nhttp://www.w3.org/pub/WWW/Protocols/\n\nRelated standards and specs:\n- RFC1808: the \"relative URL\" spec. (authoritative status)\n- RFC1738 - the \"URL standard\". (authoritative status)\n- RFC1630 - the \"URI spec\". (informational status)\n\nThe object returned by URLopener().open(file) will differ per\nprotocol.  All you know is that is has methods read(), readline(),\nreadlines(), fileno(), close() and info().  The read*(), fileno()\nand close() methods work like those of open files.\nThe info() method returns a mimetools.Message object which can be\nused to query various info about the object, if available.\n(mimetools.Message objects are queried with the getheader() method.)\n\"\"\"\n\nimport string\nimport socket\nimport os\nimport time\nimport sys\nimport base64\nimport re\n\nfrom urlparse import urljoin as basejoin\n\n__all__ = [\"urlopen\", \"URLopener\", \"FancyURLopener\", \"urlretrieve\",\n           \"urlcleanup\", \"quote\", \"quote_plus\", \"unquote\", \"unquote_plus\",\n           \"urlencode\", \"url2pathname\", \"pathname2url\", \"splittag\",\n           \"localhost\", \"thishost\", \"ftperrors\", \"basejoin\", \"unwrap\",\n           \"splittype\", \"splithost\", \"splituser\", \"splitpasswd\", \"splitport\",\n           \"splitnport\", \"splitquery\", \"splitattr\", \"splitvalue\",\n           \"getproxies\"]\n\n__version__ = '1.17'    # XXX This version is not always updated :-(\n\nMAXFTPCACHE = 10        # Trim the ftp cache beyond this size\n\n# Helper for non-unix systems\nif os.name == 'nt':\n    from nturl2path import url2pathname, pathname2url\nelif os.name == 'riscos':\n    from rourl2path import url2pathname, pathname2url\nelse:\n    def url2pathname(pathname):\n        \"\"\"OS-specific conversion from a relative URL of the 'file' scheme\n        to a file system path; not recommended for general use.\"\"\"\n        return unquote(pathname)\n\n    def pathname2url(pathname):\n        \"\"\"OS-specific conversion from a file system path to a relative URL\n        of the 'file' scheme; not recommended for general use.\"\"\"\n        return quote(pathname)\n\n# This really consists of two pieces:\n# (1) a class which handles opening of all sorts of URLs\n#     (plus assorted utilities etc.)\n# (2) a set of functions for parsing URLs\n# XXX Should these be separated out into different modules?\n\n\n# Shortcut for basic usage\n_urlopener = None\ndef urlopen(url, data=None, proxies=None, context=None):\n    \"\"\"Create a file-like object for the specified URL to read from.\"\"\"\n    from warnings import warnpy3k\n    warnpy3k(\"urllib.urlopen() has been removed in Python 3.0 in \"\n             \"favor of urllib2.urlopen()\", stacklevel=2)\n\n    global _urlopener\n    if proxies is not None or context is not None:\n        opener = FancyURLopener(proxies=proxies, context=context)\n    elif not _urlopener:\n        opener = FancyURLopener()\n        _urlopener = opener\n    else:\n        opener = _urlopener\n    if data is None:\n        return opener.open(url)\n    else:\n        return opener.open(url, data)\ndef urlretrieve(url, filename=None, reporthook=None, data=None, context=None):\n    global _urlopener\n    if context is not None:\n        opener = FancyURLopener(context=context)\n    elif not _urlopener:\n        _urlopener = opener = FancyURLopener()\n    else:\n        opener = _urlopener\n    return opener.retrieve(url, filename, reporthook, data)\ndef urlcleanup():\n    if _urlopener:\n        _urlopener.cleanup()\n    _safe_quoters.clear()\n    ftpcache.clear()\n\n# check for SSL\ntry:\n    import ssl\nexcept:\n    _have_ssl = False\nelse:\n    _have_ssl = True\n\n# exception raised when downloaded size does not match content-length\nclass ContentTooShortError(IOError):\n    def __init__(self, message, content):\n        IOError.__init__(self, message)\n        self.content = content\n\nftpcache = {}\nclass URLopener:\n    \"\"\"Class to open URLs.\n    This is a class rather than just a subroutine because we may need\n    more than one set of global protocol-specific options.\n    Note -- this is a base class for those who don't want the\n    automatic handling of errors type 302 (relocated) and 401\n    (authorization needed).\"\"\"\n\n    __tempfiles = None\n\n    version = \"Python-urllib/%s\" % __version__\n\n    # Constructor\n    def __init__(self, proxies=None, context=None, **x509):\n        if proxies is None:\n            proxies = getproxies()\n        assert hasattr(proxies, 'has_key'), \"proxies must be a mapping\"\n        self.proxies = proxies\n        self.key_file = x509.get('key_file')\n        self.cert_file = x509.get('cert_file')\n        self.context = context\n        self.addheaders = [('User-Agent', self.version)]\n        self.__tempfiles = []\n        self.__unlink = os.unlink # See cleanup()\n        self.tempcache = None\n        # Undocumented feature: if you assign {} to tempcache,\n        # it is used to cache files retrieved with\n        # self.retrieve().  This is not enabled by default\n        # since it does not work for changing documents (and I\n        # haven't got the logic to check expiration headers\n        # yet).\n        self.ftpcache = ftpcache\n        # Undocumented feature: you can use a different\n        # ftp cache by assigning to the .ftpcache member;\n        # in case you want logically independent URL openers\n        # XXX This is not threadsafe.  Bah.\n\n    def __del__(self):\n        self.close()\n\n    def close(self):\n        self.cleanup()\n\n    def cleanup(self):\n        # This code sometimes runs when the rest of this module\n        # has already been deleted, so it can't use any globals\n        # or import anything.\n        if self.__tempfiles:\n            for file in self.__tempfiles:\n                try:\n                    self.__unlink(file)\n                except OSError:\n                    pass\n            del self.__tempfiles[:]\n        if self.tempcache:\n            self.tempcache.clear()\n\n    def addheader(self, *args):\n        \"\"\"Add a header to be used by the HTTP interface only\n        e.g. u.addheader('Accept', 'sound/basic')\"\"\"\n        self.addheaders.append(args)\n\n    # External interface\n    def open(self, fullurl, data=None):\n        \"\"\"Use URLopener().open(file) instead of open(file, 'r').\"\"\"\n        fullurl = unwrap(toBytes(fullurl))\n        # percent encode url, fixing lame server errors for e.g, like space\n        # within url paths.\n        fullurl = quote(fullurl, safe=\"%/:=&?~#+!$,;'@()*[]|\")\n        if self.tempcache and fullurl in self.tempcache:\n            filename, headers = self.tempcache[fullurl]\n            fp = open(filename, 'rb')\n            return addinfourl(fp, headers, fullurl)\n        urltype, url = splittype(fullurl)\n        if not urltype:\n            urltype = 'file'\n        if urltype in self.proxies:\n            proxy = self.proxies[urltype]\n            urltype, proxyhost = splittype(proxy)\n            host, selector = splithost(proxyhost)\n            url = (host, fullurl) # Signal special case to open_*()\n        else:\n            proxy = None\n        name = 'open_' + urltype\n        self.type = urltype\n        name = name.replace('-', '_')\n        if not hasattr(self, name):\n            if proxy:\n                return self.open_unknown_proxy(proxy, fullurl, data)\n            else:\n                return self.open_unknown(fullurl, data)\n        try:\n            if data is None:\n                return getattr(self, name)(url)\n            else:\n                return getattr(self, name)(url, data)\n        except socket.error, msg:\n            raise IOError, ('socket error', msg), sys.exc_info()[2]\n\n    def open_unknown(self, fullurl, data=None):\n        \"\"\"Overridable interface to open unknown URL type.\"\"\"\n        type, url = splittype(fullurl)\n        raise IOError, ('url error', 'unknown url type', type)\n\n    def open_unknown_proxy(self, proxy, fullurl, data=None):\n        \"\"\"Overridable interface to open unknown URL type.\"\"\"\n        type, url = splittype(fullurl)\n        raise IOError, ('url error', 'invalid proxy for %s' % type, proxy)\n\n    # External interface\n    def retrieve(self, url, filename=None, reporthook=None, data=None):\n        \"\"\"retrieve(url) returns (filename, headers) for a local object\n        or (tempfilename, headers) for a remote object.\"\"\"\n        url = unwrap(toBytes(url))\n        if self.tempcache and url in self.tempcache:\n            return self.tempcache[url]\n        type, url1 = splittype(url)\n        if filename is None and (not type or type == 'file'):\n            try:\n                fp = self.open_local_file(url1)\n                hdrs = fp.info()\n                fp.close()\n                return url2pathname(splithost(url1)[1]), hdrs\n            except IOError:\n                pass\n        fp = self.open(url, data)\n        try:\n            headers = fp.info()\n            if filename:\n                tfp = open(filename, 'wb')\n            else:\n                import tempfile\n                garbage, path = splittype(url)\n                garbage, path = splithost(path or \"\")\n                path, garbage = splitquery(path or \"\")\n                path, garbage = splitattr(path or \"\")\n                suffix = os.path.splitext(path)[1]\n                (fd, filename) = tempfile.mkstemp(suffix)\n                self.__tempfiles.append(filename)\n                tfp = os.fdopen(fd, 'wb')\n            try:\n                result = filename, headers\n                if self.tempcache is not None:\n                    self.tempcache[url] = result\n                bs = 1024*8\n                size = -1\n                read = 0\n                blocknum = 0\n                if \"content-length\" in headers:\n                    size = int(headers[\"Content-Length\"])\n                if reporthook:\n                    reporthook(blocknum, bs, size)\n                while 1:\n                    block = fp.read(bs)\n                    if block == \"\":\n                        break\n                    read += len(block)\n                    tfp.write(block)\n                    blocknum += 1\n                    if reporthook:\n                        reporthook(blocknum, bs, size)\n            finally:\n                tfp.close()\n        finally:\n            fp.close()\n\n        # raise exception if actual size does not match content-length header\n        if size >= 0 and read < size:\n            raise ContentTooShortError(\"retrieval incomplete: got only %i out \"\n                                       \"of %i bytes\" % (read, size), result)\n\n        return result\n\n    # Each method named open_<type> knows how to open that type of URL\n\n    def open_http(self, url, data=None):\n        \"\"\"Use HTTP protocol.\"\"\"\n        import httplib\n        user_passwd = None\n        proxy_passwd= None\n        if isinstance(url, str):\n            host, selector = splithost(url)\n            if host:\n                user_passwd, host = splituser(host)\n                host = unquote(host)\n            realhost = host\n        else:\n            host, selector = url\n            # check whether the proxy contains authorization information\n            proxy_passwd, host = splituser(host)\n            # now we proceed with the url we want to obtain\n            urltype, rest = splittype(selector)\n            url = rest\n            user_passwd = None\n            if urltype.lower() != 'http':\n                realhost = None\n            else:\n                realhost, rest = splithost(rest)\n                if realhost:\n                    user_passwd, realhost = splituser(realhost)\n                if user_passwd:\n                    selector = \"%s://%s%s\" % (urltype, realhost, rest)\n                if proxy_bypass(realhost):\n                    host = realhost\n\n            #print \"proxy via http:\", host, selector\n        if not host: raise IOError, ('http error', 'no host given')\n\n        if proxy_passwd:\n            proxy_passwd = unquote(proxy_passwd)\n            proxy_auth = base64.b64encode(proxy_passwd).strip()\n        else:\n            proxy_auth = None\n\n        if user_passwd:\n            user_passwd = unquote(user_passwd)\n            auth = base64.b64encode(user_passwd).strip()\n        else:\n            auth = None\n        h = httplib.HTTP(host)\n        if data is not None:\n            h.putrequest('POST', selector)\n            h.putheader('Content-Type', 'application/x-www-form-urlencoded')\n            h.putheader('Content-Length', '%d' % len(data))\n        else:\n            h.putrequest('GET', selector)\n        if proxy_auth: h.putheader('Proxy-Authorization', 'Basic %s' % proxy_auth)\n        if auth: h.putheader('Authorization', 'Basic %s' % auth)\n        if realhost: h.putheader('Host', realhost)\n        for args in self.addheaders: h.putheader(*args)\n        h.endheaders(data)\n        errcode, errmsg, headers = h.getreply()\n        fp = h.getfile()\n        if errcode == -1:\n            if fp: fp.close()\n            # something went wrong with the HTTP status line\n            raise IOError, ('http protocol error', 0,\n                            'got a bad status line', None)\n        # According to RFC 2616, \"2xx\" code indicates that the client's\n        # request was successfully received, understood, and accepted.\n        if (200 <= errcode < 300):\n            return addinfourl(fp, headers, \"http:\" + url, errcode)\n        else:\n            if data is None:\n                return self.http_error(url, fp, errcode, errmsg, headers)\n            else:\n                return self.http_error(url, fp, errcode, errmsg, headers, data)\n\n    def http_error(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Handle http errors.\n        Derived class can override this, or provide specific handlers\n        named http_error_DDD where DDD is the 3-digit error code.\"\"\"\n        # First check if there's a specific handler for this error\n        name = 'http_error_%d' % errcode\n        if hasattr(self, name):\n            method = getattr(self, name)\n            if data is None:\n                result = method(url, fp, errcode, errmsg, headers)\n            else:\n                result = method(url, fp, errcode, errmsg, headers, data)\n            if result: return result\n        return self.http_error_default(url, fp, errcode, errmsg, headers)\n\n    def http_error_default(self, url, fp, errcode, errmsg, headers):\n        \"\"\"Default error handler: close the connection and raise IOError.\"\"\"\n        fp.close()\n        raise IOError, ('http error', errcode, errmsg, headers)\n\n    if _have_ssl:\n        def open_https(self, url, data=None):\n            \"\"\"Use HTTPS protocol.\"\"\"\n\n            import httplib\n            user_passwd = None\n            proxy_passwd = None\n            if isinstance(url, str):\n                host, selector = splithost(url)\n                if host:\n                    user_passwd, host = splituser(host)\n                    host = unquote(host)\n                realhost = host\n            else:\n                host, selector = url\n                # here, we determine, whether the proxy contains authorization information\n                proxy_passwd, host = splituser(host)\n                urltype, rest = splittype(selector)\n                url = rest\n                user_passwd = None\n                if urltype.lower() != 'https':\n                    realhost = None\n                else:\n                    realhost, rest = splithost(rest)\n                    if realhost:\n                        user_passwd, realhost = splituser(realhost)\n                    if user_passwd:\n                        selector = \"%s://%s%s\" % (urltype, realhost, rest)\n                #print \"proxy via https:\", host, selector\n            if not host: raise IOError, ('https error', 'no host given')\n            if proxy_passwd:\n                proxy_passwd = unquote(proxy_passwd)\n                proxy_auth = base64.b64encode(proxy_passwd).strip()\n            else:\n                proxy_auth = None\n            if user_passwd:\n                user_passwd = unquote(user_passwd)\n                auth = base64.b64encode(user_passwd).strip()\n            else:\n                auth = None\n            h = httplib.HTTPS(host, 0,\n                              key_file=self.key_file,\n                              cert_file=self.cert_file,\n                              context=self.context)\n            if data is not None:\n                h.putrequest('POST', selector)\n                h.putheader('Content-Type',\n                            'application/x-www-form-urlencoded')\n                h.putheader('Content-Length', '%d' % len(data))\n            else:\n                h.putrequest('GET', selector)\n            if proxy_auth: h.putheader('Proxy-Authorization', 'Basic %s' % proxy_auth)\n            if auth: h.putheader('Authorization', 'Basic %s' % auth)\n            if realhost: h.putheader('Host', realhost)\n            for args in self.addheaders: h.putheader(*args)\n            h.endheaders(data)\n            errcode, errmsg, headers = h.getreply()\n            fp = h.getfile()\n            if errcode == -1:\n                if fp: fp.close()\n                # something went wrong with the HTTP status line\n                raise IOError, ('http protocol error', 0,\n                                'got a bad status line', None)\n            # According to RFC 2616, \"2xx\" code indicates that the client's\n            # request was successfully received, understood, and accepted.\n            if (200 <= errcode < 300):\n                return addinfourl(fp, headers, \"https:\" + url, errcode)\n            else:\n                if data is None:\n                    return self.http_error(url, fp, errcode, errmsg, headers)\n                else:\n                    return self.http_error(url, fp, errcode, errmsg, headers,\n                                           data)\n\n    def open_file(self, url):\n        \"\"\"Use local file or FTP depending on form of URL.\"\"\"\n        if not isinstance(url, str):\n            raise IOError, ('file error', 'proxy support for file protocol currently not implemented')\n        if url[:2] == '//' and url[2:3] != '/' and url[2:12].lower() != 'localhost/':\n            return self.open_ftp(url)\n        else:\n            return self.open_local_file(url)\n\n    def open_local_file(self, url):\n        \"\"\"Use local file.\"\"\"\n        import mimetypes, mimetools, email.utils\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        host, file = splithost(url)\n        localname = url2pathname(file)\n        try:\n            stats = os.stat(localname)\n        except OSError, e:\n            raise IOError(e.errno, e.strerror, e.filename)\n        size = stats.st_size\n        modified = email.utils.formatdate(stats.st_mtime, usegmt=True)\n        mtype = mimetypes.guess_type(url)[0]\n        headers = mimetools.Message(StringIO(\n            'Content-Type: %s\\nContent-Length: %d\\nLast-modified: %s\\n' %\n            (mtype or 'text/plain', size, modified)))\n        if not host:\n            urlfile = file\n            if file[:1] == '/':\n                urlfile = 'file://' + file\n            elif file[:2] == './':\n                raise ValueError(\"local file url may start with / or file:. Unknown url of type: %s\" % url)\n            return addinfourl(open(localname, 'rb'),\n                              headers, urlfile)\n        host, port = splitport(host)\n        if not port \\\n           and socket.gethostbyname(host) in (localhost(), thishost()):\n            urlfile = file\n            if file[:1] == '/':\n                urlfile = 'file://' + file\n            return addinfourl(open(localname, 'rb'),\n                              headers, urlfile)\n        raise IOError, ('local file error', 'not on local host')\n\n    def open_ftp(self, url):\n        \"\"\"Use FTP protocol.\"\"\"\n        if not isinstance(url, str):\n            raise IOError, ('ftp error', 'proxy support for ftp protocol currently not implemented')\n        import mimetypes, mimetools\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        host, path = splithost(url)\n        if not host: raise IOError, ('ftp error', 'no host given')\n        host, port = splitport(host)\n        user, host = splituser(host)\n        if user: user, passwd = splitpasswd(user)\n        else: passwd = None\n        host = unquote(host)\n        user = user or ''\n        passwd = passwd or ''\n        host = socket.gethostbyname(host)\n        if not port:\n            import ftplib\n            port = ftplib.FTP_PORT\n        else:\n            port = int(port)\n        path, attrs = splitattr(path)\n        path = unquote(path)\n        dirs = path.split('/')\n        dirs, file = dirs[:-1], dirs[-1]\n        if dirs and not dirs[0]: dirs = dirs[1:]\n        if dirs and not dirs[0]: dirs[0] = '/'\n        key = user, host, port, '/'.join(dirs)\n        # XXX thread unsafe!\n        if len(self.ftpcache) > MAXFTPCACHE:\n            # Prune the cache, rather arbitrarily\n            for k in self.ftpcache.keys():\n                if k != key:\n                    v = self.ftpcache[k]\n                    del self.ftpcache[k]\n                    v.close()\n        try:\n            if not key in self.ftpcache:\n                self.ftpcache[key] = \\\n                    ftpwrapper(user, passwd, host, port, dirs)\n            if not file: type = 'D'\n            else: type = 'I'\n            for attr in attrs:\n                attr, value = splitvalue(attr)\n                if attr.lower() == 'type' and \\\n                   value in ('a', 'A', 'i', 'I', 'd', 'D'):\n                    type = value.upper()\n            (fp, retrlen) = self.ftpcache[key].retrfile(file, type)\n            mtype = mimetypes.guess_type(\"ftp:\" + url)[0]\n            headers = \"\"\n            if mtype:\n                headers += \"Content-Type: %s\\n\" % mtype\n            if retrlen is not None and retrlen >= 0:\n                headers += \"Content-Length: %d\\n\" % retrlen\n            headers = mimetools.Message(StringIO(headers))\n            return addinfourl(fp, headers, \"ftp:\" + url)\n        except ftperrors(), msg:\n            raise IOError, ('ftp error', msg), sys.exc_info()[2]\n\n    def open_data(self, url, data=None):\n        \"\"\"Use \"data\" URL.\"\"\"\n        if not isinstance(url, str):\n            raise IOError, ('data error', 'proxy support for data protocol currently not implemented')\n        # ignore POSTed data\n        #\n        # syntax of data URLs:\n        # dataurl   := \"data:\" [ mediatype ] [ \";base64\" ] \",\" data\n        # mediatype := [ type \"/\" subtype ] *( \";\" parameter )\n        # data      := *urlchar\n        # parameter := attribute \"=\" value\n        import mimetools\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        try:\n            [type, data] = url.split(',', 1)\n        except ValueError:\n            raise IOError, ('data error', 'bad data URL')\n        if not type:\n            type = 'text/plain;charset=US-ASCII'\n        semi = type.rfind(';')\n        if semi >= 0 and '=' not in type[semi:]:\n            encoding = type[semi+1:]\n            type = type[:semi]\n        else:\n            encoding = ''\n        msg = []\n        msg.append('Date: %s'%time.strftime('%a, %d %b %Y %H:%M:%S GMT',\n                                            time.gmtime(time.time())))\n        msg.append('Content-type: %s' % type)\n        if encoding == 'base64':\n            data = base64.decodestring(data)\n        else:\n            data = unquote(data)\n        msg.append('Content-Length: %d' % len(data))\n        msg.append('')\n        msg.append(data)\n        msg = '\\n'.join(msg)\n        f = StringIO(msg)\n        headers = mimetools.Message(f, 0)\n        #f.fileno = None     # needed for addinfourl\n        return addinfourl(f, headers, url)\n\n\nclass FancyURLopener(URLopener):\n    \"\"\"Derived class with handlers for errors we can handle (perhaps).\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        URLopener.__init__(self, *args, **kwargs)\n        self.auth_cache = {}\n        self.tries = 0\n        self.maxtries = 10\n\n    def http_error_default(self, url, fp, errcode, errmsg, headers):\n        \"\"\"Default error handling -- don't raise an exception.\"\"\"\n        return addinfourl(fp, headers, \"http:\" + url, errcode)\n\n    def http_error_302(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 302 -- relocated (temporarily).\"\"\"\n        self.tries += 1\n        if self.maxtries and self.tries >= self.maxtries:\n            if hasattr(self, \"http_error_500\"):\n                meth = self.http_error_500\n            else:\n                meth = self.http_error_default\n            self.tries = 0\n            return meth(url, fp, 500,\n                        \"Internal Server Error: Redirect Recursion\", headers)\n        result = self.redirect_internal(url, fp, errcode, errmsg, headers,\n                                        data)\n        self.tries = 0\n        return result\n\n    def redirect_internal(self, url, fp, errcode, errmsg, headers, data):\n        if 'location' in headers:\n            newurl = headers['location']\n        elif 'uri' in headers:\n            newurl = headers['uri']\n        else:\n            return\n        fp.close()\n        # In case the server sent a relative URL, join with original:\n        newurl = basejoin(self.type + \":\" + url, newurl)\n\n        # For security reasons we do not allow redirects to protocols\n        # other than HTTP, HTTPS or FTP.\n        newurl_lower = newurl.lower()\n        if not (newurl_lower.startswith('http://') or\n                newurl_lower.startswith('https://') or\n                newurl_lower.startswith('ftp://')):\n            raise IOError('redirect error', errcode,\n                          errmsg + \" - Redirection to url '%s' is not allowed\" %\n                          newurl,\n                          headers)\n\n        return self.open(newurl)\n\n    def http_error_301(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 301 -- also relocated (permanently).\"\"\"\n        return self.http_error_302(url, fp, errcode, errmsg, headers, data)\n\n    def http_error_303(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 303 -- also relocated (essentially identical to 302).\"\"\"\n        return self.http_error_302(url, fp, errcode, errmsg, headers, data)\n\n    def http_error_307(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 307 -- relocated, but turn POST into error.\"\"\"\n        if data is None:\n            return self.http_error_302(url, fp, errcode, errmsg, headers, data)\n        else:\n            return self.http_error_default(url, fp, errcode, errmsg, headers)\n\n    def http_error_401(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 401 -- authentication required.\n        This function supports Basic authentication only.\"\"\"\n        if not 'www-authenticate' in headers:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        stuff = headers['www-authenticate']\n        import re\n        match = re.match('[ \\t]*([^ \\t]+)[ \\t]+realm=\"([^\"]*)\"', stuff)\n        if not match:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        scheme, realm = match.groups()\n        if scheme.lower() != 'basic':\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        name = 'retry_' + self.type + '_basic_auth'\n        if data is None:\n            return getattr(self,name)(url, realm)\n        else:\n            return getattr(self,name)(url, realm, data)\n\n    def http_error_407(self, url, fp, errcode, errmsg, headers, data=None):\n        \"\"\"Error 407 -- proxy authentication required.\n        This function supports Basic authentication only.\"\"\"\n        if not 'proxy-authenticate' in headers:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        stuff = headers['proxy-authenticate']\n        import re\n        match = re.match('[ \\t]*([^ \\t]+)[ \\t]+realm=\"([^\"]*)\"', stuff)\n        if not match:\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        scheme, realm = match.groups()\n        if scheme.lower() != 'basic':\n            URLopener.http_error_default(self, url, fp,\n                                         errcode, errmsg, headers)\n        name = 'retry_proxy_' + self.type + '_basic_auth'\n        if data is None:\n            return getattr(self,name)(url, realm)\n        else:\n            return getattr(self,name)(url, realm, data)\n\n    def retry_proxy_http_basic_auth(self, url, realm, data=None):\n        host, selector = splithost(url)\n        newurl = 'http://' + host + selector\n        proxy = self.proxies['http']\n        urltype, proxyhost = splittype(proxy)\n        proxyhost, proxyselector = splithost(proxyhost)\n        i = proxyhost.find('@') + 1\n        proxyhost = proxyhost[i:]\n        user, passwd = self.get_user_passwd(proxyhost, realm, i)\n        if not (user or passwd): return None\n        proxyhost = quote(user, safe='') + ':' + quote(passwd, safe='') + '@' + proxyhost\n        self.proxies['http'] = 'http://' + proxyhost + proxyselector\n        if data is None:\n            return self.open(newurl)\n        else:\n            return self.open(newurl, data)\n\n    def retry_proxy_https_basic_auth(self, url, realm, data=None):\n        host, selector = splithost(url)\n        newurl = 'https://' + host + selector\n        proxy = self.proxies['https']\n        urltype, proxyhost = splittype(proxy)\n        proxyhost, proxyselector = splithost(proxyhost)\n        i = proxyhost.find('@') + 1\n        proxyhost = proxyhost[i:]\n        user, passwd = self.get_user_passwd(proxyhost, realm, i)\n        if not (user or passwd): return None\n        proxyhost = quote(user, safe='') + ':' + quote(passwd, safe='') + '@' + proxyhost\n        self.proxies['https'] = 'https://' + proxyhost + proxyselector\n        if data is None:\n            return self.open(newurl)\n        else:\n            return self.open(newurl, data)\n\n    def retry_http_basic_auth(self, url, realm, data=None):\n        host, selector = splithost(url)\n        i = host.find('@') + 1\n        host = host[i:]\n        user, passwd = self.get_user_passwd(host, realm, i)\n        if not (user or passwd): return None\n        host = quote(user, safe='') + ':' + quote(passwd, safe='') + '@' + host\n        newurl = 'http://' + host + selector\n        if data is None:\n            return self.open(newurl)\n        else:\n            return self.open(newurl, data)\n\n    def retry_https_basic_auth(self, url, realm, data=None):\n        host, selector = splithost(url)\n        i = host.find('@') + 1\n        host = host[i:]\n        user, passwd = self.get_user_passwd(host, realm, i)\n        if not (user or passwd): return None\n        host = quote(user, safe='') + ':' + quote(passwd, safe='') + '@' + host\n        newurl = 'https://' + host + selector\n        if data is None:\n            return self.open(newurl)\n        else:\n            return self.open(newurl, data)\n\n    def get_user_passwd(self, host, realm, clear_cache=0):\n        key = realm + '@' + host.lower()\n        if key in self.auth_cache:\n            if clear_cache:\n                del self.auth_cache[key]\n            else:\n                return self.auth_cache[key]\n        user, passwd = self.prompt_user_passwd(host, realm)\n        if user or passwd: self.auth_cache[key] = (user, passwd)\n        return user, passwd\n\n    def prompt_user_passwd(self, host, realm):\n        \"\"\"Override this in a GUI environment!\"\"\"\n        import getpass\n        try:\n            user = raw_input(\"Enter username for %s at %s: \" % (realm,\n                                                                host))\n            passwd = getpass.getpass(\"Enter password for %s in %s at %s: \" %\n                (user, realm, host))\n            return user, passwd\n        except KeyboardInterrupt:\n            print\n            return None, None\n\n\n# Utility functions\n\n_localhost = None\ndef localhost():\n    \"\"\"Return the IP address of the magic hostname 'localhost'.\"\"\"\n    global _localhost\n    if _localhost is None:\n        _localhost = socket.gethostbyname('localhost')\n    return _localhost\n\n_thishost = None\ndef thishost():\n    \"\"\"Return the IP address of the current host.\"\"\"\n    global _thishost\n    if _thishost is None:\n        try:\n            _thishost = socket.gethostbyname(socket.gethostname())\n        except socket.gaierror:\n            _thishost = socket.gethostbyname('localhost')\n    return _thishost\n\n_ftperrors = None\ndef ftperrors():\n    \"\"\"Return the set of errors raised by the FTP class.\"\"\"\n    global _ftperrors\n    if _ftperrors is None:\n        import ftplib\n        _ftperrors = ftplib.all_errors\n    return _ftperrors\n\n_noheaders = None\ndef noheaders():\n    \"\"\"Return an empty mimetools.Message object.\"\"\"\n    global _noheaders\n    if _noheaders is None:\n        import mimetools\n        try:\n            from cStringIO import StringIO\n        except ImportError:\n            from StringIO import StringIO\n        _noheaders = mimetools.Message(StringIO(), 0)\n        _noheaders.fp.close()   # Recycle file descriptor\n    return _noheaders\n\n\n# Utility classes\n\nclass ftpwrapper:\n    \"\"\"Class used by open_ftp() for cache of open FTP connections.\"\"\"\n\n    def __init__(self, user, passwd, host, port, dirs,\n                 timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n                 persistent=True):\n        self.user = user\n        self.passwd = passwd\n        self.host = host\n        self.port = port\n        self.dirs = dirs\n        self.timeout = timeout\n        self.refcount = 0\n        self.keepalive = persistent\n        self.init()\n\n    def init(self):\n        import ftplib\n        self.busy = 0\n        self.ftp = ftplib.FTP()\n        self.ftp.connect(self.host, self.port, self.timeout)\n        self.ftp.login(self.user, self.passwd)\n        _target = '/'.join(self.dirs)\n        self.ftp.cwd(_target)\n\n    def retrfile(self, file, type):\n        import ftplib\n        self.endtransfer()\n        if type in ('d', 'D'): cmd = 'TYPE A'; isdir = 1\n        else: cmd = 'TYPE ' + type; isdir = 0\n        try:\n            self.ftp.voidcmd(cmd)\n        except ftplib.all_errors:\n            self.init()\n            self.ftp.voidcmd(cmd)\n        conn = None\n        if file and not isdir:\n            # Try to retrieve as a file\n            try:\n                cmd = 'RETR ' + file\n                conn, retrlen = self.ftp.ntransfercmd(cmd)\n            except ftplib.error_perm, reason:\n                if str(reason)[:3] != '550':\n                    raise IOError, ('ftp error', reason), sys.exc_info()[2]\n        if not conn:\n            # Set transfer mode to ASCII!\n            self.ftp.voidcmd('TYPE A')\n            # Try a directory listing. Verify that directory exists.\n            if file:\n                pwd = self.ftp.pwd()\n                try:\n                    try:\n                        self.ftp.cwd(file)\n                    except ftplib.error_perm, reason:\n                        raise IOError, ('ftp error', reason), sys.exc_info()[2]\n                finally:\n                    self.ftp.cwd(pwd)\n                cmd = 'LIST ' + file\n            else:\n                cmd = 'LIST'\n            conn, retrlen = self.ftp.ntransfercmd(cmd)\n        self.busy = 1\n        ftpobj = addclosehook(conn.makefile('rb'), self.file_close)\n        self.refcount += 1\n        conn.close()\n        # Pass back both a suitably decorated object and a retrieval length\n        return (ftpobj, retrlen)\n\n    def endtransfer(self):\n        if not self.busy:\n            return\n        self.busy = 0\n        try:\n            self.ftp.voidresp()\n        except ftperrors():\n            pass\n\n    def close(self):\n        self.keepalive = False\n        if self.refcount <= 0:\n            self.real_close()\n\n    def file_close(self):\n        self.endtransfer()\n        self.refcount -= 1\n        if self.refcount <= 0 and not self.keepalive:\n            self.real_close()\n\n    def real_close(self):\n        self.endtransfer()\n        try:\n            self.ftp.close()\n        except ftperrors():\n            pass\n\nclass addbase:\n    \"\"\"Base class for addinfo and addclosehook.\"\"\"\n\n    def __init__(self, fp):\n        self.fp = fp\n        self.read = self.fp.read\n        self.readline = self.fp.readline\n        if hasattr(self.fp, \"readlines\"): self.readlines = self.fp.readlines\n        if hasattr(self.fp, \"fileno\"):\n            self.fileno = self.fp.fileno\n        else:\n            self.fileno = lambda: None\n        if hasattr(self.fp, \"__iter__\"):\n            self.__iter__ = self.fp.__iter__\n            if hasattr(self.fp, \"next\"):\n                self.next = self.fp.next\n\n    def __repr__(self):\n        return '<%s at %r whose fp = %r>' % (self.__class__.__name__,\n                                             id(self), self.fp)\n\n    def close(self):\n        self.read = None\n        self.readline = None\n        self.readlines = None\n        self.fileno = None\n        if self.fp: self.fp.close()\n        self.fp = None\n\nclass addclosehook(addbase):\n    \"\"\"Class to add a close hook to an open file.\"\"\"\n\n    def __init__(self, fp, closehook, *hookargs):\n        addbase.__init__(self, fp)\n        self.closehook = closehook\n        self.hookargs = hookargs\n\n    def close(self):\n        if self.closehook:\n            self.closehook(*self.hookargs)\n            self.closehook = None\n            self.hookargs = None\n        addbase.close(self)\n\nclass addinfo(addbase):\n    \"\"\"class to add an info() method to an open file.\"\"\"\n\n    def __init__(self, fp, headers):\n        addbase.__init__(self, fp)\n        self.headers = headers\n\n    def info(self):\n        return self.headers\n\nclass addinfourl(addbase):\n    \"\"\"class to add info() and geturl() methods to an open file.\"\"\"\n\n    def __init__(self, fp, headers, url, code=None):\n        addbase.__init__(self, fp)\n        self.headers = headers\n        self.url = url\n        self.code = code\n\n    def info(self):\n        return self.headers\n\n    def getcode(self):\n        return self.code\n\n    def geturl(self):\n        return self.url\n\n\n# Utilities to parse URLs (most of these return None for missing parts):\n# unwrap('<URL:type://host/path>') --> 'type://host/path'\n# splittype('type:opaquestring') --> 'type', 'opaquestring'\n# splithost('//host[:port]/path') --> 'host[:port]', '/path'\n# splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'\n# splitpasswd('user:passwd') -> 'user', 'passwd'\n# splitport('host:port') --> 'host', 'port'\n# splitquery('/path?query') --> '/path', 'query'\n# splittag('/path#tag') --> '/path', 'tag'\n# splitattr('/path;attr1=value1;attr2=value2;...') ->\n#   '/path', ['attr1=value1', 'attr2=value2', ...]\n# splitvalue('attr=value') --> 'attr', 'value'\n# unquote('abc%20def') -> 'abc def'\n# quote('abc def') -> 'abc%20def')\n\ntry:\n    unicode\nexcept NameError:\n    def _is_unicode(x):\n        return 0\nelse:\n    def _is_unicode(x):\n        return isinstance(x, unicode)\n\ndef toBytes(url):\n    \"\"\"toBytes(u\"URL\") --> 'URL'.\"\"\"\n    # Most URL schemes require ASCII. If that changes, the conversion\n    # can be relaxed\n    if _is_unicode(url):\n        try:\n            url = url.encode(\"ASCII\")\n        except UnicodeError:\n            raise UnicodeError(\"URL \" + repr(url) +\n                               \" contains non-ASCII characters\")\n    return url\n\ndef unwrap(url):\n    \"\"\"unwrap('<URL:type://host/path>') --> 'type://host/path'.\"\"\"\n    url = url.strip()\n    if url[:1] == '<' and url[-1:] == '>':\n        url = url[1:-1].strip()\n    if url[:4] == 'URL:': url = url[4:].strip()\n    return url\n\n_typeprog = None\ndef splittype(url):\n    \"\"\"splittype('type:opaquestring') --> 'type', 'opaquestring'.\"\"\"\n    global _typeprog\n    if _typeprog is None:\n        import re\n        _typeprog = re.compile('^([^/:]+):')\n\n    match = _typeprog.match(url)\n    if match:\n        scheme = match.group(1)\n        return scheme.lower(), url[len(scheme) + 1:]\n    return None, url\n\n_hostprog = None\ndef splithost(url):\n    \"\"\"splithost('//host[:port]/path') --> 'host[:port]', '/path'.\"\"\"\n    global _hostprog\n    if _hostprog is None:\n        import re\n        _hostprog = re.compile('^//([^/?]*)(.*)$')\n\n    match = _hostprog.match(url)\n    if match:\n        host_port = match.group(1)\n        path = match.group(2)\n        if path and not path.startswith('/'):\n            path = '/' + path\n        return host_port, path\n    return None, url\n\n_userprog = None\ndef splituser(host):\n    \"\"\"splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    global _userprog\n    if _userprog is None:\n        import re\n        _userprog = re.compile('^(.*)@(.*)$')\n\n    match = _userprog.match(host)\n    if match: return match.group(1, 2)\n    return None, host\n\n_passwdprog = None\ndef splitpasswd(user):\n    \"\"\"splitpasswd('user:passwd') -> 'user', 'passwd'.\"\"\"\n    global _passwdprog\n    if _passwdprog is None:\n        import re\n        _passwdprog = re.compile('^([^:]*):(.*)$',re.S)\n\n    match = _passwdprog.match(user)\n    if match: return match.group(1, 2)\n    return user, None\n\n# splittag('/path#tag') --> '/path', 'tag'\n_portprog = None\ndef splitport(host):\n    \"\"\"splitport('host:port') --> 'host', 'port'.\"\"\"\n    global _portprog\n    if _portprog is None:\n        import re\n        _portprog = re.compile('^(.*):([0-9]*)$')\n\n    match = _portprog.match(host)\n    if match:\n        host, port = match.groups()\n        if port:\n            return host, port\n    return host, None\n\n_nportprog = None\ndef splitnport(host, defport=-1):\n    \"\"\"Split host and port, returning numeric port.\n    Return given default port if no ':' found; defaults to -1.\n    Return numerical port if a valid number are found after ':'.\n    Return None if ':' but not a valid number.\"\"\"\n    global _nportprog\n    if _nportprog is None:\n        import re\n        _nportprog = re.compile('^(.*):(.*)$')\n\n    match = _nportprog.match(host)\n    if match:\n        host, port = match.group(1, 2)\n        if port:\n            try:\n                nport = int(port)\n            except ValueError:\n                nport = None\n            return host, nport\n    return host, defport\n\n_queryprog = None\ndef splitquery(url):\n    \"\"\"splitquery('/path?query') --> '/path', 'query'.\"\"\"\n    global _queryprog\n    if _queryprog is None:\n        import re\n        _queryprog = re.compile('^(.*)\\?([^?]*)$')\n\n    match = _queryprog.match(url)\n    if match: return match.group(1, 2)\n    return url, None\n\n_tagprog = None\ndef splittag(url):\n    \"\"\"splittag('/path#tag') --> '/path', 'tag'.\"\"\"\n    global _tagprog\n    if _tagprog is None:\n        import re\n        _tagprog = re.compile('^(.*)#([^#]*)$')\n\n    match = _tagprog.match(url)\n    if match: return match.group(1, 2)\n    return url, None\n\ndef splitattr(url):\n    \"\"\"splitattr('/path;attr1=value1;attr2=value2;...') ->\n        '/path', ['attr1=value1', 'attr2=value2', ...].\"\"\"\n    words = url.split(';')\n    return words[0], words[1:]\n\n_valueprog = None\ndef splitvalue(attr):\n    \"\"\"splitvalue('attr=value') --> 'attr', 'value'.\"\"\"\n    global _valueprog\n    if _valueprog is None:\n        import re\n        _valueprog = re.compile('^([^=]*)=(.*)$')\n\n    match = _valueprog.match(attr)\n    if match: return match.group(1, 2)\n    return attr, None\n\n# urlparse contains a duplicate of this method to avoid a circular import.  If\n# you update this method, also update the copy in urlparse.  This code\n# duplication does not exist in Python3.\n\n_hexdig = '0123456789ABCDEFabcdef'\n_hextochr = dict((a + b, chr(int(a + b, 16)))\n                 for a in _hexdig for b in _hexdig)\n_asciire = re.compile('([\\x00-\\x7f]+)')\n\ndef unquote(s):\n    \"\"\"unquote('abc%20def') -> 'abc def'.\"\"\"\n    if _is_unicode(s):\n        if '%' not in s:\n            return s\n        bits = _asciire.split(s)\n        res = [bits[0]]\n        append = res.append\n        for i in range(1, len(bits), 2):\n            append(unquote(str(bits[i])).decode('latin1'))\n            append(bits[i + 1])\n        return ''.join(res)\n\n    bits = s.split('%')\n    # fastpath\n    if len(bits) == 1:\n        return s\n    res = [bits[0]]\n    append = res.append\n    for j in xrange(1, len(bits)):\n        item = bits[j]\n        try:\n            append(_hextochr[item[:2]])\n            append(item[2:])\n        except KeyError:\n            append('%')\n            append(item)\n    return ''.join(res)\n\ndef unquote_plus(s):\n    \"\"\"unquote('%7e/abc+def') -> '~/abc def'\"\"\"\n    s = s.replace('+', ' ')\n    return unquote(s)\n\nalways_safe = ('ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n               'abcdefghijklmnopqrstuvwxyz'\n               '0123456789' '_.-')\n_safe_map = {}\nfor i, c in zip(xrange(256), str(bytearray(xrange(256)))):\n    _safe_map[c] = c if (i < 128 and c in always_safe) else '%{:02X}'.format(i)\n_safe_quoters = {}\n\ndef quote(s, safe='/'):\n    \"\"\"quote('abc def') -> 'abc%20def'\n\n    Each part of a URL, e.g. the path info, the query, etc., has a\n    different set of reserved characters that must be quoted.\n\n    RFC 2396 Uniform Resource Identifiers (URI): Generic Syntax lists\n    the following reserved characters.\n\n    reserved    = \";\" | \"/\" | \"?\" | \":\" | \"@\" | \"&\" | \"=\" | \"+\" |\n                  \"$\" | \",\"\n\n    Each of these characters is reserved in some component of a URL,\n    but not necessarily in all of them.\n\n    By default, the quote function is intended for quoting the path\n    section of a URL.  Thus, it will not encode '/'.  This character\n    is reserved, but in typical usage the quote function is being\n    called on a path where the existing slash characters are used as\n    reserved characters.\n    \"\"\"\n    # fastpath\n    if not s:\n        if s is None:\n            raise TypeError('None object cannot be quoted')\n        return s\n    cachekey = (safe, always_safe)\n    try:\n        (quoter, safe) = _safe_quoters[cachekey]\n    except KeyError:\n        safe_map = _safe_map.copy()\n        safe_map.update([(c, c) for c in safe])\n        quoter = safe_map.__getitem__\n        safe = always_safe + safe\n        _safe_quoters[cachekey] = (quoter, safe)\n    if not s.rstrip(safe):\n        return s\n    return ''.join(map(quoter, s))\n\ndef quote_plus(s, safe=''):\n    \"\"\"Quote the query fragment of a URL; replacing ' ' with '+'\"\"\"\n    if ' ' in s:\n        s = quote(s, safe + ' ')\n        return s.replace(' ', '+')\n    return quote(s, safe)\n\ndef urlencode(query, doseq=0):\n    \"\"\"Encode a sequence of two-element tuples or dictionary into a URL query string.\n\n    If any values in the query arg are sequences and doseq is true, each\n    sequence element is converted to a separate parameter.\n\n    If the query arg is a sequence of two-element tuples, the order of the\n    parameters in the output will match the order of parameters in the\n    input.\n    \"\"\"\n\n    if hasattr(query,\"items\"):\n        # mapping objects\n        query = query.items()\n    else:\n        # it's a bother at times that strings and string-like objects are\n        # sequences...\n        try:\n            # non-sequence items should not work with len()\n            # non-empty strings will fail this\n            if len(query) and not isinstance(query[0], tuple):\n                raise TypeError\n            # zero-length sequences of all types will get here and succeed,\n            # but that's a minor nit - since the original implementation\n            # allowed empty dicts that type of behavior probably should be\n            # preserved for consistency\n        except TypeError:\n            ty,va,tb = sys.exc_info()\n            raise TypeError, \"not a valid non-string sequence or mapping object\", tb\n\n    l = []\n    if not doseq:\n        # preserve old behavior\n        for k, v in query:\n            k = quote_plus(str(k))\n            v = quote_plus(str(v))\n            l.append(k + '=' + v)\n    else:\n        for k, v in query:\n            k = quote_plus(str(k))\n            if isinstance(v, str):\n                v = quote_plus(v)\n                l.append(k + '=' + v)\n            elif _is_unicode(v):\n                # is there a reasonable way to convert to ASCII?\n                # encode generates a string, but \"replace\" or \"ignore\"\n                # lose information and \"strict\" can raise UnicodeError\n                v = quote_plus(v.encode(\"ASCII\",\"replace\"))\n                l.append(k + '=' + v)\n            else:\n                try:\n                    # is this a sufficient test for sequence-ness?\n                    len(v)\n                except TypeError:\n                    # not a sequence\n                    v = quote_plus(str(v))\n                    l.append(k + '=' + v)\n                else:\n                    # loop over the sequence\n                    for elt in v:\n                        l.append(k + '=' + quote_plus(str(elt)))\n    return '&'.join(l)\n\n# Proxy handling\ndef getproxies_environment():\n    \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n    Scan the environment for variables named <scheme>_proxy;\n    this seems to be the standard convention.  If you need a\n    different way, you can pass a proxies dictionary to the\n    [Fancy]URLopener constructor.\n\n    \"\"\"\n    proxies = {}\n    for name, value in os.environ.items():\n        name = name.lower()\n        if value and name[-6:] == '_proxy':\n            proxies[name[:-6]] = value\n    return proxies\n\ndef proxy_bypass_environment(host):\n    \"\"\"Test if proxies should not be used for a particular host.\n\n    Checks the environment for a variable named no_proxy, which should\n    be a list of DNS suffixes separated by commas, or '*' for all hosts.\n    \"\"\"\n    no_proxy = os.environ.get('no_proxy', '') or os.environ.get('NO_PROXY', '')\n    # '*' is special case for always bypass\n    if no_proxy == '*':\n        return 1\n    # strip port off host\n    hostonly, port = splitport(host)\n    # check if the host ends with any of the DNS suffixes\n    no_proxy_list = [proxy.strip() for proxy in no_proxy.split(',')]\n    for name in no_proxy_list:\n        if name and (hostonly.endswith(name) or host.endswith(name)):\n            return 1\n    # otherwise, don't bypass\n    return 0\n\n\nif sys.platform == 'darwin':\n    from _scproxy import _get_proxy_settings, _get_proxies\n\n    def proxy_bypass_macosx_sysconf(host):\n        \"\"\"\n        Return True iff this host shouldn't be accessed using a proxy\n\n        This function uses the MacOSX framework SystemConfiguration\n        to fetch the proxy information.\n        \"\"\"\n        import re\n        import socket\n        from fnmatch import fnmatch\n\n        hostonly, port = splitport(host)\n\n        def ip2num(ipAddr):\n            parts = ipAddr.split('.')\n            parts = map(int, parts)\n            if len(parts) != 4:\n                parts = (parts + [0, 0, 0, 0])[:4]\n            return (parts[0] << 24) | (parts[1] << 16) | (parts[2] << 8) | parts[3]\n\n        proxy_settings = _get_proxy_settings()\n\n        # Check for simple host names:\n        if '.' not in host:\n            if proxy_settings['exclude_simple']:\n                return True\n\n        hostIP = None\n\n        for value in proxy_settings.get('exceptions', ()):\n            # Items in the list are strings like these: *.local, 169.254/16\n            if not value: continue\n\n            m = re.match(r\"(\\d+(?:\\.\\d+)*)(/\\d+)?\", value)\n            if m is not None:\n                if hostIP is None:\n                    try:\n                        hostIP = socket.gethostbyname(hostonly)\n                        hostIP = ip2num(hostIP)\n                    except socket.error:\n                        continue\n\n                base = ip2num(m.group(1))\n                mask = m.group(2)\n                if mask is None:\n                    mask = 8 * (m.group(1).count('.') + 1)\n\n                else:\n                    mask = int(mask[1:])\n                mask = 32 - mask\n\n                if (hostIP >> mask) == (base >> mask):\n                    return True\n\n            elif fnmatch(host, value):\n                return True\n\n        return False\n\n    def getproxies_macosx_sysconf():\n        \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n        This function uses the MacOSX framework SystemConfiguration\n        to fetch the proxy information.\n        \"\"\"\n        return _get_proxies()\n\n    def proxy_bypass(host):\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_macosx_sysconf(host)\n\n    def getproxies():\n        return getproxies_environment() or getproxies_macosx_sysconf()\n\nelif os.name == 'nt':\n    def getproxies_registry():\n        \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n        Win32 uses the registry to store proxies.\n\n        \"\"\"\n        proxies = {}\n        try:\n            import _winreg\n        except ImportError:\n            # Std module, so should be around - but you never know!\n            return proxies\n        try:\n            internetSettings = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER,\n                r'Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings')\n            proxyEnable = _winreg.QueryValueEx(internetSettings,\n                                               'ProxyEnable')[0]\n            if proxyEnable:\n                # Returned as Unicode but problems if not converted to ASCII\n                proxyServer = str(_winreg.QueryValueEx(internetSettings,\n                                                       'ProxyServer')[0])\n                if '=' in proxyServer:\n                    # Per-protocol settings\n                    for p in proxyServer.split(';'):\n                        protocol, address = p.split('=', 1)\n                        # See if address has a type:// prefix\n                        import re\n                        if not re.match('^([^/:]+)://', address):\n                            address = '%s://%s' % (protocol, address)\n                        proxies[protocol] = address\n                else:\n                    # Use one setting for all protocols\n                    if proxyServer[:5] == 'http:':\n                        proxies['http'] = proxyServer\n                    else:\n                        proxies['http'] = 'http://%s' % proxyServer\n                        proxies['https'] = 'https://%s' % proxyServer\n                        proxies['ftp'] = 'ftp://%s' % proxyServer\n            internetSettings.Close()\n        except (WindowsError, ValueError, TypeError):\n            # Either registry key not found etc, or the value in an\n            # unexpected format.\n            # proxies already set up to be empty so nothing to do\n            pass\n        return proxies\n\n    def getproxies():\n        \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n        Returns settings gathered from the environment, if specified,\n        or the registry.\n\n        \"\"\"\n        return getproxies_environment() or getproxies_registry()\n\n    def proxy_bypass_registry(host):\n        try:\n            import _winreg\n            import re\n        except ImportError:\n            # Std modules, so should be around - but you never know!\n            return 0\n        try:\n            internetSettings = _winreg.OpenKey(_winreg.HKEY_CURRENT_USER,\n                r'Software\\Microsoft\\Windows\\CurrentVersion\\Internet Settings')\n            proxyEnable = _winreg.QueryValueEx(internetSettings,\n                                               'ProxyEnable')[0]\n            proxyOverride = str(_winreg.QueryValueEx(internetSettings,\n                                                     'ProxyOverride')[0])\n            # ^^^^ Returned as Unicode but problems if not converted to ASCII\n        except WindowsError:\n            return 0\n        if not proxyEnable or not proxyOverride:\n            return 0\n        # try to make a host list from name and IP address.\n        rawHost, port = splitport(host)\n        host = [rawHost]\n        try:\n            addr = socket.gethostbyname(rawHost)\n            if addr != rawHost:\n                host.append(addr)\n        except socket.error:\n            pass\n        try:\n            fqdn = socket.getfqdn(rawHost)\n            if fqdn != rawHost:\n                host.append(fqdn)\n        except socket.error:\n            pass\n        # make a check value list from the registry entry: replace the\n        # '<local>' string by the localhost entry and the corresponding\n        # canonical entry.\n        proxyOverride = proxyOverride.split(';')\n        # now check if we match one of the registry values.\n        for test in proxyOverride:\n            if test == '<local>':\n                if '.' not in rawHost:\n                    return 1\n            test = test.replace(\".\", r\"\\.\")     # mask dots\n            test = test.replace(\"*\", r\".*\")     # change glob sequence\n            test = test.replace(\"?\", r\".\")      # change glob char\n            for val in host:\n                # print \"%s <--> %s\" %( test, val )\n                if re.match(test, val, re.I):\n                    return 1\n        return 0\n\n    def proxy_bypass(host):\n        \"\"\"Return a dictionary of scheme -> proxy server URL mappings.\n\n        Returns settings gathered from the environment, if specified,\n        or the registry.\n\n        \"\"\"\n        if getproxies_environment():\n            return proxy_bypass_environment(host)\n        else:\n            return proxy_bypass_registry(host)\n\nelse:\n    # By default use environment variables\n    getproxies = getproxies_environment\n    proxy_bypass = proxy_bypass_environment\n\n# Test and time quote() and unquote()\ndef test1():\n    s = ''\n    for i in range(256): s = s + chr(i)\n    s = s*4\n    t0 = time.time()\n    qs = quote(s)\n    uqs = unquote(qs)\n    t1 = time.time()\n    if uqs != s:\n        print 'Wrong!'\n    print repr(s)\n    print repr(qs)\n    print repr(uqs)\n    print round(t1 - t0, 3), 'sec'\n\n\ndef reporthook(blocknum, blocksize, totalsize):\n    # Report during remote transfers\n    print \"Block number: %d, Block size: %d, Total size: %d\" % (\n        blocknum, blocksize, totalsize)\n", 
    "urllib2": "\"\"\"An extensible library for opening URLs using a variety of protocols\n\nThe simplest way to use this module is to call the urlopen function,\nwhich accepts a string containing a URL or a Request object (described\nbelow).  It opens the URL and returns the results as file-like\nobject; the returned object has some extra methods described below.\n\nThe OpenerDirector manages a collection of Handler objects that do\nall the actual work.  Each Handler implements a particular protocol or\noption.  The OpenerDirector is a composite object that invokes the\nHandlers needed to open the requested URL.  For example, the\nHTTPHandler performs HTTP GET and POST requests and deals with\nnon-error returns.  The HTTPRedirectHandler automatically deals with\nHTTP 301, 302, 303 and 307 redirect errors, and the HTTPDigestAuthHandler\ndeals with digest authentication.\n\nurlopen(url, data=None) -- Basic usage is the same as original\nurllib.  pass the url and optionally data to post to an HTTP URL, and\nget a file-like object back.  One difference is that you can also pass\na Request instance instead of URL.  Raises a URLError (subclass of\nIOError); for HTTP errors, raises an HTTPError, which can also be\ntreated as a valid response.\n\nbuild_opener -- Function that creates a new OpenerDirector instance.\nWill install the default handlers.  Accepts one or more Handlers as\narguments, either instances or Handler classes that it will\ninstantiate.  If one of the argument is a subclass of the default\nhandler, the argument will be installed instead of the default.\n\ninstall_opener -- Installs a new opener as the default opener.\n\nobjects of interest:\n\nOpenerDirector -- Sets up the User Agent as the Python-urllib client and manages\nthe Handler classes, while dealing with requests and responses.\n\nRequest -- An object that encapsulates the state of a request.  The\nstate can be as simple as the URL.  It can also include extra HTTP\nheaders, e.g. a User-Agent.\n\nBaseHandler --\n\nexceptions:\nURLError -- A subclass of IOError, individual protocols have their own\nspecific subclass.\n\nHTTPError -- Also a valid HTTP response, so you can treat an HTTP error\nas an exceptional event or valid response.\n\ninternals:\nBaseHandler and parent\n_call_chain conventions\n\nExample usage:\n\nimport urllib2\n\n# set up authentication info\nauthinfo = urllib2.HTTPBasicAuthHandler()\nauthinfo.add_password(realm='PDQ Application',\n                      uri='https://mahler:8092/site-updates.py',\n                      user='klem',\n                      passwd='geheim$parole')\n\nproxy_support = urllib2.ProxyHandler({\"http\" : \"http://ahad-haam:3128\"})\n\n# build a new opener that adds authentication and caching FTP handlers\nopener = urllib2.build_opener(proxy_support, authinfo, urllib2.CacheFTPHandler)\n\n# install it\nurllib2.install_opener(opener)\n\nf = urllib2.urlopen('http://www.python.org/')\n\n\n\"\"\"\n\n# XXX issues:\n# If an authentication error handler that tries to perform\n# authentication for some reason but fails, how should the error be\n# signalled?  The client needs to know the HTTP error code.  But if\n# the handler knows that the problem was, e.g., that it didn't know\n# that hash algo that requested in the challenge, it would be good to\n# pass that information along to the client, too.\n# ftp errors aren't handled cleanly\n# check digest against correct (i.e. non-apache) implementation\n\n# Possible extensions:\n# complex proxies  XXX not sure what exactly was meant by this\n# abstract factory for opener\n\nimport base64\nimport hashlib\nimport httplib\nimport mimetools\nimport os\nimport posixpath\nimport random\nimport re\nimport socket\nimport sys\nimport time\nimport urlparse\nimport bisect\nimport warnings\n\ntry:\n    from cStringIO import StringIO\nexcept ImportError:\n    from StringIO import StringIO\n\n# check for SSL\ntry:\n    import ssl\nexcept ImportError:\n    _have_ssl = False\nelse:\n    _have_ssl = True\n\nfrom urllib import (unwrap, unquote, splittype, splithost, quote,\n     addinfourl, splitport, splittag, toBytes,\n     splitattr, ftpwrapper, splituser, splitpasswd, splitvalue)\n\n# support for FileHandler, proxies via environment variables\nfrom urllib import localhost, url2pathname, getproxies, proxy_bypass\n\n# used in User-Agent header sent\n__version__ = sys.version[:3]\n\n_opener = None\ndef urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,\n            cafile=None, capath=None, cadefault=False, context=None):\n    global _opener\n    if cafile or capath or cadefault:\n        if context is not None:\n            raise ValueError(\n                \"You can't pass both context and any of cafile, capath, and \"\n                \"cadefault\"\n            )\n        if not _have_ssl:\n            raise ValueError('SSL support not available')\n        context = ssl.create_default_context(purpose=ssl.Purpose.SERVER_AUTH,\n                                             cafile=cafile,\n                                             capath=capath)\n        https_handler = HTTPSHandler(context=context)\n        opener = build_opener(https_handler)\n    elif context:\n        https_handler = HTTPSHandler(context=context)\n        opener = build_opener(https_handler)\n    elif _opener is None:\n        _opener = opener = build_opener()\n    else:\n        opener = _opener\n    return opener.open(url, data, timeout)\n\ndef install_opener(opener):\n    global _opener\n    _opener = opener\n\n# do these error classes make sense?\n# make sure all of the IOError stuff is overridden.  we just want to be\n# subtypes.\n\nclass URLError(IOError):\n    # URLError is a sub-type of IOError, but it doesn't share any of\n    # the implementation.  need to override __init__ and __str__.\n    # It sets self.args for compatibility with other EnvironmentError\n    # subclasses, but args doesn't have the typical format with errno in\n    # slot 0 and strerror in slot 1.  This may be better than nothing.\n    def __init__(self, reason):\n        self.args = reason,\n        self.reason = reason\n\n    def __str__(self):\n        return '<urlopen error %s>' % self.reason\n\nclass HTTPError(URLError, addinfourl):\n    \"\"\"Raised when HTTP error occurs, but also acts like non-error return\"\"\"\n    __super_init = addinfourl.__init__\n\n    def __init__(self, url, code, msg, hdrs, fp):\n        self.code = code\n        self.msg = msg\n        self.hdrs = hdrs\n        self.fp = fp\n        self.filename = url\n        # The addinfourl classes depend on fp being a valid file\n        # object.  In some cases, the HTTPError may not have a valid\n        # file object.  If this happens, the simplest workaround is to\n        # not initialize the base classes.\n        if fp is not None:\n            self.__super_init(fp, hdrs, url, code)\n\n    def __str__(self):\n        return 'HTTP Error %s: %s' % (self.code, self.msg)\n\n    # since URLError specifies a .reason attribute, HTTPError should also\n    #  provide this attribute. See issue13211 fo discussion.\n    @property\n    def reason(self):\n        return self.msg\n\n    def info(self):\n        return self.hdrs\n\n# copied from cookielib.py\n_cut_port_re = re.compile(r\":\\d+$\")\ndef request_host(request):\n    \"\"\"Return request-host, as defined by RFC 2965.\n\n    Variation from RFC: returned value is lowercased, for convenient\n    comparison.\n\n    \"\"\"\n    url = request.get_full_url()\n    host = urlparse.urlparse(url)[1]\n    if host == \"\":\n        host = request.get_header(\"Host\", \"\")\n\n    # remove port, if present\n    host = _cut_port_re.sub(\"\", host, 1)\n    return host.lower()\n\nclass Request:\n\n    def __init__(self, url, data=None, headers={},\n                 origin_req_host=None, unverifiable=False):\n        # unwrap('<URL:type://host/path>') --> 'type://host/path'\n        self.__original = unwrap(url)\n        self.__original, self.__fragment = splittag(self.__original)\n        self.type = None\n        # self.__r_type is what's left after doing the splittype\n        self.host = None\n        self.port = None\n        self._tunnel_host = None\n        self.data = data\n        self.headers = {}\n        for key, value in headers.items():\n            self.add_header(key, value)\n        self.unredirected_hdrs = {}\n        if origin_req_host is None:\n            origin_req_host = request_host(self)\n        self.origin_req_host = origin_req_host\n        self.unverifiable = unverifiable\n\n    def __getattr__(self, attr):\n        # XXX this is a fallback mechanism to guard against these\n        # methods getting called in a non-standard order.  this may be\n        # too complicated and/or unnecessary.\n        # XXX should the __r_XXX attributes be public?\n        if attr[:12] == '_Request__r_':\n            name = attr[12:]\n            if hasattr(Request, 'get_' + name):\n                getattr(self, 'get_' + name)()\n                return getattr(self, attr)\n        raise AttributeError, attr\n\n    def get_method(self):\n        if self.has_data():\n            return \"POST\"\n        else:\n            return \"GET\"\n\n    # XXX these helper methods are lame\n\n    def add_data(self, data):\n        self.data = data\n\n    def has_data(self):\n        return self.data is not None\n\n    def get_data(self):\n        return self.data\n\n    def get_full_url(self):\n        if self.__fragment:\n            return '%s#%s' % (self.__original, self.__fragment)\n        else:\n            return self.__original\n\n    def get_type(self):\n        if self.type is None:\n            self.type, self.__r_type = splittype(self.__original)\n            if self.type is None:\n                raise ValueError, \"unknown url type: %s\" % self.__original\n        return self.type\n\n    def get_host(self):\n        if self.host is None:\n            self.host, self.__r_host = splithost(self.__r_type)\n            if self.host:\n                self.host = unquote(self.host)\n        return self.host\n\n    def get_selector(self):\n        return self.__r_host\n\n    def set_proxy(self, host, type):\n        if self.type == 'https' and not self._tunnel_host:\n            self._tunnel_host = self.host\n        else:\n            self.type = type\n            self.__r_host = self.__original\n\n        self.host = host\n\n    def has_proxy(self):\n        return self.__r_host == self.__original\n\n    def get_origin_req_host(self):\n        return self.origin_req_host\n\n    def is_unverifiable(self):\n        return self.unverifiable\n\n    def add_header(self, key, val):\n        # useful for something like authentication\n        self.headers[key.capitalize()] = val\n\n    def add_unredirected_header(self, key, val):\n        # will not be added to a redirected request\n        self.unredirected_hdrs[key.capitalize()] = val\n\n    def has_header(self, header_name):\n        return (header_name in self.headers or\n                header_name in self.unredirected_hdrs)\n\n    def get_header(self, header_name, default=None):\n        return self.headers.get(\n            header_name,\n            self.unredirected_hdrs.get(header_name, default))\n\n    def header_items(self):\n        hdrs = self.unredirected_hdrs.copy()\n        hdrs.update(self.headers)\n        return hdrs.items()\n\nclass OpenerDirector:\n    def __init__(self):\n        client_version = \"Python-urllib/%s\" % __version__\n        self.addheaders = [('User-agent', client_version)]\n        # self.handlers is retained only for backward compatibility\n        self.handlers = []\n        # manage the individual handlers\n        self.handle_open = {}\n        self.handle_error = {}\n        self.process_response = {}\n        self.process_request = {}\n\n    def add_handler(self, handler):\n        if not hasattr(handler, \"add_parent\"):\n            raise TypeError(\"expected BaseHandler instance, got %r\" %\n                            type(handler))\n\n        added = False\n        for meth in dir(handler):\n            if meth in [\"redirect_request\", \"do_open\", \"proxy_open\"]:\n                # oops, coincidental match\n                continue\n\n            i = meth.find(\"_\")\n            protocol = meth[:i]\n            condition = meth[i+1:]\n\n            if condition.startswith(\"error\"):\n                j = condition.find(\"_\") + i + 1\n                kind = meth[j+1:]\n                try:\n                    kind = int(kind)\n                except ValueError:\n                    pass\n                lookup = self.handle_error.get(protocol, {})\n                self.handle_error[protocol] = lookup\n            elif condition == \"open\":\n                kind = protocol\n                lookup = self.handle_open\n            elif condition == \"response\":\n                kind = protocol\n                lookup = self.process_response\n            elif condition == \"request\":\n                kind = protocol\n                lookup = self.process_request\n            else:\n                continue\n\n            handlers = lookup.setdefault(kind, [])\n            if handlers:\n                bisect.insort(handlers, handler)\n            else:\n                handlers.append(handler)\n            added = True\n\n        if added:\n            bisect.insort(self.handlers, handler)\n            handler.add_parent(self)\n\n    def close(self):\n        # Only exists for backwards compatibility.\n        pass\n\n    def _call_chain(self, chain, kind, meth_name, *args):\n        # Handlers raise an exception if no one else should try to handle\n        # the request, or return None if they can't but another handler\n        # could.  Otherwise, they return the response.\n        handlers = chain.get(kind, ())\n        for handler in handlers:\n            func = getattr(handler, meth_name)\n\n            result = func(*args)\n            if result is not None:\n                return result\n\n    def open(self, fullurl, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT):\n        # accept a URL or a Request object\n        if isinstance(fullurl, basestring):\n            req = Request(fullurl, data)\n        else:\n            req = fullurl\n            if data is not None:\n                req.add_data(data)\n\n        req.timeout = timeout\n        protocol = req.get_type()\n\n        # pre-process request\n        meth_name = protocol+\"_request\"\n        for processor in self.process_request.get(protocol, []):\n            meth = getattr(processor, meth_name)\n            req = meth(req)\n\n        response = self._open(req, data)\n\n        # post-process response\n        meth_name = protocol+\"_response\"\n        for processor in self.process_response.get(protocol, []):\n            meth = getattr(processor, meth_name)\n            response = meth(req, response)\n\n        return response\n\n    def _open(self, req, data=None):\n        result = self._call_chain(self.handle_open, 'default',\n                                  'default_open', req)\n        if result:\n            return result\n\n        protocol = req.get_type()\n        result = self._call_chain(self.handle_open, protocol, protocol +\n                                  '_open', req)\n        if result:\n            return result\n\n        return self._call_chain(self.handle_open, 'unknown',\n                                'unknown_open', req)\n\n    def error(self, proto, *args):\n        if proto in ('http', 'https'):\n            # XXX http[s] protocols are special-cased\n            dict = self.handle_error['http'] # https is not different than http\n            proto = args[2]  # YUCK!\n            meth_name = 'http_error_%s' % proto\n            http_err = 1\n            orig_args = args\n        else:\n            dict = self.handle_error\n            meth_name = proto + '_error'\n            http_err = 0\n        args = (dict, proto, meth_name) + args\n        result = self._call_chain(*args)\n        if result:\n            return result\n\n        if http_err:\n            args = (dict, 'default', 'http_error_default') + orig_args\n            return self._call_chain(*args)\n\n# XXX probably also want an abstract factory that knows when it makes\n# sense to skip a superclass in favor of a subclass and when it might\n# make sense to include both\n\ndef build_opener(*handlers):\n    \"\"\"Create an opener object from a list of handlers.\n\n    The opener will use several default handlers, including support\n    for HTTP, FTP and when applicable, HTTPS.\n\n    If any of the handlers passed as arguments are subclasses of the\n    default handlers, the default handlers will not be used.\n    \"\"\"\n    import types\n    def isclass(obj):\n        return isinstance(obj, (types.ClassType, type))\n\n    opener = OpenerDirector()\n    default_classes = [ProxyHandler, UnknownHandler, HTTPHandler,\n                       HTTPDefaultErrorHandler, HTTPRedirectHandler,\n                       FTPHandler, FileHandler, HTTPErrorProcessor]\n    if hasattr(httplib, 'HTTPS'):\n        default_classes.append(HTTPSHandler)\n    skip = set()\n    for klass in default_classes:\n        for check in handlers:\n            if isclass(check):\n                if issubclass(check, klass):\n                    skip.add(klass)\n            elif isinstance(check, klass):\n                skip.add(klass)\n    for klass in skip:\n        default_classes.remove(klass)\n\n    for klass in default_classes:\n        opener.add_handler(klass())\n\n    for h in handlers:\n        if isclass(h):\n            h = h()\n        opener.add_handler(h)\n    return opener\n\nclass BaseHandler:\n    handler_order = 500\n\n    def add_parent(self, parent):\n        self.parent = parent\n\n    def close(self):\n        # Only exists for backwards compatibility\n        pass\n\n    def __lt__(self, other):\n        if not hasattr(other, \"handler_order\"):\n            # Try to preserve the old behavior of having custom classes\n            # inserted after default ones (works only for custom user\n            # classes which are not aware of handler_order).\n            return True\n        return self.handler_order < other.handler_order\n\n\nclass HTTPErrorProcessor(BaseHandler):\n    \"\"\"Process HTTP error responses.\"\"\"\n    handler_order = 1000  # after all other processing\n\n    def http_response(self, request, response):\n        code, msg, hdrs = response.code, response.msg, response.info()\n\n        # According to RFC 2616, \"2xx\" code indicates that the client's\n        # request was successfully received, understood, and accepted.\n        if not (200 <= code < 300):\n            response = self.parent.error(\n                'http', request, response, code, msg, hdrs)\n\n        return response\n\n    https_response = http_response\n\nclass HTTPDefaultErrorHandler(BaseHandler):\n    def http_error_default(self, req, fp, code, msg, hdrs):\n        raise HTTPError(req.get_full_url(), code, msg, hdrs, fp)\n\nclass HTTPRedirectHandler(BaseHandler):\n    # maximum number of redirections to any single URL\n    # this is needed because of the state that cookies introduce\n    max_repeats = 4\n    # maximum total number of redirections (regardless of URL) before\n    # assuming we're in a loop\n    max_redirections = 10\n\n    def redirect_request(self, req, fp, code, msg, headers, newurl):\n        \"\"\"Return a Request or None in response to a redirect.\n\n        This is called by the http_error_30x methods when a\n        redirection response is received.  If a redirection should\n        take place, return a new Request to allow http_error_30x to\n        perform the redirect.  Otherwise, raise HTTPError if no-one\n        else should try to handle this url.  Return None if you can't\n        but another Handler might.\n        \"\"\"\n        m = req.get_method()\n        if (code in (301, 302, 303, 307) and m in (\"GET\", \"HEAD\")\n            or code in (301, 302, 303) and m == \"POST\"):\n            # Strictly (according to RFC 2616), 301 or 302 in response\n            # to a POST MUST NOT cause a redirection without confirmation\n            # from the user (of urllib2, in this case).  In practice,\n            # essentially all clients do redirect in this case, so we\n            # do the same.\n            # be conciliant with URIs containing a space\n            newurl = newurl.replace(' ', '%20')\n            newheaders = dict((k,v) for k,v in req.headers.items()\n                              if k.lower() not in (\"content-length\", \"content-type\")\n                             )\n            return Request(newurl,\n                           headers=newheaders,\n                           origin_req_host=req.get_origin_req_host(),\n                           unverifiable=True)\n        else:\n            raise HTTPError(req.get_full_url(), code, msg, headers, fp)\n\n    # Implementation note: To avoid the server sending us into an\n    # infinite loop, the request object needs to track what URLs we\n    # have already seen.  Do this by adding a handler-specific\n    # attribute to the Request object.\n    def http_error_302(self, req, fp, code, msg, headers):\n        # Some servers (incorrectly) return multiple Location headers\n        # (so probably same goes for URI).  Use first header.\n        if 'location' in headers:\n            newurl = headers.getheaders('location')[0]\n        elif 'uri' in headers:\n            newurl = headers.getheaders('uri')[0]\n        else:\n            return\n\n        # fix a possible malformed URL\n        urlparts = urlparse.urlparse(newurl)\n        if not urlparts.path:\n            urlparts = list(urlparts)\n            urlparts[2] = \"/\"\n        newurl = urlparse.urlunparse(urlparts)\n\n        newurl = urlparse.urljoin(req.get_full_url(), newurl)\n\n        # For security reasons we do not allow redirects to protocols\n        # other than HTTP, HTTPS or FTP.\n        newurl_lower = newurl.lower()\n        if not (newurl_lower.startswith('http://') or\n                newurl_lower.startswith('https://') or\n                newurl_lower.startswith('ftp://')):\n            raise HTTPError(newurl, code,\n                            msg + \" - Redirection to url '%s' is not allowed\" %\n                            newurl,\n                            headers, fp)\n\n        # XXX Probably want to forget about the state of the current\n        # request, although that might interact poorly with other\n        # handlers that also use handler-specific request attributes\n        new = self.redirect_request(req, fp, code, msg, headers, newurl)\n        if new is None:\n            return\n\n        # loop detection\n        # .redirect_dict has a key url if url was previously visited.\n        if hasattr(req, 'redirect_dict'):\n            visited = new.redirect_dict = req.redirect_dict\n            if (visited.get(newurl, 0) >= self.max_repeats or\n                len(visited) >= self.max_redirections):\n                raise HTTPError(req.get_full_url(), code,\n                                self.inf_msg + msg, headers, fp)\n        else:\n            visited = new.redirect_dict = req.redirect_dict = {}\n        visited[newurl] = visited.get(newurl, 0) + 1\n\n        # Don't close the fp until we are sure that we won't use it\n        # with HTTPError.\n        fp.read()\n        fp.close()\n\n        return self.parent.open(new, timeout=req.timeout)\n\n    http_error_301 = http_error_303 = http_error_307 = http_error_302\n\n    inf_msg = \"The HTTP server returned a redirect error that would \" \\\n              \"lead to an infinite loop.\\n\" \\\n              \"The last 30x error message was:\\n\"\n\n\ndef _parse_proxy(proxy):\n    \"\"\"Return (scheme, user, password, host/port) given a URL or an authority.\n\n    If a URL is supplied, it must have an authority (host:port) component.\n    According to RFC 3986, having an authority component means the URL must\n    have two slashes after the scheme:\n\n    >>> _parse_proxy('file:/ftp.example.com/')\n    Traceback (most recent call last):\n    ValueError: proxy URL with no authority: 'file:/ftp.example.com/'\n\n    The first three items of the returned tuple may be None.\n\n    Examples of authority parsing:\n\n    >>> _parse_proxy('proxy.example.com')\n    (None, None, None, 'proxy.example.com')\n    >>> _parse_proxy('proxy.example.com:3128')\n    (None, None, None, 'proxy.example.com:3128')\n\n    The authority component may optionally include userinfo (assumed to be\n    username:password):\n\n    >>> _parse_proxy('joe:password@proxy.example.com')\n    (None, 'joe', 'password', 'proxy.example.com')\n    >>> _parse_proxy('joe:password@proxy.example.com:3128')\n    (None, 'joe', 'password', 'proxy.example.com:3128')\n\n    Same examples, but with URLs instead:\n\n    >>> _parse_proxy('http://proxy.example.com/')\n    ('http', None, None, 'proxy.example.com')\n    >>> _parse_proxy('http://proxy.example.com:3128/')\n    ('http', None, None, 'proxy.example.com:3128')\n    >>> _parse_proxy('http://joe:password@proxy.example.com/')\n    ('http', 'joe', 'password', 'proxy.example.com')\n    >>> _parse_proxy('http://joe:password@proxy.example.com:3128')\n    ('http', 'joe', 'password', 'proxy.example.com:3128')\n\n    Everything after the authority is ignored:\n\n    >>> _parse_proxy('ftp://joe:password@proxy.example.com/rubbish:3128')\n    ('ftp', 'joe', 'password', 'proxy.example.com')\n\n    Test for no trailing '/' case:\n\n    >>> _parse_proxy('http://joe:password@proxy.example.com')\n    ('http', 'joe', 'password', 'proxy.example.com')\n\n    \"\"\"\n    scheme, r_scheme = splittype(proxy)\n    if not r_scheme.startswith(\"/\"):\n        # authority\n        scheme = None\n        authority = proxy\n    else:\n        # URL\n        if not r_scheme.startswith(\"//\"):\n            raise ValueError(\"proxy URL with no authority: %r\" % proxy)\n        # We have an authority, so for RFC 3986-compliant URLs (by ss 3.\n        # and 3.3.), path is empty or starts with '/'\n        end = r_scheme.find(\"/\", 2)\n        if end == -1:\n            end = None\n        authority = r_scheme[2:end]\n    userinfo, hostport = splituser(authority)\n    if userinfo is not None:\n        user, password = splitpasswd(userinfo)\n    else:\n        user = password = None\n    return scheme, user, password, hostport\n\nclass ProxyHandler(BaseHandler):\n    # Proxies must be in front\n    handler_order = 100\n\n    def __init__(self, proxies=None):\n        if proxies is None:\n            proxies = getproxies()\n        assert hasattr(proxies, 'has_key'), \"proxies must be a mapping\"\n        self.proxies = proxies\n        for type, url in proxies.items():\n            setattr(self, '%s_open' % type,\n                    lambda r, proxy=url, type=type, meth=self.proxy_open: \\\n                    meth(r, proxy, type))\n\n    def proxy_open(self, req, proxy, type):\n        orig_type = req.get_type()\n        proxy_type, user, password, hostport = _parse_proxy(proxy)\n\n        if proxy_type is None:\n            proxy_type = orig_type\n\n        if req.host and proxy_bypass(req.host):\n            return None\n\n        if user and password:\n            user_pass = '%s:%s' % (unquote(user), unquote(password))\n            creds = base64.b64encode(user_pass).strip()\n            req.add_header('Proxy-authorization', 'Basic ' + creds)\n        hostport = unquote(hostport)\n        req.set_proxy(hostport, proxy_type)\n\n        if orig_type == proxy_type or orig_type == 'https':\n            # let other handlers take care of it\n            return None\n        else:\n            # need to start over, because the other handlers don't\n            # grok the proxy's URL type\n            # e.g. if we have a constructor arg proxies like so:\n            # {'http': 'ftp://proxy.example.com'}, we may end up turning\n            # a request for http://acme.example.com/a into one for\n            # ftp://proxy.example.com/a\n            return self.parent.open(req, timeout=req.timeout)\n\nclass HTTPPasswordMgr:\n\n    def __init__(self):\n        self.passwd = {}\n\n    def add_password(self, realm, uri, user, passwd):\n        # uri could be a single URI or a sequence\n        if isinstance(uri, basestring):\n            uri = [uri]\n        if not realm in self.passwd:\n            self.passwd[realm] = {}\n        for default_port in True, False:\n            reduced_uri = tuple(\n                [self.reduce_uri(u, default_port) for u in uri])\n            self.passwd[realm][reduced_uri] = (user, passwd)\n\n    def find_user_password(self, realm, authuri):\n        domains = self.passwd.get(realm, {})\n        for default_port in True, False:\n            reduced_authuri = self.reduce_uri(authuri, default_port)\n            for uris, authinfo in domains.iteritems():\n                for uri in uris:\n                    if self.is_suburi(uri, reduced_authuri):\n                        return authinfo\n        return None, None\n\n    def reduce_uri(self, uri, default_port=True):\n        \"\"\"Accept authority or URI and extract only the authority and path.\"\"\"\n        # note HTTP URLs do not have a userinfo component\n        parts = urlparse.urlsplit(uri)\n        if parts[1]:\n            # URI\n            scheme = parts[0]\n            authority = parts[1]\n            path = parts[2] or '/'\n        else:\n            # host or host:port\n            scheme = None\n            authority = uri\n            path = '/'\n        host, port = splitport(authority)\n        if default_port and port is None and scheme is not None:\n            dport = {\"http\": 80,\n                     \"https\": 443,\n                     }.get(scheme)\n            if dport is not None:\n                authority = \"%s:%d\" % (host, dport)\n        return authority, path\n\n    def is_suburi(self, base, test):\n        \"\"\"Check if test is below base in a URI tree\n\n        Both args must be URIs in reduced form.\n        \"\"\"\n        if base == test:\n            return True\n        if base[0] != test[0]:\n            return False\n        common = posixpath.commonprefix((base[1], test[1]))\n        if len(common) == len(base[1]):\n            return True\n        return False\n\n\nclass HTTPPasswordMgrWithDefaultRealm(HTTPPasswordMgr):\n\n    def find_user_password(self, realm, authuri):\n        user, password = HTTPPasswordMgr.find_user_password(self, realm,\n                                                            authuri)\n        if user is not None:\n            return user, password\n        return HTTPPasswordMgr.find_user_password(self, None, authuri)\n\n\nclass AbstractBasicAuthHandler:\n\n    # XXX this allows for multiple auth-schemes, but will stupidly pick\n    # the last one with a realm specified.\n\n    # allow for double- and single-quoted realm values\n    # (single quotes are a violation of the RFC, but appear in the wild)\n    rx = re.compile('(?:.*,)*[ \\t]*([^ \\t]+)[ \\t]+'\n                    'realm=([\"\\']?)([^\"\\']*)\\\\2', re.I)\n\n    # XXX could pre-emptively send auth info already accepted (RFC 2617,\n    # end of section 2, and section 1.2 immediately after \"credentials\"\n    # production).\n\n    def __init__(self, password_mgr=None):\n        if password_mgr is None:\n            password_mgr = HTTPPasswordMgr()\n        self.passwd = password_mgr\n        self.add_password = self.passwd.add_password\n\n\n    def http_error_auth_reqed(self, authreq, host, req, headers):\n        # host may be an authority (without userinfo) or a URL with an\n        # authority\n        # XXX could be multiple headers\n        authreq = headers.get(authreq, None)\n\n        if authreq:\n            mo = AbstractBasicAuthHandler.rx.search(authreq)\n            if mo:\n                scheme, quote, realm = mo.groups()\n                if quote not in ['\"', \"'\"]:\n                    warnings.warn(\"Basic Auth Realm was unquoted\",\n                                  UserWarning, 2)\n                if scheme.lower() == 'basic':\n                    return self.retry_http_basic_auth(host, req, realm)\n\n    def retry_http_basic_auth(self, host, req, realm):\n        user, pw = self.passwd.find_user_password(realm, host)\n        if pw is not None:\n            raw = \"%s:%s\" % (user, pw)\n            auth = 'Basic %s' % base64.b64encode(raw).strip()\n            if req.get_header(self.auth_header, None) == auth:\n                return None\n            req.add_unredirected_header(self.auth_header, auth)\n            return self.parent.open(req, timeout=req.timeout)\n        else:\n            return None\n\n\nclass HTTPBasicAuthHandler(AbstractBasicAuthHandler, BaseHandler):\n\n    auth_header = 'Authorization'\n\n    def http_error_401(self, req, fp, code, msg, headers):\n        url = req.get_full_url()\n        response = self.http_error_auth_reqed('www-authenticate',\n                                              url, req, headers)\n        return response\n\n\nclass ProxyBasicAuthHandler(AbstractBasicAuthHandler, BaseHandler):\n\n    auth_header = 'Proxy-authorization'\n\n    def http_error_407(self, req, fp, code, msg, headers):\n        # http_error_auth_reqed requires that there is no userinfo component in\n        # authority.  Assume there isn't one, since urllib2 does not (and\n        # should not, RFC 3986 s. 3.2.1) support requests for URLs containing\n        # userinfo.\n        authority = req.get_host()\n        response = self.http_error_auth_reqed('proxy-authenticate',\n                                          authority, req, headers)\n        return response\n\n\ndef randombytes(n):\n    \"\"\"Return n random bytes.\"\"\"\n    # Use /dev/urandom if it is available.  Fall back to random module\n    # if not.  It might be worthwhile to extend this function to use\n    # other platform-specific mechanisms for getting random bytes.\n    if os.path.exists(\"/dev/urandom\"):\n        f = open(\"/dev/urandom\")\n        s = f.read(n)\n        f.close()\n        return s\n    else:\n        L = [chr(random.randrange(0, 256)) for i in range(n)]\n        return \"\".join(L)\n\nclass AbstractDigestAuthHandler:\n    # Digest authentication is specified in RFC 2617.\n\n    # XXX The client does not inspect the Authentication-Info header\n    # in a successful response.\n\n    # XXX It should be possible to test this implementation against\n    # a mock server that just generates a static set of challenges.\n\n    # XXX qop=\"auth-int\" supports is shaky\n\n    def __init__(self, passwd=None):\n        if passwd is None:\n            passwd = HTTPPasswordMgr()\n        self.passwd = passwd\n        self.add_password = self.passwd.add_password\n        self.retried = 0\n        self.nonce_count = 0\n        self.last_nonce = None\n\n    def reset_retry_count(self):\n        self.retried = 0\n\n    def http_error_auth_reqed(self, auth_header, host, req, headers):\n        authreq = headers.get(auth_header, None)\n        if self.retried > 5:\n            # Don't fail endlessly - if we failed once, we'll probably\n            # fail a second time. Hm. Unless the Password Manager is\n            # prompting for the information. Crap. This isn't great\n            # but it's better than the current 'repeat until recursion\n            # depth exceeded' approach <wink>\n            raise HTTPError(req.get_full_url(), 401, \"digest auth failed\",\n                            headers, None)\n        else:\n            self.retried += 1\n        if authreq:\n            scheme = authreq.split()[0]\n            if scheme.lower() == 'digest':\n                return self.retry_http_digest_auth(req, authreq)\n\n    def retry_http_digest_auth(self, req, auth):\n        token, challenge = auth.split(' ', 1)\n        chal = parse_keqv_list(parse_http_list(challenge))\n        auth = self.get_authorization(req, chal)\n        if auth:\n            auth_val = 'Digest %s' % auth\n            if req.headers.get(self.auth_header, None) == auth_val:\n                return None\n            req.add_unredirected_header(self.auth_header, auth_val)\n            resp = self.parent.open(req, timeout=req.timeout)\n            return resp\n\n    def get_cnonce(self, nonce):\n        # The cnonce-value is an opaque\n        # quoted string value provided by the client and used by both client\n        # and server to avoid chosen plaintext attacks, to provide mutual\n        # authentication, and to provide some message integrity protection.\n        # This isn't a fabulous effort, but it's probably Good Enough.\n        dig = hashlib.sha1(\"%s:%s:%s:%s\" % (self.nonce_count, nonce, time.ctime(),\n                                            randombytes(8))).hexdigest()\n        return dig[:16]\n\n    def get_authorization(self, req, chal):\n        try:\n            realm = chal['realm']\n            nonce = chal['nonce']\n            qop = chal.get('qop')\n            algorithm = chal.get('algorithm', 'MD5')\n            # mod_digest doesn't send an opaque, even though it isn't\n            # supposed to be optional\n            opaque = chal.get('opaque', None)\n        except KeyError:\n            return None\n\n        H, KD = self.get_algorithm_impls(algorithm)\n        if H is None:\n            return None\n\n        user, pw = self.passwd.find_user_password(realm, req.get_full_url())\n        if user is None:\n            return None\n\n        # XXX not implemented yet\n        if req.has_data():\n            entdig = self.get_entity_digest(req.get_data(), chal)\n        else:\n            entdig = None\n\n        A1 = \"%s:%s:%s\" % (user, realm, pw)\n        A2 = \"%s:%s\" % (req.get_method(),\n                        # XXX selector: what about proxies and full urls\n                        req.get_selector())\n        if qop == 'auth':\n            if nonce == self.last_nonce:\n                self.nonce_count += 1\n            else:\n                self.nonce_count = 1\n                self.last_nonce = nonce\n\n            ncvalue = '%08x' % self.nonce_count\n            cnonce = self.get_cnonce(nonce)\n            noncebit = \"%s:%s:%s:%s:%s\" % (nonce, ncvalue, cnonce, qop, H(A2))\n            respdig = KD(H(A1), noncebit)\n        elif qop is None:\n            respdig = KD(H(A1), \"%s:%s\" % (nonce, H(A2)))\n        else:\n            # XXX handle auth-int.\n            raise URLError(\"qop '%s' is not supported.\" % qop)\n\n        # XXX should the partial digests be encoded too?\n\n        base = 'username=\"%s\", realm=\"%s\", nonce=\"%s\", uri=\"%s\", ' \\\n               'response=\"%s\"' % (user, realm, nonce, req.get_selector(),\n                                  respdig)\n        if opaque:\n            base += ', opaque=\"%s\"' % opaque\n        if entdig:\n            base += ', digest=\"%s\"' % entdig\n        base += ', algorithm=\"%s\"' % algorithm\n        if qop:\n            base += ', qop=auth, nc=%s, cnonce=\"%s\"' % (ncvalue, cnonce)\n        return base\n\n    def get_algorithm_impls(self, algorithm):\n        # algorithm should be case-insensitive according to RFC2617\n        algorithm = algorithm.upper()\n        # lambdas assume digest modules are imported at the top level\n        if algorithm == 'MD5':\n            H = lambda x: hashlib.md5(x).hexdigest()\n        elif algorithm == 'SHA':\n            H = lambda x: hashlib.sha1(x).hexdigest()\n        # XXX MD5-sess\n        KD = lambda s, d: H(\"%s:%s\" % (s, d))\n        return H, KD\n\n    def get_entity_digest(self, data, chal):\n        # XXX not implemented yet\n        return None\n\n\nclass HTTPDigestAuthHandler(BaseHandler, AbstractDigestAuthHandler):\n    \"\"\"An authentication protocol defined by RFC 2069\n\n    Digest authentication improves on basic authentication because it\n    does not transmit passwords in the clear.\n    \"\"\"\n\n    auth_header = 'Authorization'\n    handler_order = 490  # before Basic auth\n\n    def http_error_401(self, req, fp, code, msg, headers):\n        host = urlparse.urlparse(req.get_full_url())[1]\n        retry = self.http_error_auth_reqed('www-authenticate',\n                                           host, req, headers)\n        self.reset_retry_count()\n        return retry\n\n\nclass ProxyDigestAuthHandler(BaseHandler, AbstractDigestAuthHandler):\n\n    auth_header = 'Proxy-Authorization'\n    handler_order = 490  # before Basic auth\n\n    def http_error_407(self, req, fp, code, msg, headers):\n        host = req.get_host()\n        retry = self.http_error_auth_reqed('proxy-authenticate',\n                                           host, req, headers)\n        self.reset_retry_count()\n        return retry\n\nclass AbstractHTTPHandler(BaseHandler):\n\n    def __init__(self, debuglevel=0):\n        self._debuglevel = debuglevel\n\n    def set_http_debuglevel(self, level):\n        self._debuglevel = level\n\n    def do_request_(self, request):\n        host = request.get_host()\n        if not host:\n            raise URLError('no host given')\n\n        if request.has_data():  # POST\n            data = request.get_data()\n            if not request.has_header('Content-type'):\n                request.add_unredirected_header(\n                    'Content-type',\n                    'application/x-www-form-urlencoded')\n            if not request.has_header('Content-length'):\n                request.add_unredirected_header(\n                    'Content-length', '%d' % len(data))\n\n        sel_host = host\n        if request.has_proxy():\n            scheme, sel = splittype(request.get_selector())\n            sel_host, sel_path = splithost(sel)\n\n        if not request.has_header('Host'):\n            request.add_unredirected_header('Host', sel_host)\n        for name, value in self.parent.addheaders:\n            name = name.capitalize()\n            if not request.has_header(name):\n                request.add_unredirected_header(name, value)\n\n        return request\n\n    def do_open(self, http_class, req, **http_conn_args):\n        \"\"\"Return an addinfourl object for the request, using http_class.\n\n        http_class must implement the HTTPConnection API from httplib.\n        The addinfourl return value is a file-like object.  It also\n        has methods and attributes including:\n            - info(): return a mimetools.Message object for the headers\n            - geturl(): return the original request URL\n            - code: HTTP status code\n        \"\"\"\n        host = req.get_host()\n        if not host:\n            raise URLError('no host given')\n\n        # will parse host:port\n        h = http_class(host, timeout=req.timeout, **http_conn_args)\n        h.set_debuglevel(self._debuglevel)\n\n        headers = dict(req.unredirected_hdrs)\n        headers.update(dict((k, v) for k, v in req.headers.items()\n                            if k not in headers))\n\n        # We want to make an HTTP/1.1 request, but the addinfourl\n        # class isn't prepared to deal with a persistent connection.\n        # It will try to read all remaining data from the socket,\n        # which will block while the server waits for the next request.\n        # So make sure the connection gets closed after the (only)\n        # request.\n        headers[\"Connection\"] = \"close\"\n        headers = dict(\n            (name.title(), val) for name, val in headers.items())\n\n        if req._tunnel_host:\n            tunnel_headers = {}\n            proxy_auth_hdr = \"Proxy-Authorization\"\n            if proxy_auth_hdr in headers:\n                tunnel_headers[proxy_auth_hdr] = headers[proxy_auth_hdr]\n                # Proxy-Authorization should not be sent to origin\n                # server.\n                del headers[proxy_auth_hdr]\n            h.set_tunnel(req._tunnel_host, headers=tunnel_headers)\n\n        try:\n            h.request(req.get_method(), req.get_selector(), req.data, headers)\n        except socket.error, err: # XXX what error?\n            h.close()\n            raise URLError(err)\n        else:\n            try:\n                r = h.getresponse(buffering=True)\n            except TypeError: # buffering kw not supported\n                r = h.getresponse()\n\n        # Pick apart the HTTPResponse object to get the addinfourl\n        # object initialized properly.\n\n        # Wrap the HTTPResponse object in socket's file object adapter\n        # for Windows.  That adapter calls recv(), so delegate recv()\n        # to read().  This weird wrapping allows the returned object to\n        # have readline() and readlines() methods.\n\n        # XXX It might be better to extract the read buffering code\n        # out of socket._fileobject() and into a base class.\n\n        r.recv = r.read\n        r._reuse = lambda: None\n        r._drop = lambda: None\n        fp = socket._fileobject(r, close=True)\n\n        resp = addinfourl(fp, r.msg, req.get_full_url())\n        resp.code = r.status\n        resp.msg = r.reason\n        return resp\n\n\nclass HTTPHandler(AbstractHTTPHandler):\n\n    def http_open(self, req):\n        return self.do_open(httplib.HTTPConnection, req)\n\n    http_request = AbstractHTTPHandler.do_request_\n\nif hasattr(httplib, 'HTTPS'):\n    class HTTPSHandler(AbstractHTTPHandler):\n\n        def __init__(self, debuglevel=0, context=None):\n            AbstractHTTPHandler.__init__(self, debuglevel)\n            self._context = context\n\n        def https_open(self, req):\n            return self.do_open(httplib.HTTPSConnection, req,\n                context=self._context)\n\n        https_request = AbstractHTTPHandler.do_request_\n\nclass HTTPCookieProcessor(BaseHandler):\n    def __init__(self, cookiejar=None):\n        import cookielib\n        if cookiejar is None:\n            cookiejar = cookielib.CookieJar()\n        self.cookiejar = cookiejar\n\n    def http_request(self, request):\n        self.cookiejar.add_cookie_header(request)\n        return request\n\n    def http_response(self, request, response):\n        self.cookiejar.extract_cookies(response, request)\n        return response\n\n    https_request = http_request\n    https_response = http_response\n\nclass UnknownHandler(BaseHandler):\n    def unknown_open(self, req):\n        type = req.get_type()\n        raise URLError('unknown url type: %s' % type)\n\ndef parse_keqv_list(l):\n    \"\"\"Parse list of key=value strings where keys are not duplicated.\"\"\"\n    parsed = {}\n    for elt in l:\n        k, v = elt.split('=', 1)\n        if v[0] == '\"' and v[-1] == '\"':\n            v = v[1:-1]\n        parsed[k] = v\n    return parsed\n\ndef parse_http_list(s):\n    \"\"\"Parse lists as described by RFC 2068 Section 2.\n\n    In particular, parse comma-separated lists where the elements of\n    the list may include quoted-strings.  A quoted-string could\n    contain a comma.  A non-quoted string could have quotes in the\n    middle.  Neither commas nor quotes count if they are escaped.\n    Only double-quotes count, not single-quotes.\n    \"\"\"\n    res = []\n    part = ''\n\n    escape = quote = False\n    for cur in s:\n        if escape:\n            part += cur\n            escape = False\n            continue\n        if quote:\n            if cur == '\\\\':\n                escape = True\n                continue\n            elif cur == '\"':\n                quote = False\n            part += cur\n            continue\n\n        if cur == ',':\n            res.append(part)\n            part = ''\n            continue\n\n        if cur == '\"':\n            quote = True\n\n        part += cur\n\n    # append last part\n    if part:\n        res.append(part)\n\n    return [part.strip() for part in res]\n\ndef _safe_gethostbyname(host):\n    try:\n        return socket.gethostbyname(host)\n    except socket.gaierror:\n        return None\n\nclass FileHandler(BaseHandler):\n    # Use local file or FTP depending on form of URL\n    def file_open(self, req):\n        url = req.get_selector()\n        if url[:2] == '//' and url[2:3] != '/' and (req.host and\n                req.host != 'localhost'):\n            req.type = 'ftp'\n            return self.parent.open(req)\n        else:\n            return self.open_local_file(req)\n\n    # names for the localhost\n    names = None\n    def get_names(self):\n        if FileHandler.names is None:\n            try:\n                FileHandler.names = tuple(\n                    socket.gethostbyname_ex('localhost')[2] +\n                    socket.gethostbyname_ex(socket.gethostname())[2])\n            except socket.gaierror:\n                FileHandler.names = (socket.gethostbyname('localhost'),)\n        return FileHandler.names\n\n    # not entirely sure what the rules are here\n    def open_local_file(self, req):\n        import email.utils\n        import mimetypes\n        host = req.get_host()\n        filename = req.get_selector()\n        localfile = url2pathname(filename)\n        try:\n            stats = os.stat(localfile)\n            size = stats.st_size\n            modified = email.utils.formatdate(stats.st_mtime, usegmt=True)\n            mtype = mimetypes.guess_type(filename)[0]\n            headers = mimetools.Message(StringIO(\n                'Content-type: %s\\nContent-length: %d\\nLast-modified: %s\\n' %\n                (mtype or 'text/plain', size, modified)))\n            if host:\n                host, port = splitport(host)\n            if not host or \\\n                (not port and _safe_gethostbyname(host) in self.get_names()):\n                if host:\n                    origurl = 'file://' + host + filename\n                else:\n                    origurl = 'file://' + filename\n                return addinfourl(open(localfile, 'rb'), headers, origurl)\n        except OSError, msg:\n            # urllib2 users shouldn't expect OSErrors coming from urlopen()\n            raise URLError(msg)\n        raise URLError('file not on local host')\n\nclass FTPHandler(BaseHandler):\n    def ftp_open(self, req):\n        import ftplib\n        import mimetypes\n        host = req.get_host()\n        if not host:\n            raise URLError('ftp error: no host given')\n        host, port = splitport(host)\n        if port is None:\n            port = ftplib.FTP_PORT\n        else:\n            port = int(port)\n\n        # username/password handling\n        user, host = splituser(host)\n        if user:\n            user, passwd = splitpasswd(user)\n        else:\n            passwd = None\n        host = unquote(host)\n        user = user or ''\n        passwd = passwd or ''\n\n        try:\n            host = socket.gethostbyname(host)\n        except socket.error, msg:\n            raise URLError(msg)\n        path, attrs = splitattr(req.get_selector())\n        dirs = path.split('/')\n        dirs = map(unquote, dirs)\n        dirs, file = dirs[:-1], dirs[-1]\n        if dirs and not dirs[0]:\n            dirs = dirs[1:]\n        try:\n            fw = self.connect_ftp(user, passwd, host, port, dirs, req.timeout)\n            type = file and 'I' or 'D'\n            for attr in attrs:\n                attr, value = splitvalue(attr)\n                if attr.lower() == 'type' and \\\n                   value in ('a', 'A', 'i', 'I', 'd', 'D'):\n                    type = value.upper()\n            fp, retrlen = fw.retrfile(file, type)\n            headers = \"\"\n            mtype = mimetypes.guess_type(req.get_full_url())[0]\n            if mtype:\n                headers += \"Content-type: %s\\n\" % mtype\n            if retrlen is not None and retrlen >= 0:\n                headers += \"Content-length: %d\\n\" % retrlen\n            sf = StringIO(headers)\n            headers = mimetools.Message(sf)\n            return addinfourl(fp, headers, req.get_full_url())\n        except ftplib.all_errors, msg:\n            raise URLError, ('ftp error: %s' % msg), sys.exc_info()[2]\n\n    def connect_ftp(self, user, passwd, host, port, dirs, timeout):\n        fw = ftpwrapper(user, passwd, host, port, dirs, timeout,\n                        persistent=False)\n##        fw.ftp.set_debuglevel(1)\n        return fw\n\nclass CacheFTPHandler(FTPHandler):\n    # XXX would be nice to have pluggable cache strategies\n    # XXX this stuff is definitely not thread safe\n    def __init__(self):\n        self.cache = {}\n        self.timeout = {}\n        self.soonest = 0\n        self.delay = 60\n        self.max_conns = 16\n\n    def setTimeout(self, t):\n        self.delay = t\n\n    def setMaxConns(self, m):\n        self.max_conns = m\n\n    def connect_ftp(self, user, passwd, host, port, dirs, timeout):\n        key = user, host, port, '/'.join(dirs), timeout\n        if key in self.cache:\n            self.timeout[key] = time.time() + self.delay\n        else:\n            self.cache[key] = ftpwrapper(user, passwd, host, port, dirs, timeout)\n            self.timeout[key] = time.time() + self.delay\n        self.check_cache()\n        return self.cache[key]\n\n    def check_cache(self):\n        # first check for old ones\n        t = time.time()\n        if self.soonest <= t:\n            for k, v in self.timeout.items():\n                if v < t:\n                    self.cache[k].close()\n                    del self.cache[k]\n                    del self.timeout[k]\n        self.soonest = min(self.timeout.values())\n\n        # then check the size\n        if len(self.cache) == self.max_conns:\n            for k, v in self.timeout.items():\n                if v == self.soonest:\n                    del self.cache[k]\n                    del self.timeout[k]\n                    break\n            self.soonest = min(self.timeout.values())\n\n    def clear_cache(self):\n        for conn in self.cache.values():\n            conn.close()\n        self.cache.clear()\n        self.timeout.clear()\n", 
    "urlparse": "\"\"\"Parse (absolute and relative) URLs.\n\nurlparse module is based upon the following RFC specifications.\n\nRFC 3986 (STD66): \"Uniform Resource Identifiers\" by T. Berners-Lee, R. Fielding\nand L.  Masinter, January 2005.\n\nRFC 2732 : \"Format for Literal IPv6 Addresses in URL's by R.Hinden, B.Carpenter\nand L.Masinter, December 1999.\n\nRFC 2396:  \"Uniform Resource Identifiers (URI)\": Generic Syntax by T.\nBerners-Lee, R. Fielding, and L. Masinter, August 1998.\n\nRFC 2368: \"The mailto URL scheme\", by P.Hoffman , L Masinter, J. Zwinski, July 1998.\n\nRFC 1808: \"Relative Uniform Resource Locators\", by R. Fielding, UC Irvine, June\n1995.\n\nRFC 1738: \"Uniform Resource Locators (URL)\" by T. Berners-Lee, L. Masinter, M.\nMcCahill, December 1994\n\nRFC 3986 is considered the current standard and any future changes to\nurlparse module should conform with it.  The urlparse module is\ncurrently not entirely compliant with this RFC due to defacto\nscenarios for parsing, and for backward compatibility purposes, some\nparsing quirks from older RFCs are retained. The testcases in\ntest_urlparse.py provides a good indicator of parsing behavior.\n\n\"\"\"\n\nimport re\n\n__all__ = [\"urlparse\", \"urlunparse\", \"urljoin\", \"urldefrag\",\n           \"urlsplit\", \"urlunsplit\", \"parse_qs\", \"parse_qsl\"]\n\n# A classification of schemes ('' means apply by default)\nuses_relative = ['ftp', 'http', 'gopher', 'nntp', 'imap',\n                 'wais', 'file', 'https', 'shttp', 'mms',\n                 'prospero', 'rtsp', 'rtspu', '', 'sftp',\n                 'svn', 'svn+ssh']\nuses_netloc = ['ftp', 'http', 'gopher', 'nntp', 'telnet',\n               'imap', 'wais', 'file', 'mms', 'https', 'shttp',\n               'snews', 'prospero', 'rtsp', 'rtspu', 'rsync', '',\n               'svn', 'svn+ssh', 'sftp','nfs','git', 'git+ssh']\nuses_params = ['ftp', 'hdl', 'prospero', 'http', 'imap',\n               'https', 'shttp', 'rtsp', 'rtspu', 'sip', 'sips',\n               'mms', '', 'sftp', 'tel']\n\n# These are not actually used anymore, but should stay for backwards\n# compatibility.  (They are undocumented, but have a public-looking name.)\nnon_hierarchical = ['gopher', 'hdl', 'mailto', 'news',\n                    'telnet', 'wais', 'imap', 'snews', 'sip', 'sips']\nuses_query = ['http', 'wais', 'imap', 'https', 'shttp', 'mms',\n              'gopher', 'rtsp', 'rtspu', 'sip', 'sips', '']\nuses_fragment = ['ftp', 'hdl', 'http', 'gopher', 'news',\n                 'nntp', 'wais', 'https', 'shttp', 'snews',\n                 'file', 'prospero', '']\n\n# Characters valid in scheme names\nscheme_chars = ('abcdefghijklmnopqrstuvwxyz'\n                'ABCDEFGHIJKLMNOPQRSTUVWXYZ'\n                '0123456789'\n                '+-.')\n\nMAX_CACHE_SIZE = 20\n_parse_cache = {}\n\ndef clear_cache():\n    \"\"\"Clear the parse cache.\"\"\"\n    _parse_cache.clear()\n\n\nclass ResultMixin(object):\n    \"\"\"Shared methods for the parsed result objects.\"\"\"\n\n    @property\n    def username(self):\n        netloc = self.netloc\n        if \"@\" in netloc:\n            userinfo = netloc.rsplit(\"@\", 1)[0]\n            if \":\" in userinfo:\n                userinfo = userinfo.split(\":\", 1)[0]\n            return userinfo\n        return None\n\n    @property\n    def password(self):\n        netloc = self.netloc\n        if \"@\" in netloc:\n            userinfo = netloc.rsplit(\"@\", 1)[0]\n            if \":\" in userinfo:\n                return userinfo.split(\":\", 1)[1]\n        return None\n\n    @property\n    def hostname(self):\n        netloc = self.netloc.split('@')[-1]\n        if '[' in netloc and ']' in netloc:\n            return netloc.split(']')[0][1:].lower()\n        elif ':' in netloc:\n            return netloc.split(':')[0].lower()\n        elif netloc == '':\n            return None\n        else:\n            return netloc.lower()\n\n    @property\n    def port(self):\n        netloc = self.netloc.split('@')[-1].split(']')[-1]\n        if ':' in netloc:\n            port = netloc.split(':')[1]\n            if port:\n                port = int(port, 10)\n                # verify legal port\n                if (0 <= port <= 65535):\n                    return port\n        return None\n\nfrom collections import namedtuple\n\nclass SplitResult(namedtuple('SplitResult', 'scheme netloc path query fragment'), ResultMixin):\n\n    __slots__ = ()\n\n    def geturl(self):\n        return urlunsplit(self)\n\n\nclass ParseResult(namedtuple('ParseResult', 'scheme netloc path params query fragment'), ResultMixin):\n\n    __slots__ = ()\n\n    def geturl(self):\n        return urlunparse(self)\n\n\ndef urlparse(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 6 components:\n    <scheme>://<netloc>/<path>;<params>?<query>#<fragment>\n    Return a 6-tuple: (scheme, netloc, path, params, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    tuple = urlsplit(url, scheme, allow_fragments)\n    scheme, netloc, url, query, fragment = tuple\n    if scheme in uses_params and ';' in url:\n        url, params = _splitparams(url)\n    else:\n        params = ''\n    return ParseResult(scheme, netloc, url, params, query, fragment)\n\ndef _splitparams(url):\n    if '/'  in url:\n        i = url.find(';', url.rfind('/'))\n        if i < 0:\n            return url, ''\n    else:\n        i = url.find(';')\n    return url[:i], url[i+1:]\n\ndef _splitnetloc(url, start=0):\n    delim = len(url)   # position of end of domain part of url, default is end\n    for c in '/?#':    # look for delimiters; the order is NOT important\n        wdelim = url.find(c, start)        # find first of this delim\n        if wdelim >= 0:                    # if found\n            delim = min(delim, wdelim)     # use earliest delim position\n    return url[start:delim], url[delim:]   # return (domain, rest)\n\ndef urlsplit(url, scheme='', allow_fragments=True):\n    \"\"\"Parse a URL into 5 components:\n    <scheme>://<netloc>/<path>?<query>#<fragment>\n    Return a 5-tuple: (scheme, netloc, path, query, fragment).\n    Note that we don't break the components up in smaller bits\n    (e.g. netloc is a single string) and we don't expand % escapes.\"\"\"\n    allow_fragments = bool(allow_fragments)\n    key = url, scheme, allow_fragments, type(url), type(scheme)\n    cached = _parse_cache.get(key, None)\n    if cached:\n        return cached\n    if len(_parse_cache) >= MAX_CACHE_SIZE: # avoid runaway growth\n        clear_cache()\n    netloc = query = fragment = ''\n    i = url.find(':')\n    if i > 0:\n        if url[:i] == 'http': # optimize the common case\n            scheme = url[:i].lower()\n            url = url[i+1:]\n            if url[:2] == '//':\n                netloc, url = _splitnetloc(url, 2)\n                if (('[' in netloc and ']' not in netloc) or\n                        (']' in netloc and '[' not in netloc)):\n                    raise ValueError(\"Invalid IPv6 URL\")\n            if allow_fragments and '#' in url:\n                url, fragment = url.split('#', 1)\n            if '?' in url:\n                url, query = url.split('?', 1)\n            v = SplitResult(scheme, netloc, url, query, fragment)\n            _parse_cache[key] = v\n            return v\n        for c in url[:i]:\n            if c not in scheme_chars:\n                break\n        else:\n            # make sure \"url\" is not actually a port number (in which case\n            # \"scheme\" is really part of the path)\n            rest = url[i+1:]\n            if not rest or any(c not in '0123456789' for c in rest):\n                # not a port number\n                scheme, url = url[:i].lower(), rest\n\n    if url[:2] == '//':\n        netloc, url = _splitnetloc(url, 2)\n        if (('[' in netloc and ']' not in netloc) or\n                (']' in netloc and '[' not in netloc)):\n            raise ValueError(\"Invalid IPv6 URL\")\n    if allow_fragments and '#' in url:\n        url, fragment = url.split('#', 1)\n    if '?' in url:\n        url, query = url.split('?', 1)\n    v = SplitResult(scheme, netloc, url, query, fragment)\n    _parse_cache[key] = v\n    return v\n\ndef urlunparse(data):\n    \"\"\"Put a parsed URL back together again.  This may result in a\n    slightly different, but equivalent URL, if the URL that was parsed\n    originally had redundant delimiters, e.g. a ? with an empty query\n    (the draft states that these are equivalent).\"\"\"\n    scheme, netloc, url, params, query, fragment = data\n    if params:\n        url = \"%s;%s\" % (url, params)\n    return urlunsplit((scheme, netloc, url, query, fragment))\n\ndef urlunsplit(data):\n    \"\"\"Combine the elements of a tuple as returned by urlsplit() into a\n    complete URL as a string. The data argument can be any five-item iterable.\n    This may result in a slightly different, but equivalent URL, if the URL that\n    was parsed originally had unnecessary delimiters (for example, a ? with an\n    empty query; the RFC states that these are equivalent).\"\"\"\n    scheme, netloc, url, query, fragment = data\n    if netloc or (scheme and scheme in uses_netloc and url[:2] != '//'):\n        if url and url[:1] != '/': url = '/' + url\n        url = '//' + (netloc or '') + url\n    if scheme:\n        url = scheme + ':' + url\n    if query:\n        url = url + '?' + query\n    if fragment:\n        url = url + '#' + fragment\n    return url\n\ndef urljoin(base, url, allow_fragments=True):\n    \"\"\"Join a base URL and a possibly relative URL to form an absolute\n    interpretation of the latter.\"\"\"\n    if not base:\n        return url\n    if not url:\n        return base\n    bscheme, bnetloc, bpath, bparams, bquery, bfragment = \\\n            urlparse(base, '', allow_fragments)\n    scheme, netloc, path, params, query, fragment = \\\n            urlparse(url, bscheme, allow_fragments)\n    if scheme != bscheme or scheme not in uses_relative:\n        return url\n    if scheme in uses_netloc:\n        if netloc:\n            return urlunparse((scheme, netloc, path,\n                               params, query, fragment))\n        netloc = bnetloc\n    if path[:1] == '/':\n        return urlunparse((scheme, netloc, path,\n                           params, query, fragment))\n    if not path and not params:\n        path = bpath\n        params = bparams\n        if not query:\n            query = bquery\n        return urlunparse((scheme, netloc, path,\n                           params, query, fragment))\n    segments = bpath.split('/')[:-1] + path.split('/')\n    # XXX The stuff below is bogus in various ways...\n    if segments[-1] == '.':\n        segments[-1] = ''\n    while '.' in segments:\n        segments.remove('.')\n    while 1:\n        i = 1\n        n = len(segments) - 1\n        while i < n:\n            if (segments[i] == '..'\n                and segments[i-1] not in ('', '..')):\n                del segments[i-1:i+1]\n                break\n            i = i+1\n        else:\n            break\n    if segments == ['', '..']:\n        segments[-1] = ''\n    elif len(segments) >= 2 and segments[-1] == '..':\n        segments[-2:] = ['']\n    return urlunparse((scheme, netloc, '/'.join(segments),\n                       params, query, fragment))\n\ndef urldefrag(url):\n    \"\"\"Removes any existing fragment from URL.\n\n    Returns a tuple of the defragmented URL and the fragment.  If\n    the URL contained no fragments, the second element is the\n    empty string.\n    \"\"\"\n    if '#' in url:\n        s, n, p, a, q, frag = urlparse(url)\n        defrag = urlunparse((s, n, p, a, q, ''))\n        return defrag, frag\n    else:\n        return url, ''\n\ntry:\n    unicode\nexcept NameError:\n    def _is_unicode(x):\n        return 0\nelse:\n    def _is_unicode(x):\n        return isinstance(x, unicode)\n\n# unquote method for parse_qs and parse_qsl\n# Cannot use directly from urllib as it would create a circular reference\n# because urllib uses urlparse methods (urljoin).  If you update this function,\n# update it also in urllib.  This code duplication does not existin in Python3.\n\n_hexdig = '0123456789ABCDEFabcdef'\n_hextochr = dict((a+b, chr(int(a+b,16)))\n                 for a in _hexdig for b in _hexdig)\n_asciire = re.compile('([\\x00-\\x7f]+)')\n\ndef unquote(s):\n    \"\"\"unquote('abc%20def') -> 'abc def'.\"\"\"\n    if _is_unicode(s):\n        if '%' not in s:\n            return s\n        bits = _asciire.split(s)\n        res = [bits[0]]\n        append = res.append\n        for i in range(1, len(bits), 2):\n            append(unquote(str(bits[i])).decode('latin1'))\n            append(bits[i + 1])\n        return ''.join(res)\n\n    bits = s.split('%')\n    # fastpath\n    if len(bits) == 1:\n        return s\n    res = [bits[0]]\n    append = res.append\n    for j in xrange(1, len(bits)):\n        item = bits[j]\n        try:\n            append(_hextochr[item[:2]])\n            append(item[2:])\n        except KeyError:\n            append('%')\n            append(item)\n    return ''.join(res)\n\ndef parse_qs(qs, keep_blank_values=0, strict_parsing=0):\n    \"\"\"Parse a query given as a string argument.\n\n        Arguments:\n\n        qs: percent-encoded query string to be parsed\n\n        keep_blank_values: flag indicating whether blank values in\n            percent-encoded queries should be treated as blank strings.\n            A true value indicates that blanks should be retained as\n            blank strings.  The default false value indicates that\n            blank values are to be ignored and treated as if they were\n            not included.\n\n        strict_parsing: flag indicating what to do with parsing errors.\n            If false (the default), errors are silently ignored.\n            If true, errors raise a ValueError exception.\n    \"\"\"\n    dict = {}\n    for name, value in parse_qsl(qs, keep_blank_values, strict_parsing):\n        if name in dict:\n            dict[name].append(value)\n        else:\n            dict[name] = [value]\n    return dict\n\ndef parse_qsl(qs, keep_blank_values=0, strict_parsing=0):\n    \"\"\"Parse a query given as a string argument.\n\n    Arguments:\n\n    qs: percent-encoded query string to be parsed\n\n    keep_blank_values: flag indicating whether blank values in\n        percent-encoded queries should be treated as blank strings.  A\n        true value indicates that blanks should be retained as blank\n        strings.  The default false value indicates that blank values\n        are to be ignored and treated as if they were  not included.\n\n    strict_parsing: flag indicating what to do with parsing errors. If\n        false (the default), errors are silently ignored. If true,\n        errors raise a ValueError exception.\n\n    Returns a list, as G-d intended.\n    \"\"\"\n    pairs = [s2 for s1 in qs.split('&') for s2 in s1.split(';')]\n    r = []\n    for name_value in pairs:\n        if not name_value and not strict_parsing:\n            continue\n        nv = name_value.split('=', 1)\n        if len(nv) != 2:\n            if strict_parsing:\n                raise ValueError, \"bad query field: %r\" % (name_value,)\n            # Handle case of a control-name with no equal sign\n            if keep_blank_values:\n                nv.append('')\n            else:\n                continue\n        if len(nv[1]) or keep_blank_values:\n            name = unquote(nv[0].replace('+', ' '))\n            value = unquote(nv[1].replace('+', ' '))\n            r.append((name, value))\n\n    return r\n", 
    "uu": "#! /usr/bin/env python\n\n# Copyright 1994 by Lance Ellinghouse\n# Cathedral City, California Republic, United States of America.\n#                        All Rights Reserved\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose and without fee is hereby granted,\n# provided that the above copyright notice appear in all copies and that\n# both that copyright notice and this permission notice appear in\n# supporting documentation, and that the name of Lance Ellinghouse\n# not be used in advertising or publicity pertaining to distribution\n# of the software without specific, written prior permission.\n# LANCE ELLINGHOUSE DISCLAIMS ALL WARRANTIES WITH REGARD TO\n# THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND\n# FITNESS, IN NO EVENT SHALL LANCE ELLINGHOUSE CENTRUM BE LIABLE\n# FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n#\n# Modified by Jack Jansen, CWI, July 1995:\n# - Use binascii module to do the actual line-by-line conversion\n#   between ascii and binary. This results in a 1000-fold speedup. The C\n#   version is still 5 times faster, though.\n# - Arguments more compliant with python standard\n\n\"\"\"Implementation of the UUencode and UUdecode functions.\n\nencode(in_file, out_file [,name, mode])\ndecode(in_file [, out_file, mode])\n\"\"\"\n\nimport binascii\nimport os\nimport sys\n\n__all__ = [\"Error\", \"encode\", \"decode\"]\n\nclass Error(Exception):\n    pass\n\ndef encode(in_file, out_file, name=None, mode=None):\n    \"\"\"Uuencode file\"\"\"\n    #\n    # If in_file is a pathname open it and change defaults\n    #\n    opened_files = []\n    try:\n        if in_file == '-':\n            in_file = sys.stdin\n        elif isinstance(in_file, basestring):\n            if name is None:\n                name = os.path.basename(in_file)\n            if mode is None:\n                try:\n                    mode = os.stat(in_file).st_mode\n                except AttributeError:\n                    pass\n            in_file = open(in_file, 'rb')\n            opened_files.append(in_file)\n        #\n        # Open out_file if it is a pathname\n        #\n        if out_file == '-':\n            out_file = sys.stdout\n        elif isinstance(out_file, basestring):\n            out_file = open(out_file, 'wb')\n            opened_files.append(out_file)\n        #\n        # Set defaults for name and mode\n        #\n        if name is None:\n            name = '-'\n        if mode is None:\n            mode = 0666\n        #\n        # Write the data\n        #\n        out_file.write('begin %o %s\\n' % ((mode&0777),name))\n        data = in_file.read(45)\n        while len(data) > 0:\n            out_file.write(binascii.b2a_uu(data))\n            data = in_file.read(45)\n        out_file.write(' \\nend\\n')\n    finally:\n        for f in opened_files:\n            f.close()\n\n\ndef decode(in_file, out_file=None, mode=None, quiet=0):\n    \"\"\"Decode uuencoded file\"\"\"\n    #\n    # Open the input file, if needed.\n    #\n    opened_files = []\n    if in_file == '-':\n        in_file = sys.stdin\n    elif isinstance(in_file, basestring):\n        in_file = open(in_file)\n        opened_files.append(in_file)\n    try:\n        #\n        # Read until a begin is encountered or we've exhausted the file\n        #\n        while True:\n            hdr = in_file.readline()\n            if not hdr:\n                raise Error('No valid begin line found in input file')\n            if not hdr.startswith('begin'):\n                continue\n            hdrfields = hdr.split(' ', 2)\n            if len(hdrfields) == 3 and hdrfields[0] == 'begin':\n                try:\n                    int(hdrfields[1], 8)\n                    break\n                except ValueError:\n                    pass\n        if out_file is None:\n            out_file = hdrfields[2].rstrip()\n            if os.path.exists(out_file):\n                raise Error('Cannot overwrite existing file: %s' % out_file)\n        if mode is None:\n            mode = int(hdrfields[1], 8)\n        #\n        # Open the output file\n        #\n        if out_file == '-':\n            out_file = sys.stdout\n        elif isinstance(out_file, basestring):\n            fp = open(out_file, 'wb')\n            try:\n                os.path.chmod(out_file, mode)\n            except AttributeError:\n                pass\n            out_file = fp\n            opened_files.append(out_file)\n        #\n        # Main decoding loop\n        #\n        s = in_file.readline()\n        while s and s.strip() != 'end':\n            try:\n                data = binascii.a2b_uu(s)\n            except binascii.Error, v:\n                # Workaround for broken uuencoders by /Fredrik Lundh\n                nbytes = (((ord(s[0])-32) & 63) * 4 + 5) // 3\n                data = binascii.a2b_uu(s[:nbytes])\n                if not quiet:\n                    sys.stderr.write(\"Warning: %s\\n\" % v)\n            out_file.write(data)\n            s = in_file.readline()\n        if not s:\n            raise Error('Truncated input file')\n    finally:\n        for f in opened_files:\n            f.close()\n\ndef test():\n    \"\"\"uuencode/uudecode main program\"\"\"\n\n    import optparse\n    parser = optparse.OptionParser(usage='usage: %prog [-d] [-t] [input [output]]')\n    parser.add_option('-d', '--decode', dest='decode', help='Decode (instead of encode)?', default=False, action='store_true')\n    parser.add_option('-t', '--text', dest='text', help='data is text, encoded format unix-compatible text?', default=False, action='store_true')\n\n    (options, args) = parser.parse_args()\n    if len(args) > 2:\n        parser.error('incorrect number of arguments')\n        sys.exit(1)\n\n    input = sys.stdin\n    output = sys.stdout\n    if len(args) > 0:\n        input = args[0]\n    if len(args) > 1:\n        output = args[1]\n\n    if options.decode:\n        if options.text:\n            if isinstance(output, basestring):\n                output = open(output, 'w')\n            else:\n                print sys.argv[0], ': cannot do -t to stdout'\n                sys.exit(1)\n        decode(input, output)\n    else:\n        if options.text:\n            if isinstance(input, basestring):\n                input = open(input, 'r')\n            else:\n                print sys.argv[0], ': cannot do -t from stdin'\n                sys.exit(1)\n        encode(input, output)\n\nif __name__ == '__main__':\n    test()\n", 
    "warnings": "\"\"\"Python part of the warnings subsystem.\"\"\"\n\n# Note: function level imports should *not* be used\n# in this module as it may cause import lock deadlock.\n# See bug 683658.\nimport linecache\nimport sys\nimport types\n\n__all__ = [\"warn\", \"warn_explicit\", \"showwarning\",\n           \"formatwarning\", \"filterwarnings\", \"simplefilter\",\n           \"resetwarnings\", \"catch_warnings\"]\n\n\ndef warnpy3k(message, category=None, stacklevel=1):\n    \"\"\"Issue a deprecation warning for Python 3.x related changes.\n\n    Warnings are omitted unless Python is started with the -3 option.\n    \"\"\"\n    if sys.py3kwarning:\n        if category is None:\n            category = DeprecationWarning\n        warn(message, category, stacklevel+1)\n\ndef _show_warning(message, category, filename, lineno, file=None, line=None):\n    \"\"\"Hook to write a warning to a file; replace if you like.\"\"\"\n    if file is None:\n        file = sys.stderr\n    try:\n        file.write(formatwarning(message, category, filename, lineno, line))\n    except IOError:\n        pass # the file (probably stderr) is invalid - this warning gets lost.\n# Keep a working version around in case the deprecation of the old API is\n# triggered.\nshowwarning = _show_warning\n\ndef formatwarning(message, category, filename, lineno, line=None):\n    \"\"\"Function to format a warning the standard way.\"\"\"\n    s =  \"%s:%s: %s: %s\\n\" % (filename, lineno, category.__name__, message)\n    line = linecache.getline(filename, lineno) if line is None else line\n    if line:\n        line = line.strip()\n        s += \"  %s\\n\" % line\n    return s\n\ndef filterwarnings(action, message=\"\", category=Warning, module=\"\", lineno=0,\n                   append=0):\n    \"\"\"Insert an entry into the list of warnings filters (at the front).\n\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'message' -- a regex that the warning message must match\n    'category' -- a class that the warning must be a subclass of\n    'module' -- a regex that the module name must match\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    import re\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(message, basestring), \"message must be a string\"\n    assert isinstance(category, (type, types.ClassType)), \\\n           \"category must be a class\"\n    assert issubclass(category, Warning), \"category must be a Warning subclass\"\n    assert isinstance(module, basestring), \"module must be a string\"\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    item = (action, re.compile(message, re.I), category,\n            re.compile(module), lineno)\n    if append:\n        filters.append(item)\n    else:\n        filters.insert(0, item)\n\ndef simplefilter(action, category=Warning, lineno=0, append=0):\n    \"\"\"Insert a simple entry into the list of warnings filters (at the front).\n\n    A simple filter matches all modules and messages.\n    'action' -- one of \"error\", \"ignore\", \"always\", \"default\", \"module\",\n                or \"once\"\n    'category' -- a class that the warning must be a subclass of\n    'lineno' -- an integer line number, 0 matches all warnings\n    'append' -- if true, append to the list of filters\n    \"\"\"\n    assert action in (\"error\", \"ignore\", \"always\", \"default\", \"module\",\n                      \"once\"), \"invalid action: %r\" % (action,)\n    assert isinstance(lineno, int) and lineno >= 0, \\\n           \"lineno must be an int >= 0\"\n    item = (action, None, category, None, lineno)\n    if append:\n        filters.append(item)\n    else:\n        filters.insert(0, item)\n\ndef resetwarnings():\n    \"\"\"Clear the list of warning filters, so that no filters are active.\"\"\"\n    filters[:] = []\n\nclass _OptionError(Exception):\n    \"\"\"Exception used by option processing helpers.\"\"\"\n    pass\n\n# Helper to process -W options passed via sys.warnoptions\ndef _processoptions(args):\n    for arg in args:\n        try:\n            _setoption(arg)\n        except _OptionError, msg:\n            print >>sys.stderr, \"Invalid -W option ignored:\", msg\n\n# Helper for _processoptions()\ndef _setoption(arg):\n    import re\n    parts = arg.split(':')\n    if len(parts) > 5:\n        raise _OptionError(\"too many fields (max 5): %r\" % (arg,))\n    while len(parts) < 5:\n        parts.append('')\n    action, message, category, module, lineno = [s.strip()\n                                                 for s in parts]\n    action = _getaction(action)\n    message = re.escape(message)\n    category = _getcategory(category)\n    module = re.escape(module)\n    if module:\n        module = module + '$'\n    if lineno:\n        try:\n            lineno = int(lineno)\n            if lineno < 0:\n                raise ValueError\n        except (ValueError, OverflowError):\n            raise _OptionError(\"invalid lineno %r\" % (lineno,))\n    else:\n        lineno = 0\n    filterwarnings(action, message, category, module, lineno)\n\n# Helper for _setoption()\ndef _getaction(action):\n    if not action:\n        return \"default\"\n    if action == \"all\": return \"always\" # Alias\n    for a in ('default', 'always', 'ignore', 'module', 'once', 'error'):\n        if a.startswith(action):\n            return a\n    raise _OptionError(\"invalid action: %r\" % (action,))\n\n# Helper for _setoption()\ndef _getcategory(category):\n    import re\n    if not category:\n        return Warning\n    if re.match(\"^[a-zA-Z0-9_]+$\", category):\n        try:\n            cat = eval(category)\n        except NameError:\n            raise _OptionError(\"unknown warning category: %r\" % (category,))\n    else:\n        i = category.rfind(\".\")\n        module = category[:i]\n        klass = category[i+1:]\n        try:\n            m = __import__(module, None, None, [klass])\n        except ImportError:\n            raise _OptionError(\"invalid module name: %r\" % (module,))\n        try:\n            cat = getattr(m, klass)\n        except AttributeError:\n            raise _OptionError(\"unknown warning category: %r\" % (category,))\n    if not issubclass(cat, Warning):\n        raise _OptionError(\"invalid warning category: %r\" % (category,))\n    return cat\n\n\n# Code typically replaced by _warnings\ndef warn(message, category=None, stacklevel=1):\n    \"\"\"Issue a warning, or maybe ignore it or raise an exception.\"\"\"\n    # Check if message is already a Warning object\n    if isinstance(message, Warning):\n        category = message.__class__\n    # Check category argument\n    if category is None:\n        category = UserWarning\n    assert issubclass(category, Warning)\n    # Get context information\n    try:\n        caller = sys._getframe(stacklevel)\n    except ValueError:\n        globals = sys.__dict__\n        lineno = 1\n    else:\n        globals = caller.f_globals\n        lineno = caller.f_lineno\n    if '__name__' in globals:\n        module = globals['__name__']\n    else:\n        module = \"<string>\"\n    filename = globals.get('__file__')\n    if filename:\n        fnl = filename.lower()\n        if fnl.endswith((\".pyc\", \".pyo\")):\n            filename = filename[:-1]\n    else:\n        if module == \"__main__\":\n            try:\n                filename = sys.argv[0]\n            except AttributeError:\n                # embedded interpreters don't have sys.argv, see bug #839151\n                filename = '__main__'\n        if not filename:\n            filename = module\n    registry = globals.setdefault(\"__warningregistry__\", {})\n    warn_explicit(message, category, filename, lineno, module, registry,\n                  globals)\n\ndef warn_explicit(message, category, filename, lineno,\n                  module=None, registry=None, module_globals=None):\n    lineno = int(lineno)\n    if module is None:\n        module = filename or \"<unknown>\"\n        if module[-3:].lower() == \".py\":\n            module = module[:-3] # XXX What about leading pathname?\n    if registry is None:\n        registry = {}\n    if isinstance(message, Warning):\n        text = str(message)\n        category = message.__class__\n    else:\n        text = message\n        message = category(message)\n    key = (text, category, lineno)\n    # Quick test for common case\n    if registry.get(key):\n        return\n    # Search the filters\n    for item in filters:\n        action, msg, cat, mod, ln = item\n        if ((msg is None or msg.match(text)) and\n            issubclass(category, cat) and\n            (mod is None or mod.match(module)) and\n            (ln == 0 or lineno == ln)):\n            break\n    else:\n        action = defaultaction\n    # Early exit actions\n    if action == \"ignore\":\n        registry[key] = 1\n        return\n\n    # Prime the linecache for formatting, in case the\n    # \"file\" is actually in a zipfile or something.\n    linecache.getlines(filename, module_globals)\n\n    if action == \"error\":\n        raise message\n    # Other actions\n    if action == \"once\":\n        registry[key] = 1\n        oncekey = (text, category)\n        if onceregistry.get(oncekey):\n            return\n        onceregistry[oncekey] = 1\n    elif action == \"always\":\n        pass\n    elif action == \"module\":\n        registry[key] = 1\n        altkey = (text, category, 0)\n        if registry.get(altkey):\n            return\n        registry[altkey] = 1\n    elif action == \"default\":\n        registry[key] = 1\n    else:\n        # Unrecognized actions are errors\n        raise RuntimeError(\n              \"Unrecognized action (%r) in warnings.filters:\\n %s\" %\n              (action, item))\n    # Print message and context\n    showwarning(message, category, filename, lineno)\n\n\nclass WarningMessage(object):\n\n    \"\"\"Holds the result of a single showwarning() call.\"\"\"\n\n    _WARNING_DETAILS = (\"message\", \"category\", \"filename\", \"lineno\", \"file\",\n                        \"line\")\n\n    def __init__(self, message, category, filename, lineno, file=None,\n                    line=None):\n        local_values = locals()\n        for attr in self._WARNING_DETAILS:\n            setattr(self, attr, local_values[attr])\n        self._category_name = category.__name__ if category else None\n\n    def __str__(self):\n        return (\"{message : %r, category : %r, filename : %r, lineno : %s, \"\n                    \"line : %r}\" % (self.message, self._category_name,\n                                    self.filename, self.lineno, self.line))\n\n\nclass catch_warnings(object):\n\n    \"\"\"A context manager that copies and restores the warnings filter upon\n    exiting the context.\n\n    The 'record' argument specifies whether warnings should be captured by a\n    custom implementation of warnings.showwarning() and be appended to a list\n    returned by the context manager. Otherwise None is returned by the context\n    manager. The objects appended to the list are arguments whose attributes\n    mirror the arguments to showwarning().\n\n    The 'module' argument is to specify an alternative module to the module\n    named 'warnings' and imported under that name. This argument is only useful\n    when testing the warnings module itself.\n\n    \"\"\"\n\n    def __init__(self, record=False, module=None):\n        \"\"\"Specify whether to record warnings and if an alternative module\n        should be used other than sys.modules['warnings'].\n\n        For compatibility with Python 3.0, please consider all arguments to be\n        keyword-only.\n\n        \"\"\"\n        self._record = record\n        self._module = sys.modules['warnings'] if module is None else module\n        self._entered = False\n\n    def __repr__(self):\n        args = []\n        if self._record:\n            args.append(\"record=True\")\n        if self._module is not sys.modules['warnings']:\n            args.append(\"module=%r\" % self._module)\n        name = type(self).__name__\n        return \"%s(%s)\" % (name, \", \".join(args))\n\n    def __enter__(self):\n        if self._entered:\n            raise RuntimeError(\"Cannot enter %r twice\" % self)\n        self._entered = True\n        self._filters = self._module.filters\n        self._module.filters = self._filters[:]\n        self._showwarning = self._module.showwarning\n        if self._record:\n            log = []\n            def showwarning(*args, **kwargs):\n                log.append(WarningMessage(*args, **kwargs))\n            self._module.showwarning = showwarning\n            return log\n        else:\n            return None\n\n    def __exit__(self, *exc_info):\n        if not self._entered:\n            raise RuntimeError(\"Cannot exit %r without entering first\" % self)\n        self._module.filters = self._filters\n        self._module.showwarning = self._showwarning\n\n\n# filters contains a sequence of filter 5-tuples\n# The components of the 5-tuple are:\n# - an action: error, ignore, always, default, module, or once\n# - a compiled regex that must match the warning message\n# - a class representing the warning category\n# - a compiled regex that must match the module that is being warned\n# - a line number for the line being warning, or 0 to mean any line\n# If either if the compiled regexs are None, match anything.\n_warnings_defaults = False\ntry:\n    from _warnings import (filters, default_action, once_registry,\n                            warn, warn_explicit)\n    defaultaction = default_action\n    onceregistry = once_registry\n    _warnings_defaults = True\nexcept ImportError:\n    filters = []\n    defaultaction = \"default\"\n    onceregistry = {}\n\n\n# Module initialization\n_processoptions(sys.warnoptions)\nif not _warnings_defaults:\n    silence = [ImportWarning, PendingDeprecationWarning]\n    # Don't silence DeprecationWarning if -3 or -Q was used.\n    if not sys.py3kwarning and not sys.flags.division_warning:\n        silence.append(DeprecationWarning)\n    for cls in silence:\n        simplefilter(\"ignore\", category=cls)\n    bytes_warning = sys.flags.bytes_warning\n    if bytes_warning > 1:\n        bytes_action = \"error\"\n    elif bytes_warning:\n        bytes_action = \"default\"\n    else:\n        bytes_action = \"ignore\"\n    simplefilter(bytes_action, category=BytesWarning, append=1)\ndel _warnings_defaults\n", 
    "weakref": "\"\"\"Weak reference support for Python.\n\nThis module is an implementation of PEP 205:\n\nhttp://www.python.org/dev/peps/pep-0205/\n\"\"\"\n\n# Naming convention: Variables named \"wr\" are weak reference objects;\n# they are called this instead of \"ref\" to avoid name collisions with\n# the module-global ref() function imported from _weakref.\n\nimport UserDict\n\nfrom _weakref import (\n     getweakrefcount,\n     getweakrefs,\n     ref,\n     proxy,\n     CallableProxyType,\n     ProxyType,\n     ReferenceType)\n\nfrom _weakrefset import WeakSet, _IterationGuard\n\nfrom exceptions import ReferenceError\n\n\nProxyTypes = (ProxyType, CallableProxyType)\n\n__all__ = [\"ref\", \"proxy\", \"getweakrefcount\", \"getweakrefs\",\n           \"WeakKeyDictionary\", \"ReferenceError\", \"ReferenceType\", \"ProxyType\",\n           \"CallableProxyType\", \"ProxyTypes\", \"WeakValueDictionary\", 'WeakSet']\n\n\nclass WeakValueDictionary(UserDict.UserDict):\n    \"\"\"Mapping class that references values weakly.\n\n    Entries in the dictionary will be discarded when no strong\n    reference to the value exists anymore\n    \"\"\"\n    # We inherit the constructor without worrying about the input\n    # dictionary; since it uses our .update() method, we get the right\n    # checks (if the other dictionary is a WeakValueDictionary,\n    # objects are unwrapped on the way out, and we always wrap on the\n    # way in).\n\n    def __init__(self, *args, **kw):\n        def remove(wr, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(wr.key)\n                else:\n                    # Changed this for PyPy: made more resistent.  The\n                    # issue is that in some corner cases, self.data\n                    # might already be changed or removed by the time\n                    # this weakref's callback is called.  If that is\n                    # the case, we don't want to randomly kill an\n                    # unrelated entry.\n                    if self.data.get(wr.key) is wr:\n                        del self.data[wr.key]\n        self._remove = remove\n        # A list of keys to be removed\n        self._pending_removals = []\n        self._iterating = set()\n        UserDict.UserDict.__init__(self, *args, **kw)\n\n    def _commit_removals(self):\n        l = self._pending_removals\n        d = self.data\n        # We shouldn't encounter any KeyError, because this method should\n        # always be called *before* mutating the dict.\n        while l:\n            del d[l.pop()]\n\n    def __getitem__(self, key):\n        o = self.data[key]()\n        if o is None:\n            raise KeyError, key\n        else:\n            return o\n\n    def __delitem__(self, key):\n        if self._pending_removals:\n            self._commit_removals()\n        del self.data[key]\n\n    def __contains__(self, key):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def has_key(self, key):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            return False\n        return o is not None\n\n    def __repr__(self):\n        return \"<WeakValueDictionary at %s>\" % id(self)\n\n    def __setitem__(self, key, value):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data[key] = KeyedRef(value, self._remove, key)\n\n    def clear(self):\n        if self._pending_removals:\n            self._commit_removals()\n        self.data.clear()\n\n    def copy(self):\n        new = WeakValueDictionary()\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                new[key] = o\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                new[deepcopy(key, memo)] = o\n        return new\n\n    def get(self, key, default=None):\n        try:\n            wr = self.data[key]\n        except KeyError:\n            return default\n        else:\n            o = wr()\n            if o is None:\n                # This should only happen\n                return default\n            else:\n                return o\n\n    def items(self):\n        L = []\n        for key, wr in self.data.items():\n            o = wr()\n            if o is not None:\n                L.append((key, o))\n        return L\n\n    def iteritems(self):\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                value = wr()\n                if value is not None:\n                    yield wr.key, value\n\n    def iterkeys(self):\n        with _IterationGuard(self):\n            for k in self.data.iterkeys():\n                yield k\n\n    __iter__ = iterkeys\n\n    def itervaluerefs(self):\n        \"\"\"Return an iterator that yields the weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                yield wr\n\n    def itervalues(self):\n        with _IterationGuard(self):\n            for wr in self.data.itervalues():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    def popitem(self):\n        if self._pending_removals:\n            self._commit_removals()\n        while 1:\n            key, wr = self.data.popitem()\n            o = wr()\n            if o is not None:\n                return key, o\n\n    def pop(self, key, *args):\n        if self._pending_removals:\n            self._commit_removals()\n        try:\n            o = self.data.pop(key)()\n        except KeyError:\n            o = None\n        if o is None:\n            if args:\n                return args[0]\n            raise KeyError, key\n        else:\n            return o\n        # The logic above was fixed in PyPy\n\n    def setdefault(self, key, default=None):\n        try:\n            o = self.data[key]()\n        except KeyError:\n            o = None\n        if o is None:\n            if self._pending_removals:\n                self._commit_removals()\n            self.data[key] = KeyedRef(default, self._remove, key)\n            return default\n        else:\n            return o\n        # The logic above was fixed in PyPy\n\n    def update(self, dict=None, **kwargs):\n        if self._pending_removals:\n            self._commit_removals()\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, o in dict.items():\n                d[key] = KeyedRef(o, self._remove, key)\n        if len(kwargs):\n            self.update(kwargs)\n\n    def valuerefs(self):\n        \"\"\"Return a list of weak references to the values.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the values around longer than needed.\n\n        \"\"\"\n        return self.data.values()\n\n    def values(self):\n        L = []\n        for wr in self.data.values():\n            o = wr()\n            if o is not None:\n                L.append(o)\n        return L\n\n\nclass KeyedRef(ref):\n    \"\"\"Specialized reference that includes a key corresponding to the value.\n\n    This is used in the WeakValueDictionary to avoid having to create\n    a function object for each key stored in the mapping.  A shared\n    callback object can use the 'key' attribute of a KeyedRef instead\n    of getting a reference to the key from an enclosing scope.\n\n    \"\"\"\n\n    __slots__ = \"key\",\n\n    def __new__(type, ob, callback, key):\n        self = ref.__new__(type, ob, callback)\n        self.key = key\n        return self\n\n    def __init__(self, ob, callback, key):\n        super(KeyedRef,  self).__init__(ob, callback)\n\n\nclass WeakKeyDictionary(UserDict.UserDict):\n    \"\"\" Mapping class that references keys weakly.\n\n    Entries in the dictionary will be discarded when there is no\n    longer a strong reference to the key. This can be used to\n    associate additional data with an object owned by other parts of\n    an application without adding attributes to those objects. This\n    can be especially useful with objects that override attribute\n    accesses.\n    \"\"\"\n\n    def __init__(self, dict=None):\n        self.data = {}\n        def remove(k, selfref=ref(self)):\n            self = selfref()\n            if self is not None:\n                if self._iterating:\n                    self._pending_removals.append(k)\n                else:\n                    del self.data[k]\n        self._remove = remove\n        # A list of dead weakrefs (keys to be removed)\n        self._pending_removals = []\n        self._iterating = set()\n        if dict is not None:\n            self.update(dict)\n\n    def _commit_removals(self):\n        # NOTE: We don't need to call this method before mutating the dict,\n        # because a dead weakref never compares equal to a live weakref,\n        # even if they happened to refer to equal objects.\n        # However, it means keys may already have been removed.\n        l = self._pending_removals\n        d = self.data\n        while l:\n            try:\n                del d[l.pop()]\n            except KeyError:\n                pass\n\n    def __delitem__(self, key):\n        del self.data[ref(key)]\n\n    def __getitem__(self, key):\n        return self.data[ref(key)]\n\n    def __repr__(self):\n        return \"<WeakKeyDictionary at %s>\" % id(self)\n\n    def __setitem__(self, key, value):\n        self.data[ref(key, self._remove)] = value\n\n    def copy(self):\n        new = WeakKeyDictionary()\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                new[o] = value\n        return new\n\n    __copy__ = copy\n\n    def __deepcopy__(self, memo):\n        from copy import deepcopy\n        new = self.__class__()\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                new[o] = deepcopy(value, memo)\n        return new\n\n    def get(self, key, default=None):\n        return self.data.get(ref(key),default)\n\n    def has_key(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return 0\n        return wr in self.data\n\n    def __contains__(self, key):\n        try:\n            wr = ref(key)\n        except TypeError:\n            return 0\n        return wr in self.data\n\n    def items(self):\n        L = []\n        for key, value in self.data.items():\n            o = key()\n            if o is not None:\n                L.append((o, value))\n        return L\n\n    def iteritems(self):\n        with _IterationGuard(self):\n            for wr, value in self.data.iteritems():\n                key = wr()\n                if key is not None:\n                    yield key, value\n\n    def iterkeyrefs(self):\n        \"\"\"Return an iterator that yields the weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        with _IterationGuard(self):\n            for wr in self.data.iterkeys():\n                yield wr\n\n    def iterkeys(self):\n        with _IterationGuard(self):\n            for wr in self.data.iterkeys():\n                obj = wr()\n                if obj is not None:\n                    yield obj\n\n    __iter__ = iterkeys\n\n    def itervalues(self):\n        with _IterationGuard(self):\n            for value in self.data.itervalues():\n                yield value\n\n    def keyrefs(self):\n        \"\"\"Return a list of weak references to the keys.\n\n        The references are not guaranteed to be 'live' at the time\n        they are used, so the result of calling the references needs\n        to be checked before being used.  This can be used to avoid\n        creating references that will cause the garbage collector to\n        keep the keys around longer than needed.\n\n        \"\"\"\n        return self.data.keys()\n\n    def keys(self):\n        L = []\n        for wr in self.data.keys():\n            o = wr()\n            if o is not None:\n                L.append(o)\n        return L\n\n    def popitem(self):\n        while 1:\n            key, value = self.data.popitem()\n            o = key()\n            if o is not None:\n                return o, value\n\n    def pop(self, key, *args):\n        return self.data.pop(ref(key), *args)\n\n    def setdefault(self, key, default=None):\n        return self.data.setdefault(ref(key, self._remove),default)\n\n    def update(self, dict=None, **kwargs):\n        d = self.data\n        if dict is not None:\n            if not hasattr(dict, \"items\"):\n                dict = type({})(dict)\n            for key, value in dict.items():\n                d[ref(key, self._remove)] = value\n        if len(kwargs):\n            self.update(kwargs)\n", 
    "xml.__init__": "\"\"\"Core XML support for Python.\n\nThis package contains four sub-packages:\n\ndom -- The W3C Document Object Model.  This supports DOM Level 1 +\n       Namespaces.\n\nparsers -- Python wrappers for XML parsers (currently only supports Expat).\n\nsax -- The Simple API for XML, developed by XML-Dev, led by David\n       Megginson and ported to Python by Lars Marius Garshol.  This\n       supports the SAX 2 API.\n\netree -- The ElementTree XML library.  This is a subset of the full\n       ElementTree XML release.\n\n\"\"\"\n\n\n__all__ = [\"dom\", \"parsers\", \"sax\", \"etree\"]\n\n_MINIMUM_XMLPLUS_VERSION = (0, 8, 4)\n\n\ntry:\n    import _xmlplus\nexcept ImportError:\n    pass\nelse:\n    try:\n        v = _xmlplus.version_info\n    except AttributeError:\n        # _xmlplus is too old; ignore it\n        pass\n    else:\n        if v >= _MINIMUM_XMLPLUS_VERSION:\n            import sys\n            _xmlplus.__path__.extend(__path__)\n            sys.modules[__name__] = _xmlplus\n        else:\n            del v\n", 
    "xml.parsers.__init__": "\"\"\"Python interfaces to XML parsers.\n\nThis package contains one module:\n\nexpat -- Python wrapper for James Clark's Expat parser, with namespace\n         support.\n\n\"\"\"\n", 
    "xml.parsers.expat": "\"\"\"Interface to the Expat non-validating XML parser.\"\"\"\n__version__ = '$Revision: 17640 $'\n\nfrom pyexpat import *\n", 
    "xmllib": "\"\"\"A parser for XML, using the derived class as static DTD.\"\"\"\n\n# Author: Sjoerd Mullender.\n\nimport re\nimport string\n\nimport warnings\nwarnings.warn(\"The xmllib module is obsolete.  Use xml.sax instead.\",\n              DeprecationWarning, 2)\ndel warnings\n\nversion = '0.3'\n\nclass Error(RuntimeError):\n    pass\n\n# Regular expressions used for parsing\n\n_S = '[ \\t\\r\\n]+'                       # white space\n_opS = '[ \\t\\r\\n]*'                     # optional white space\n_Name = '[a-zA-Z_:][-a-zA-Z0-9._:]*'    # valid XML name\n_QStr = \"(?:'[^']*'|\\\"[^\\\"]*\\\")\"        # quoted XML string\nillegal = re.compile('[^\\t\\r\\n -\\176\\240-\\377]') # illegal chars in content\ninteresting = re.compile('[]&<]')\n\namp = re.compile('&')\nref = re.compile('&(' + _Name + '|#[0-9]+|#x[0-9a-fA-F]+)[^-a-zA-Z0-9._:]')\nentityref = re.compile('&(?P<name>' + _Name + ')[^-a-zA-Z0-9._:]')\ncharref = re.compile('&#(?P<char>[0-9]+[^0-9]|x[0-9a-fA-F]+[^0-9a-fA-F])')\nspace = re.compile(_S + '$')\nnewline = re.compile('\\n')\n\nattrfind = re.compile(\n    _S + '(?P<name>' + _Name + ')'\n    '(' + _opS + '=' + _opS +\n    '(?P<value>'+_QStr+'|[-a-zA-Z0-9.:+*%?!\\(\\)_#=~]+))?')\nstarttagopen = re.compile('<' + _Name)\nstarttagend = re.compile(_opS + '(?P<slash>/?)>')\nstarttagmatch = re.compile('<(?P<tagname>'+_Name+')'\n                      '(?P<attrs>(?:'+attrfind.pattern+')*)'+\n                      starttagend.pattern)\nendtagopen = re.compile('</')\nendbracket = re.compile(_opS + '>')\nendbracketfind = re.compile('(?:[^>\\'\"]|'+_QStr+')*>')\ntagfind = re.compile(_Name)\ncdataopen = re.compile(r'<!\\[CDATA\\[')\ncdataclose = re.compile(r'\\]\\]>')\n# this matches one of the following:\n# SYSTEM SystemLiteral\n# PUBLIC PubidLiteral SystemLiteral\n_SystemLiteral = '(?P<%s>'+_QStr+')'\n_PublicLiteral = '(?P<%s>\"[-\\'\\(\\)+,./:=?;!*#@$_%% \\n\\ra-zA-Z0-9]*\"|' \\\n                        \"'[-\\(\\)+,./:=?;!*#@$_%% \\n\\ra-zA-Z0-9]*')\"\n_ExternalId = '(?:SYSTEM|' \\\n                 'PUBLIC'+_S+_PublicLiteral%'pubid'+ \\\n              ')'+_S+_SystemLiteral%'syslit'\ndoctype = re.compile('<!DOCTYPE'+_S+'(?P<name>'+_Name+')'\n                     '(?:'+_S+_ExternalId+')?'+_opS)\nxmldecl = re.compile('<\\?xml'+_S+\n                     'version'+_opS+'='+_opS+'(?P<version>'+_QStr+')'+\n                     '(?:'+_S+'encoding'+_opS+'='+_opS+\n                        \"(?P<encoding>'[A-Za-z][-A-Za-z0-9._]*'|\"\n                        '\"[A-Za-z][-A-Za-z0-9._]*\"))?'\n                     '(?:'+_S+'standalone'+_opS+'='+_opS+\n                        '(?P<standalone>\\'(?:yes|no)\\'|\"(?:yes|no)\"))?'+\n                     _opS+'\\?>')\nprocopen = re.compile(r'<\\?(?P<proc>' + _Name + ')' + _opS)\nprocclose = re.compile(_opS + r'\\?>')\ncommentopen = re.compile('<!--')\ncommentclose = re.compile('-->')\ndoubledash = re.compile('--')\nattrtrans = string.maketrans(' \\r\\n\\t', '    ')\n\n# definitions for XML namespaces\n_NCName = '[a-zA-Z_][-a-zA-Z0-9._]*'    # XML Name, minus the \":\"\nncname = re.compile(_NCName + '$')\nqname = re.compile('(?:(?P<prefix>' + _NCName + '):)?' # optional prefix\n                   '(?P<local>' + _NCName + ')$')\n\nxmlns = re.compile('xmlns(?::(?P<ncname>'+_NCName+'))?$')\n\n# XML parser base class -- find tags and call handler functions.\n# Usage: p = XMLParser(); p.feed(data); ...; p.close().\n# The dtd is defined by deriving a class which defines methods with\n# special names to handle tags: start_foo and end_foo to handle <foo>\n# and </foo>, respectively.  The data between tags is passed to the\n# parser by calling self.handle_data() with some data as argument (the\n# data may be split up in arbitrary chunks).\n\nclass XMLParser:\n    attributes = {}                     # default, to be overridden\n    elements = {}                       # default, to be overridden\n\n    # parsing options, settable using keyword args in __init__\n    __accept_unquoted_attributes = 0\n    __accept_missing_endtag_name = 0\n    __map_case = 0\n    __accept_utf8 = 0\n    __translate_attribute_references = 1\n\n    # Interface -- initialize and reset this instance\n    def __init__(self, **kw):\n        self.__fixed = 0\n        if 'accept_unquoted_attributes' in kw:\n            self.__accept_unquoted_attributes = kw['accept_unquoted_attributes']\n        if 'accept_missing_endtag_name' in kw:\n            self.__accept_missing_endtag_name = kw['accept_missing_endtag_name']\n        if 'map_case' in kw:\n            self.__map_case = kw['map_case']\n        if 'accept_utf8' in kw:\n            self.__accept_utf8 = kw['accept_utf8']\n        if 'translate_attribute_references' in kw:\n            self.__translate_attribute_references = kw['translate_attribute_references']\n        self.reset()\n\n    def __fixelements(self):\n        self.__fixed = 1\n        self.elements = {}\n        self.__fixdict(self.__dict__)\n        self.__fixclass(self.__class__)\n\n    def __fixclass(self, kl):\n        self.__fixdict(kl.__dict__)\n        for k in kl.__bases__:\n            self.__fixclass(k)\n\n    def __fixdict(self, dict):\n        for key in dict.keys():\n            if key[:6] == 'start_':\n                tag = key[6:]\n                start, end = self.elements.get(tag, (None, None))\n                if start is None:\n                    self.elements[tag] = getattr(self, key), end\n            elif key[:4] == 'end_':\n                tag = key[4:]\n                start, end = self.elements.get(tag, (None, None))\n                if end is None:\n                    self.elements[tag] = start, getattr(self, key)\n\n    # Interface -- reset this instance.  Loses all unprocessed data\n    def reset(self):\n        self.rawdata = ''\n        self.stack = []\n        self.nomoretags = 0\n        self.literal = 0\n        self.lineno = 1\n        self.__at_start = 1\n        self.__seen_doctype = None\n        self.__seen_starttag = 0\n        self.__use_namespaces = 0\n        self.__namespaces = {'xml':None}   # xml is implicitly declared\n        # backward compatibility hack: if elements not overridden,\n        # fill it in ourselves\n        if self.elements is XMLParser.elements:\n            self.__fixelements()\n\n    # For derived classes only -- enter literal mode (CDATA) till EOF\n    def setnomoretags(self):\n        self.nomoretags = self.literal = 1\n\n    # For derived classes only -- enter literal mode (CDATA)\n    def setliteral(self, *args):\n        self.literal = 1\n\n    # Interface -- feed some data to the parser.  Call this as\n    # often as you want, with as little or as much text as you\n    # want (may include '\\n').  (This just saves the text, all the\n    # processing is done by goahead().)\n    def feed(self, data):\n        self.rawdata = self.rawdata + data\n        self.goahead(0)\n\n    # Interface -- handle the remaining data\n    def close(self):\n        self.goahead(1)\n        if self.__fixed:\n            self.__fixed = 0\n            # remove self.elements so that we don't leak\n            del self.elements\n\n    # Interface -- translate references\n    def translate_references(self, data, all = 1):\n        if not self.__translate_attribute_references:\n            return data\n        i = 0\n        while 1:\n            res = amp.search(data, i)\n            if res is None:\n                return data\n            s = res.start(0)\n            res = ref.match(data, s)\n            if res is None:\n                self.syntax_error(\"bogus `&'\")\n                i = s+1\n                continue\n            i = res.end(0)\n            str = res.group(1)\n            rescan = 0\n            if str[0] == '#':\n                if str[1] == 'x':\n                    str = chr(int(str[2:], 16))\n                else:\n                    str = chr(int(str[1:]))\n                if data[i - 1] != ';':\n                    self.syntax_error(\"`;' missing after char reference\")\n                    i = i-1\n            elif all:\n                if str in self.entitydefs:\n                    str = self.entitydefs[str]\n                    rescan = 1\n                elif data[i - 1] != ';':\n                    self.syntax_error(\"bogus `&'\")\n                    i = s + 1 # just past the &\n                    continue\n                else:\n                    self.syntax_error(\"reference to unknown entity `&%s;'\" % str)\n                    str = '&' + str + ';'\n            elif data[i - 1] != ';':\n                self.syntax_error(\"bogus `&'\")\n                i = s + 1 # just past the &\n                continue\n\n            # when we get here, str contains the translated text and i points\n            # to the end of the string that is to be replaced\n            data = data[:s] + str + data[i:]\n            if rescan:\n                i = s\n            else:\n                i = s + len(str)\n\n    # Interface - return a dictionary of all namespaces currently valid\n    def getnamespace(self):\n        nsdict = {}\n        for t, d, nst in self.stack:\n            nsdict.update(d)\n        return nsdict\n\n    # Internal -- handle data as far as reasonable.  May leave state\n    # and data to be processed by a subsequent call.  If 'end' is\n    # true, force handling all data as if followed by EOF marker.\n    def goahead(self, end):\n        rawdata = self.rawdata\n        i = 0\n        n = len(rawdata)\n        while i < n:\n            if i > 0:\n                self.__at_start = 0\n            if self.nomoretags:\n                data = rawdata[i:n]\n                self.handle_data(data)\n                self.lineno = self.lineno + data.count('\\n')\n                i = n\n                break\n            res = interesting.search(rawdata, i)\n            if res:\n                j = res.start(0)\n            else:\n                j = n\n            if i < j:\n                data = rawdata[i:j]\n                if self.__at_start and space.match(data) is None:\n                    self.syntax_error('illegal data at start of file')\n                self.__at_start = 0\n                if not self.stack and space.match(data) is None:\n                    self.syntax_error('data not in content')\n                if not self.__accept_utf8 and illegal.search(data):\n                    self.syntax_error('illegal character in content')\n                self.handle_data(data)\n                self.lineno = self.lineno + data.count('\\n')\n            i = j\n            if i == n: break\n            if rawdata[i] == '<':\n                if starttagopen.match(rawdata, i):\n                    if self.literal:\n                        data = rawdata[i]\n                        self.handle_data(data)\n                        self.lineno = self.lineno + data.count('\\n')\n                        i = i+1\n                        continue\n                    k = self.parse_starttag(i)\n                    if k < 0: break\n                    self.__seen_starttag = 1\n                    self.lineno = self.lineno + rawdata[i:k].count('\\n')\n                    i = k\n                    continue\n                if endtagopen.match(rawdata, i):\n                    k = self.parse_endtag(i)\n                    if k < 0: break\n                    self.lineno = self.lineno + rawdata[i:k].count('\\n')\n                    i =  k\n                    continue\n                if commentopen.match(rawdata, i):\n                    if self.literal:\n                        data = rawdata[i]\n                        self.handle_data(data)\n                        self.lineno = self.lineno + data.count('\\n')\n                        i = i+1\n                        continue\n                    k = self.parse_comment(i)\n                    if k < 0: break\n                    self.lineno = self.lineno + rawdata[i:k].count('\\n')\n                    i = k\n                    continue\n                if cdataopen.match(rawdata, i):\n                    k = self.parse_cdata(i)\n                    if k < 0: break\n                    self.lineno = self.lineno + rawdata[i:k].count('\\n')\n                    i = k\n                    continue\n                res = xmldecl.match(rawdata, i)\n                if res:\n                    if not self.__at_start:\n                        self.syntax_error(\"<?xml?> declaration not at start of document\")\n                    version, encoding, standalone = res.group('version',\n                                                              'encoding',\n                                                              'standalone')\n                    if version[1:-1] != '1.0':\n                        raise Error('only XML version 1.0 supported')\n                    if encoding: encoding = encoding[1:-1]\n                    if standalone: standalone = standalone[1:-1]\n                    self.handle_xml(encoding, standalone)\n                    i = res.end(0)\n                    continue\n                res = procopen.match(rawdata, i)\n                if res:\n                    k = self.parse_proc(i)\n                    if k < 0: break\n                    self.lineno = self.lineno + rawdata[i:k].count('\\n')\n                    i = k\n                    continue\n                res = doctype.match(rawdata, i)\n                if res:\n                    if self.literal:\n                        data = rawdata[i]\n                        self.handle_data(data)\n                        self.lineno = self.lineno + data.count('\\n')\n                        i = i+1\n                        continue\n                    if self.__seen_doctype:\n                        self.syntax_error('multiple DOCTYPE elements')\n                    if self.__seen_starttag:\n                        self.syntax_error('DOCTYPE not at beginning of document')\n                    k = self.parse_doctype(res)\n                    if k < 0: break\n                    self.__seen_doctype = res.group('name')\n                    if self.__map_case:\n                        self.__seen_doctype = self.__seen_doctype.lower()\n                    self.lineno = self.lineno + rawdata[i:k].count('\\n')\n                    i = k\n                    continue\n            elif rawdata[i] == '&':\n                if self.literal:\n                    data = rawdata[i]\n                    self.handle_data(data)\n                    i = i+1\n                    continue\n                res = charref.match(rawdata, i)\n                if res is not None:\n                    i = res.end(0)\n                    if rawdata[i-1] != ';':\n                        self.syntax_error(\"`;' missing in charref\")\n                        i = i-1\n                    if not self.stack:\n                        self.syntax_error('data not in content')\n                    self.handle_charref(res.group('char')[:-1])\n                    self.lineno = self.lineno + res.group(0).count('\\n')\n                    continue\n                res = entityref.match(rawdata, i)\n                if res is not None:\n                    i = res.end(0)\n                    if rawdata[i-1] != ';':\n                        self.syntax_error(\"`;' missing in entityref\")\n                        i = i-1\n                    name = res.group('name')\n                    if self.__map_case:\n                        name = name.lower()\n                    if name in self.entitydefs:\n                        self.rawdata = rawdata = rawdata[:res.start(0)] + self.entitydefs[name] + rawdata[i:]\n                        n = len(rawdata)\n                        i = res.start(0)\n                    else:\n                        self.unknown_entityref(name)\n                    self.lineno = self.lineno + res.group(0).count('\\n')\n                    continue\n            elif rawdata[i] == ']':\n                if self.literal:\n                    data = rawdata[i]\n                    self.handle_data(data)\n                    i = i+1\n                    continue\n                if n-i < 3:\n                    break\n                if cdataclose.match(rawdata, i):\n                    self.syntax_error(\"bogus `]]>'\")\n                self.handle_data(rawdata[i])\n                i = i+1\n                continue\n            else:\n                raise Error('neither < nor & ??')\n            # We get here only if incomplete matches but\n            # nothing else\n            break\n        # end while\n        if i > 0:\n            self.__at_start = 0\n        if end and i < n:\n            data = rawdata[i]\n            self.syntax_error(\"bogus `%s'\" % data)\n            if not self.__accept_utf8 and illegal.search(data):\n                self.syntax_error('illegal character in content')\n            self.handle_data(data)\n            self.lineno = self.lineno + data.count('\\n')\n            self.rawdata = rawdata[i+1:]\n            return self.goahead(end)\n        self.rawdata = rawdata[i:]\n        if end:\n            if not self.__seen_starttag:\n                self.syntax_error('no elements in file')\n            if self.stack:\n                self.syntax_error('missing end tags')\n                while self.stack:\n                    self.finish_endtag(self.stack[-1][0])\n\n    # Internal -- parse comment, return length or -1 if not terminated\n    def parse_comment(self, i):\n        rawdata = self.rawdata\n        if rawdata[i:i+4] != '<!--':\n            raise Error('unexpected call to handle_comment')\n        res = commentclose.search(rawdata, i+4)\n        if res is None:\n            return -1\n        if doubledash.search(rawdata, i+4, res.start(0)):\n            self.syntax_error(\"`--' inside comment\")\n        if rawdata[res.start(0)-1] == '-':\n            self.syntax_error('comment cannot end in three dashes')\n        if not self.__accept_utf8 and \\\n           illegal.search(rawdata, i+4, res.start(0)):\n            self.syntax_error('illegal character in comment')\n        self.handle_comment(rawdata[i+4: res.start(0)])\n        return res.end(0)\n\n    # Internal -- handle DOCTYPE tag, return length or -1 if not terminated\n    def parse_doctype(self, res):\n        rawdata = self.rawdata\n        n = len(rawdata)\n        name = res.group('name')\n        if self.__map_case:\n            name = name.lower()\n        pubid, syslit = res.group('pubid', 'syslit')\n        if pubid is not None:\n            pubid = pubid[1:-1]         # remove quotes\n            pubid = ' '.join(pubid.split()) # normalize\n        if syslit is not None: syslit = syslit[1:-1] # remove quotes\n        j = k = res.end(0)\n        if k >= n:\n            return -1\n        if rawdata[k] == '[':\n            level = 0\n            k = k+1\n            dq = sq = 0\n            while k < n:\n                c = rawdata[k]\n                if not sq and c == '\"':\n                    dq = not dq\n                elif not dq and c == \"'\":\n                    sq = not sq\n                elif sq or dq:\n                    pass\n                elif level <= 0 and c == ']':\n                    res = endbracket.match(rawdata, k+1)\n                    if res is None:\n                        return -1\n                    self.handle_doctype(name, pubid, syslit, rawdata[j+1:k])\n                    return res.end(0)\n                elif c == '<':\n                    level = level + 1\n                elif c == '>':\n                    level = level - 1\n                    if level < 0:\n                        self.syntax_error(\"bogus `>' in DOCTYPE\")\n                k = k+1\n        res = endbracketfind.match(rawdata, k)\n        if res is None:\n            return -1\n        if endbracket.match(rawdata, k) is None:\n            self.syntax_error('garbage in DOCTYPE')\n        self.handle_doctype(name, pubid, syslit, None)\n        return res.end(0)\n\n    # Internal -- handle CDATA tag, return length or -1 if not terminated\n    def parse_cdata(self, i):\n        rawdata = self.rawdata\n        if rawdata[i:i+9] != '<![CDATA[':\n            raise Error('unexpected call to parse_cdata')\n        res = cdataclose.search(rawdata, i+9)\n        if res is None:\n            return -1\n        if not self.__accept_utf8 and \\\n           illegal.search(rawdata, i+9, res.start(0)):\n            self.syntax_error('illegal character in CDATA')\n        if not self.stack:\n            self.syntax_error('CDATA not in content')\n        self.handle_cdata(rawdata[i+9:res.start(0)])\n        return res.end(0)\n\n    __xml_namespace_attributes = {'ns':None, 'src':None, 'prefix':None}\n    # Internal -- handle a processing instruction tag\n    def parse_proc(self, i):\n        rawdata = self.rawdata\n        end = procclose.search(rawdata, i)\n        if end is None:\n            return -1\n        j = end.start(0)\n        if not self.__accept_utf8 and illegal.search(rawdata, i+2, j):\n            self.syntax_error('illegal character in processing instruction')\n        res = tagfind.match(rawdata, i+2)\n        if res is None:\n            raise Error('unexpected call to parse_proc')\n        k = res.end(0)\n        name = res.group(0)\n        if self.__map_case:\n            name = name.lower()\n        if name == 'xml:namespace':\n            self.syntax_error('old-fashioned namespace declaration')\n            self.__use_namespaces = -1\n            # namespace declaration\n            # this must come after the <?xml?> declaration (if any)\n            # and before the <!DOCTYPE> (if any).\n            if self.__seen_doctype or self.__seen_starttag:\n                self.syntax_error('xml:namespace declaration too late in document')\n            attrdict, namespace, k = self.parse_attributes(name, k, j)\n            if namespace:\n                self.syntax_error('namespace declaration inside namespace declaration')\n            for attrname in attrdict.keys():\n                if not attrname in self.__xml_namespace_attributes:\n                    self.syntax_error(\"unknown attribute `%s' in xml:namespace tag\" % attrname)\n            if not 'ns' in attrdict or not 'prefix' in attrdict:\n                self.syntax_error('xml:namespace without required attributes')\n            prefix = attrdict.get('prefix')\n            if ncname.match(prefix) is None:\n                self.syntax_error('xml:namespace illegal prefix value')\n                return end.end(0)\n            if prefix in self.__namespaces:\n                self.syntax_error('xml:namespace prefix not unique')\n            self.__namespaces[prefix] = attrdict['ns']\n        else:\n            if name.lower() == 'xml':\n                self.syntax_error('illegal processing instruction target name')\n            self.handle_proc(name, rawdata[k:j])\n        return end.end(0)\n\n    # Internal -- parse attributes between i and j\n    def parse_attributes(self, tag, i, j):\n        rawdata = self.rawdata\n        attrdict = {}\n        namespace = {}\n        while i < j:\n            res = attrfind.match(rawdata, i)\n            if res is None:\n                break\n            attrname, attrvalue = res.group('name', 'value')\n            if self.__map_case:\n                attrname = attrname.lower()\n            i = res.end(0)\n            if attrvalue is None:\n                self.syntax_error(\"no value specified for attribute `%s'\" % attrname)\n                attrvalue = attrname\n            elif attrvalue[:1] == \"'\" == attrvalue[-1:] or \\\n                 attrvalue[:1] == '\"' == attrvalue[-1:]:\n                attrvalue = attrvalue[1:-1]\n            elif not self.__accept_unquoted_attributes:\n                self.syntax_error(\"attribute `%s' value not quoted\" % attrname)\n            res = xmlns.match(attrname)\n            if res is not None:\n                # namespace declaration\n                ncname = res.group('ncname')\n                namespace[ncname or ''] = attrvalue or None\n                if not self.__use_namespaces:\n                    self.__use_namespaces = len(self.stack)+1\n                continue\n            if '<' in attrvalue:\n                self.syntax_error(\"`<' illegal in attribute value\")\n            if attrname in attrdict:\n                self.syntax_error(\"attribute `%s' specified twice\" % attrname)\n            attrvalue = attrvalue.translate(attrtrans)\n            attrdict[attrname] = self.translate_references(attrvalue)\n        return attrdict, namespace, i\n\n    # Internal -- handle starttag, return length or -1 if not terminated\n    def parse_starttag(self, i):\n        rawdata = self.rawdata\n        # i points to start of tag\n        end = endbracketfind.match(rawdata, i+1)\n        if end is None:\n            return -1\n        tag = starttagmatch.match(rawdata, i)\n        if tag is None or tag.end(0) != end.end(0):\n            self.syntax_error('garbage in starttag')\n            return end.end(0)\n        nstag = tagname = tag.group('tagname')\n        if self.__map_case:\n            nstag = tagname = nstag.lower()\n        if not self.__seen_starttag and self.__seen_doctype and \\\n           tagname != self.__seen_doctype:\n            self.syntax_error('starttag does not match DOCTYPE')\n        if self.__seen_starttag and not self.stack:\n            self.syntax_error('multiple elements on top level')\n        k, j = tag.span('attrs')\n        attrdict, nsdict, k = self.parse_attributes(tagname, k, j)\n        self.stack.append((tagname, nsdict, nstag))\n        if self.__use_namespaces:\n            res = qname.match(tagname)\n        else:\n            res = None\n        if res is not None:\n            prefix, nstag = res.group('prefix', 'local')\n            if prefix is None:\n                prefix = ''\n            ns = None\n            for t, d, nst in self.stack:\n                if prefix in d:\n                    ns = d[prefix]\n            if ns is None and prefix != '':\n                ns = self.__namespaces.get(prefix)\n            if ns is not None:\n                nstag = ns + ' ' + nstag\n            elif prefix != '':\n                nstag = prefix + ':' + nstag # undo split\n            self.stack[-1] = tagname, nsdict, nstag\n        # translate namespace of attributes\n        attrnamemap = {} # map from new name to old name (used for error reporting)\n        for key in attrdict.keys():\n            attrnamemap[key] = key\n        if self.__use_namespaces:\n            nattrdict = {}\n            for key, val in attrdict.items():\n                okey = key\n                res = qname.match(key)\n                if res is not None:\n                    aprefix, key = res.group('prefix', 'local')\n                    if self.__map_case:\n                        key = key.lower()\n                    if aprefix is not None:\n                        ans = None\n                        for t, d, nst in self.stack:\n                            if aprefix in d:\n                                ans = d[aprefix]\n                        if ans is None:\n                            ans = self.__namespaces.get(aprefix)\n                        if ans is not None:\n                            key = ans + ' ' + key\n                        else:\n                            key = aprefix + ':' + key\n                nattrdict[key] = val\n                attrnamemap[key] = okey\n            attrdict = nattrdict\n        attributes = self.attributes.get(nstag)\n        if attributes is not None:\n            for key in attrdict.keys():\n                if not key in attributes:\n                    self.syntax_error(\"unknown attribute `%s' in tag `%s'\" % (attrnamemap[key], tagname))\n            for key, val in attributes.items():\n                if val is not None and not key in attrdict:\n                    attrdict[key] = val\n        method = self.elements.get(nstag, (None, None))[0]\n        self.finish_starttag(nstag, attrdict, method)\n        if tag.group('slash') == '/':\n            self.finish_endtag(tagname)\n        return tag.end(0)\n\n    # Internal -- parse endtag\n    def parse_endtag(self, i):\n        rawdata = self.rawdata\n        end = endbracketfind.match(rawdata, i+1)\n        if end is None:\n            return -1\n        res = tagfind.match(rawdata, i+2)\n        if res is None:\n            if self.literal:\n                self.handle_data(rawdata[i])\n                return i+1\n            if not self.__accept_missing_endtag_name:\n                self.syntax_error('no name specified in end tag')\n            tag = self.stack[-1][0]\n            k = i+2\n        else:\n            tag = res.group(0)\n            if self.__map_case:\n                tag = tag.lower()\n            if self.literal:\n                if not self.stack or tag != self.stack[-1][0]:\n                    self.handle_data(rawdata[i])\n                    return i+1\n            k = res.end(0)\n        if endbracket.match(rawdata, k) is None:\n            self.syntax_error('garbage in end tag')\n        self.finish_endtag(tag)\n        return end.end(0)\n\n    # Internal -- finish processing of start tag\n    def finish_starttag(self, tagname, attrdict, method):\n        if method is not None:\n            self.handle_starttag(tagname, method, attrdict)\n        else:\n            self.unknown_starttag(tagname, attrdict)\n\n    # Internal -- finish processing of end tag\n    def finish_endtag(self, tag):\n        self.literal = 0\n        if not tag:\n            self.syntax_error('name-less end tag')\n            found = len(self.stack) - 1\n            if found < 0:\n                self.unknown_endtag(tag)\n                return\n        else:\n            found = -1\n            for i in range(len(self.stack)):\n                if tag == self.stack[i][0]:\n                    found = i\n            if found == -1:\n                self.syntax_error('unopened end tag')\n                return\n        while len(self.stack) > found:\n            if found < len(self.stack) - 1:\n                self.syntax_error('missing close tag for %s' % self.stack[-1][2])\n            nstag = self.stack[-1][2]\n            method = self.elements.get(nstag, (None, None))[1]\n            if method is not None:\n                self.handle_endtag(nstag, method)\n            else:\n                self.unknown_endtag(nstag)\n            if self.__use_namespaces == len(self.stack):\n                self.__use_namespaces = 0\n            del self.stack[-1]\n\n    # Overridable -- handle xml processing instruction\n    def handle_xml(self, encoding, standalone):\n        pass\n\n    # Overridable -- handle DOCTYPE\n    def handle_doctype(self, tag, pubid, syslit, data):\n        pass\n\n    # Overridable -- handle start tag\n    def handle_starttag(self, tag, method, attrs):\n        method(attrs)\n\n    # Overridable -- handle end tag\n    def handle_endtag(self, tag, method):\n        method()\n\n    # Example -- handle character reference, no need to override\n    def handle_charref(self, name):\n        try:\n            if name[0] == 'x':\n                n = int(name[1:], 16)\n            else:\n                n = int(name)\n        except ValueError:\n            self.unknown_charref(name)\n            return\n        if not 0 <= n <= 255:\n            self.unknown_charref(name)\n            return\n        self.handle_data(chr(n))\n\n    # Definition of entities -- derived classes may override\n    entitydefs = {'lt': '&#60;',        # must use charref\n                  'gt': '&#62;',\n                  'amp': '&#38;',       # must use charref\n                  'quot': '&#34;',\n                  'apos': '&#39;',\n                  }\n\n    # Example -- handle data, should be overridden\n    def handle_data(self, data):\n        pass\n\n    # Example -- handle cdata, could be overridden\n    def handle_cdata(self, data):\n        pass\n\n    # Example -- handle comment, could be overridden\n    def handle_comment(self, data):\n        pass\n\n    # Example -- handle processing instructions, could be overridden\n    def handle_proc(self, name, data):\n        pass\n\n    # Example -- handle relatively harmless syntax errors, could be overridden\n    def syntax_error(self, message):\n        raise Error('Syntax error at line %d: %s' % (self.lineno, message))\n\n    # To be overridden -- handlers for unknown objects\n    def unknown_starttag(self, tag, attrs): pass\n    def unknown_endtag(self, tag): pass\n    def unknown_charref(self, ref): pass\n    def unknown_entityref(self, name):\n        self.syntax_error(\"reference to unknown entity `&%s;'\" % name)\n\n\nclass TestXMLParser(XMLParser):\n\n    def __init__(self, **kw):\n        self.testdata = \"\"\n        XMLParser.__init__(self, **kw)\n\n    def handle_xml(self, encoding, standalone):\n        self.flush()\n        print 'xml: encoding =',encoding,'standalone =',standalone\n\n    def handle_doctype(self, tag, pubid, syslit, data):\n        self.flush()\n        print 'DOCTYPE:',tag, repr(data)\n\n    def handle_data(self, data):\n        self.testdata = self.testdata + data\n        if len(repr(self.testdata)) >= 70:\n            self.flush()\n\n    def flush(self):\n        data = self.testdata\n        if data:\n            self.testdata = \"\"\n            print 'data:', repr(data)\n\n    def handle_cdata(self, data):\n        self.flush()\n        print 'cdata:', repr(data)\n\n    def handle_proc(self, name, data):\n        self.flush()\n        print 'processing:',name,repr(data)\n\n    def handle_comment(self, data):\n        self.flush()\n        r = repr(data)\n        if len(r) > 68:\n            r = r[:32] + '...' + r[-32:]\n        print 'comment:', r\n\n    def syntax_error(self, message):\n        print 'error at line %d:' % self.lineno, message\n\n    def unknown_starttag(self, tag, attrs):\n        self.flush()\n        if not attrs:\n            print 'start tag: <' + tag + '>'\n        else:\n            print 'start tag: <' + tag,\n            for name, value in attrs.items():\n                print name + '=' + '\"' + value + '\"',\n            print '>'\n\n    def unknown_endtag(self, tag):\n        self.flush()\n        print 'end tag: </' + tag + '>'\n\n    def unknown_entityref(self, ref):\n        self.flush()\n        print '*** unknown entity ref: &' + ref + ';'\n\n    def unknown_charref(self, ref):\n        self.flush()\n        print '*** unknown char ref: &#' + ref + ';'\n\n    def close(self):\n        XMLParser.close(self)\n        self.flush()\n\ndef test(args = None):\n    import sys, getopt\n    from time import time\n\n    if not args:\n        args = sys.argv[1:]\n\n    opts, args = getopt.getopt(args, 'st')\n    klass = TestXMLParser\n    do_time = 0\n    for o, a in opts:\n        if o == '-s':\n            klass = XMLParser\n        elif o == '-t':\n            do_time = 1\n\n    if args:\n        file = args[0]\n    else:\n        file = 'test.xml'\n\n    if file == '-':\n        f = sys.stdin\n    else:\n        try:\n            f = open(file, 'r')\n        except IOError, msg:\n            print file, \":\", msg\n            sys.exit(1)\n\n    data = f.read()\n    if f is not sys.stdin:\n        f.close()\n\n    x = klass()\n    t0 = time()\n    try:\n        if do_time:\n            x.feed(data)\n            x.close()\n        else:\n            for c in data:\n                x.feed(c)\n            x.close()\n    except Error, msg:\n        t1 = time()\n        print msg\n        if do_time:\n            print 'total time: %g' % (t1-t0)\n        sys.exit(1)\n    t1 = time()\n    if do_time:\n        print 'total time: %g' % (t1-t0)\n\n\nif __name__ == '__main__':\n    test()\n", 
    "xmlrpclib": "#\n# XML-RPC CLIENT LIBRARY\n# $Id$\n#\n# an XML-RPC client interface for Python.\n#\n# the marshalling and response parser code can also be used to\n# implement XML-RPC servers.\n#\n# Notes:\n# this version is designed to work with Python 2.1 or newer.\n#\n# History:\n# 1999-01-14 fl  Created\n# 1999-01-15 fl  Changed dateTime to use localtime\n# 1999-01-16 fl  Added Binary/base64 element, default to RPC2 service\n# 1999-01-19 fl  Fixed array data element (from Skip Montanaro)\n# 1999-01-21 fl  Fixed dateTime constructor, etc.\n# 1999-02-02 fl  Added fault handling, handle empty sequences, etc.\n# 1999-02-10 fl  Fixed problem with empty responses (from Skip Montanaro)\n# 1999-06-20 fl  Speed improvements, pluggable parsers/transports (0.9.8)\n# 2000-11-28 fl  Changed boolean to check the truth value of its argument\n# 2001-02-24 fl  Added encoding/Unicode/SafeTransport patches\n# 2001-02-26 fl  Added compare support to wrappers (0.9.9/1.0b1)\n# 2001-03-28 fl  Make sure response tuple is a singleton\n# 2001-03-29 fl  Don't require empty params element (from Nicholas Riley)\n# 2001-06-10 fl  Folded in _xmlrpclib accelerator support (1.0b2)\n# 2001-08-20 fl  Base xmlrpclib.Error on built-in Exception (from Paul Prescod)\n# 2001-09-03 fl  Allow Transport subclass to override getparser\n# 2001-09-10 fl  Lazy import of urllib, cgi, xmllib (20x import speedup)\n# 2001-10-01 fl  Remove containers from memo cache when done with them\n# 2001-10-01 fl  Use faster escape method (80% dumps speedup)\n# 2001-10-02 fl  More dumps microtuning\n# 2001-10-04 fl  Make sure import expat gets a parser (from Guido van Rossum)\n# 2001-10-10 sm  Allow long ints to be passed as ints if they don't overflow\n# 2001-10-17 sm  Test for int and long overflow (allows use on 64-bit systems)\n# 2001-11-12 fl  Use repr() to marshal doubles (from Paul Felix)\n# 2002-03-17 fl  Avoid buffered read when possible (from James Rucker)\n# 2002-04-07 fl  Added pythondoc comments\n# 2002-04-16 fl  Added __str__ methods to datetime/binary wrappers\n# 2002-05-15 fl  Added error constants (from Andrew Kuchling)\n# 2002-06-27 fl  Merged with Python CVS version\n# 2002-10-22 fl  Added basic authentication (based on code from Phillip Eby)\n# 2003-01-22 sm  Add support for the bool type\n# 2003-02-27 gvr Remove apply calls\n# 2003-04-24 sm  Use cStringIO if available\n# 2003-04-25 ak  Add support for nil\n# 2003-06-15 gn  Add support for time.struct_time\n# 2003-07-12 gp  Correct marshalling of Faults\n# 2003-10-31 mvl Add multicall support\n# 2004-08-20 mvl Bump minimum supported Python version to 2.1\n# 2014-12-02 ch/doko  Add workaround for gzip bomb vulnerability\n#\n# Copyright (c) 1999-2002 by Secret Labs AB.\n# Copyright (c) 1999-2002 by Fredrik Lundh.\n#\n# info@pythonware.com\n# http://www.pythonware.com\n#\n# --------------------------------------------------------------------\n# The XML-RPC client interface is\n#\n# Copyright (c) 1999-2002 by Secret Labs AB\n# Copyright (c) 1999-2002 by Fredrik Lundh\n#\n# By obtaining, using, and/or copying this software and/or its\n# associated documentation, you agree that you have read, understood,\n# and will comply with the following terms and conditions:\n#\n# Permission to use, copy, modify, and distribute this software and\n# its associated documentation for any purpose and without fee is\n# hereby granted, provided that the above copyright notice appears in\n# all copies, and that both that copyright notice and this permission\n# notice appear in supporting documentation, and that the name of\n# Secret Labs AB or the author not be used in advertising or publicity\n# pertaining to distribution of the software without specific, written\n# prior permission.\n#\n# SECRET LABS AB AND THE AUTHOR DISCLAIMS ALL WARRANTIES WITH REGARD\n# TO THIS SOFTWARE, INCLUDING ALL IMPLIED WARRANTIES OF MERCHANT-\n# ABILITY AND FITNESS.  IN NO EVENT SHALL SECRET LABS AB OR THE AUTHOR\n# BE LIABLE FOR ANY SPECIAL, INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY\n# DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS,\n# WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS\n# ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE\n# OF THIS SOFTWARE.\n# --------------------------------------------------------------------\n\n#\n# things to look into some day:\n\n# TODO: sort out True/False/boolean issues for Python 2.3\n\n\"\"\"\nAn XML-RPC client interface for Python.\n\nThe marshalling and response parser code can also be used to\nimplement XML-RPC servers.\n\nExported exceptions:\n\n  Error          Base class for client errors\n  ProtocolError  Indicates an HTTP protocol error\n  ResponseError  Indicates a broken response package\n  Fault          Indicates an XML-RPC fault package\n\nExported classes:\n\n  ServerProxy    Represents a logical connection to an XML-RPC server\n\n  MultiCall      Executor of boxcared xmlrpc requests\n  Boolean        boolean wrapper to generate a \"boolean\" XML-RPC value\n  DateTime       dateTime wrapper for an ISO 8601 string or time tuple or\n                 localtime integer value to generate a \"dateTime.iso8601\"\n                 XML-RPC value\n  Binary         binary data wrapper\n\n  SlowParser     Slow but safe standard parser (based on xmllib)\n  Marshaller     Generate an XML-RPC params chunk from a Python data structure\n  Unmarshaller   Unmarshal an XML-RPC response from incoming XML event message\n  Transport      Handles an HTTP transaction to an XML-RPC server\n  SafeTransport  Handles an HTTPS transaction to an XML-RPC server\n\nExported constants:\n\n  True\n  False\n\nExported functions:\n\n  boolean        Convert any Python value to an XML-RPC boolean\n  getparser      Create instance of the fastest available parser & attach\n                 to an unmarshalling object\n  dumps          Convert an argument tuple or a Fault instance to an XML-RPC\n                 request (or response, if the methodresponse option is used).\n  loads          Convert an XML-RPC packet to unmarshalled data plus a method\n                 name (None if not present).\n\"\"\"\n\nimport re, string, time, operator\n\nfrom types import *\nimport socket\nimport errno\nimport httplib\ntry:\n    import gzip\nexcept ImportError:\n    gzip = None #python can be built without zlib/gzip support\n\n# --------------------------------------------------------------------\n# Internal stuff\n\ntry:\n    unicode\nexcept NameError:\n    unicode = None # unicode support not available\n\ntry:\n    import datetime\nexcept ImportError:\n    datetime = None\n\ntry:\n    _bool_is_builtin = False.__class__.__name__ == \"bool\"\nexcept NameError:\n    _bool_is_builtin = 0\n\ndef _decode(data, encoding, is8bit=re.compile(\"[\\x80-\\xff]\").search):\n    # decode non-ascii string (if possible)\n    if unicode and encoding and is8bit(data):\n        data = unicode(data, encoding)\n    return data\n\ndef escape(s, replace=string.replace):\n    s = replace(s, \"&\", \"&amp;\")\n    s = replace(s, \"<\", \"&lt;\")\n    return replace(s, \">\", \"&gt;\",)\n\nif unicode:\n    def _stringify(string):\n        # convert to 7-bit ascii if possible\n        try:\n            return string.encode(\"ascii\")\n        except UnicodeError:\n            return string\nelse:\n    def _stringify(string):\n        return string\n\n__version__ = \"1.0.1\"\n\n# xmlrpc integer limits\nMAXINT =  2L**31-1\nMININT = -2L**31\n\n# --------------------------------------------------------------------\n# Error constants (from Dan Libby's specification at\n# http://xmlrpc-epi.sourceforge.net/specs/rfc.fault_codes.php)\n\n# Ranges of errors\nPARSE_ERROR       = -32700\nSERVER_ERROR      = -32600\nAPPLICATION_ERROR = -32500\nSYSTEM_ERROR      = -32400\nTRANSPORT_ERROR   = -32300\n\n# Specific errors\nNOT_WELLFORMED_ERROR  = -32700\nUNSUPPORTED_ENCODING  = -32701\nINVALID_ENCODING_CHAR = -32702\nINVALID_XMLRPC        = -32600\nMETHOD_NOT_FOUND      = -32601\nINVALID_METHOD_PARAMS = -32602\nINTERNAL_ERROR        = -32603\n\n# --------------------------------------------------------------------\n# Exceptions\n\n##\n# Base class for all kinds of client-side errors.\n\nclass Error(Exception):\n    \"\"\"Base class for client errors.\"\"\"\n    def __str__(self):\n        return repr(self)\n\n##\n# Indicates an HTTP-level protocol error.  This is raised by the HTTP\n# transport layer, if the server returns an error code other than 200\n# (OK).\n#\n# @param url The target URL.\n# @param errcode The HTTP error code.\n# @param errmsg The HTTP error message.\n# @param headers The HTTP header dictionary.\n\nclass ProtocolError(Error):\n    \"\"\"Indicates an HTTP protocol error.\"\"\"\n    def __init__(self, url, errcode, errmsg, headers):\n        Error.__init__(self)\n        self.url = url\n        self.errcode = errcode\n        self.errmsg = errmsg\n        self.headers = headers\n    def __repr__(self):\n        return (\n            \"<ProtocolError for %s: %s %s>\" %\n            (self.url, self.errcode, self.errmsg)\n            )\n\n##\n# Indicates a broken XML-RPC response package.  This exception is\n# raised by the unmarshalling layer, if the XML-RPC response is\n# malformed.\n\nclass ResponseError(Error):\n    \"\"\"Indicates a broken response package.\"\"\"\n    pass\n\n##\n# Indicates an XML-RPC fault response package.  This exception is\n# raised by the unmarshalling layer, if the XML-RPC response contains\n# a fault string.  This exception can also used as a class, to\n# generate a fault XML-RPC message.\n#\n# @param faultCode The XML-RPC fault code.\n# @param faultString The XML-RPC fault string.\n\nclass Fault(Error):\n    \"\"\"Indicates an XML-RPC fault package.\"\"\"\n    def __init__(self, faultCode, faultString, **extra):\n        Error.__init__(self)\n        self.faultCode = faultCode\n        self.faultString = faultString\n    def __repr__(self):\n        return (\n            \"<Fault %s: %s>\" %\n            (self.faultCode, repr(self.faultString))\n            )\n\n# --------------------------------------------------------------------\n# Special values\n\n##\n# Wrapper for XML-RPC boolean values.  Use the xmlrpclib.True and\n# xmlrpclib.False constants, or the xmlrpclib.boolean() function, to\n# generate boolean XML-RPC values.\n#\n# @param value A boolean value.  Any true value is interpreted as True,\n#              all other values are interpreted as False.\n\nfrom sys import modules\nmod_dict = modules[__name__].__dict__\nif _bool_is_builtin:\n    boolean = Boolean = bool\n    # to avoid breaking code which references xmlrpclib.{True,False}\n    mod_dict['True'] = True\n    mod_dict['False'] = False\nelse:\n    class Boolean:\n        \"\"\"Boolean-value wrapper.\n\n        Use True or False to generate a \"boolean\" XML-RPC value.\n        \"\"\"\n\n        def __init__(self, value = 0):\n            self.value = operator.truth(value)\n\n        def encode(self, out):\n            out.write(\"<value><boolean>%d</boolean></value>\\n\" % self.value)\n\n        def __cmp__(self, other):\n            if isinstance(other, Boolean):\n                other = other.value\n            return cmp(self.value, other)\n\n        def __repr__(self):\n            if self.value:\n                return \"<Boolean True at %x>\" % id(self)\n            else:\n                return \"<Boolean False at %x>\" % id(self)\n\n        def __int__(self):\n            return self.value\n\n        def __nonzero__(self):\n            return self.value\n\n    mod_dict['True'] = Boolean(1)\n    mod_dict['False'] = Boolean(0)\n\n    ##\n    # Map true or false value to XML-RPC boolean values.\n    #\n    # @def boolean(value)\n    # @param value A boolean value.  Any true value is mapped to True,\n    #              all other values are mapped to False.\n    # @return xmlrpclib.True or xmlrpclib.False.\n    # @see Boolean\n    # @see True\n    # @see False\n\n    def boolean(value, _truefalse=(False, True)):\n        \"\"\"Convert any Python value to XML-RPC 'boolean'.\"\"\"\n        return _truefalse[operator.truth(value)]\n\ndel modules, mod_dict\n\n##\n# Wrapper for XML-RPC DateTime values.  This converts a time value to\n# the format used by XML-RPC.\n# <p>\n# The value can be given as a string in the format\n# \"yyyymmddThh:mm:ss\", as a 9-item time tuple (as returned by\n# time.localtime()), or an integer value (as returned by time.time()).\n# The wrapper uses time.localtime() to convert an integer to a time\n# tuple.\n#\n# @param value The time, given as an ISO 8601 string, a time\n#              tuple, or a integer time value.\n\ndef _strftime(value):\n    if datetime:\n        if isinstance(value, datetime.datetime):\n            return \"%04d%02d%02dT%02d:%02d:%02d\" % (\n                value.year, value.month, value.day,\n                value.hour, value.minute, value.second)\n\n    if not isinstance(value, (TupleType, time.struct_time)):\n        if value == 0:\n            value = time.time()\n        value = time.localtime(value)\n\n    return \"%04d%02d%02dT%02d:%02d:%02d\" % value[:6]\n\nclass DateTime:\n    \"\"\"DateTime wrapper for an ISO 8601 string or time tuple or\n    localtime integer value to generate 'dateTime.iso8601' XML-RPC\n    value.\n    \"\"\"\n\n    def __init__(self, value=0):\n        if isinstance(value, StringType):\n            self.value = value\n        else:\n            self.value = _strftime(value)\n\n    def make_comparable(self, other):\n        if isinstance(other, DateTime):\n            s = self.value\n            o = other.value\n        elif datetime and isinstance(other, datetime.datetime):\n            s = self.value\n            o = other.strftime(\"%Y%m%dT%H:%M:%S\")\n        elif isinstance(other, (str, unicode)):\n            s = self.value\n            o = other\n        elif hasattr(other, \"timetuple\"):\n            s = self.timetuple()\n            o = other.timetuple()\n        else:\n            otype = (hasattr(other, \"__class__\")\n                     and other.__class__.__name__\n                     or type(other))\n            raise TypeError(\"Can't compare %s and %s\" %\n                            (self.__class__.__name__, otype))\n        return s, o\n\n    def __lt__(self, other):\n        s, o = self.make_comparable(other)\n        return s < o\n\n    def __le__(self, other):\n        s, o = self.make_comparable(other)\n        return s <= o\n\n    def __gt__(self, other):\n        s, o = self.make_comparable(other)\n        return s > o\n\n    def __ge__(self, other):\n        s, o = self.make_comparable(other)\n        return s >= o\n\n    def __eq__(self, other):\n        s, o = self.make_comparable(other)\n        return s == o\n\n    def __ne__(self, other):\n        s, o = self.make_comparable(other)\n        return s != o\n\n    def timetuple(self):\n        return time.strptime(self.value, \"%Y%m%dT%H:%M:%S\")\n\n    def __cmp__(self, other):\n        s, o = self.make_comparable(other)\n        return cmp(s, o)\n\n    ##\n    # Get date/time value.\n    #\n    # @return Date/time value, as an ISO 8601 string.\n\n    def __str__(self):\n        return self.value\n\n    def __repr__(self):\n        return \"<DateTime %s at %x>\" % (repr(self.value), id(self))\n\n    def decode(self, data):\n        data = str(data)\n        self.value = string.strip(data)\n\n    def encode(self, out):\n        out.write(\"<value><dateTime.iso8601>\")\n        out.write(self.value)\n        out.write(\"</dateTime.iso8601></value>\\n\")\n\ndef _datetime(data):\n    # decode xml element contents into a DateTime structure.\n    value = DateTime()\n    value.decode(data)\n    return value\n\ndef _datetime_type(data):\n    t = time.strptime(data, \"%Y%m%dT%H:%M:%S\")\n    return datetime.datetime(*tuple(t)[:6])\n\n##\n# Wrapper for binary data.  This can be used to transport any kind\n# of binary data over XML-RPC, using BASE64 encoding.\n#\n# @param data An 8-bit string containing arbitrary data.\n\nimport base64\ntry:\n    import cStringIO as StringIO\nexcept ImportError:\n    import StringIO\n\nclass Binary:\n    \"\"\"Wrapper for binary data.\"\"\"\n\n    def __init__(self, data=None):\n        self.data = data\n\n    ##\n    # Get buffer contents.\n    #\n    # @return Buffer contents, as an 8-bit string.\n\n    def __str__(self):\n        return self.data or \"\"\n\n    def __cmp__(self, other):\n        if isinstance(other, Binary):\n            other = other.data\n        return cmp(self.data, other)\n\n    def decode(self, data):\n        self.data = base64.decodestring(data)\n\n    def encode(self, out):\n        out.write(\"<value><base64>\\n\")\n        base64.encode(StringIO.StringIO(self.data), out)\n        out.write(\"</base64></value>\\n\")\n\ndef _binary(data):\n    # decode xml element contents into a Binary structure\n    value = Binary()\n    value.decode(data)\n    return value\n\nWRAPPERS = (DateTime, Binary)\nif not _bool_is_builtin:\n    WRAPPERS = WRAPPERS + (Boolean,)\n\n# --------------------------------------------------------------------\n# XML parsers\n\ntry:\n    # optional xmlrpclib accelerator\n    import _xmlrpclib\n    FastParser = _xmlrpclib.Parser\n    FastUnmarshaller = _xmlrpclib.Unmarshaller\nexcept (AttributeError, ImportError):\n    FastParser = FastUnmarshaller = None\n\ntry:\n    import _xmlrpclib\n    FastMarshaller = _xmlrpclib.Marshaller\nexcept (AttributeError, ImportError):\n    FastMarshaller = None\n\ntry:\n    from xml.parsers import expat\n    if not hasattr(expat, \"ParserCreate\"):\n        raise ImportError\nexcept ImportError:\n    ExpatParser = None # expat not available\nelse:\n    class ExpatParser:\n        # fast expat parser for Python 2.0 and later.\n        def __init__(self, target):\n            self._parser = parser = expat.ParserCreate(None, None)\n            self._target = target\n            parser.StartElementHandler = target.start\n            parser.EndElementHandler = target.end\n            parser.CharacterDataHandler = target.data\n            encoding = None\n            if not parser.returns_unicode:\n                encoding = \"utf-8\"\n            target.xml(encoding, None)\n\n        def feed(self, data):\n            self._parser.Parse(data, 0)\n\n        def close(self):\n            self._parser.Parse(\"\", 1) # end of data\n            del self._target, self._parser # get rid of circular references\n\nclass SlowParser:\n    \"\"\"Default XML parser (based on xmllib.XMLParser).\"\"\"\n    # this is the slowest parser.\n    def __init__(self, target):\n        import xmllib # lazy subclassing (!)\n        if xmllib.XMLParser not in SlowParser.__bases__:\n            SlowParser.__bases__ = (xmllib.XMLParser,)\n        self.handle_xml = target.xml\n        self.unknown_starttag = target.start\n        self.handle_data = target.data\n        self.handle_cdata = target.data\n        self.unknown_endtag = target.end\n        try:\n            xmllib.XMLParser.__init__(self, accept_utf8=1)\n        except TypeError:\n            xmllib.XMLParser.__init__(self) # pre-2.0\n\n# --------------------------------------------------------------------\n# XML-RPC marshalling and unmarshalling code\n\n##\n# XML-RPC marshaller.\n#\n# @param encoding Default encoding for 8-bit strings.  The default\n#     value is None (interpreted as UTF-8).\n# @see dumps\n\nclass Marshaller:\n    \"\"\"Generate an XML-RPC params chunk from a Python data structure.\n\n    Create a Marshaller instance for each set of parameters, and use\n    the \"dumps\" method to convert your data (represented as a tuple)\n    to an XML-RPC params chunk.  To write a fault response, pass a\n    Fault instance instead.  You may prefer to use the \"dumps\" module\n    function for this purpose.\n    \"\"\"\n\n    # by the way, if you don't understand what's going on in here,\n    # that's perfectly ok.\n\n    def __init__(self, encoding=None, allow_none=0):\n        self.memo = {}\n        self.data = None\n        self.encoding = encoding\n        self.allow_none = allow_none\n\n    dispatch = {}\n\n    def dumps(self, values):\n        out = []\n        write = out.append\n        dump = self.__dump\n        if isinstance(values, Fault):\n            # fault instance\n            write(\"<fault>\\n\")\n            dump({'faultCode': values.faultCode,\n                  'faultString': values.faultString},\n                 write)\n            write(\"</fault>\\n\")\n        else:\n            # parameter block\n            # FIXME: the xml-rpc specification allows us to leave out\n            # the entire <params> block if there are no parameters.\n            # however, changing this may break older code (including\n            # old versions of xmlrpclib.py), so this is better left as\n            # is for now.  See @XMLRPC3 for more information. /F\n            write(\"<params>\\n\")\n            for v in values:\n                write(\"<param>\\n\")\n                dump(v, write)\n                write(\"</param>\\n\")\n            write(\"</params>\\n\")\n        result = string.join(out, \"\")\n        return result\n\n    def __dump(self, value, write):\n        try:\n            f = self.dispatch[type(value)]\n        except KeyError:\n            # check if this object can be marshalled as a structure\n            try:\n                value.__dict__\n            except:\n                raise TypeError, \"cannot marshal %s objects\" % type(value)\n            # check if this class is a sub-class of a basic type,\n            # because we don't know how to marshal these types\n            # (e.g. a string sub-class)\n            for type_ in type(value).__mro__:\n                if type_ in self.dispatch.keys():\n                    raise TypeError, \"cannot marshal %s objects\" % type(value)\n            f = self.dispatch[InstanceType]\n        f(self, value, write)\n\n    def dump_nil (self, value, write):\n        if not self.allow_none:\n            raise TypeError, \"cannot marshal None unless allow_none is enabled\"\n        write(\"<value><nil/></value>\")\n    dispatch[NoneType] = dump_nil\n\n    def dump_int(self, value, write):\n        # in case ints are > 32 bits\n        if value > MAXINT or value < MININT:\n            raise OverflowError, \"int exceeds XML-RPC limits\"\n        write(\"<value><int>\")\n        write(str(value))\n        write(\"</int></value>\\n\")\n    dispatch[IntType] = dump_int\n\n    if _bool_is_builtin:\n        def dump_bool(self, value, write):\n            write(\"<value><boolean>\")\n            write(value and \"1\" or \"0\")\n            write(\"</boolean></value>\\n\")\n        dispatch[bool] = dump_bool\n\n    def dump_long(self, value, write):\n        if value > MAXINT or value < MININT:\n            raise OverflowError, \"long int exceeds XML-RPC limits\"\n        write(\"<value><int>\")\n        write(str(int(value)))\n        write(\"</int></value>\\n\")\n    dispatch[LongType] = dump_long\n\n    def dump_double(self, value, write):\n        write(\"<value><double>\")\n        write(repr(value))\n        write(\"</double></value>\\n\")\n    dispatch[FloatType] = dump_double\n\n    def dump_string(self, value, write, escape=escape):\n        write(\"<value><string>\")\n        write(escape(value))\n        write(\"</string></value>\\n\")\n    dispatch[StringType] = dump_string\n\n    if unicode:\n        def dump_unicode(self, value, write, escape=escape):\n            value = value.encode(self.encoding)\n            write(\"<value><string>\")\n            write(escape(value))\n            write(\"</string></value>\\n\")\n        dispatch[UnicodeType] = dump_unicode\n\n    def dump_array(self, value, write):\n        i = id(value)\n        if i in self.memo:\n            raise TypeError, \"cannot marshal recursive sequences\"\n        self.memo[i] = None\n        dump = self.__dump\n        write(\"<value><array><data>\\n\")\n        for v in value:\n            dump(v, write)\n        write(\"</data></array></value>\\n\")\n        del self.memo[i]\n    dispatch[TupleType] = dump_array\n    dispatch[ListType] = dump_array\n\n    def dump_struct(self, value, write, escape=escape):\n        i = id(value)\n        if i in self.memo:\n            raise TypeError, \"cannot marshal recursive dictionaries\"\n        self.memo[i] = None\n        dump = self.__dump\n        write(\"<value><struct>\\n\")\n        for k, v in value.items():\n            write(\"<member>\\n\")\n            if type(k) is not StringType:\n                if unicode and type(k) is UnicodeType:\n                    k = k.encode(self.encoding)\n                else:\n                    raise TypeError, \"dictionary key must be string\"\n            write(\"<name>%s</name>\\n\" % escape(k))\n            dump(v, write)\n            write(\"</member>\\n\")\n        write(\"</struct></value>\\n\")\n        del self.memo[i]\n    dispatch[DictType] = dump_struct\n\n    if datetime:\n        def dump_datetime(self, value, write):\n            write(\"<value><dateTime.iso8601>\")\n            write(_strftime(value))\n            write(\"</dateTime.iso8601></value>\\n\")\n        dispatch[datetime.datetime] = dump_datetime\n\n    def dump_instance(self, value, write):\n        # check for special wrappers\n        if value.__class__ in WRAPPERS:\n            self.write = write\n            value.encode(self)\n            del self.write\n        else:\n            # store instance attributes as a struct (really?)\n            self.dump_struct(value.__dict__, write)\n    dispatch[InstanceType] = dump_instance\n\n##\n# XML-RPC unmarshaller.\n#\n# @see loads\n\nclass Unmarshaller:\n    \"\"\"Unmarshal an XML-RPC response, based on incoming XML event\n    messages (start, data, end).  Call close() to get the resulting\n    data structure.\n\n    Note that this reader is fairly tolerant, and gladly accepts bogus\n    XML-RPC data without complaining (but not bogus XML).\n    \"\"\"\n\n    # and again, if you don't understand what's going on in here,\n    # that's perfectly ok.\n\n    def __init__(self, use_datetime=0):\n        self._type = None\n        self._stack = []\n        self._marks = []\n        self._data = []\n        self._methodname = None\n        self._encoding = \"utf-8\"\n        self.append = self._stack.append\n        self._use_datetime = use_datetime\n        if use_datetime and not datetime:\n            raise ValueError, \"the datetime module is not available\"\n\n    def close(self):\n        # return response tuple and target method\n        if self._type is None or self._marks:\n            raise ResponseError()\n        if self._type == \"fault\":\n            raise Fault(**self._stack[0])\n        return tuple(self._stack)\n\n    def getmethodname(self):\n        return self._methodname\n\n    #\n    # event handlers\n\n    def xml(self, encoding, standalone):\n        self._encoding = encoding\n        # FIXME: assert standalone == 1 ???\n\n    def start(self, tag, attrs):\n        # prepare to handle this element\n        if tag == \"array\" or tag == \"struct\":\n            self._marks.append(len(self._stack))\n        self._data = []\n        self._value = (tag == \"value\")\n\n    def data(self, text):\n        self._data.append(text)\n\n    def end(self, tag, join=string.join):\n        # call the appropriate end tag handler\n        try:\n            f = self.dispatch[tag]\n        except KeyError:\n            pass # unknown tag ?\n        else:\n            return f(self, join(self._data, \"\"))\n\n    #\n    # accelerator support\n\n    def end_dispatch(self, tag, data):\n        # dispatch data\n        try:\n            f = self.dispatch[tag]\n        except KeyError:\n            pass # unknown tag ?\n        else:\n            return f(self, data)\n\n    #\n    # element decoders\n\n    dispatch = {}\n\n    def end_nil (self, data):\n        self.append(None)\n        self._value = 0\n    dispatch[\"nil\"] = end_nil\n\n    def end_boolean(self, data):\n        if data == \"0\":\n            self.append(False)\n        elif data == \"1\":\n            self.append(True)\n        else:\n            raise TypeError, \"bad boolean value\"\n        self._value = 0\n    dispatch[\"boolean\"] = end_boolean\n\n    def end_int(self, data):\n        self.append(int(data))\n        self._value = 0\n    dispatch[\"i4\"] = end_int\n    dispatch[\"i8\"] = end_int\n    dispatch[\"int\"] = end_int\n\n    def end_double(self, data):\n        self.append(float(data))\n        self._value = 0\n    dispatch[\"double\"] = end_double\n\n    def end_string(self, data):\n        if self._encoding:\n            data = _decode(data, self._encoding)\n        self.append(_stringify(data))\n        self._value = 0\n    dispatch[\"string\"] = end_string\n    dispatch[\"name\"] = end_string # struct keys are always strings\n\n    def end_array(self, data):\n        mark = self._marks.pop()\n        # map arrays to Python lists\n        self._stack[mark:] = [self._stack[mark:]]\n        self._value = 0\n    dispatch[\"array\"] = end_array\n\n    def end_struct(self, data):\n        mark = self._marks.pop()\n        # map structs to Python dictionaries\n        dict = {}\n        items = self._stack[mark:]\n        for i in range(0, len(items), 2):\n            dict[_stringify(items[i])] = items[i+1]\n        self._stack[mark:] = [dict]\n        self._value = 0\n    dispatch[\"struct\"] = end_struct\n\n    def end_base64(self, data):\n        value = Binary()\n        value.decode(data)\n        self.append(value)\n        self._value = 0\n    dispatch[\"base64\"] = end_base64\n\n    def end_dateTime(self, data):\n        value = DateTime()\n        value.decode(data)\n        if self._use_datetime:\n            value = _datetime_type(data)\n        self.append(value)\n    dispatch[\"dateTime.iso8601\"] = end_dateTime\n\n    def end_value(self, data):\n        # if we stumble upon a value element with no internal\n        # elements, treat it as a string element\n        if self._value:\n            self.end_string(data)\n    dispatch[\"value\"] = end_value\n\n    def end_params(self, data):\n        self._type = \"params\"\n    dispatch[\"params\"] = end_params\n\n    def end_fault(self, data):\n        self._type = \"fault\"\n    dispatch[\"fault\"] = end_fault\n\n    def end_methodName(self, data):\n        if self._encoding:\n            data = _decode(data, self._encoding)\n        self._methodname = data\n        self._type = \"methodName\" # no params\n    dispatch[\"methodName\"] = end_methodName\n\n## Multicall support\n#\n\nclass _MultiCallMethod:\n    # some lesser magic to store calls made to a MultiCall object\n    # for batch execution\n    def __init__(self, call_list, name):\n        self.__call_list = call_list\n        self.__name = name\n    def __getattr__(self, name):\n        return _MultiCallMethod(self.__call_list, \"%s.%s\" % (self.__name, name))\n    def __call__(self, *args):\n        self.__call_list.append((self.__name, args))\n\nclass MultiCallIterator:\n    \"\"\"Iterates over the results of a multicall. Exceptions are\n    raised in response to xmlrpc faults.\"\"\"\n\n    def __init__(self, results):\n        self.results = results\n\n    def __getitem__(self, i):\n        item = self.results[i]\n        if type(item) == type({}):\n            raise Fault(item['faultCode'], item['faultString'])\n        elif type(item) == type([]):\n            return item[0]\n        else:\n            raise ValueError,\\\n                  \"unexpected type in multicall result\"\n\nclass MultiCall:\n    \"\"\"server -> a object used to boxcar method calls\n\n    server should be a ServerProxy object.\n\n    Methods can be added to the MultiCall using normal\n    method call syntax e.g.:\n\n    multicall = MultiCall(server_proxy)\n    multicall.add(2,3)\n    multicall.get_address(\"Guido\")\n\n    To execute the multicall, call the MultiCall object e.g.:\n\n    add_result, address = multicall()\n    \"\"\"\n\n    def __init__(self, server):\n        self.__server = server\n        self.__call_list = []\n\n    def __repr__(self):\n        return \"<MultiCall at %x>\" % id(self)\n\n    __str__ = __repr__\n\n    def __getattr__(self, name):\n        return _MultiCallMethod(self.__call_list, name)\n\n    def __call__(self):\n        marshalled_list = []\n        for name, args in self.__call_list:\n            marshalled_list.append({'methodName' : name, 'params' : args})\n\n        return MultiCallIterator(self.__server.system.multicall(marshalled_list))\n\n# --------------------------------------------------------------------\n# convenience functions\n\n##\n# Create a parser object, and connect it to an unmarshalling instance.\n# This function picks the fastest available XML parser.\n#\n# return A (parser, unmarshaller) tuple.\n\ndef getparser(use_datetime=0):\n    \"\"\"getparser() -> parser, unmarshaller\n\n    Create an instance of the fastest available parser, and attach it\n    to an unmarshalling object.  Return both objects.\n    \"\"\"\n    if use_datetime and not datetime:\n        raise ValueError, \"the datetime module is not available\"\n    if FastParser and FastUnmarshaller:\n        if use_datetime:\n            mkdatetime = _datetime_type\n        else:\n            mkdatetime = _datetime\n        target = FastUnmarshaller(True, False, _binary, mkdatetime, Fault)\n        parser = FastParser(target)\n    else:\n        target = Unmarshaller(use_datetime=use_datetime)\n        if FastParser:\n            parser = FastParser(target)\n        elif ExpatParser:\n            parser = ExpatParser(target)\n        else:\n            parser = SlowParser(target)\n    return parser, target\n\n##\n# Convert a Python tuple or a Fault instance to an XML-RPC packet.\n#\n# @def dumps(params, **options)\n# @param params A tuple or Fault instance.\n# @keyparam methodname If given, create a methodCall request for\n#     this method name.\n# @keyparam methodresponse If given, create a methodResponse packet.\n#     If used with a tuple, the tuple must be a singleton (that is,\n#     it must contain exactly one element).\n# @keyparam encoding The packet encoding.\n# @return A string containing marshalled data.\n\ndef dumps(params, methodname=None, methodresponse=None, encoding=None,\n          allow_none=0):\n    \"\"\"data [,options] -> marshalled data\n\n    Convert an argument tuple or a Fault instance to an XML-RPC\n    request (or response, if the methodresponse option is used).\n\n    In addition to the data object, the following options can be given\n    as keyword arguments:\n\n        methodname: the method name for a methodCall packet\n\n        methodresponse: true to create a methodResponse packet.\n        If this option is used with a tuple, the tuple must be\n        a singleton (i.e. it can contain only one element).\n\n        encoding: the packet encoding (default is UTF-8)\n\n    All 8-bit strings in the data structure are assumed to use the\n    packet encoding.  Unicode strings are automatically converted,\n    where necessary.\n    \"\"\"\n\n    assert isinstance(params, TupleType) or isinstance(params, Fault),\\\n           \"argument must be tuple or Fault instance\"\n\n    if isinstance(params, Fault):\n        methodresponse = 1\n    elif methodresponse and isinstance(params, TupleType):\n        assert len(params) == 1, \"response tuple must be a singleton\"\n\n    if not encoding:\n        encoding = \"utf-8\"\n\n    if FastMarshaller:\n        m = FastMarshaller(encoding)\n    else:\n        m = Marshaller(encoding, allow_none)\n\n    data = m.dumps(params)\n\n    if encoding != \"utf-8\":\n        xmlheader = \"<?xml version='1.0' encoding='%s'?>\\n\" % str(encoding)\n    else:\n        xmlheader = \"<?xml version='1.0'?>\\n\" # utf-8 is default\n\n    # standard XML-RPC wrappings\n    if methodname:\n        # a method call\n        if not isinstance(methodname, StringType):\n            methodname = methodname.encode(encoding)\n        data = (\n            xmlheader,\n            \"<methodCall>\\n\"\n            \"<methodName>\", methodname, \"</methodName>\\n\",\n            data,\n            \"</methodCall>\\n\"\n            )\n    elif methodresponse:\n        # a method response, or a fault structure\n        data = (\n            xmlheader,\n            \"<methodResponse>\\n\",\n            data,\n            \"</methodResponse>\\n\"\n            )\n    else:\n        return data # return as is\n    return string.join(data, \"\")\n\n##\n# Convert an XML-RPC packet to a Python object.  If the XML-RPC packet\n# represents a fault condition, this function raises a Fault exception.\n#\n# @param data An XML-RPC packet, given as an 8-bit string.\n# @return A tuple containing the unpacked data, and the method name\n#     (None if not present).\n# @see Fault\n\ndef loads(data, use_datetime=0):\n    \"\"\"data -> unmarshalled data, method name\n\n    Convert an XML-RPC packet to unmarshalled data plus a method\n    name (None if not present).\n\n    If the XML-RPC packet represents a fault condition, this function\n    raises a Fault exception.\n    \"\"\"\n    p, u = getparser(use_datetime=use_datetime)\n    p.feed(data)\n    p.close()\n    return u.close(), u.getmethodname()\n\n##\n# Encode a string using the gzip content encoding such as specified by the\n# Content-Encoding: gzip\n# in the HTTP header, as described in RFC 1952\n#\n# @param data the unencoded data\n# @return the encoded data\n\ndef gzip_encode(data):\n    \"\"\"data -> gzip encoded data\n\n    Encode data using the gzip content encoding as described in RFC 1952\n    \"\"\"\n    if not gzip:\n        raise NotImplementedError\n    f = StringIO.StringIO()\n    gzf = gzip.GzipFile(mode=\"wb\", fileobj=f, compresslevel=1)\n    gzf.write(data)\n    gzf.close()\n    encoded = f.getvalue()\n    f.close()\n    return encoded\n\n##\n# Decode a string using the gzip content encoding such as specified by the\n# Content-Encoding: gzip\n# in the HTTP header, as described in RFC 1952\n#\n# @param data The encoded data\n# @keyparam max_decode Maximum bytes to decode (20MB default), use negative\n#    values for unlimited decoding\n# @return the unencoded data\n# @raises ValueError if data is not correctly coded.\n# @raises ValueError if max gzipped payload length exceeded\n\ndef gzip_decode(data, max_decode=20971520):\n    \"\"\"gzip encoded data -> unencoded data\n\n    Decode data using the gzip content encoding as described in RFC 1952\n    \"\"\"\n    if not gzip:\n        raise NotImplementedError\n    f = StringIO.StringIO(data)\n    gzf = gzip.GzipFile(mode=\"rb\", fileobj=f)\n    try:\n        if max_decode < 0: # no limit\n            decoded = gzf.read()\n        else:\n            decoded = gzf.read(max_decode + 1)\n    except IOError:\n        raise ValueError(\"invalid data\")\n    f.close()\n    gzf.close()\n    if max_decode >= 0 and len(decoded) > max_decode:\n        raise ValueError(\"max gzipped payload length exceeded\")\n    return decoded\n\n##\n# Return a decoded file-like object for the gzip encoding\n# as described in RFC 1952.\n#\n# @param response A stream supporting a read() method\n# @return a file-like object that the decoded data can be read() from\n\nclass GzipDecodedResponse(gzip.GzipFile if gzip else object):\n    \"\"\"a file-like object to decode a response encoded with the gzip\n    method, as described in RFC 1952.\n    \"\"\"\n    def __init__(self, response):\n        #response doesn't support tell() and read(), required by\n        #GzipFile\n        if not gzip:\n            raise NotImplementedError\n        self.stringio = StringIO.StringIO(response.read())\n        gzip.GzipFile.__init__(self, mode=\"rb\", fileobj=self.stringio)\n\n    def close(self):\n        gzip.GzipFile.close(self)\n        self.stringio.close()\n\n\n# --------------------------------------------------------------------\n# request dispatcher\n\nclass _Method:\n    # some magic to bind an XML-RPC method to an RPC server.\n    # supports \"nested\" methods (e.g. examples.getStateName)\n    def __init__(self, send, name):\n        self.__send = send\n        self.__name = name\n    def __getattr__(self, name):\n        return _Method(self.__send, \"%s.%s\" % (self.__name, name))\n    def __call__(self, *args):\n        return self.__send(self.__name, args)\n\n##\n# Standard transport class for XML-RPC over HTTP.\n# <p>\n# You can create custom transports by subclassing this method, and\n# overriding selected methods.\n\nclass Transport:\n    \"\"\"Handles an HTTP transaction to an XML-RPC server.\"\"\"\n\n    # client identifier (may be overridden)\n    user_agent = \"xmlrpclib.py/%s (by www.pythonware.com)\" % __version__\n\n    #if true, we'll request gzip encoding\n    accept_gzip_encoding = True\n\n    # if positive, encode request using gzip if it exceeds this threshold\n    # note that many server will get confused, so only use it if you know\n    # that they can decode such a request\n    encode_threshold = None #None = don't encode\n\n    def __init__(self, use_datetime=0):\n        self._use_datetime = use_datetime\n        self._connection = (None, None)\n        self._extra_headers = []\n    ##\n    # Send a complete request, and parse the response.\n    # Retry request if a cached connection has disconnected.\n    #\n    # @param host Target host.\n    # @param handler Target PRC handler.\n    # @param request_body XML-RPC request body.\n    # @param verbose Debugging flag.\n    # @return Parsed response.\n\n    def request(self, host, handler, request_body, verbose=0):\n        #retry request once if cached connection has gone cold\n        for i in (0, 1):\n            try:\n                return self.single_request(host, handler, request_body, verbose)\n            except socket.error, e:\n                if i or e.errno not in (errno.ECONNRESET, errno.ECONNABORTED, errno.EPIPE):\n                    raise\n            except httplib.BadStatusLine: #close after we sent request\n                if i:\n                    raise\n\n    ##\n    # Send a complete request, and parse the response.\n    #\n    # @param host Target host.\n    # @param handler Target PRC handler.\n    # @param request_body XML-RPC request body.\n    # @param verbose Debugging flag.\n    # @return Parsed response.\n\n    def single_request(self, host, handler, request_body, verbose=0):\n        # issue XML-RPC request\n\n        h = self.make_connection(host)\n        if verbose:\n            h.set_debuglevel(1)\n\n        try:\n            self.send_request(h, handler, request_body)\n            self.send_host(h, host)\n            self.send_user_agent(h)\n            self.send_content(h, request_body)\n\n            response = h.getresponse(buffering=True)\n            if response.status == 200:\n                self.verbose = verbose\n                return self.parse_response(response)\n        except Fault:\n            raise\n        except Exception:\n            # All unexpected errors leave connection in\n            # a strange state, so we clear it.\n            self.close()\n            raise\n\n        #discard any response data and raise exception\n        if (response.getheader(\"content-length\", 0)):\n            response.read()\n        raise ProtocolError(\n            host + handler,\n            response.status, response.reason,\n            response.msg,\n            )\n\n    ##\n    # Create parser.\n    #\n    # @return A 2-tuple containing a parser and a unmarshaller.\n\n    def getparser(self):\n        # get parser and unmarshaller\n        return getparser(use_datetime=self._use_datetime)\n\n    ##\n    # Get authorization info from host parameter\n    # Host may be a string, or a (host, x509-dict) tuple; if a string,\n    # it is checked for a \"user:pw@host\" format, and a \"Basic\n    # Authentication\" header is added if appropriate.\n    #\n    # @param host Host descriptor (URL or (URL, x509 info) tuple).\n    # @return A 3-tuple containing (actual host, extra headers,\n    #     x509 info).  The header and x509 fields may be None.\n\n    def get_host_info(self, host):\n\n        x509 = {}\n        if isinstance(host, TupleType):\n            host, x509 = host\n\n        import urllib\n        auth, host = urllib.splituser(host)\n\n        if auth:\n            import base64\n            auth = base64.encodestring(urllib.unquote(auth))\n            auth = string.join(string.split(auth), \"\") # get rid of whitespace\n            extra_headers = [\n                (\"Authorization\", \"Basic \" + auth)\n                ]\n        else:\n            extra_headers = None\n\n        return host, extra_headers, x509\n\n    ##\n    # Connect to server.\n    #\n    # @param host Target host.\n    # @return A connection handle.\n\n    def make_connection(self, host):\n        #return an existing connection if possible.  This allows\n        #HTTP/1.1 keep-alive.\n        if self._connection and host == self._connection[0]:\n            return self._connection[1]\n\n        # create a HTTP connection object from a host descriptor\n        chost, self._extra_headers, x509 = self.get_host_info(host)\n        #store the host argument along with the connection object\n        self._connection = host, httplib.HTTPConnection(chost)\n        return self._connection[1]\n\n    ##\n    # Clear any cached connection object.\n    # Used in the event of socket errors.\n    #\n    def close(self):\n        if self._connection[1]:\n            self._connection[1].close()\n            self._connection = (None, None)\n\n    ##\n    # Send request header.\n    #\n    # @param connection Connection handle.\n    # @param handler Target RPC handler.\n    # @param request_body XML-RPC body.\n\n    def send_request(self, connection, handler, request_body):\n        if (self.accept_gzip_encoding and gzip):\n            connection.putrequest(\"POST\", handler, skip_accept_encoding=True)\n            connection.putheader(\"Accept-Encoding\", \"gzip\")\n        else:\n            connection.putrequest(\"POST\", handler)\n\n    ##\n    # Send host name.\n    #\n    # @param connection Connection handle.\n    # @param host Host name.\n    #\n    # Note: This function doesn't actually add the \"Host\"\n    # header anymore, it is done as part of the connection.putrequest() in\n    # send_request() above.\n\n    def send_host(self, connection, host):\n        extra_headers = self._extra_headers\n        if extra_headers:\n            if isinstance(extra_headers, DictType):\n                extra_headers = extra_headers.items()\n            for key, value in extra_headers:\n                connection.putheader(key, value)\n\n    ##\n    # Send user-agent identifier.\n    #\n    # @param connection Connection handle.\n\n    def send_user_agent(self, connection):\n        connection.putheader(\"User-Agent\", self.user_agent)\n\n    ##\n    # Send request body.\n    #\n    # @param connection Connection handle.\n    # @param request_body XML-RPC request body.\n\n    def send_content(self, connection, request_body):\n        connection.putheader(\"Content-Type\", \"text/xml\")\n\n        #optionally encode the request\n        if (self.encode_threshold is not None and\n            self.encode_threshold < len(request_body) and\n            gzip):\n            connection.putheader(\"Content-Encoding\", \"gzip\")\n            request_body = gzip_encode(request_body)\n\n        connection.putheader(\"Content-Length\", str(len(request_body)))\n        connection.endheaders(request_body)\n\n    ##\n    # Parse response.\n    #\n    # @param file Stream.\n    # @return Response tuple and target method.\n\n    def parse_response(self, response):\n        # read response data from httpresponse, and parse it\n\n        # Check for new http response object, else it is a file object\n        if hasattr(response,'getheader'):\n            if response.getheader(\"Content-Encoding\", \"\") == \"gzip\":\n                stream = GzipDecodedResponse(response)\n            else:\n                stream = response\n        else:\n            stream = response\n\n        p, u = self.getparser()\n\n        while 1:\n            data = stream.read(1024)\n            if not data:\n                break\n            if self.verbose:\n                print \"body:\", repr(data)\n            p.feed(data)\n\n        if stream is not response:\n            stream.close()\n        p.close()\n\n        return u.close()\n\n##\n# Standard transport class for XML-RPC over HTTPS.\n\nclass SafeTransport(Transport):\n    \"\"\"Handles an HTTPS transaction to an XML-RPC server.\"\"\"\n\n    def __init__(self, use_datetime=0, context=None):\n        Transport.__init__(self, use_datetime=use_datetime)\n        self.context = context\n\n    # FIXME: mostly untested\n\n    def make_connection(self, host):\n        if self._connection and host == self._connection[0]:\n            return self._connection[1]\n        # create a HTTPS connection object from a host descriptor\n        # host may be a string, or a (host, x509-dict) tuple\n        try:\n            HTTPS = httplib.HTTPSConnection\n        except AttributeError:\n            raise NotImplementedError(\n                \"your version of httplib doesn't support HTTPS\"\n                )\n        else:\n            chost, self._extra_headers, x509 = self.get_host_info(host)\n            self._connection = host, HTTPS(chost, None, context=self.context, **(x509 or {}))\n            return self._connection[1]\n\n##\n# Standard server proxy.  This class establishes a virtual connection\n# to an XML-RPC server.\n# <p>\n# This class is available as ServerProxy and Server.  New code should\n# use ServerProxy, to avoid confusion.\n#\n# @def ServerProxy(uri, **options)\n# @param uri The connection point on the server.\n# @keyparam transport A transport factory, compatible with the\n#    standard transport class.\n# @keyparam encoding The default encoding used for 8-bit strings\n#    (default is UTF-8).\n# @keyparam verbose Use a true value to enable debugging output.\n#    (printed to standard output).\n# @see Transport\n\nclass ServerProxy:\n    \"\"\"uri [,options] -> a logical connection to an XML-RPC server\n\n    uri is the connection point on the server, given as\n    scheme://host/target.\n\n    The standard implementation always supports the \"http\" scheme.  If\n    SSL socket support is available (Python 2.0), it also supports\n    \"https\".\n\n    If the target part and the slash preceding it are both omitted,\n    \"/RPC2\" is assumed.\n\n    The following options can be given as keyword arguments:\n\n        transport: a transport factory\n        encoding: the request encoding (default is UTF-8)\n\n    All 8-bit strings passed to the server proxy are assumed to use\n    the given encoding.\n    \"\"\"\n\n    def __init__(self, uri, transport=None, encoding=None, verbose=0,\n                 allow_none=0, use_datetime=0, context=None):\n        # establish a \"logical\" server connection\n\n        if isinstance(uri, unicode):\n            uri = uri.encode('ISO-8859-1')\n\n        # get the url\n        import urllib\n        type, uri = urllib.splittype(uri)\n        if type not in (\"http\", \"https\"):\n            raise IOError, \"unsupported XML-RPC protocol\"\n        self.__host, self.__handler = urllib.splithost(uri)\n        if not self.__handler:\n            self.__handler = \"/RPC2\"\n\n        if transport is None:\n            if type == \"https\":\n                transport = SafeTransport(use_datetime=use_datetime, context=context)\n            else:\n                transport = Transport(use_datetime=use_datetime)\n        self.__transport = transport\n\n        self.__encoding = encoding\n        self.__verbose = verbose\n        self.__allow_none = allow_none\n\n    def __close(self):\n        self.__transport.close()\n\n    def __request(self, methodname, params):\n        # call a method on the remote server\n\n        request = dumps(params, methodname, encoding=self.__encoding,\n                        allow_none=self.__allow_none)\n\n        response = self.__transport.request(\n            self.__host,\n            self.__handler,\n            request,\n            verbose=self.__verbose\n            )\n\n        if len(response) == 1:\n            response = response[0]\n\n        return response\n\n    def __repr__(self):\n        return (\n            \"<ServerProxy for %s%s>\" %\n            (self.__host, self.__handler)\n            )\n\n    __str__ = __repr__\n\n    def __getattr__(self, name):\n        # magic method dispatcher\n        return _Method(self.__request, name)\n\n    # note: to call a remote object with an non-standard name, use\n    # result getattr(server, \"strange-python-name\")(args)\n\n    def __call__(self, attr):\n        \"\"\"A workaround to get special attributes on the ServerProxy\n           without interfering with the magic __getattr__\n        \"\"\"\n        if attr == \"close\":\n            return self.__close\n        elif attr == \"transport\":\n            return self.__transport\n        raise AttributeError(\"Attribute %r not found\" % (attr,))\n\n# compatibility\n\nServer = ServerProxy\n\n# --------------------------------------------------------------------\n# test code\n\nif __name__ == \"__main__\":\n\n    server = ServerProxy(\"http://localhost:8000\")\n\n    print server\n\n    multi = MultiCall(server)\n    multi.pow(2, 9)\n    multi.add(5, 1)\n    multi.add(24, 11)\n    try:\n        for response in multi():\n            print response\n    except Error, v:\n        print \"ERROR\", v\n", 
    "zipfile": "\"\"\"\nRead and write ZIP files.\n\"\"\"\nimport struct, os, time, sys, shutil\nimport binascii, cStringIO, stat\nimport io\nimport re\nimport string\n\ntry:\n    import zlib # We may need its compression method\n    crc32 = zlib.crc32\nexcept ImportError:\n    zlib = None\n    crc32 = binascii.crc32\n\n__all__ = [\"BadZipfile\", \"error\", \"ZIP_STORED\", \"ZIP_DEFLATED\", \"is_zipfile\",\n           \"ZipInfo\", \"ZipFile\", \"PyZipFile\", \"LargeZipFile\" ]\n\nclass BadZipfile(Exception):\n    pass\n\n\nclass LargeZipFile(Exception):\n    \"\"\"\n    Raised when writing a zipfile, the zipfile requires ZIP64 extensions\n    and those extensions are disabled.\n    \"\"\"\n\nerror = BadZipfile      # The exception raised by this module\n\nZIP64_LIMIT = (1 << 31) - 1\nZIP_FILECOUNT_LIMIT = (1 << 16) - 1\nZIP_MAX_COMMENT = (1 << 16) - 1\n\n# constants for Zip file compression methods\nZIP_STORED = 0\nZIP_DEFLATED = 8\n# Other ZIP compression methods not supported\n\n# Below are some formats and associated data for reading/writing headers using\n# the struct module.  The names and structures of headers/records are those used\n# in the PKWARE description of the ZIP file format:\n#     http://www.pkware.com/documents/casestudies/APPNOTE.TXT\n# (URL valid as of January 2008)\n\n# The \"end of central directory\" structure, magic number, size, and indices\n# (section V.I in the format document)\nstructEndArchive = \"<4s4H2LH\"\nstringEndArchive = \"PK\\005\\006\"\nsizeEndCentDir = struct.calcsize(structEndArchive)\n\n_ECD_SIGNATURE = 0\n_ECD_DISK_NUMBER = 1\n_ECD_DISK_START = 2\n_ECD_ENTRIES_THIS_DISK = 3\n_ECD_ENTRIES_TOTAL = 4\n_ECD_SIZE = 5\n_ECD_OFFSET = 6\n_ECD_COMMENT_SIZE = 7\n# These last two indices are not part of the structure as defined in the\n# spec, but they are used internally by this module as a convenience\n_ECD_COMMENT = 8\n_ECD_LOCATION = 9\n\n# The \"central directory\" structure, magic number, size, and indices\n# of entries in the structure (section V.F in the format document)\nstructCentralDir = \"<4s4B4HL2L5H2L\"\nstringCentralDir = \"PK\\001\\002\"\nsizeCentralDir = struct.calcsize(structCentralDir)\n\n# indexes of entries in the central directory structure\n_CD_SIGNATURE = 0\n_CD_CREATE_VERSION = 1\n_CD_CREATE_SYSTEM = 2\n_CD_EXTRACT_VERSION = 3\n_CD_EXTRACT_SYSTEM = 4\n_CD_FLAG_BITS = 5\n_CD_COMPRESS_TYPE = 6\n_CD_TIME = 7\n_CD_DATE = 8\n_CD_CRC = 9\n_CD_COMPRESSED_SIZE = 10\n_CD_UNCOMPRESSED_SIZE = 11\n_CD_FILENAME_LENGTH = 12\n_CD_EXTRA_FIELD_LENGTH = 13\n_CD_COMMENT_LENGTH = 14\n_CD_DISK_NUMBER_START = 15\n_CD_INTERNAL_FILE_ATTRIBUTES = 16\n_CD_EXTERNAL_FILE_ATTRIBUTES = 17\n_CD_LOCAL_HEADER_OFFSET = 18\n\n# The \"local file header\" structure, magic number, size, and indices\n# (section V.A in the format document)\nstructFileHeader = \"<4s2B4HL2L2H\"\nstringFileHeader = \"PK\\003\\004\"\nsizeFileHeader = struct.calcsize(structFileHeader)\n\n_FH_SIGNATURE = 0\n_FH_EXTRACT_VERSION = 1\n_FH_EXTRACT_SYSTEM = 2\n_FH_GENERAL_PURPOSE_FLAG_BITS = 3\n_FH_COMPRESSION_METHOD = 4\n_FH_LAST_MOD_TIME = 5\n_FH_LAST_MOD_DATE = 6\n_FH_CRC = 7\n_FH_COMPRESSED_SIZE = 8\n_FH_UNCOMPRESSED_SIZE = 9\n_FH_FILENAME_LENGTH = 10\n_FH_EXTRA_FIELD_LENGTH = 11\n\n# The \"Zip64 end of central directory locator\" structure, magic number, and size\nstructEndArchive64Locator = \"<4sLQL\"\nstringEndArchive64Locator = \"PK\\x06\\x07\"\nsizeEndCentDir64Locator = struct.calcsize(structEndArchive64Locator)\n\n# The \"Zip64 end of central directory\" record, magic number, size, and indices\n# (section V.G in the format document)\nstructEndArchive64 = \"<4sQ2H2L4Q\"\nstringEndArchive64 = \"PK\\x06\\x06\"\nsizeEndCentDir64 = struct.calcsize(structEndArchive64)\n\n_CD64_SIGNATURE = 0\n_CD64_DIRECTORY_RECSIZE = 1\n_CD64_CREATE_VERSION = 2\n_CD64_EXTRACT_VERSION = 3\n_CD64_DISK_NUMBER = 4\n_CD64_DISK_NUMBER_START = 5\n_CD64_NUMBER_ENTRIES_THIS_DISK = 6\n_CD64_NUMBER_ENTRIES_TOTAL = 7\n_CD64_DIRECTORY_SIZE = 8\n_CD64_OFFSET_START_CENTDIR = 9\n\ndef _check_zipfile(fp):\n    try:\n        if _EndRecData(fp):\n            return True         # file has correct magic number\n    except IOError:\n        pass\n    return False\n\ndef is_zipfile(filename):\n    \"\"\"Quickly see if a file is a ZIP file by checking the magic number.\n\n    The filename argument may be a file or file-like object too.\n    \"\"\"\n    result = False\n    try:\n        if hasattr(filename, \"read\"):\n            result = _check_zipfile(fp=filename)\n        else:\n            with open(filename, \"rb\") as fp:\n                result = _check_zipfile(fp)\n    except IOError:\n        pass\n    return result\n\ndef _EndRecData64(fpin, offset, endrec):\n    \"\"\"\n    Read the ZIP64 end-of-archive records and use that to update endrec\n    \"\"\"\n    try:\n        fpin.seek(offset - sizeEndCentDir64Locator, 2)\n    except IOError:\n        # If the seek fails, the file is not large enough to contain a ZIP64\n        # end-of-archive record, so just return the end record we were given.\n        return endrec\n\n    data = fpin.read(sizeEndCentDir64Locator)\n    if len(data) != sizeEndCentDir64Locator:\n        return endrec\n    sig, diskno, reloff, disks = struct.unpack(structEndArchive64Locator, data)\n    if sig != stringEndArchive64Locator:\n        return endrec\n\n    if diskno != 0 or disks != 1:\n        raise BadZipfile(\"zipfiles that span multiple disks are not supported\")\n\n    # Assume no 'zip64 extensible data'\n    fpin.seek(offset - sizeEndCentDir64Locator - sizeEndCentDir64, 2)\n    data = fpin.read(sizeEndCentDir64)\n    if len(data) != sizeEndCentDir64:\n        return endrec\n    sig, sz, create_version, read_version, disk_num, disk_dir, \\\n            dircount, dircount2, dirsize, diroffset = \\\n            struct.unpack(structEndArchive64, data)\n    if sig != stringEndArchive64:\n        return endrec\n\n    # Update the original endrec using data from the ZIP64 record\n    endrec[_ECD_SIGNATURE] = sig\n    endrec[_ECD_DISK_NUMBER] = disk_num\n    endrec[_ECD_DISK_START] = disk_dir\n    endrec[_ECD_ENTRIES_THIS_DISK] = dircount\n    endrec[_ECD_ENTRIES_TOTAL] = dircount2\n    endrec[_ECD_SIZE] = dirsize\n    endrec[_ECD_OFFSET] = diroffset\n    return endrec\n\n\ndef _EndRecData(fpin):\n    \"\"\"Return data from the \"End of Central Directory\" record, or None.\n\n    The data is a list of the nine items in the ZIP \"End of central dir\"\n    record followed by a tenth item, the file seek offset of this record.\"\"\"\n\n    # Determine file size\n    fpin.seek(0, 2)\n    filesize = fpin.tell()\n\n    # Check to see if this is ZIP file with no archive comment (the\n    # \"end of central directory\" structure should be the last item in the\n    # file if this is the case).\n    try:\n        fpin.seek(-sizeEndCentDir, 2)\n    except IOError:\n        return None\n    data = fpin.read()\n    if (len(data) == sizeEndCentDir and\n        data[0:4] == stringEndArchive and\n        data[-2:] == b\"\\000\\000\"):\n        # the signature is correct and there's no comment, unpack structure\n        endrec = struct.unpack(structEndArchive, data)\n        endrec=list(endrec)\n\n        # Append a blank comment and record start offset\n        endrec.append(\"\")\n        endrec.append(filesize - sizeEndCentDir)\n\n        # Try to read the \"Zip64 end of central directory\" structure\n        return _EndRecData64(fpin, -sizeEndCentDir, endrec)\n\n    # Either this is not a ZIP file, or it is a ZIP file with an archive\n    # comment.  Search the end of the file for the \"end of central directory\"\n    # record signature. The comment is the last item in the ZIP file and may be\n    # up to 64K long.  It is assumed that the \"end of central directory\" magic\n    # number does not appear in the comment.\n    maxCommentStart = max(filesize - (1 << 16) - sizeEndCentDir, 0)\n    fpin.seek(maxCommentStart, 0)\n    data = fpin.read()\n    start = data.rfind(stringEndArchive)\n    if start >= 0:\n        # found the magic number; attempt to unpack and interpret\n        recData = data[start:start+sizeEndCentDir]\n        if len(recData) != sizeEndCentDir:\n            # Zip file is corrupted.\n            return None\n        endrec = list(struct.unpack(structEndArchive, recData))\n        commentSize = endrec[_ECD_COMMENT_SIZE] #as claimed by the zip file\n        comment = data[start+sizeEndCentDir:start+sizeEndCentDir+commentSize]\n        endrec.append(comment)\n        endrec.append(maxCommentStart + start)\n\n        # Try to read the \"Zip64 end of central directory\" structure\n        return _EndRecData64(fpin, maxCommentStart + start - filesize,\n                             endrec)\n\n    # Unable to find a valid end of central directory structure\n    return None\n\n\nclass ZipInfo (object):\n    \"\"\"Class with attributes describing each file in the ZIP archive.\"\"\"\n\n    __slots__ = (\n            'orig_filename',\n            'filename',\n            'date_time',\n            'compress_type',\n            'comment',\n            'extra',\n            'create_system',\n            'create_version',\n            'extract_version',\n            'reserved',\n            'flag_bits',\n            'volume',\n            'internal_attr',\n            'external_attr',\n            'header_offset',\n            'CRC',\n            'compress_size',\n            'file_size',\n            '_raw_time',\n        )\n\n    def __init__(self, filename=\"NoName\", date_time=(1980,1,1,0,0,0)):\n        self.orig_filename = filename   # Original file name in archive\n\n        # Terminate the file name at the first null byte.  Null bytes in file\n        # names are used as tricks by viruses in archives.\n        null_byte = filename.find(chr(0))\n        if null_byte >= 0:\n            filename = filename[0:null_byte]\n        # This is used to ensure paths in generated ZIP files always use\n        # forward slashes as the directory separator, as required by the\n        # ZIP format specification.\n        if os.sep != \"/\" and os.sep in filename:\n            filename = filename.replace(os.sep, \"/\")\n\n        self.filename = filename        # Normalized file name\n        self.date_time = date_time      # year, month, day, hour, min, sec\n\n        if date_time[0] < 1980:\n            raise ValueError('ZIP does not support timestamps before 1980')\n\n        # Standard values:\n        self.compress_type = ZIP_STORED # Type of compression for the file\n        self.comment = \"\"               # Comment for each file\n        self.extra = \"\"                 # ZIP extra data\n        if sys.platform == 'win32':\n            self.create_system = 0          # System which created ZIP archive\n        else:\n            # Assume everything else is unix-y\n            self.create_system = 3          # System which created ZIP archive\n        self.create_version = 20        # Version which created ZIP archive\n        self.extract_version = 20       # Version needed to extract archive\n        self.reserved = 0               # Must be zero\n        self.flag_bits = 0              # ZIP flag bits\n        self.volume = 0                 # Volume number of file header\n        self.internal_attr = 0          # Internal attributes\n        self.external_attr = 0          # External file attributes\n        # Other attributes are set by class ZipFile:\n        # header_offset         Byte offset to the file header\n        # CRC                   CRC-32 of the uncompressed file\n        # compress_size         Size of the compressed file\n        # file_size             Size of the uncompressed file\n\n    def FileHeader(self, zip64=None):\n        \"\"\"Return the per-file header as a string.\"\"\"\n        dt = self.date_time\n        dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]\n        dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)\n        if self.flag_bits & 0x08:\n            # Set these to zero because we write them after the file data\n            CRC = compress_size = file_size = 0\n        else:\n            CRC = self.CRC\n            compress_size = self.compress_size\n            file_size = self.file_size\n\n        extra = self.extra\n\n        if zip64 is None:\n            zip64 = file_size > ZIP64_LIMIT or compress_size > ZIP64_LIMIT\n        if zip64:\n            fmt = '<HHQQ'\n            extra = extra + struct.pack(fmt,\n                    1, struct.calcsize(fmt)-4, file_size, compress_size)\n        if file_size > ZIP64_LIMIT or compress_size > ZIP64_LIMIT:\n            if not zip64:\n                raise LargeZipFile(\"Filesize would require ZIP64 extensions\")\n            # File is larger than what fits into a 4 byte integer,\n            # fall back to the ZIP64 extension\n            file_size = 0xffffffff\n            compress_size = 0xffffffff\n            self.extract_version = max(45, self.extract_version)\n            self.create_version = max(45, self.extract_version)\n\n        filename, flag_bits = self._encodeFilenameFlags()\n        header = struct.pack(structFileHeader, stringFileHeader,\n                 self.extract_version, self.reserved, flag_bits,\n                 self.compress_type, dostime, dosdate, CRC,\n                 compress_size, file_size,\n                 len(filename), len(extra))\n        return header + filename + extra\n\n    def _encodeFilenameFlags(self):\n        if isinstance(self.filename, unicode):\n            try:\n                return self.filename.encode('ascii'), self.flag_bits\n            except UnicodeEncodeError:\n                return self.filename.encode('utf-8'), self.flag_bits | 0x800\n        else:\n            return self.filename, self.flag_bits\n\n    def _decodeFilename(self):\n        if self.flag_bits & 0x800:\n            return self.filename.decode('utf-8')\n        else:\n            return self.filename\n\n    def _decodeExtra(self):\n        # Try to decode the extra field.\n        extra = self.extra\n        unpack = struct.unpack\n        while len(extra) >= 4:\n            tp, ln = unpack('<HH', extra[:4])\n            if tp == 1:\n                if ln >= 24:\n                    counts = unpack('<QQQ', extra[4:28])\n                elif ln == 16:\n                    counts = unpack('<QQ', extra[4:20])\n                elif ln == 8:\n                    counts = unpack('<Q', extra[4:12])\n                elif ln == 0:\n                    counts = ()\n                else:\n                    raise RuntimeError, \"Corrupt extra field %s\"%(ln,)\n\n                idx = 0\n\n                # ZIP64 extension (large files and/or large archives)\n                if self.file_size in (0xffffffffffffffffL, 0xffffffffL):\n                    self.file_size = counts[idx]\n                    idx += 1\n\n                if self.compress_size == 0xFFFFFFFFL:\n                    self.compress_size = counts[idx]\n                    idx += 1\n\n                if self.header_offset == 0xffffffffL:\n                    old = self.header_offset\n                    self.header_offset = counts[idx]\n                    idx+=1\n\n            extra = extra[ln+4:]\n\n\nclass _ZipDecrypter:\n    \"\"\"Class to handle decryption of files stored within a ZIP archive.\n\n    ZIP supports a password-based form of encryption. Even though known\n    plaintext attacks have been found against it, it is still useful\n    to be able to get data out of such a file.\n\n    Usage:\n        zd = _ZipDecrypter(mypwd)\n        plain_char = zd(cypher_char)\n        plain_text = map(zd, cypher_text)\n    \"\"\"\n\n    def _GenerateCRCTable():\n        \"\"\"Generate a CRC-32 table.\n\n        ZIP encryption uses the CRC32 one-byte primitive for scrambling some\n        internal keys. We noticed that a direct implementation is faster than\n        relying on binascii.crc32().\n        \"\"\"\n        poly = 0xedb88320\n        table = [0] * 256\n        for i in range(256):\n            crc = i\n            for j in range(8):\n                if crc & 1:\n                    crc = ((crc >> 1) & 0x7FFFFFFF) ^ poly\n                else:\n                    crc = ((crc >> 1) & 0x7FFFFFFF)\n            table[i] = crc\n        return table\n    crctable = _GenerateCRCTable()\n\n    def _crc32(self, ch, crc):\n        \"\"\"Compute the CRC32 primitive on one byte.\"\"\"\n        return ((crc >> 8) & 0xffffff) ^ self.crctable[(crc ^ ord(ch)) & 0xff]\n\n    def __init__(self, pwd):\n        self.key0 = 305419896\n        self.key1 = 591751049\n        self.key2 = 878082192\n        for p in pwd:\n            self._UpdateKeys(p)\n\n    def _UpdateKeys(self, c):\n        self.key0 = self._crc32(c, self.key0)\n        self.key1 = (self.key1 + (self.key0 & 255)) & 4294967295\n        self.key1 = (self.key1 * 134775813 + 1) & 4294967295\n        self.key2 = self._crc32(chr((self.key1 >> 24) & 255), self.key2)\n\n    def __call__(self, c):\n        \"\"\"Decrypt a single character.\"\"\"\n        c = ord(c)\n        k = self.key2 | 2\n        c = c ^ (((k * (k^1)) >> 8) & 255)\n        c = chr(c)\n        self._UpdateKeys(c)\n        return c\n\n\ncompressor_names = {\n    0: 'store',\n    1: 'shrink',\n    2: 'reduce',\n    3: 'reduce',\n    4: 'reduce',\n    5: 'reduce',\n    6: 'implode',\n    7: 'tokenize',\n    8: 'deflate',\n    9: 'deflate64',\n    10: 'implode',\n    12: 'bzip2',\n    14: 'lzma',\n    18: 'terse',\n    19: 'lz77',\n    97: 'wavpack',\n    98: 'ppmd',\n}\n\n\nclass ZipExtFile(io.BufferedIOBase):\n    \"\"\"File-like object for reading an archive member.\n       Is returned by ZipFile.open().\n    \"\"\"\n\n    # Max size supported by decompressor.\n    MAX_N = 1 << 31 - 1\n\n    # Read from compressed files in 4k blocks.\n    MIN_READ_SIZE = 4096\n\n    # Search for universal newlines or line chunks.\n    PATTERN = re.compile(r'^(?P<chunk>[^\\r\\n]+)|(?P<newline>\\n|\\r\\n?)')\n\n    def __init__(self, fileobj, mode, zipinfo, decrypter=None,\n            close_fileobj=False):\n        self._fileobj = fileobj\n        self._decrypter = decrypter\n        self._close_fileobj = close_fileobj\n\n        self._compress_type = zipinfo.compress_type\n        self._compress_size = zipinfo.compress_size\n        self._compress_left = zipinfo.compress_size\n\n        if self._compress_type == ZIP_DEFLATED:\n            self._decompressor = zlib.decompressobj(-15)\n        elif self._compress_type != ZIP_STORED:\n            descr = compressor_names.get(self._compress_type)\n            if descr:\n                raise NotImplementedError(\"compression type %d (%s)\" % (self._compress_type, descr))\n            else:\n                raise NotImplementedError(\"compression type %d\" % (self._compress_type,))\n        self._unconsumed = ''\n\n        self._readbuffer = ''\n        self._offset = 0\n\n        self._universal = 'U' in mode\n        self.newlines = None\n\n        # Adjust read size for encrypted files since the first 12 bytes\n        # are for the encryption/password information.\n        if self._decrypter is not None:\n            self._compress_left -= 12\n\n        self.mode = mode\n        self.name = zipinfo.filename\n\n        if hasattr(zipinfo, 'CRC'):\n            self._expected_crc = zipinfo.CRC\n            self._running_crc = crc32(b'') & 0xffffffff\n        else:\n            self._expected_crc = None\n\n    def readline(self, limit=-1):\n        \"\"\"Read and return a line from the stream.\n\n        If limit is specified, at most limit bytes will be read.\n        \"\"\"\n\n        if not self._universal and limit < 0:\n            # Shortcut common case - newline found in buffer.\n            i = self._readbuffer.find('\\n', self._offset) + 1\n            if i > 0:\n                line = self._readbuffer[self._offset: i]\n                self._offset = i\n                return line\n\n        if not self._universal:\n            return io.BufferedIOBase.readline(self, limit)\n\n        line = ''\n        while limit < 0 or len(line) < limit:\n            readahead = self.peek(2)\n            if readahead == '':\n                return line\n\n            #\n            # Search for universal newlines or line chunks.\n            #\n            # The pattern returns either a line chunk or a newline, but not\n            # both. Combined with peek(2), we are assured that the sequence\n            # '\\r\\n' is always retrieved completely and never split into\n            # separate newlines - '\\r', '\\n' due to coincidental readaheads.\n            #\n            match = self.PATTERN.search(readahead)\n            newline = match.group('newline')\n            if newline is not None:\n                if self.newlines is None:\n                    self.newlines = []\n                if newline not in self.newlines:\n                    self.newlines.append(newline)\n                self._offset += len(newline)\n                return line + '\\n'\n\n            chunk = match.group('chunk')\n            if limit >= 0:\n                chunk = chunk[: limit - len(line)]\n\n            self._offset += len(chunk)\n            line += chunk\n\n        return line\n\n    def peek(self, n=1):\n        \"\"\"Returns buffered bytes without advancing the position.\"\"\"\n        if n > len(self._readbuffer) - self._offset:\n            chunk = self.read(n)\n            if len(chunk) > self._offset:\n                self._readbuffer = chunk + self._readbuffer[self._offset:]\n                self._offset = 0\n            else:\n                self._offset -= len(chunk)\n\n        # Return up to 512 bytes to reduce allocation overhead for tight loops.\n        return self._readbuffer[self._offset: self._offset + 512]\n\n    def readable(self):\n        return True\n\n    def read(self, n=-1):\n        \"\"\"Read and return up to n bytes.\n        If the argument is omitted, None, or negative, data is read and returned until EOF is reached..\n        \"\"\"\n        buf = ''\n        if n is None:\n            n = -1\n        while True:\n            if n < 0:\n                data = self.read1(n)\n            elif n > len(buf):\n                data = self.read1(n - len(buf))\n            else:\n                return buf\n            if len(data) == 0:\n                return buf\n            buf += data\n\n    def _update_crc(self, newdata, eof):\n        # Update the CRC using the given data.\n        if self._expected_crc is None:\n            # No need to compute the CRC if we don't have a reference value\n            return\n        self._running_crc = crc32(newdata, self._running_crc) & 0xffffffff\n        # Check the CRC if we're at the end of the file\n        if eof and self._running_crc != self._expected_crc:\n            raise BadZipfile(\"Bad CRC-32 for file %r\" % self.name)\n\n    def read1(self, n):\n        \"\"\"Read up to n bytes with at most one read() system call.\"\"\"\n\n        # Simplify algorithm (branching) by transforming negative n to large n.\n        if n < 0 or n is None:\n            n = self.MAX_N\n\n        # Bytes available in read buffer.\n        len_readbuffer = len(self._readbuffer) - self._offset\n\n        # Read from file.\n        if self._compress_left > 0 and n > len_readbuffer + len(self._unconsumed):\n            nbytes = n - len_readbuffer - len(self._unconsumed)\n            nbytes = max(nbytes, self.MIN_READ_SIZE)\n            nbytes = min(nbytes, self._compress_left)\n\n            data = self._fileobj.read(nbytes)\n            self._compress_left -= len(data)\n\n            if data and self._decrypter is not None:\n                data = ''.join(map(self._decrypter, data))\n\n            if self._compress_type == ZIP_STORED:\n                self._update_crc(data, eof=(self._compress_left==0))\n                self._readbuffer = self._readbuffer[self._offset:] + data\n                self._offset = 0\n            else:\n                # Prepare deflated bytes for decompression.\n                self._unconsumed += data\n\n        # Handle unconsumed data.\n        if (len(self._unconsumed) > 0 and n > len_readbuffer and\n            self._compress_type == ZIP_DEFLATED):\n            data = self._decompressor.decompress(\n                self._unconsumed,\n                max(n - len_readbuffer, self.MIN_READ_SIZE)\n            )\n\n            self._unconsumed = self._decompressor.unconsumed_tail\n            eof = len(self._unconsumed) == 0 and self._compress_left == 0\n            if eof:\n                data += self._decompressor.flush()\n\n            self._update_crc(data, eof=eof)\n            self._readbuffer = self._readbuffer[self._offset:] + data\n            self._offset = 0\n\n        # Read from buffer.\n        data = self._readbuffer[self._offset: self._offset + n]\n        self._offset += len(data)\n        return data\n\n    def close(self):\n        try :\n            if self._close_fileobj:\n                self._fileobj.close()\n        finally:\n            super(ZipExtFile, self).close()\n\n\nclass ZipFile(object):\n    \"\"\" Class with methods to open, read, write, close, list zip files.\n\n    z = ZipFile(file, mode=\"r\", compression=ZIP_STORED, allowZip64=False)\n\n    file: Either the path to the file, or a file-like object.\n          If it is a path, the file will be opened and closed by ZipFile.\n    mode: The mode can be either read \"r\", write \"w\" or append \"a\".\n    compression: ZIP_STORED (no compression) or ZIP_DEFLATED (requires zlib).\n    allowZip64: if True ZipFile will create files with ZIP64 extensions when\n                needed, otherwise it will raise an exception when this would\n                be necessary.\n\n    \"\"\"\n\n    fp = None                   # Set here since __del__ checks it\n\n    def __init__(self, file, mode=\"r\", compression=ZIP_STORED, allowZip64=False):\n        \"\"\"Open the ZIP file with mode read \"r\", write \"w\" or append \"a\".\"\"\"\n        if mode not in (\"r\", \"w\", \"a\"):\n            raise RuntimeError('ZipFile() requires mode \"r\", \"w\", or \"a\"')\n\n        if compression == ZIP_STORED:\n            pass\n        elif compression == ZIP_DEFLATED:\n            if not zlib:\n                raise RuntimeError,\\\n                      \"Compression requires the (missing) zlib module\"\n        else:\n            raise RuntimeError, \"That compression method is not supported\"\n\n        self._allowZip64 = allowZip64\n        self._didModify = False\n        self.debug = 0  # Level of printing: 0 through 3\n        self.NameToInfo = {}    # Find file info given name\n        self.filelist = []      # List of ZipInfo instances for archive\n        self.compression = compression  # Method of compression\n        self.mode = key = mode.replace('b', '')[0]\n        self.pwd = None\n        self._comment = ''\n\n        # Check if we were passed a file-like object\n        if isinstance(file, basestring):\n            self._filePassed = 0\n            self.filename = file\n            modeDict = {'r' : 'rb', 'w': 'wb', 'a' : 'r+b'}\n            try:\n                self.fp = open(file, modeDict[mode])\n            except IOError:\n                if mode == 'a':\n                    mode = key = 'w'\n                    self.fp = open(file, modeDict[mode])\n                else:\n                    raise\n        else:\n            self._filePassed = 1\n            self.fp = file\n            self.filename = getattr(file, 'name', None)\n\n        try:\n            if key == 'r':\n                self._RealGetContents()\n            elif key == 'w':\n                # set the modified flag so central directory gets written\n                # even if no files are added to the archive\n                self._didModify = True\n            elif key == 'a':\n                try:\n                    # See if file is a zip file\n                    self._RealGetContents()\n                    # seek to start of directory and overwrite\n                    self.fp.seek(self.start_dir, 0)\n                except BadZipfile:\n                    # file is not a zip file, just append\n                    self.fp.seek(0, 2)\n\n                    # set the modified flag so central directory gets written\n                    # even if no files are added to the archive\n                    self._didModify = True\n            else:\n                raise RuntimeError('Mode must be \"r\", \"w\" or \"a\"')\n        except:\n            fp = self.fp\n            self.fp = None\n            if not self._filePassed:\n                fp.close()\n            raise\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, type, value, traceback):\n        self.close()\n\n    def _RealGetContents(self):\n        \"\"\"Read in the table of contents for the ZIP file.\"\"\"\n        fp = self.fp\n        try:\n            endrec = _EndRecData(fp)\n        except IOError:\n            raise BadZipfile(\"File is not a zip file\")\n        if not endrec:\n            raise BadZipfile, \"File is not a zip file\"\n        if self.debug > 1:\n            print endrec\n        size_cd = endrec[_ECD_SIZE]             # bytes in central directory\n        offset_cd = endrec[_ECD_OFFSET]         # offset of central directory\n        self._comment = endrec[_ECD_COMMENT]    # archive comment\n\n        # \"concat\" is zero, unless zip was concatenated to another file\n        concat = endrec[_ECD_LOCATION] - size_cd - offset_cd\n        if endrec[_ECD_SIGNATURE] == stringEndArchive64:\n            # If Zip64 extension structures are present, account for them\n            concat -= (sizeEndCentDir64 + sizeEndCentDir64Locator)\n\n        if self.debug > 2:\n            inferred = concat + offset_cd\n            print \"given, inferred, offset\", offset_cd, inferred, concat\n        # self.start_dir:  Position of start of central directory\n        self.start_dir = offset_cd + concat\n        fp.seek(self.start_dir, 0)\n        data = fp.read(size_cd)\n        fp = cStringIO.StringIO(data)\n        total = 0\n        while total < size_cd:\n            centdir = fp.read(sizeCentralDir)\n            if len(centdir) != sizeCentralDir:\n                raise BadZipfile(\"Truncated central directory\")\n            centdir = struct.unpack(structCentralDir, centdir)\n            if centdir[_CD_SIGNATURE] != stringCentralDir:\n                raise BadZipfile(\"Bad magic number for central directory\")\n            if self.debug > 2:\n                print centdir\n            filename = fp.read(centdir[_CD_FILENAME_LENGTH])\n            # Create ZipInfo instance to store file information\n            x = ZipInfo(filename)\n            x.extra = fp.read(centdir[_CD_EXTRA_FIELD_LENGTH])\n            x.comment = fp.read(centdir[_CD_COMMENT_LENGTH])\n            x.header_offset = centdir[_CD_LOCAL_HEADER_OFFSET]\n            (x.create_version, x.create_system, x.extract_version, x.reserved,\n                x.flag_bits, x.compress_type, t, d,\n                x.CRC, x.compress_size, x.file_size) = centdir[1:12]\n            x.volume, x.internal_attr, x.external_attr = centdir[15:18]\n            # Convert date/time code to (year, month, day, hour, min, sec)\n            x._raw_time = t\n            x.date_time = ( (d>>9)+1980, (d>>5)&0xF, d&0x1F,\n                                     t>>11, (t>>5)&0x3F, (t&0x1F) * 2 )\n\n            x._decodeExtra()\n            x.header_offset = x.header_offset + concat\n            x.filename = x._decodeFilename()\n            self.filelist.append(x)\n            self.NameToInfo[x.filename] = x\n\n            # update total bytes read from central directory\n            total = (total + sizeCentralDir + centdir[_CD_FILENAME_LENGTH]\n                     + centdir[_CD_EXTRA_FIELD_LENGTH]\n                     + centdir[_CD_COMMENT_LENGTH])\n\n            if self.debug > 2:\n                print \"total\", total\n\n\n    def namelist(self):\n        \"\"\"Return a list of file names in the archive.\"\"\"\n        l = []\n        for data in self.filelist:\n            l.append(data.filename)\n        return l\n\n    def infolist(self):\n        \"\"\"Return a list of class ZipInfo instances for files in the\n        archive.\"\"\"\n        return self.filelist\n\n    def printdir(self):\n        \"\"\"Print a table of contents for the zip file.\"\"\"\n        print \"%-46s %19s %12s\" % (\"File Name\", \"Modified    \", \"Size\")\n        for zinfo in self.filelist:\n            date = \"%d-%02d-%02d %02d:%02d:%02d\" % zinfo.date_time[:6]\n            print \"%-46s %s %12d\" % (zinfo.filename, date, zinfo.file_size)\n\n    def testzip(self):\n        \"\"\"Read all the files and check the CRC.\"\"\"\n        chunk_size = 2 ** 20\n        for zinfo in self.filelist:\n            try:\n                # Read by chunks, to avoid an OverflowError or a\n                # MemoryError with very large embedded files.\n                with self.open(zinfo.filename, \"r\") as f:\n                    while f.read(chunk_size):     # Check CRC-32\n                        pass\n            except BadZipfile:\n                return zinfo.filename\n\n    def getinfo(self, name):\n        \"\"\"Return the instance of ZipInfo given 'name'.\"\"\"\n        info = self.NameToInfo.get(name)\n        if info is None:\n            raise KeyError(\n                'There is no item named %r in the archive' % name)\n\n        return info\n\n    def setpassword(self, pwd):\n        \"\"\"Set default password for encrypted files.\"\"\"\n        self.pwd = pwd\n\n    @property\n    def comment(self):\n        \"\"\"The comment text associated with the ZIP file.\"\"\"\n        return self._comment\n\n    @comment.setter\n    def comment(self, comment):\n        # check for valid comment length\n        if len(comment) > ZIP_MAX_COMMENT:\n            import warnings\n            warnings.warn('Archive comment is too long; truncating to %d bytes'\n                          % ZIP_MAX_COMMENT, stacklevel=2)\n            comment = comment[:ZIP_MAX_COMMENT]\n        self._comment = comment\n        self._didModify = True\n\n    def read(self, name, pwd=None):\n        \"\"\"Return file bytes (as a string) for name.\"\"\"\n        with self.open(name, \"r\", pwd) as fp:\n            return fp.read()\n\n    def open(self, name, mode=\"r\", pwd=None):\n        \"\"\"Return file-like object for 'name'.\"\"\"\n        if mode not in (\"r\", \"U\", \"rU\"):\n            raise RuntimeError, 'open() requires mode \"r\", \"U\", or \"rU\"'\n        if not self.fp:\n            raise RuntimeError, \\\n                  \"Attempt to read ZIP archive that was already closed\"\n\n        # Only open a new file for instances where we were not\n        # given a file object in the constructor\n        if self._filePassed:\n            zef_file = self.fp\n            should_close = False\n        else:\n            zef_file = open(self.filename, 'rb')\n            should_close = True\n\n        try:\n            # Make sure we have an info object\n            if isinstance(name, ZipInfo):\n                # 'name' is already an info object\n                zinfo = name\n            else:\n                # Get info object for name\n                zinfo = self.getinfo(name)\n\n            zef_file.seek(zinfo.header_offset, 0)\n\n            # Skip the file header:\n            fheader = zef_file.read(sizeFileHeader)\n            if len(fheader) != sizeFileHeader:\n                raise BadZipfile(\"Truncated file header\")\n            fheader = struct.unpack(structFileHeader, fheader)\n            if fheader[_FH_SIGNATURE] != stringFileHeader:\n                raise BadZipfile(\"Bad magic number for file header\")\n\n            fname = zef_file.read(fheader[_FH_FILENAME_LENGTH])\n            if fheader[_FH_EXTRA_FIELD_LENGTH]:\n                zef_file.read(fheader[_FH_EXTRA_FIELD_LENGTH])\n\n            if fname != zinfo.orig_filename:\n                raise BadZipfile, \\\n                        'File name in directory \"%s\" and header \"%s\" differ.' % (\n                            zinfo.orig_filename, fname)\n\n            # check for encrypted flag & handle password\n            is_encrypted = zinfo.flag_bits & 0x1\n            zd = None\n            if is_encrypted:\n                if not pwd:\n                    pwd = self.pwd\n                if not pwd:\n                    raise RuntimeError, \"File %s is encrypted, \" \\\n                        \"password required for extraction\" % name\n\n                zd = _ZipDecrypter(pwd)\n                # The first 12 bytes in the cypher stream is an encryption header\n                #  used to strengthen the algorithm. The first 11 bytes are\n                #  completely random, while the 12th contains the MSB of the CRC,\n                #  or the MSB of the file time depending on the header type\n                #  and is used to check the correctness of the password.\n                bytes = zef_file.read(12)\n                h = map(zd, bytes[0:12])\n                if zinfo.flag_bits & 0x8:\n                    # compare against the file type from extended local headers\n                    check_byte = (zinfo._raw_time >> 8) & 0xff\n                else:\n                    # compare against the CRC otherwise\n                    check_byte = (zinfo.CRC >> 24) & 0xff\n                if ord(h[11]) != check_byte:\n                    raise RuntimeError(\"Bad password for file\", name)\n\n            return ZipExtFile(zef_file, mode, zinfo, zd,\n                    close_fileobj=should_close)\n        except:\n            if should_close:\n                zef_file.close()\n            raise\n\n    def extract(self, member, path=None, pwd=None):\n        \"\"\"Extract a member from the archive to the current working directory,\n           using its full name. Its file information is extracted as accurately\n           as possible. `member' may be a filename or a ZipInfo object. You can\n           specify a different directory using `path'.\n        \"\"\"\n        if not isinstance(member, ZipInfo):\n            member = self.getinfo(member)\n\n        if path is None:\n            path = os.getcwd()\n\n        return self._extract_member(member, path, pwd)\n\n    def extractall(self, path=None, members=None, pwd=None):\n        \"\"\"Extract all members from the archive to the current working\n           directory. `path' specifies a different directory to extract to.\n           `members' is optional and must be a subset of the list returned\n           by namelist().\n        \"\"\"\n        if members is None:\n            members = self.namelist()\n\n        for zipinfo in members:\n            self.extract(zipinfo, path, pwd)\n\n    def _extract_member(self, member, targetpath, pwd):\n        \"\"\"Extract the ZipInfo object 'member' to a physical\n           file on the path targetpath.\n        \"\"\"\n        # build the destination pathname, replacing\n        # forward slashes to platform specific separators.\n        arcname = member.filename.replace('/', os.path.sep)\n\n        if os.path.altsep:\n            arcname = arcname.replace(os.path.altsep, os.path.sep)\n        # interpret absolute pathname as relative, remove drive letter or\n        # UNC path, redundant separators, \".\" and \"..\" components.\n        arcname = os.path.splitdrive(arcname)[1]\n        arcname = os.path.sep.join(x for x in arcname.split(os.path.sep)\n                    if x not in ('', os.path.curdir, os.path.pardir))\n        if os.path.sep == '\\\\':\n            # filter illegal characters on Windows\n            illegal = ':<>|\"?*'\n            if isinstance(arcname, unicode):\n                table = {ord(c): ord('_') for c in illegal}\n            else:\n                table = string.maketrans(illegal, '_' * len(illegal))\n            arcname = arcname.translate(table)\n            # remove trailing dots\n            arcname = (x.rstrip('.') for x in arcname.split(os.path.sep))\n            arcname = os.path.sep.join(x for x in arcname if x)\n\n        targetpath = os.path.join(targetpath, arcname)\n        targetpath = os.path.normpath(targetpath)\n\n        # Create all upper directories if necessary.\n        upperdirs = os.path.dirname(targetpath)\n        if upperdirs and not os.path.exists(upperdirs):\n            os.makedirs(upperdirs)\n\n        if member.filename[-1] == '/':\n            if not os.path.isdir(targetpath):\n                os.mkdir(targetpath)\n            return targetpath\n\n        with self.open(member, pwd=pwd) as source, \\\n             file(targetpath, \"wb\") as target:\n            shutil.copyfileobj(source, target)\n\n        return targetpath\n\n    def _writecheck(self, zinfo):\n        \"\"\"Check for errors before writing a file to the archive.\"\"\"\n        if zinfo.filename in self.NameToInfo:\n            import warnings\n            warnings.warn('Duplicate name: %r' % zinfo.filename, stacklevel=3)\n        if self.mode not in (\"w\", \"a\"):\n            raise RuntimeError, 'write() requires mode \"w\" or \"a\"'\n        if not self.fp:\n            raise RuntimeError, \\\n                  \"Attempt to write ZIP archive that was already closed\"\n        if zinfo.compress_type == ZIP_DEFLATED and not zlib:\n            raise RuntimeError, \\\n                  \"Compression requires the (missing) zlib module\"\n        if zinfo.compress_type not in (ZIP_STORED, ZIP_DEFLATED):\n            raise RuntimeError, \\\n                  \"That compression method is not supported\"\n        if not self._allowZip64:\n            requires_zip64 = None\n            if len(self.filelist) >= ZIP_FILECOUNT_LIMIT:\n                requires_zip64 = \"Files count\"\n            elif zinfo.file_size > ZIP64_LIMIT:\n                requires_zip64 = \"Filesize\"\n            elif zinfo.header_offset > ZIP64_LIMIT:\n                requires_zip64 = \"Zipfile size\"\n            if requires_zip64:\n                raise LargeZipFile(requires_zip64 +\n                                   \" would require ZIP64 extensions\")\n\n    def write(self, filename, arcname=None, compress_type=None):\n        \"\"\"Put the bytes from filename into the archive under the name\n        arcname.\"\"\"\n        if not self.fp:\n            raise RuntimeError(\n                  \"Attempt to write to ZIP archive that was already closed\")\n\n        st = os.stat(filename)\n        isdir = stat.S_ISDIR(st.st_mode)\n        mtime = time.localtime(st.st_mtime)\n        date_time = mtime[0:6]\n        # Create ZipInfo instance to store file information\n        if arcname is None:\n            arcname = filename\n        arcname = os.path.normpath(os.path.splitdrive(arcname)[1])\n        while arcname[0] in (os.sep, os.altsep):\n            arcname = arcname[1:]\n        if isdir:\n            arcname += '/'\n        zinfo = ZipInfo(arcname, date_time)\n        zinfo.external_attr = (st[0] & 0xFFFF) << 16L      # Unix attributes\n        if compress_type is None:\n            zinfo.compress_type = self.compression\n        else:\n            zinfo.compress_type = compress_type\n\n        zinfo.file_size = st.st_size\n        zinfo.flag_bits = 0x00\n        zinfo.header_offset = self.fp.tell()    # Start of header bytes\n\n        self._writecheck(zinfo)\n        self._didModify = True\n\n        if isdir:\n            zinfo.file_size = 0\n            zinfo.compress_size = 0\n            zinfo.CRC = 0\n            zinfo.external_attr |= 0x10  # MS-DOS directory flag\n            self.filelist.append(zinfo)\n            self.NameToInfo[zinfo.filename] = zinfo\n            self.fp.write(zinfo.FileHeader(False))\n            return\n\n        with open(filename, \"rb\") as fp:\n            # Must overwrite CRC and sizes with correct data later\n            zinfo.CRC = CRC = 0\n            zinfo.compress_size = compress_size = 0\n            # Compressed size can be larger than uncompressed size\n            zip64 = self._allowZip64 and \\\n                    zinfo.file_size * 1.05 > ZIP64_LIMIT\n            self.fp.write(zinfo.FileHeader(zip64))\n            if zinfo.compress_type == ZIP_DEFLATED:\n                cmpr = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION,\n                     zlib.DEFLATED, -15)\n            else:\n                cmpr = None\n            file_size = 0\n            while 1:\n                buf = fp.read(1024 * 8)\n                if not buf:\n                    break\n                file_size = file_size + len(buf)\n                CRC = crc32(buf, CRC) & 0xffffffff\n                if cmpr:\n                    buf = cmpr.compress(buf)\n                    compress_size = compress_size + len(buf)\n                self.fp.write(buf)\n        if cmpr:\n            buf = cmpr.flush()\n            compress_size = compress_size + len(buf)\n            self.fp.write(buf)\n            zinfo.compress_size = compress_size\n        else:\n            zinfo.compress_size = file_size\n        zinfo.CRC = CRC\n        zinfo.file_size = file_size\n        if not zip64 and self._allowZip64:\n            if file_size > ZIP64_LIMIT:\n                raise RuntimeError('File size has increased during compressing')\n            if compress_size > ZIP64_LIMIT:\n                raise RuntimeError('Compressed size larger than uncompressed size')\n        # Seek backwards and write file header (which will now include\n        # correct CRC and file sizes)\n        position = self.fp.tell()       # Preserve current position in file\n        self.fp.seek(zinfo.header_offset, 0)\n        self.fp.write(zinfo.FileHeader(zip64))\n        self.fp.seek(position, 0)\n        self.filelist.append(zinfo)\n        self.NameToInfo[zinfo.filename] = zinfo\n\n    def writestr(self, zinfo_or_arcname, bytes, compress_type=None):\n        \"\"\"Write a file into the archive.  The contents is the string\n        'bytes'.  'zinfo_or_arcname' is either a ZipInfo instance or\n        the name of the file in the archive.\"\"\"\n        if not isinstance(zinfo_or_arcname, ZipInfo):\n            zinfo = ZipInfo(filename=zinfo_or_arcname,\n                            date_time=time.localtime(time.time())[:6])\n\n            zinfo.compress_type = self.compression\n            if zinfo.filename[-1] == '/':\n                zinfo.external_attr = 0o40775 << 16   # drwxrwxr-x\n                zinfo.external_attr |= 0x10           # MS-DOS directory flag\n            else:\n                zinfo.external_attr = 0o600 << 16     # ?rw-------\n        else:\n            zinfo = zinfo_or_arcname\n\n        if not self.fp:\n            raise RuntimeError(\n                  \"Attempt to write to ZIP archive that was already closed\")\n\n        if compress_type is not None:\n            zinfo.compress_type = compress_type\n\n        zinfo.file_size = len(bytes)            # Uncompressed size\n        zinfo.header_offset = self.fp.tell()    # Start of header bytes\n        self._writecheck(zinfo)\n        self._didModify = True\n        zinfo.CRC = crc32(bytes) & 0xffffffff       # CRC-32 checksum\n        if zinfo.compress_type == ZIP_DEFLATED:\n            co = zlib.compressobj(zlib.Z_DEFAULT_COMPRESSION,\n                 zlib.DEFLATED, -15)\n            bytes = co.compress(bytes) + co.flush()\n            zinfo.compress_size = len(bytes)    # Compressed size\n        else:\n            zinfo.compress_size = zinfo.file_size\n        zip64 = zinfo.file_size > ZIP64_LIMIT or \\\n                zinfo.compress_size > ZIP64_LIMIT\n        if zip64 and not self._allowZip64:\n            raise LargeZipFile(\"Filesize would require ZIP64 extensions\")\n        self.fp.write(zinfo.FileHeader(zip64))\n        self.fp.write(bytes)\n        if zinfo.flag_bits & 0x08:\n            # Write CRC and file sizes after the file data\n            fmt = '<LQQ' if zip64 else '<LLL'\n            self.fp.write(struct.pack(fmt, zinfo.CRC, zinfo.compress_size,\n                  zinfo.file_size))\n        self.fp.flush()\n        self.filelist.append(zinfo)\n        self.NameToInfo[zinfo.filename] = zinfo\n\n    def __del__(self):\n        \"\"\"Call the \"close()\" method in case the user forgot.\"\"\"\n        self.close()\n\n    def close(self):\n        \"\"\"Close the file, and for mode \"w\" and \"a\" write the ending\n        records.\"\"\"\n        if self.fp is None:\n            return\n\n        try:\n            if self.mode in (\"w\", \"a\") and self._didModify: # write ending records\n                pos1 = self.fp.tell()\n                for zinfo in self.filelist:         # write central directory\n                    dt = zinfo.date_time\n                    dosdate = (dt[0] - 1980) << 9 | dt[1] << 5 | dt[2]\n                    dostime = dt[3] << 11 | dt[4] << 5 | (dt[5] // 2)\n                    extra = []\n                    if zinfo.file_size > ZIP64_LIMIT \\\n                            or zinfo.compress_size > ZIP64_LIMIT:\n                        extra.append(zinfo.file_size)\n                        extra.append(zinfo.compress_size)\n                        file_size = 0xffffffff\n                        compress_size = 0xffffffff\n                    else:\n                        file_size = zinfo.file_size\n                        compress_size = zinfo.compress_size\n\n                    if zinfo.header_offset > ZIP64_LIMIT:\n                        extra.append(zinfo.header_offset)\n                        header_offset = 0xffffffffL\n                    else:\n                        header_offset = zinfo.header_offset\n\n                    extra_data = zinfo.extra\n                    if extra:\n                        # Append a ZIP64 field to the extra's\n                        extra_data = struct.pack(\n                                '<HH' + 'Q'*len(extra),\n                                1, 8*len(extra), *extra) + extra_data\n\n                        extract_version = max(45, zinfo.extract_version)\n                        create_version = max(45, zinfo.create_version)\n                    else:\n                        extract_version = zinfo.extract_version\n                        create_version = zinfo.create_version\n\n                    try:\n                        filename, flag_bits = zinfo._encodeFilenameFlags()\n                        centdir = struct.pack(structCentralDir,\n                        stringCentralDir, create_version,\n                        zinfo.create_system, extract_version, zinfo.reserved,\n                        flag_bits, zinfo.compress_type, dostime, dosdate,\n                        zinfo.CRC, compress_size, file_size,\n                        len(filename), len(extra_data), len(zinfo.comment),\n                        0, zinfo.internal_attr, zinfo.external_attr,\n                        header_offset)\n                    except DeprecationWarning:\n                        print >>sys.stderr, (structCentralDir,\n                        stringCentralDir, create_version,\n                        zinfo.create_system, extract_version, zinfo.reserved,\n                        zinfo.flag_bits, zinfo.compress_type, dostime, dosdate,\n                        zinfo.CRC, compress_size, file_size,\n                        len(zinfo.filename), len(extra_data), len(zinfo.comment),\n                        0, zinfo.internal_attr, zinfo.external_attr,\n                        header_offset)\n                        raise\n                    self.fp.write(centdir)\n                    self.fp.write(filename)\n                    self.fp.write(extra_data)\n                    self.fp.write(zinfo.comment)\n\n                pos2 = self.fp.tell()\n                # Write end-of-zip-archive record\n                centDirCount = len(self.filelist)\n                centDirSize = pos2 - pos1\n                centDirOffset = pos1\n                requires_zip64 = None\n                if centDirCount > ZIP_FILECOUNT_LIMIT:\n                    requires_zip64 = \"Files count\"\n                elif centDirOffset > ZIP64_LIMIT:\n                    requires_zip64 = \"Central directory offset\"\n                elif centDirSize > ZIP64_LIMIT:\n                    requires_zip64 = \"Central directory size\"\n                if requires_zip64:\n                    # Need to write the ZIP64 end-of-archive records\n                    if not self._allowZip64:\n                        raise LargeZipFile(requires_zip64 +\n                                           \" would require ZIP64 extensions\")\n                    zip64endrec = struct.pack(\n                            structEndArchive64, stringEndArchive64,\n                            44, 45, 45, 0, 0, centDirCount, centDirCount,\n                            centDirSize, centDirOffset)\n                    self.fp.write(zip64endrec)\n\n                    zip64locrec = struct.pack(\n                            structEndArchive64Locator,\n                            stringEndArchive64Locator, 0, pos2, 1)\n                    self.fp.write(zip64locrec)\n                    centDirCount = min(centDirCount, 0xFFFF)\n                    centDirSize = min(centDirSize, 0xFFFFFFFF)\n                    centDirOffset = min(centDirOffset, 0xFFFFFFFF)\n\n                endrec = struct.pack(structEndArchive, stringEndArchive,\n                                    0, 0, centDirCount, centDirCount,\n                                    centDirSize, centDirOffset, len(self._comment))\n                self.fp.write(endrec)\n                self.fp.write(self._comment)\n                self.fp.flush()\n        finally:\n            fp = self.fp\n            self.fp = None\n            if not self._filePassed:\n                fp.close()\n\n\nclass PyZipFile(ZipFile):\n    \"\"\"Class to create ZIP archives with Python library files and packages.\"\"\"\n\n    def writepy(self, pathname, basename = \"\"):\n        \"\"\"Add all files from \"pathname\" to the ZIP archive.\n\n        If pathname is a package directory, search the directory and\n        all package subdirectories recursively for all *.py and enter\n        the modules into the archive.  If pathname is a plain\n        directory, listdir *.py and enter all modules.  Else, pathname\n        must be a Python *.py file and the module will be put into the\n        archive.  Added modules are always module.pyo or module.pyc.\n        This method will compile the module.py into module.pyc if\n        necessary.\n        \"\"\"\n        dir, name = os.path.split(pathname)\n        if os.path.isdir(pathname):\n            initname = os.path.join(pathname, \"__init__.py\")\n            if os.path.isfile(initname):\n                # This is a package directory, add it\n                if basename:\n                    basename = \"%s/%s\" % (basename, name)\n                else:\n                    basename = name\n                if self.debug:\n                    print \"Adding package in\", pathname, \"as\", basename\n                fname, arcname = self._get_codename(initname[0:-3], basename)\n                if self.debug:\n                    print \"Adding\", arcname\n                self.write(fname, arcname)\n                dirlist = os.listdir(pathname)\n                dirlist.remove(\"__init__.py\")\n                # Add all *.py files and package subdirectories\n                for filename in dirlist:\n                    path = os.path.join(pathname, filename)\n                    root, ext = os.path.splitext(filename)\n                    if os.path.isdir(path):\n                        if os.path.isfile(os.path.join(path, \"__init__.py\")):\n                            # This is a package directory, add it\n                            self.writepy(path, basename)  # Recursive call\n                    elif ext == \".py\":\n                        fname, arcname = self._get_codename(path[0:-3],\n                                         basename)\n                        if self.debug:\n                            print \"Adding\", arcname\n                        self.write(fname, arcname)\n            else:\n                # This is NOT a package directory, add its files at top level\n                if self.debug:\n                    print \"Adding files from directory\", pathname\n                for filename in os.listdir(pathname):\n                    path = os.path.join(pathname, filename)\n                    root, ext = os.path.splitext(filename)\n                    if ext == \".py\":\n                        fname, arcname = self._get_codename(path[0:-3],\n                                         basename)\n                        if self.debug:\n                            print \"Adding\", arcname\n                        self.write(fname, arcname)\n        else:\n            if pathname[-3:] != \".py\":\n                raise RuntimeError, \\\n                      'Files added with writepy() must end with \".py\"'\n            fname, arcname = self._get_codename(pathname[0:-3], basename)\n            if self.debug:\n                print \"Adding file\", arcname\n            self.write(fname, arcname)\n\n    def _get_codename(self, pathname, basename):\n        \"\"\"Return (filename, archivename) for the path.\n\n        Given a module name path, return the correct file path and\n        archive name, compiling if necessary.  For example, given\n        /python/lib/string, return (/python/lib/string.pyc, string).\n        \"\"\"\n        file_py  = pathname + \".py\"\n        file_pyc = pathname + \".pyc\"\n        file_pyo = pathname + \".pyo\"\n        if os.path.isfile(file_pyo) and \\\n                            os.stat(file_pyo).st_mtime >= os.stat(file_py).st_mtime:\n            fname = file_pyo    # Use .pyo file\n        elif not os.path.isfile(file_pyc) or \\\n             os.stat(file_pyc).st_mtime < os.stat(file_py).st_mtime:\n            import py_compile\n            if self.debug:\n                print \"Compiling\", file_py\n            try:\n                py_compile.compile(file_py, file_pyc, None, True)\n            except py_compile.PyCompileError,err:\n                print err.msg\n            fname = file_pyc\n        else:\n            fname = file_pyc\n        archivename = os.path.split(fname)[1]\n        if basename:\n            archivename = \"%s/%s\" % (basename, archivename)\n        return (fname, archivename)\n\n\ndef main(args = None):\n    import textwrap\n    USAGE=textwrap.dedent(\"\"\"\\\n        Usage:\n            zipfile.py -l zipfile.zip        # Show listing of a zipfile\n            zipfile.py -t zipfile.zip        # Test if a zipfile is valid\n            zipfile.py -e zipfile.zip target # Extract zipfile into target dir\n            zipfile.py -c zipfile.zip src ... # Create zipfile from sources\n        \"\"\")\n    if args is None:\n        args = sys.argv[1:]\n\n    if not args or args[0] not in ('-l', '-c', '-e', '-t'):\n        print USAGE\n        sys.exit(1)\n\n    if args[0] == '-l':\n        if len(args) != 2:\n            print USAGE\n            sys.exit(1)\n        with ZipFile(args[1], 'r') as zf:\n            zf.printdir()\n\n    elif args[0] == '-t':\n        if len(args) != 2:\n            print USAGE\n            sys.exit(1)\n        with ZipFile(args[1], 'r') as zf:\n            badfile = zf.testzip()\n        if badfile:\n            print(\"The following enclosed file is corrupted: {!r}\".format(badfile))\n        print \"Done testing\"\n\n    elif args[0] == '-e':\n        if len(args) != 3:\n            print USAGE\n            sys.exit(1)\n\n        with ZipFile(args[1], 'r') as zf:\n            zf.extractall(args[2])\n\n    elif args[0] == '-c':\n        if len(args) < 3:\n            print USAGE\n            sys.exit(1)\n\n        def addToZip(zf, path, zippath):\n            if os.path.isfile(path):\n                zf.write(path, zippath, ZIP_DEFLATED)\n            elif os.path.isdir(path):\n                if zippath:\n                    zf.write(path, zippath)\n                for nm in os.listdir(path):\n                    addToZip(zf,\n                            os.path.join(path, nm), os.path.join(zippath, nm))\n            # else: ignore\n\n        with ZipFile(args[1], 'w', allowZip64=True) as zf:\n            for path in args[2:]:\n                zippath = os.path.basename(path)\n                if not zippath:\n                    zippath = os.path.basename(os.path.dirname(path))\n                if zippath in ('', os.curdir, os.pardir):\n                    zippath = ''\n                addToZip(zf, path, zippath)\n\nif __name__ == \"__main__\":\n    main()\n"
  }
}